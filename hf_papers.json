{
    "date": {
        "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 15",
        "zh": "12æœˆ15æ—¥"
    },
    "time_utc": "2025-12-15 21:22",
    "weekday": 0,
    "issue_id": 69,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.11558",
            "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
            "url": "https://huggingface.co/papers/2512.11558",
            "abstract": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
            "score": 37,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "4d36c99c02abd4b4",
            "authors": [
                "Zhenyang Cai",
                "Jiaming Zhang",
                "Junjie Zhao",
                "Ziyi Zeng",
                "Yanchao Li",
                "Jingyi Liang",
                "Junying Chen",
                "Yunjin Yang",
                "Jiajun You",
                "Shuzhi Deng",
                "Tongfei Wang",
                "Wanting Chen",
                "Chunxiu Hao",
                "Ruiqi Xie",
                "Zhenwei Wen",
                "Xiangyi Feng",
                "Zou Ting",
                "Jin Zou Lin",
                "Jianquan Li",
                "Guangjun Yu",
                "Liangyi Chen",
                "Junwen Wang",
                "Shan Jiang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Beijing Institute of Collaborative Innovation",
                "Division of Applied Oral Sciences & Community Dental Care Faculty of Dentistry, The University of Hong Kong",
                "Freedom AI",
                "National Health Data Institute, Shenzhen",
                "Shenzhen Institute of Big Data",
                "Shenzhen Loop Area Institute",
                "Shenzhen Stomatology Hospital (Pingshan) of Southern Medical University",
                "State Key Laboratory of Membrane Biology, Beijing Key Laboratory of Cardiometabolic Molecular Medicine, Institute of Molecular Medicine, National Biomedical Imaging Center, School of Future Technology, Peking University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11558.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#benchmark",
                    "#multimodal",
                    "#science",
                    "#reasoning",
                    "#open_source",
                    "#synthetic",
                    "#training",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ¦·",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "DentalGPT â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 120k ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‚Ğ¸ Ñ€Ñ‚Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (reinforcement learning) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), DentalGPT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "DentalGPT: Revolutionizing Dental Diagnosis with Multimodal AI",
                    "desc": "DentalGPT is a specialized multimodal large language model designed for the dental field, which excels in disease classification and visual question answering (VQA) tasks. It was developed using a large annotated dataset of over 120,000 dental images paired with detailed descriptions, enhancing its ability to understand complex dental visuals. The model incorporates reinforcement learning to improve its reasoning capabilities, allowing for more accurate diagnoses. Evaluations show that DentalGPT outperforms many existing models, demonstrating the effectiveness of combining high-quality dental data with advanced training techniques."
                },
                "zh": {
                    "title": "DentalGPTï¼šç‰™ç§‘é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "DentalGPTæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹ç‰™ç§‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ³¨å…¥é«˜è´¨é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†ç–¾ç—…åˆ†ç±»å’Œç‰™ç§‘è§†è§‰é—®ç­”ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†å…‹æœç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•æ‰ç»†è‡´ç‰™ç§‘è§†è§‰ç»†èŠ‚å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒDentalGPTæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç‰™ç§‘æ³¨é‡Šå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡12ä¸‡å¼ ç‰™ç§‘å›¾åƒåŠå…¶è¯¦ç»†æè¿°ã€‚è¯¥æ¨¡å‹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ç‰™ç§‘çŠ¶å†µçš„è§†è§‰ç†è§£ï¼Œåç»­çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æå‡äº†å…¶å¤šæ¨¡æ€å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDentalGPTåœ¨ç–¾ç—…åˆ†ç±»å’Œç‰™ç§‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°½ç®¡å…¶å‚æ•°ä»…ä¸º70äº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08269",
            "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
            "url": "https://huggingface.co/papers/2512.08269",
            "abstract": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
            "score": 36,
            "issue_id": 53,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "3bb2b096dc745e11",
            "authors": [
                "Taewoong Kang",
                "Kinam Kim",
                "Dohyeon Kim",
                "Minho Park",
                "Junha Hyung",
                "Jaegul Choo"
            ],
            "affiliations": [
                "KAIST AI",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08269.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#video",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° ÑĞ±Ğ¾ĞºÑƒ Ğº Ğ²Ğ·Ğ³Ğ»ÑĞ´Ñƒ Ğ¸Ğ·Ğ½ÑƒÑ‚Ñ€Ğ¸: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "EgoX â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ LoRA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ self-attention, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Perspectives: From Exocentric to Immersive Egocentric Videos",
                    "desc": "The EgoX framework is designed to create egocentric videos from exocentric inputs by utilizing advanced video diffusion models. It employs LoRA adaptation to enhance the model's performance while integrating a unified conditioning strategy that merges different video perspectives. A geometry-guided self-attention mechanism is implemented to focus on relevant spatial areas, ensuring that the generated videos maintain visual coherence and fidelity. This innovative approach allows for the effective synthesis of first-person videos, even in challenging scenarios with varying camera angles and limited overlap."
                },
                "zh": {
                    "title": "EgoXï¼šä»å¤–éƒ¨è§†è§’ç”Ÿæˆè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "EgoXæ¡†æ¶é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘ï¼Œä½¿ç”¨LoRAé€‚åº”ã€ç»Ÿä¸€æ¡ä»¶å’Œå‡ ä½•å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç¡®ä¿ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†å¤–éƒ¨è§†è§’ï¼ˆç¬¬ä¸‰äººç§°ï¼‰è§†é¢‘è½¬æ¢ä¸ºè‡ªæˆ‘è§†è§’ï¼ˆç¬¬ä¸€äººç§°ï¼‰è§†é¢‘ï¼Œå°½ç®¡é¢ä¸´æå¤§çš„ç›¸æœºå§¿æ€å˜åŒ–å’Œè§†è§’é‡å ä¸è¶³çš„æŒ‘æˆ˜ã€‚EgoXåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒæ—¶ç©ºçŸ¥è¯†ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„LoRAé€‚åº”å¼•å…¥ç»Ÿä¸€çš„æ¡ä»¶ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨å’Œè‡ªæˆ‘ä¸­å¿ƒçš„å…ˆéªŒä¿¡æ¯ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´ä¸”çœŸå®çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11749",
            "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
            "url": "https://huggingface.co/papers/2512.11749",
            "abstract": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
            "score": 32,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "3b2603c9f5e073db",
            "authors": [
                "Minglei Shi",
                "Haolin Wang",
                "Borui Zhang",
                "Wenzhao Zheng",
                "Bohan Zeng",
                "Ziyang Yuan",
                "Xiaoshi Wu",
                "Yuanxing Zhang",
                "Huan Yang",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11749.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#open_source",
                    "#training",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Visual Foundation Model",
                    "desc": "SVG-T2I â€” ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Visual Foundation Model. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ pipeline Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞ³Ğ¾ Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ VFM-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GenEval (0.75) Ğ¸ DPG-Bench (85.78), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ VFM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ pipeline-Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "SVG-T2I: Bridging Text and Image with VFM Power",
                    "desc": "SVG-T2I is a new framework that enhances text-to-image synthesis by utilizing the Visual Foundation Model (VFM) feature domain. It focuses on generating high-quality images from text descriptions while leveraging self-supervised representations for better performance. The framework demonstrates competitive results in generative tasks, achieving notable scores on evaluation benchmarks. By open-sourcing the project, it aims to promote further research in the area of representation-driven visual generation."
                },
                "zh": {
                    "title": "é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ–°è·¯å¾„",
                    "desc": "SVG-T2Iæ˜¯ä¸€ä¸ªæ‰©å±•çš„SVGæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰åŸºç¡€æ¨¡å‹ç‰¹å¾åŸŸä¸­ç›´æ¥å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ ‡å‡†çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç®¡é“ï¼Œè¾¾åˆ°äº†ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›ã€‚å°½ç®¡åœ¨è§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ï¼Œä½†SVG-T2Iä¸ºæ­¤æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†é¡¹ç›®å®Œå…¨å¼€æºï¼ŒåŒ…æ‹¬è‡ªç¼–ç å™¨å’Œç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠå®ƒä»¬çš„è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ç®¡é“å’Œé¢„è®­ç»ƒæƒé‡ï¼Œä»¥ä¿ƒè¿›åŸºäºè¡¨ç¤ºçš„è§†è§‰ç”Ÿæˆçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11799",
            "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
            "url": "https://huggingface.co/papers/2512.11799",
            "abstract": "V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
            "score": 24,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "fa4bfef8afa61334",
            "authors": [
                "Ye Fang",
                "Tong Wu",
                "Valentin Deschaintre",
                "Duygu Ceylan",
                "Iliyan Georgiev",
                "Chun-Hao Paul Huang",
                "Yiwei Hu",
                "Xuelin Chen",
                "Tuanfeng Yang Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Fudan University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11799.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "V-RGBX â€” ÑÑ‚Ğ¾ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² ÑÑ†ĞµĞ½Ñ‹ (Ğ°Ğ»ÑŒĞ±ĞµĞ´Ğ¾, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ), Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing Video Editing with Intrinsic Awareness",
                    "desc": "V-RGBX is a novel framework designed for intrinsic-aware video editing, integrating video inverse rendering, photorealistic synthesis, and keyframe-based editing. It uniquely understands intrinsic scene properties like albedo and material, allowing for realistic video generation and manipulation. The framework employs an interleaved conditioning mechanism that facilitates intuitive editing based on user-selected keyframes, ensuring edits are consistent and physically plausible. V-RGBX outperforms previous methods in producing high-quality, temporally coherent videos, making it effective for various applications such as object appearance editing and scene relighting."
                },
                "zh": {
                    "title": "å†…åœ¨æ„ŸçŸ¥è§†é¢‘ç¼–è¾‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "V-RGBXæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œä¸“æ³¨äºå†…åœ¨æ„ŸçŸ¥çš„è§†é¢‘ç¼–è¾‘ã€‚å®ƒç»“åˆäº†è§†é¢‘é€†æ¸²æŸ“ã€é€¼çœŸçš„åˆæˆå’ŒåŸºäºå…³é”®å¸§çš„ç¼–è¾‘ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸€è‡´ä¸”ç‰©ç†ä¸Šåˆç†çš„ç¼–è¾‘æ•ˆæœã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯äº¤é”™æ¡ä»¶æœºåˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡é€‰æ‹©å…³é”®å¸§è¿›è¡Œç›´è§‚çš„ã€åŸºäºç‰©ç†çš„ç¼–è¾‘ã€‚V-RGBXåœ¨å¤šä¸ªåº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ç‰©ä½“å¤–è§‚ç¼–è¾‘å’Œåœºæ™¯é‡ç…§æ˜ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10411",
            "title": "Sliding Window Attention Adaptation",
            "url": "https://huggingface.co/papers/2512.10411",
            "abstract": "Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
            "score": 13,
            "issue_id": 54,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "e30d3f22fa97adbb",
            "authors": [
                "Yijiong Yu",
                "Jiale Liu",
                "Qingyun Wu",
                "Huazheng Wang",
                "Ji Pei"
            ],
            "affiliations": [
                "DeepSolution",
                "Oregon State University",
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10411.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸªŸ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SWAA) Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº: Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²-ÑĞºĞ¾Ñ€ĞµĞ¹, Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Long-Context Adaptation for Transformers with SWAA",
                    "desc": "The paper introduces Sliding Window Attention Adaptation (SWAA), a method that allows Transformer-based Large Language Models (LLMs) to efficiently use sliding window attention without the need for retraining. This approach addresses the challenge of long-context performance degradation when switching from full attention to sliding window attention during inference. SWAA combines several techniques, including selective application of sliding window attention, preservation of specific tokens, and interleaving different attention layers, to enhance adaptation. The results demonstrate that while adapting these models is complex, effective combinations of methods can restore their long-context capabilities while maintaining efficiency."
                },
                "zh": {
                    "title": "æ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚åº”ï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚åº”ï¼ˆSWAAï¼‰ä½¿åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼Œä»è€Œé€šè¿‡é€‚åº”æŠ€æœ¯ç»„åˆæ¢å¤é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦éšç€è¾“å…¥é•¿åº¦çš„å¢åŠ è€Œå‘ˆå¹³æ–¹å¢é•¿ï¼Œå¯¼è‡´é•¿ä¸Šä¸‹æ–‡æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚SWAAæå‡ºäº†ä¸€ç³»åˆ—å®ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨é¢„å¡«å……æœŸé—´åº”ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ã€ä¿ç•™â€œæ²‰æ²¡â€æ ‡è®°ã€äº¤é”™å…¨æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›å±‚ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä»¥åŠå¾®è°ƒï¼Œä»¥å®ç°æ›´å¥½çš„é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›çš„é€‚åº”æ˜¯å¯è¡Œçš„ï¼Œä½†å¹¶éç®€å•ï¼Œç‰¹å®šçš„ååŒç»„åˆèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤åŸå§‹çš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11253",
            "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
            "url": "https://huggingface.co/papers/2512.11253",
            "abstract": "PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming.  \t\t\t\t\tAI-generated summary \t\t\t\t Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
            "score": 10,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "63e3852998e651d2",
            "authors": [
                "Zhiyuan Li",
                "Chi-Man Pun",
                "Chen Fang",
                "Jue Wang",
                "Xiaodong Cun"
            ],
            "affiliations": [
                "Dzine.ai",
                "GVC Lab, Great Bay University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11253.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² 7-22 Ñ€Ğ°Ğ·Ğ°",
                    "desc": "PersonaLive â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Real-Time Portrait Animation Revolutionized with PersonaLive",
                    "desc": "PersonaLive is a cutting-edge framework designed for real-time portrait animation using diffusion models. It focuses on improving the speed and efficiency of animation generation while maintaining high visual quality. The framework employs multi-stage training, hybrid implicit signals for better motion control, and a unique appearance distillation method to streamline the denoising process. Additionally, it introduces an autoregressive micro-chunk streaming approach that allows for low-latency video generation, achieving significant speed improvements over existing models."
                },
                "zh": {
                    "title": "PersonaLiveï¼šå®æ—¶è‚–åƒåŠ¨ç”»çš„æ–°çªç ´",
                    "desc": "PersonaLive æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å®æ—¶è‚–åƒåŠ¨ç”»æ¡†æ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒã€æ··åˆéšå¼ä¿¡å·ã€å¤–è§‚è’¸é¦å’Œè‡ªå›å½’å¾®å—æµæ¥æé«˜é€Ÿåº¦å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†å½“å‰è‚–åƒåŠ¨ç”»æ¨¡å‹åœ¨å®æ—¶æ€§èƒ½ä¸Šçš„ä¸è¶³ï¼Œä½¿å…¶æ›´é€‚åˆç›´æ’­åœºæ™¯ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆéšå¼ä¿¡å·æ¥å®ç°å›¾åƒçº§è¿åŠ¨æ§åˆ¶ï¼Œå¹¶æå‡ºäº†å‡å°‘å¤–è§‚å†—ä½™çš„è’¸é¦ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚æœ€ç»ˆï¼ŒPersonaLive å®ç°äº†æ¯”ä»¥å¾€æ¨¡å‹å¿« 7-22 å€çš„æ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨å®æ—¶è§†é¢‘ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11464",
            "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
            "url": "https://huggingface.co/papers/2512.11464",
            "abstract": "MetaCanvas leverages multimodal large language models as latent-space planners to enhance precise and structured image and video generation, outperforming global-conditioning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
            "score": 8,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "337ee6656cf51654",
            "authors": [
                "Han Lin",
                "Xichen Pan",
                "Ziqi Huang",
                "Ji Hou",
                "Jialiang Wang",
                "Weifeng Chen",
                "Zecheng He",
                "Felix Juefei-Xu",
                "Junzhe Sun",
                "Zhipeng Fan",
                "Ali Thabet",
                "Mohit Bansal",
                "Chu Wang"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "Nanyang Technological University",
                "New York University",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11464.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MetaCanvas â€” Ğ»ĞµĞ³ĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ MLLMs Ñ€Ğ¾Ğ»ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ ÑĞ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Empowering Image and Video Generation with Latent-Space Planning",
                    "desc": "MetaCanvas is a novel framework that utilizes multimodal large language models (MLLMs) as planners in latent spaces to improve the generation of images and videos. Unlike traditional methods that rely on global text encoders, MetaCanvas allows MLLMs to engage in reasoning and planning directly within spatial and spatiotemporal contexts. This approach enhances the precision and structure of generated content, addressing the limitations of existing global-conditioning techniques. Through extensive evaluation across various tasks, MetaCanvas demonstrates superior performance, highlighting its potential to bridge the gap between multimodal understanding and generation."
                },
                "zh": {
                    "title": "MetaCanvasï¼šæå‡å›¾åƒè§†é¢‘ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´è§„åˆ’è€…",
                    "desc": "MetaCanvas æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’è€…ï¼Œæå‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆçš„ç²¾ç¡®æ€§å’Œç»“æ„åŒ–ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡æ€å­¦ä¹ é€šå¸¸å°†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ç®€åŒ–ä¸ºå…¨å±€æ–‡æœ¬ç¼–ç å™¨ï¼Œå¯¼è‡´å…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ã€‚MetaCanvas å…è®¸ MLLMs ç›´æ¥åœ¨ç©ºé—´å’Œæ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†å’Œè§„åˆ’ï¼Œå¹¶ä¸æ‰©æ•£ç”Ÿæˆå™¨ç´§å¯†ç»“åˆã€‚é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå®è¯è¯„ä¼°ï¼ŒMetaCanvas æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆç²¾ç¡®å¸ƒå±€å’Œå¼ºå±æ€§ç»‘å®šæ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¡¨æ˜å°† MLLMs è§†ä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’è€…æ˜¯ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´å·®è·çš„æœ‰æ•ˆæ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06818",
            "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
            "url": "https://huggingface.co/papers/2512.06818",
            "abstract": "MeshSplatting, a mesh-based reconstruction method, enhances novel view synthesis by optimizing geometry and appearance through differentiable rendering, improving quality and efficiency over existing techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
            "score": 6,
            "issue_id": 55,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "38a05e08f469d8ee",
            "authors": [
                "Jan Held",
                "Sanghyun Son",
                "Renaud Vandeghen",
                "Daniel Rebain",
                "Matheus Gadelha",
                "Yi Zhou",
                "Anthony Cioppa",
                "Ming C. Lin",
                "Marc Van Droogenbroeck",
                "Andrea Tagliasacchi"
            ],
            "affiliations": [
                "Adobe Research",
                "Simon Fraser University",
                "University of British Columbia",
                "University of LiÃ¨ge",
                "University of Maryland",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.06818.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡ĞµÑ‚ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ²: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "MeshSplatting â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ñ€Ğ¸Ğ°Ğ½Ğ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ”ĞµĞ»Ğ¾Ğ½Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 0.69 Ğ´Ğ‘ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PSNR Ğ¸ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ, Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸ AR/VR Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with MeshSplatting",
                    "desc": "MeshSplatting is a new method for creating 3D models that improves how we generate images from different viewpoints. It uses a technique called differentiable rendering to optimize both the shape and appearance of the models, making them look better and faster than older methods. This approach connects points in a way that keeps the surfaces smooth and consistent, which is important for applications like augmented reality and video games. By achieving higher image quality and faster training times, MeshSplatting effectively combines advanced neural rendering with real-time graphics."
                },
                "zh": {
                    "title": "MeshSplattingï¼šæå‡æ–°è§†è§’åˆæˆçš„ç½‘æ ¼é‡å»ºæ–¹æ³•",
                    "desc": "MeshSplattingæ˜¯ä¸€ç§åŸºäºç½‘æ ¼çš„é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“ä¼˜åŒ–å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ï¼Œä»è€Œæå‡æ–°è§†è§’åˆæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡é™åˆ¶çš„å¾·åŠ³å†…ä¸‰è§’å‰–åˆ†æ¥å¼ºåˆ¶è¿æ¥æ€§ï¼Œå¹¶æ”¹å–„è¡¨é¢ä¸€è‡´æ€§ï¼Œç”Ÿæˆå¹³æ»‘ä¸”è§†è§‰è´¨é‡é«˜çš„ç½‘æ ¼ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶3Då¼•æ“ä¸­é«˜æ•ˆæ¸²æŸ“ã€‚ä¸å½“å‰æœ€å…ˆè¿›çš„MiLoæ–¹æ³•ç›¸æ¯”ï¼ŒMeshSplattingåœ¨Mip-NeRF360æ•°æ®é›†ä¸Šæé«˜äº†0.69 dBçš„PSNRï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº†2å€ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘äº†2å€ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†ç¥ç»æ¸²æŸ“ä¸äº¤äº’å¼3Då›¾å½¢ç»“åˆï¼Œå®ç°æ— ç¼çš„å®æ—¶åœºæ™¯äº¤äº’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11792",
            "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
            "url": "https://huggingface.co/papers/2512.11792",
            "abstract": "SAM2VideoX improves realistic motion generation in video models by integrating structure-preserving priors from an autoregressive model into a bidirectional diffusion model with novel feature fusion and local alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
            "score": 5,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "94f116052c2d1ea3",
            "authors": [
                "Yang Fei",
                "George Stoica",
                "Jingyuan Liu",
                "Qifeng Chen",
                "Ranjay Krishna",
                "Xiaojuan Wang",
                "Benlin Liu"
            ],
            "affiliations": [
                "Adobe",
                "Georgia Tech",
                "HKUST",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11792.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAM2VideoX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ SAM2 Ğ² Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CogVideoX. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Local Gram Flow loss Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: +2.60% Ğ½Ğ° VBench, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ FVD Ğ½Ğ° 21-22% Ğ¸ 71.4% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Realistic Motion Generation in Video Models",
                    "desc": "SAM2VideoX enhances video generation by combining structure-preserving motion priors from an autoregressive model with a bidirectional diffusion model. It introduces a bidirectional feature fusion module to capture global motion patterns and a Local Gram Flow loss to ensure local feature alignment. This approach addresses the challenges of generating realistic motion for articulated and deformable objects, which traditional methods struggle with. Experimental results demonstrate significant improvements in motion fidelity and human preference compared to existing models."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®è¿åŠ¨è¡¨ç°",
                    "desc": "SAM2VideoXé€šè¿‡å°†è‡ªå›å½’æ¨¡å‹ä¸­çš„ç»“æ„ä¿æŒå…ˆéªŒä¸åŒå‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæ”¹å–„äº†è§†é¢‘æ¨¡å‹ä¸­çš„çœŸå®è¿åŠ¨ç”Ÿæˆã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°çš„ç‰¹å¾èåˆå’Œå±€éƒ¨å¯¹é½æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨æ—¢çœŸå®åˆä¿æŒç»“æ„ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå…³èŠ‚å’Œå¯å˜å½¢ç‰©ä½“ï¼ˆå¦‚äººç±»å’ŒåŠ¨ç‰©ï¼‰çš„çœŸå®è¿åŠ¨æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAM2VideoXåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10605",
            "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
            "url": "https://huggingface.co/papers/2512.10605",
            "abstract": "A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
            "score": 4,
            "issue_id": 57,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "61e63e51edb593ff",
            "authors": [
                "Lihuang Chen",
                "Xiangyu Luo",
                "Jun Meng"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10605.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ LEO-RobotAgent â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… â€” Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ°Ñ…, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°ĞµÑ‚ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Robots with Language for Smarter Interaction and Task Planning",
                    "desc": "LEO-RobotAgent is a versatile framework that uses large language models (LLMs) to improve how robots interact with humans and plan tasks. It allows different types of robots to handle complex and unpredictable tasks in various environments. The framework is designed to be efficient and robust, making it easier for robots to understand human intentions and collaborate effectively. By simplifying the structure of task planning, LEO-RobotAgent enhances the adaptability of robots across multiple platforms, such as UAVs and robotic arms."
                },
                "zh": {
                    "title": "é€šç”¨è¯­è¨€é©±åŠ¨çš„æœºå™¨äººæ™ºèƒ½æ¡†æ¶",
                    "desc": "LEO-RobotAgentæ˜¯ä¸€ä¸ªé€šç”¨çš„è¯­è¨€é©±åŠ¨æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºäººæœºäº¤äº’å’Œä»»åŠ¡è§„åˆ’ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ“ä½œä¸åŒç±»å‹çš„æœºå™¨äººï¼Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§å’Œé«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿé™ä½äººæœºäº¤äº’çš„é—¨æ§›ã€‚é€šè¿‡æ¨¡å—åŒ–çš„å·¥å…·é›†ï¼ŒLEO-RobotAgentå¯ä»¥çµæ´»è°ƒç”¨å„ç§å·¥å…·ï¼Œé€‚åº”ä¸åŒçš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11437",
            "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
            "url": "https://huggingface.co/papers/2512.11437",
            "abstract": "CLINIC is a multilingual benchmark that evaluates the trustworthiness of language models in healthcare across five dimensions, revealing challenges in factual correctness, bias, privacy, and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.",
            "score": 3,
            "issue_id": 66,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "45ca38731a1257e8",
            "authors": [
                "Akash Ghosh",
                "Srivarshinee Sridhar",
                "Raghav Kaushik Ravi",
                "Muhsin Muhsin",
                "Sriparna Saha",
                "Chirag Agarwal"
            ],
            "affiliations": [
                "IGIMS, Patna",
                "Indian Institute of Technology Patna",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11437.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#security",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#low_resource",
                    "#healthcare",
                    "#ethics"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "CLINIC â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¸Ğ»Ğ¾ 15 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ 18 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ñ„Ğ°ĞºĞ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº ÑƒÑ‚ĞµÑ‡ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Trust in Multilingual Healthcare AI",
                    "desc": "CLINIC is a multilingual benchmark designed to assess the trustworthiness of language models (LMs) in healthcare. It evaluates LMs across five critical dimensions: truthfulness, fairness, safety, robustness, and privacy, using 18 diverse tasks in 15 languages. The study reveals that current LMs face significant challenges, including issues with factual correctness, bias against different demographic and linguistic groups, and vulnerabilities to privacy breaches and adversarial attacks. By identifying these weaknesses, CLINIC aims to improve the reliability and global applicability of LMs in healthcare settings."
                },
                "zh": {
                    "title": "æå‡åŒ»ç–—è¯­è¨€æ¨¡å‹å¯ä¿¡åº¦çš„å¤šè¯­è¨€åŸºå‡†",
                    "desc": "CLINICæ˜¯ä¸€ä¸ªå¤šè¯­è¨€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„å¯ä¿¡åº¦ï¼Œæ¶µç›–äº”ä¸ªç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨äº‹å®æ­£ç¡®æ€§ã€å…¬å¹³æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§å’Œéšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚å°¤å…¶æ˜¯åœ¨ä¸­ä½èµ„æºè¯­è¨€çš„åŒ»ç–—æŸ¥è¯¢ä¸­ï¼Œç°æœ‰æ¨¡å‹çš„è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶åœ¨å…¨çƒåŒ»ç–—ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼ŒCLINICä¸ºæå‡è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–è¯­è¨€ä¸­çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11150",
            "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
            "url": "https://huggingface.co/papers/2512.11150",
            "abstract": "CJE framework improves model evaluation accuracy and efficiency by addressing calibration, weight stabilization, and uncertainty in confidence intervals.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.",
            "score": 3,
            "issue_id": 66,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "dadf1ce0556d564a",
            "authors": [
                "Eddie Landesberg"
            ],
            "affiliations": [
                "CIMO Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11150.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑÑƒĞ´ĞµĞ¹ LLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° CJE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: AutoCal-R Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ·Ğ¾Ñ‚Ğ¾Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ, SIMCal-W Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚ĞµĞºĞ¸Ğ½Ğ³, Ğ¸ OUA Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Chatbot Arena CJE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 99% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 5% Ğ¾Ñ€Ğ°ĞºÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ² 14 Ñ€Ğ°Ğ·. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ SNIPS Ğ¸ IPS Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ Ğ½Ğµ Ğ¿Ğ¾ÑĞµÑ‰Ğ°ÑÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "CJE: Revolutionizing Model Evaluation with Calibration and Stability",
                    "desc": "The Causal Judge Evaluation (CJE) framework enhances the accuracy and efficiency of model evaluation by addressing key issues such as calibration, weight stabilization, and uncertainty in confidence intervals. Traditional methods, like LLM-as-judge, often produce uncalibrated scores that can mislead preferences and yield unreliable confidence intervals. CJE achieves high pairwise ranking accuracy while significantly reducing costs by utilizing a calibrated judge with minimal oracle labels. The framework incorporates three main components: AutoCal-R for reward calibration, SIMCal-W for weight stabilization, and Oracle-Uncertainty Aware inference to improve confidence interval coverage."
                },
                "zh": {
                    "title": "CJEæ¡†æ¶ï¼šæå‡æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§ä¸æ•ˆç‡",
                    "desc": "CJEæ¡†æ¶é€šè¿‡è§£å†³æ ¡å‡†ã€æƒé‡ç¨³å®šæ€§å’Œç½®ä¿¡åŒºé—´çš„ä¸ç¡®å®šæ€§ï¼Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¼ ç»Ÿçš„LLMè¯„ä¼°æ–¹æ³•åœ¨ç»Ÿè®¡ä¸Šæ˜¯ä¸å¯é çš„ï¼Œæœªæ ¡å‡†çš„åˆ†æ•°å¯èƒ½ä¼šé¢ å€’åå¥½ï¼Œè€Œç®€å•çš„ç½®ä¿¡åŒºé—´åœ¨æœªæ ¡å‡†çš„åˆ†æ•°ä¸Šå‡ ä¹æ²¡æœ‰è¦†ç›–ç‡ã€‚CJEç»“åˆäº†è‡ªåŠ¨æ ¡å‡†ã€æƒé‡ç¨³å®šå’Œä¸ç¡®å®šæ€§æ¨æ–­ä¸‰ä¸ªç»„ä»¶ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCJEåœ¨å¤šä¸ªé…ç½®ä¸‹çš„é…å¯¹æ’åå‡†ç¡®ç‡è¾¾åˆ°94%ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨æ›´å°‘çš„æ ‡ç­¾æ—¶ä»èƒ½ä¿æŒé«˜è´¨é‡çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06951",
            "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
            "url": "https://huggingface.co/papers/2512.06951",
            "abstract": "A vision-action policy using correlated noise for flow matching and learnable mixed-layer attention wins the 2025 BEHAVIOR Challenge with high performance across diverse household tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.   Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.   Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
            "score": 3,
            "issue_id": 51,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "7a391e30872f5e27",
            "authors": [
                "Ilia Larchenko",
                "Gleb Zarin",
                "Akash Karnatak"
            ],
            "affiliations": [
                "Independent Researchers"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.06951.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#cv",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ BEHAVIOR Challenge 2025 Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 50 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Pi0.5 Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ flow matching, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 26% q-score Ğ½Ğ° Ğ²ÑĞµÑ… 50 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Winning with Correlated Noise and Attention in Household Tasks",
                    "desc": "This paper presents a vision-action policy that achieved first place in the 2025 BEHAVIOR Challenge, which involved complex household tasks requiring advanced manipulation and decision-making. The authors introduce correlated noise for flow matching, enhancing training efficiency and enabling smoother action sequences through correlation-aware inpainting. They also implement learnable mixed-layer attention and System 2 stage tracking to resolve ambiguities during task execution. The proposed method demonstrates a significant performance improvement, achieving a 26% q-score across a diverse set of 50 tasks."
                },
                "zh": {
                    "title": "ç›¸å…³å™ªå£°ä¸æ··åˆå±‚æ³¨æ„åŠ›çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§†è§‰-åŠ¨ä½œç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨2025å¹´BEHAVIORæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼Œå±•ç¤ºäº†åœ¨50ä¸ªå¤šæ ·åŒ–å®¶åº­ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åŒæ‰‹æ“ä½œã€å¯¼èˆªå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ã€‚æˆ‘ä»¬æå‡ºçš„ä¸»è¦åˆ›æ–°æ˜¯ä½¿ç”¨ç›¸å…³å™ªå£°è¿›è¡ŒæµåŒ¹é…ï¼Œè¿™æé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶å®ç°äº†å¹³æ»‘çš„åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡å¤šæ ·æœ¬æµåŒ¹é…å’Œå¯å­¦ä¹ çš„æ··åˆå±‚æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°è§£å†³äº†æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µåº”ç”¨äº†åŠ¨ä½œå‹ç¼©å’Œç‰¹å®šæŒ‘æˆ˜çš„ä¿®æ­£è§„åˆ™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02901",
            "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
            "url": "https://huggingface.co/papers/2512.02901",
            "abstract": "Fairy2i converts pre-trained real-valued models to complex form, enabling efficient low-bit quantization while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
            "score": 3,
            "issue_id": 62,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "9b24707f7c40821d",
            "authors": [
                "Feiyu Wang",
                "Xinyu Tan",
                "Bokai Huang",
                "Yihao Zhang",
                "Guoan Wang",
                "Peizhuang Cong",
                "Tong Yang"
            ],
            "affiliations": [
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.02901.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ° ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ²ĞµÑ€Ñ…ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fairy2i â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ğ²ĞµÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ¸ÑĞµĞ» Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ²Ğ¿Ğ»Ğ¾Ñ‚ÑŒ Ğ´Ğ¾ 2-Ğ±Ğ¸Ñ‚Ğ¾Ğ²) Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ„Ğ°Ğ·Ğ¾Ğ²Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ¼ Ñ‡ĞµÑ‚Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ñ… ĞºĞ¾Ñ€Ğ½ĞµĞ¹ Ğ¸Ğ· ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaMA-2 7B Ğ¿Ñ€Ğ¸ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Models for Efficient Low-Bit Quantization",
                    "desc": "Fairy2i is a framework that converts pre-trained real-valued machine learning models into complex-valued forms, allowing for efficient low-bit quantization without losing performance. This approach leverages the advantages of complex-valued representations, which can better handle low-bit quantization compared to traditional real-valued models. By establishing a lossless mathematical equivalence, Fairy2i enables the use of existing pre-trained models while applying a novel phase-aware quantization method. The results show that this method can achieve performance levels close to full precision, making it a significant advancement in efficient model deployment on standard hardware."
                },
                "zh": {
                    "title": "é«˜æ•ˆä½ä½é‡åŒ–çš„æ–°æ–¹æ³•ï¼šFairy2i",
                    "desc": "Fairy2i æ˜¯ä¸€ä¸ªå°†é¢„è®­ç»ƒçš„å®å€¼æ¨¡å‹è½¬æ¢ä¸ºå¤æ•°å½¢å¼çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„ä½ä½é‡åŒ–ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯æ˜å®å€¼å’Œå¹¿ä¹‰çº¿æ€§æ˜ å°„ä¹‹é—´çš„æ— æŸæ•°å­¦ç­‰ä»·æ€§ï¼Œå°†æ ‡å‡†çš„ Transformer è½¬æ¢åˆ°å¤æ•°åŸŸã€‚Fairy2i é‡‡ç”¨äº†ä¸€ç§ç›¸ä½æ„ŸçŸ¥çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥é€’å½’æ®‹å·®é‡åŒ–æœºåˆ¶ï¼Œé€æ­¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚å®éªŒè¡¨æ˜ï¼ŒFairy2i åœ¨æœ‰æ•ˆçš„ 2 ä½ç²¾åº¦ä¸‹ï¼Œèƒ½å¤Ÿæ¢å¤ LLaMA-2 7B çš„æ€§èƒ½ï¼Œæ¥è¿‘å…¨ç²¾åº¦åŸºçº¿ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å®å€¼äºŒè¿›åˆ¶å’Œä¸‰è¿›åˆ¶é‡åŒ–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10858",
            "title": "Scaling Behavior of Discrete Diffusion Language Models",
            "url": "https://huggingface.co/papers/2512.10858",
            "abstract": "Research on DLMs explores their scaling behavior under different noise types, revealing that uniform diffusion is more parameter-efficient and data-efficient compared to masked diffusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.   We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for 10^{22} FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
            "score": 2,
            "issue_id": 60,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "c0f8579773cf08c3",
            "authors": [
                "Dimitri von RÃ¼tte",
                "Janis Fluri",
                "Omead Pooladzandi",
                "Bernhard SchÃ¶lkopf",
                "Thomas Hofmann",
                "Antonio Orvieto"
            ],
            "affiliations": [
                "ELLIS Institute Tubingen",
                "ETH Zurich",
                "Max Planck Institute for Intelligent Systems, Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10858.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ â€” Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (DLM) Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹, Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞµ Ñ‚Ğ¸Ğ¿Ñ‹ ÑˆÑƒĞ¼Ğ° ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğº Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ 10 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ 10^{22} FLOPs, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ ÑĞ°Ğ¼Ğ¾Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Uniform Diffusion: The Efficient Path for DLMs",
                    "desc": "This paper investigates the scaling behavior of discrete diffusion language models (DLMs) under various noise types, particularly focusing on uniform and masked diffusion. The authors find that uniform diffusion is more efficient in terms of parameters and data usage compared to masked diffusion, especially in compute-bound scenarios. Their experiments highlight that the choice of noise type significantly influences the performance and efficiency of DLMs, distinguishing them from autoregressive language models (ALMs). By scaling their uniform diffusion model to 10 billion parameters, they confirm the predicted scaling behavior, establishing it as the largest known model of its kind."
                },
                "zh": {
                    "title": "å‡åŒ€æ‰©æ•£ï¼šé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹é€‰æ‹©",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰åœ¨ä¸åŒå™ªå£°ç±»å‹ä¸‹çš„æ‰©å±•è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œå‡åŒ€æ‰©æ•£åœ¨å‚æ•°æ•ˆç‡å’Œæ•°æ®æ•ˆç‡ä¸Šä¼˜äºæ©è”½æ‰©æ•£ã€‚é€šè¿‡å¹³æ»‘æ’å€¼ï¼Œæˆ‘ä»¬åˆ†æäº†è¶…å‚æ•°å¦‚æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‡åŒ€æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¯æ•°æ®å—é™ç¯å¢ƒä¸‹çš„æœ‰å¸Œæœ›çš„é€‰æ‹©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10715",
            "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
            "url": "https://huggingface.co/papers/2512.10715",
            "abstract": "Uncertainty estimation in landmark-based segmentation of chest X-rays using hybrid neural network architectures improves reliability and robustness in clinical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
            "score": 2,
            "issue_id": 56,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "9dd547fc5e9ff57c",
            "authors": [
                "Matias Cosarinsky",
                "Nicolas Gaggion",
                "Rodrigo Echeveste",
                "Enzo Ferrante"
            ],
            "affiliations": [
                "APOLO Biotech, Buenos Aires, Argentina",
                "Laboratory of Applied Artificial Intelligence, Institute of Computer Sciences, CONICET - Universidad de Buenos Aires, Argentina",
                "Research Institute for Signals, Systems and Computational Intelligence, sinc(i), CONICET - Universidad Nacional del Litoral, Argentina",
                "Weizmann Institute of Science, Rehovot, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10715.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#healthcare",
                    "#architecture",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ«",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ… Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ° Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CheXmask-U Ñ 657,566 ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒĞ·Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Enhancing Reliability in Chest X-ray Segmentation through Uncertainty Estimation",
                    "desc": "This paper focuses on improving the reliability of landmark-based segmentation in chest X-rays by incorporating uncertainty estimation. It introduces a hybrid neural network architecture that combines convolutional encoders with graph-based decoders to derive two types of uncertainty: latent uncertainty and predictive uncertainty. The study shows that these uncertainty measures can effectively identify unreliable predictions and enhance out-of-distribution detection. Additionally, the authors provide a large dataset, CheXmask-U, which includes uncertainty estimates for anatomical landmark segmentations, facilitating better assessment of segmentation quality."
                },
                "zh": {
                    "title": "æå‡èƒ¸éƒ¨Xå…‰åˆ†å‰²å¯é æ€§çš„å…³é”®åœ¨äºä¸ç¡®å®šæ€§ä¼°è®¡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè§£å‰–æ ‡å¿—çš„èƒ¸éƒ¨Xå…‰å›¾åƒåˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†æ ‡å‡†å›¾åƒå·ç§¯ç¼–ç å™¨å’ŒåŸºäºå›¾çš„ç”Ÿæˆè§£ç å™¨ï¼Œä»¥æé«˜åˆ†å‰²çš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚é€šè¿‡å¼•å…¥æ½œåœ¨ä¸ç¡®å®šæ€§å’Œé¢„æµ‹ä¸ç¡®å®šæ€§ä¸¤ç§äº’è¡¥åº¦é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«ä¸å¯é çš„é¢„æµ‹å¹¶æ”¯æŒä¸´åºŠåº”ç”¨ä¸­çš„äººå·¥ç›‘ç£ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«657,566ä¸ªèƒ¸éƒ¨Xå…‰æ ‡å¿—åˆ†å‰²åŠå…¶ä¸ç¡®å®šæ€§ä¼°è®¡çš„å¤§å‹æ•°æ®é›†ï¼Œä¿ƒè¿›äº†ç›¸å…³ç ”ç©¶çš„å¼€å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11393",
            "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
            "url": "https://huggingface.co/papers/2512.11393",
            "abstract": "A model learns to parallelize tasks from a single egocentric video by addressing spatial and object conflicts, achieving improved action coverage and reduced collisions.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
            "score": 1,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "ee646b596d383e5a",
            "authors": [
                "Zhifan Zhu",
                "Yifei Huang",
                "Yoichi Sato",
                "Dima Damen"
            ],
            "affiliations": [
                "The University of Tokyo",
                "University of Bristol"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11393.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision-Language Model Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° 45% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ»Ğ¸Ğ·Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 55%, 45% Ğ¸ 55% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Learning to Parallelize Tasks from Egocentric Video",
                    "desc": "This paper presents a method for a model to learn how to parallelize tasks observed in a single egocentric video. It introduces the N-Body Problem, which involves multiple individuals performing tasks while avoiding conflicts such as spatial collisions and object usage overlaps. The authors propose metrics to evaluate both the efficiency of task execution and the feasibility of the proposed actions. By using a structured prompting strategy with a Vision-Language Model, the model significantly improves action coverage and reduces conflicts in task execution."
                },
                "zh": {
                    "title": "ä»è§†é¢‘ä¸­å­¦ä¹ ä»»åŠ¡å¹¶è¡ŒåŒ–çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•ä¸ªè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­å­¦ä¹ å¦‚ä½•å¹¶è¡ŒåŒ–ä»»åŠ¡ï¼Œè§£å†³ç©ºé—´å’Œç‰©ä½“å†²çªé—®é¢˜ã€‚æˆ‘ä»¬å®šä¹‰äº†Nä½“é—®é¢˜ï¼Œæ¢è®¨Nä¸ªä¸ªä½“å¦‚ä½•åœ¨è§‚å¯Ÿåˆ°çš„ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°å¹¶è¡Œæ‰§è¡Œã€‚é€šè¿‡å¼•å…¥ä¸€å¥—è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬ä¸ä»…å…³æ³¨ä»»åŠ¡çš„è¦†ç›–ç‡å’Œé€Ÿåº¦æå‡ï¼Œè¿˜è€ƒè™‘äº†å®é™…æ‰§è¡Œä¸­çš„å¯è¡Œæ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘ä¸Šæ˜¾è‘—æé«˜äº†åŠ¨ä½œè¦†ç›–ç‡ï¼Œå¹¶å‡å°‘äº†å†²çªå’Œç¢°æ’çš„å‘ç”Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11130",
            "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
            "url": "https://huggingface.co/papers/2512.11130",
            "abstract": "Fast-FoundationStereo achieves real-time zero-shot stereo generalization by combining knowledge distillation, blockwise neural architecture search, and structured pruning.  \t\t\t\t\tAI-generated summary \t\t\t\t Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/",
            "score": 1,
            "issue_id": 68,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "ad26844b06d27c8f",
            "authors": [
                "Bowen Wen",
                "Shaurya Dewan",
                "Stan Birchfield"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11130.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset",
                    "#inference",
                    "#synthetic",
                    "#cv",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Fast-FoundationStereo â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ, Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 1.4 Ğ¼Ğ»Ğ½ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ FoundationStereo, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ñ‚ĞµÑ€ÑÑ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°."
                },
                "en": {
                    "title": "Real-Time Stereo Vision Made Efficient!",
                    "desc": "Fast-FoundationStereo is a novel approach that enables real-time zero-shot stereo generalization by integrating three advanced techniques: knowledge distillation, blockwise neural architecture search, and structured pruning. This method addresses the challenge of achieving high performance in stereo vision without the need for extensive fine-tuning on specific datasets. By compressing a complex model into a more efficient one through knowledge distillation and optimizing architecture design, Fast-FoundationStereo significantly reduces computational costs while maintaining accuracy. The introduction of a large dataset for training further enhances the model's robustness, allowing it to operate over 10 times faster than previous methods while achieving comparable results."
                },
                "zh": {
                    "title": "å®æ—¶é›¶-shot ç«‹ä½“è§†è§‰çš„æ–°çªç ´",
                    "desc": "Fast-FoundationStereo æ˜¯ä¸€ç§æ–°å‹çš„ç«‹ä½“è§†è§‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶æ¡ä»¶ä¸‹å®ç°é›¶-shot æ³›åŒ–ã€‚å®ƒç»“åˆäº†çŸ¥è¯†è’¸é¦ã€åˆ†å—ç¥ç»æ¶æ„æœç´¢å’Œç»“æ„åŒ–å‰ªæä¸‰ç§æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼ŒFast-FoundationStereo å¯ä»¥åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¿è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿæ¨¡å‹å¿«åå€ä»¥ä¸Šã€‚è¯¥æ¨¡å‹çš„åˆ›æ–°ä½¿å¾—å®æ—¶ç«‹ä½“è§†è§‰åº”ç”¨å˜å¾—æ›´åŠ å¯è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10685",
            "title": "Sharp Monocular View Synthesis in Less Than a Second",
            "url": "https://huggingface.co/papers/2512.10685",
            "abstract": "SHARP synthesizes photorealistic views from a single image using a 3D Gaussian representation, achieving state-of-the-art results with rapid processing.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
            "score": 1,
            "issue_id": 59,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "49143c212a60b0bb",
            "authors": [
                "Lars Mescheder",
                "Wei Dong",
                "Shiwei Li",
                "Xuyang Bai",
                "Marcel Santos",
                "Peiyun Hu",
                "Bruno Lecouat",
                "Mingmin Zhen",
                "AmaÃ«l Delaunoy",
                "Tian Fang",
                "Yanghai Tsin",
                "Stephan R. Richter",
                "Vladlen Koltun"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10685.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğº Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ½Ğ¾Ğ¼Ñƒ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "SHARP â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° GPU Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñƒ ÑÑ†ĞµĞ½Ñ‹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ LPIPS Ğ¸ DISTS Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Single Images into Stunning 3D Views with SHARP",
                    "desc": "SHARP is a novel method for creating photorealistic images from a single input photograph using a 3D Gaussian model. It efficiently regresses the parameters of this model through a neural network, allowing for rapid processing on standard GPUs. The resulting 3D representation can be rendered in real time, producing high-quality images for different viewpoints. SHARP achieves significant improvements in image quality metrics and processing speed compared to previous methods, demonstrating strong generalization across various datasets."
                },
                "zh": {
                    "title": "SHARPï¼šå¿«é€Ÿç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„æ–°æ–¹æ³•",
                    "desc": "SHARPæ˜¯ä¸€ç§ä»å•å¼ å›¾åƒåˆæˆç…§ç‰‡çº§çœŸå®æ„Ÿè§†å›¾çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å›å½’åœºæ™¯çš„3Dé«˜æ–¯è¡¨ç¤ºçš„å‚æ•°ï¼Œåœ¨æ ‡å‡†GPUä¸Šä»…éœ€ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å®Œæˆå¤„ç†ã€‚SHARPç”Ÿæˆçš„3Dé«˜æ–¯è¡¨ç¤ºå¯ä»¥å®æ—¶æ¸²æŸ“ï¼Œæä¾›é«˜åˆ†è¾¨ç‡çš„çœŸå®æ„Ÿå›¾åƒï¼Œå¹¶æ”¯æŒåº¦é‡ç›¸æœºè¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSHARPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåˆæˆæ—¶é—´å‡å°‘äº†ä¸‰ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶åœ¨å›¾åƒè´¨é‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11798",
            "title": "Particulate: Feed-Forward 3D Object Articulation",
            "url": "https://huggingface.co/papers/2512.11798",
            "abstract": "Particulate is a feed-forward method using a transformer network to infer articulated 3D structures from single static meshes, achieving faster and more accurate results than prior approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
            "score": 0,
            "issue_id": 65,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "70c5da4dfa7293e6",
            "authors": [
                "Ruining Li",
                "Yuxin Yao",
                "Chuanxia Zheng",
                "Christian Rupprecht",
                "Joan Lasenby",
                "Shangzhe Wu",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "University of Cambridge",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11798.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€",
                    "desc": "Particulate â€” ÑÑ‚Ğ¾ feed-forward Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸. Ğ¡ĞµÑ‚ÑŒ Part Articulation Transformer Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ end-to-end Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ 3D Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Fast and Accurate 3D Articulation with Particulate",
                    "desc": "Particulate is a novel feed-forward method that utilizes a transformer network to derive articulated 3D structures from a single static mesh. It effectively predicts various attributes of the 3D object, such as its parts, kinematic structure, and motion constraints, all while supporting multi-joint configurations. The model is trained end-to-end on a diverse dataset of articulated 3D assets, allowing it to generate fully articulated models quickly during inference. Additionally, Particulate can process both real and AI-generated 3D assets, making it a versatile tool for 3D articulation estimation."
                },
                "zh": {
                    "title": "å¿«é€Ÿå‡†ç¡®çš„3Då…³èŠ‚åŒ–ç»“æ„æ¨æ–­",
                    "desc": "Particulateæ˜¯ä¸€ç§å‰é¦ˆæ–¹æ³•ï¼Œåˆ©ç”¨å˜æ¢å™¨ç½‘ç»œä»å•ä¸ªé™æ€ç½‘æ ¼æ¨æ–­å‡ºå…³èŠ‚åŒ–çš„3Dç»“æ„ï¼Œé€Ÿåº¦æ›´å¿«ä¸”å‡†ç¡®æ€§æ›´é«˜ã€‚è¯¥æ–¹æ³•å¯ä»¥ç›´æ¥ä»æ—¥å¸¸ç‰©ä½“çš„é™æ€3Dç½‘æ ¼ä¸­æ¨æ–­å‡ºå…¶æ‰€æœ‰å±æ€§ï¼ŒåŒ…æ‹¬3Déƒ¨ä»¶ã€è¿åŠ¨ç»“æ„å’Œè¿åŠ¨çº¦æŸã€‚å…¶æ ¸å¿ƒæ˜¯Part Articulation Transformerç½‘ç»œï¼Œèƒ½å¤Ÿå¤„ç†è¾“å…¥ç½‘æ ¼çš„ç‚¹äº‘ï¼Œé¢„æµ‹æ‰€æœ‰ç›¸å…³å±æ€§ï¼Œå¹¶æ”¯æŒå¤šå…³èŠ‚ç»“æ„ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„å…³èŠ‚åŒ–3Dèµ„äº§ä¸Šè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼ŒParticulateèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆå®Œæ•´çš„å…³èŠ‚åŒ–3Dæ¨¡å‹ï¼Œæ˜¾è‘—ä¼˜äºéœ€è¦é€ä¸ªå¯¹è±¡ä¼˜åŒ–çš„ä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-12.html",
    "link_next": "2025-12-16.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 9,
        "#healthcare": 3,
        "#training": 10,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}