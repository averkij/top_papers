{
    "date": {
        "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 2",
        "zh": "4æœˆ2æ—¥"
    },
    "time_utc": "2025-04-02 20:12",
    "weekday": 2,
    "issue_id": 3035,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.24379",
            "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.24379",
            "abstract": "To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/",
            "score": 43,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "dce65db5da1b8c34",
            "authors": [
                "Shengqiong Wu",
                "Weicai Ye",
                "Jiahao Wang",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Shuicheng Yan",
                "Hao Fei",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24379.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Any2Caption - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Any2CapIns Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Any2Caption",
                    "desc": "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."
                },
                "zh": {
                    "title": "å¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šAny2Caption",
                    "desc": "ä¸ºäº†å…‹æœå½“å‰è§†é¢‘ç”Ÿæˆé¢†åŸŸä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†Any2Captionï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¯æ§è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤è§£è€¦ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘åŠç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œç›¸æœºå§¿æ€ï¼‰è½¬åŒ–ä¸ºå¯†é›†çš„ç»“æ„åŒ–å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Any2CapInsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«337Kå®ä¾‹å’Œ407Kæ¡ä»¶çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00050",
            "title": "JudgeLRM: Large Reasoning Models as a Judge",
            "url": "https://huggingface.co/papers/2504.00050",
            "abstract": "The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.",
            "score": 29,
            "issue_id": 3022,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "5060d3d364f635eb",
            "authors": [
                "Nuo Chen",
                "Zhiyuan Hu",
                "Qingyun Zou",
                "Jiaying Wu",
                "Qian Wang",
                "Bryan Hooi",
                "Bingsheng He"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00050.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-ÑÑƒĞ´ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ JudgeLRM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹. JudgeLRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ JudgeLRM-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.79% Ğ¿Ğ¾ F1-Ğ¼ĞµÑ€Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DeepSeek-R1."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts LLMs for Complex Judging Tasks",
                    "desc": "This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning."
                },
                "zh": {
                    "title": "JudgeLRMï¼šæ·±åº¦æ¨ç†çš„è¯„åˆ¤è€…",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è€…çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦æ·±åº¦æ¨ç†çš„æ ·æœ¬æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†JudgeLRMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„è¯„åˆ¤å¯¼å‘LLMï¼Œèƒ½å¤Ÿæä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJudgeLRMæ¨¡å‹åœ¨è¯„åˆ¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„SFTæ¨¡å‹å’Œæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œå°¤å…¶åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23145",
            "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
            "url": "https://huggingface.co/papers/2503.23145",
            "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.",
            "score": 26,
            "issue_id": 3021,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "945cc4b51522e668",
            "authors": [
                "Anjiang Wei",
                "Tarun Suresh",
                "Jiannan Cao",
                "Naveen Kannan",
                "Yuheng Wu",
                "Kai Yan",
                "Thiago S. F. X. Teixeira",
                "Ke Wang",
                "Alex Aiken"
            ],
            "affiliations": [
                "Intel",
                "MIT",
                "Stanford University",
                "University of Illinois Urbana-Champaign",
                "Visa Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23145.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#plp",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CodeARC: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CodeARC - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², CodeARC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1114 Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 52.7% ÑƒÑĞ¿ĞµÑ…Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "CodeARC: A New Frontier in Inductive Program Synthesis Evaluation",
                    "desc": "This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks."
                },
                "zh": {
                    "title": "CodeARCï¼šæå‡ç¨‹åºåˆæˆçš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "å½’çº³ç¨‹åºåˆæˆï¼Œä¹Ÿç§°ä¸ºç¤ºä¾‹ç¼–ç¨‹ï¼Œæ˜¯ä»è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­åˆæˆå‡½æ•°çš„è¿‡ç¨‹ï¼Œè¦æ±‚èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è¾“å…¥ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å½’çº³ç¨‹åºåˆæˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°åè®®ä¾èµ–äºé™æ€ç¤ºä¾‹é›†å’Œä¿ç•™æµ‹è¯•ï¼Œæ— æ³•åœ¨åˆæˆå‡½æ•°é”™è¯¯æ—¶æä¾›åé¦ˆï¼Œä¹Ÿæœªèƒ½åæ˜ ç°å®ä¸–ç•Œä¸­çš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†CodeARCï¼Œä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå…è®¸ä»£ç†é€šè¿‡æŸ¥è¯¢éšè—çš„ç›®æ ‡å‡½æ•°ä¸ä¹‹äº’åŠ¨ï¼Œä»è€Œåˆæˆå€™é€‰å‡½æ•°å¹¶æ ¹æ®åé¦ˆè¿­ä»£æ”¹è¿›è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24376",
            "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
            "url": "https://huggingface.co/papers/2503.24376",
            "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.",
            "score": 24,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "d22966d0969ded43",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Rui Wang",
                "Yixiao Ge",
                "Lu Qiu",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24376.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SEED-Bench-R1 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2-VL-Instruct-7B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ½Ğµ ĞµĞ³Ğ¾. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ½Ğ¾ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµĞ½ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with SEED-Bench-R1",
                    "desc": "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "æœ€è¿‘ï¼Œé“¾å¼æ€ç»´ï¼ˆCOTï¼‰ç”Ÿæˆçš„è¿›å±•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒæ–¹æ³•ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»§æ‰¿äº†è¿™ç§æ¨ç†æ½œåŠ›ï¼Œä½†åœ¨éœ€è¦æ„ŸçŸ¥å’Œé€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEED-Bench-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMsåœ¨è§†é¢‘ç†è§£ä¸­åè®­ç»ƒæ–¹æ³•çš„åŸºå‡†ï¼ŒåŒ…å«å¤æ‚çš„çœŸå®è§†é¢‘å’Œå¤šé¡¹é€‰æ‹©é¢˜çš„æ—¥å¸¸è§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†åœ¨é€»è¾‘è¿è´¯æ€§æ–¹é¢å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00698",
            "title": "Command A: An Enterprise-Ready Large Language Model",
            "url": "https://huggingface.co/papers/2504.00698",
            "abstract": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.",
            "score": 16,
            "issue_id": 3020,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "8670e6d1cc4f6bee",
            "authors": [
                "Team Cohere",
                "Aakanksha",
                "Arash Ahmadian",
                "Marwan Ahmed",
                "Jay Alammar",
                "Yazeed Alnumay",
                "Sophia Althammer",
                "Arkady Arkhangorodsky",
                "Viraat Aryabumi",
                "Dennis Aumiller",
                "RaphaÃ«l Avalos",
                "Zahara Aviv",
                "Sammie Bae",
                "Saurabh Baji",
                "Alexandre Barbet",
                "Max Bartolo",
                "BjÃ¶rn Bebensee",
                "Neeral Beladia",
                "Walter Beller-Morales",
                "Alexandre BÃ©rard",
                "Andrew Berneshawi",
                "Anna Bialas",
                "Phil Blunsom",
                "Matt Bobkin",
                "Adi Bongale",
                "Sam Braun",
                "Maxime Brunet",
                "Samuel Cahyawijaya",
                "David Cairuz",
                "Jon Ander Campos",
                "Cassie Cao",
                "Kris Cao",
                "Roman CastagnÃ©",
                "JuliÃ¡n Cendrero",
                "Leila Chan Currie",
                "Yash Chandak",
                "Diane Chang",
                "Giannis Chatziveroglou",
                "Hongyu Chen",
                "Claire Cheng",
                "Alexis Chevalier",
                "Justin T. Chiu",
                "Eugene Cho",
                "Eugene Choi",
                "Eujeong Choi",
                "Tim Chung",
                "Volkan Cirik",
                "Ana Cismaru",
                "Pierre Clavier",
                "Henry Conklin",
                "Lucas Crawhall-Stein",
                "Devon Crouse",
                "Andres Felipe Cruz-Salinas",
                "Ben Cyrus",
                "Daniel D'souza",
                "Hugo Dalla-Torre",
                "John Dang",
                "William Darling",
                "Omar Darwiche Domingues",
                "Saurabh Dash",
                "Antoine Debugne",
                "ThÃ©o Dehaze",
                "Shaan Desai",
                "Joan Devassy",
                "Rishit Dholakia",
                "Kyle Duffy",
                "Ali Edalati",
                "Ace Eldeib",
                "Abdullah Elkady",
                "Sarah Elsharkawy",
                "Irem ErgÃ¼n",
                "Beyza Ermis",
                "Marzieh Fadaee",
                "Boyu Fan",
                "Lucas Fayoux",
                "Yannis Flet-Berliac",
                "Nick Frosst",
                "Matthias GallÃ©",
                "Wojciech Galuba",
                "Utsav Garg",
                "Matthieu Geist",
                "Mohammad Gheshlaghi Azar",
                "Seraphina Goldfarb-Tarrant",
                "Tomas Goldsack",
                "Aidan Gomez",
                "Victor Machado Gonzaga",
                "Nithya Govindarajan",
                "Manoj Govindassamy",
                "Nathan Grinsztajn",
                "Nikolas Gritsch",
                "Patrick Gu",
                "Shangmin Guo",
                "Kilian Haefeli",
                "Rod Hajjar",
                "Tim Hawes",
                "Jingyi He",
                "Sebastian HofstÃ¤tter",
                "Sungjin Hong",
                "Sara Hooker",
                "Tom Hosking",
                "Stephanie Howe",
                "Eric Hu",
                "Renjie Huang",
                "Hemant Jain",
                "Ritika Jain",
                "Nick Jakobi",
                "Madeline Jenkins",
                "JJ Jordan",
                "Dhruti Joshi",
                "Jason Jung",
                "Trushant Kalyanpur",
                "Siddhartha Rao Kamalakara",
                "Julia Kedrzycki",
                "Gokce Keskin",
                "Edward Kim",
                "Joon Kim",
                "Wei-Yin Ko",
                "Tom Kocmi",
                "Michael Kozakov",
                "Wojciech KryÅ›ciÅ„ski",
                "Arnav Kumar Jain",
                "Komal Kumar Teru",
                "Sander Land",
                "Michael Lasby",
                "Olivia Lasche",
                "Justin Lee",
                "Patrick Lewis",
                "Jeffrey Li",
                "Jonathan Li",
                "Hangyu Lin",
                "Acyr Locatelli",
                "Kevin Luong",
                "Raymond Ma",
                "Lukas Mach",
                "Marina Machado",
                "Joanne Magbitang",
                "Brenda Malacara Lopez",
                "Aryan Mann",
                "Kelly Marchisio",
                "Olivia Markham",
                "Alexandre Matton",
                "Alex McKinney",
                "Dominic McLoughlin",
                "Jozef Mokry",
                "Adrien Morisot",
                "Autumn Moulder",
                "Harry Moynehan",
                "Maximilian Mozes",
                "Vivek Muppalla",
                "Lidiya Murakhovska",
                "Hemangani Nagarajan",
                "Alekhya Nandula",
                "Hisham Nasir",
                "Shauna Nehra",
                "Josh Netto-Rosen",
                "Daniel Ohashi",
                "James Owers-Bardsley",
                "Jason Ozuzu",
                "Dennis Padilla",
                "Gloria Park",
                "Sam Passaglia",
                "Jeremy Pekmez",
                "Laura Penstone",
                "Aleksandra Piktus",
                "Case Ploeg",
                "Andrew Poulton",
                "Youran Qi",
                "Shubha Raghvendra",
                "Miguel Ramos",
                "Ekagra Ranjan",
                "Pierre Richemond",
                "CÃ©cile Robert-Michon",
                "AurÃ©lien Rodriguez",
                "Sudip Roy",
                "Laura Ruis",
                "Louise Rust",
                "Anubhav Sachan",
                "Alejandro Salamanca",
                "Kailash Karthik Saravanakumar",
                "Isha Satyakam",
                "Alice Schoenauer Sebag",
                "Priyanka Sen",
                "Sholeh Sepehri",
                "Preethi Seshadri",
                "Ye Shen",
                "Tom Sherborne",
                "Sylvie Chang Shi",
                "Sanal Shivaprasad",
                "Vladyslav Shmyhlo",
                "Anirudh Shrinivason",
                "Inna Shteinbuk",
                "Amir Shukayev",
                "Mathieu Simard",
                "Ella Snyder",
                "Ava Spataru",
                "Victoria Spooner",
                "Trisha Starostina",
                "Florian Strub",
                "Yixuan Su",
                "Jimin Sun",
                "Dwarak Talupuru",
                "Eugene Tarassov",
                "Elena Tommasone",
                "Jennifer Tracey",
                "Billy Trend",
                "Evren Tumer",
                "Ahmet ÃœstÃ¼n",
                "Bharat Venkitesh",
                "David Venuto",
                "Pat Verga",
                "Maxime Voisin",
                "Alex Wang",
                "Donglu Wang",
                "Shijian Wang",
                "Edmond Wen",
                "Naomi White",
                "Jesse Willman",
                "Marysia Winkels",
                "Chen Xia",
                "Jessica Xie",
                "Minjie Xu",
                "Bowen Yang",
                "Tan Yi-Chern",
                "Ivan Zhang",
                "Zhenyu Zhao",
                "Zhoujie Zhao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.00698.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#low_resource",
                    "#agents",
                    "#open_source",
                    "#rag",
                    "#multilingual",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Command A: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Command A - Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 23 ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Command A Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Retrieval Augmented Generation (RAG) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering Enterprises with Command A: The Future of Language Models",
                    "desc": "This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results."
                },
                "zh": {
                    "title": "Command Aï¼šä¼ä¸šåº”ç”¨çš„å¼ºå¤§è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Command Açš„å¼€å‘ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºä¼ä¸šå®é™…åº”ç”¨è€Œè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚Command Aå…·å¤‡å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ”¯æŒ23ç§å…¨çƒå•†ä¸šè¯­è¨€ï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„æ··åˆæ¶æ„ï¼Œå…¼é¡¾æ•ˆç‡ä¸é«˜æ€§èƒ½ã€‚å®ƒæä¾›äº†æœ€ä½³çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å·¥å…·ä½¿ç”¨å’ŒåŸºç¡€çŸ¥è¯†æ”¯æŒæ¥è‡ªåŠ¨åŒ–å¤æ‚çš„ä¸šåŠ¡æµç¨‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸Command Aç›¸ä¼¼çš„Command R7Bæ¨¡å‹çš„ç»“æœï¼Œå¹¶å‘å¸ƒäº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„æƒé‡ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01016",
            "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.01016",
            "abstract": "Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.",
            "score": 15,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "9430b45c3324fb61",
            "authors": [
                "Tian-Xing Xu",
                "Xiangjun Gao",
                "Wenbo Hu",
                "Xiaoyu Li",
                "Song-Hai Zhang",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "HKUST",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01016.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "GeometryCrafter: Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
                    "desc": "GeometryCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ 3D/4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GeometryCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps",
                    "desc": "This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets."
                },
                "zh": {
                    "title": "GeometryCrafterï¼šé«˜ä¿çœŸè§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°æ¡†æ¶",
                    "desc": "å°½ç®¡è§†é¢‘æ·±åº¦ä¼°è®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•ä¿çœŸåº¦æ–¹é¢å­˜åœ¨å›ºæœ‰å±€é™ï¼Œé™åˆ¶äº†å…¶åœ¨é‡å»ºå’Œå…¶ä»–åº¦é‡åŸºç¡€ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†GeometryCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å¼€æ”¾ä¸–ç•Œè§†é¢‘ä¸­æ¢å¤å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„é«˜ä¿çœŸç‚¹å›¾åºåˆ—ï¼Œä»è€Œå®ç°å‡†ç¡®çš„3D/4Dé‡å»ºå’Œç›¸æœºå‚æ•°ä¼°è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯ä¸€ä¸ªç‚¹å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒå­¦ä¹ ä¸€ä¸ªä¸è§†é¢‘æ½œåœ¨åˆ†å¸ƒæ— å…³çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æœ‰æ•ˆåœ°è¿›è¡Œç‚¹å›¾ç¼–ç å’Œè§£ç ã€‚é€šè¿‡åˆ©ç”¨VAEï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å»ºæ¨¡åŸºäºè¾“å…¥è§†é¢‘çš„ç‚¹å›¾åºåˆ—çš„åˆ†å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00595",
            "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
            "url": "https://huggingface.co/papers/2504.00595",
            "abstract": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.",
            "score": 15,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "3e8c667bd93d754e",
            "authors": [
                "Weizhi Wang",
                "Yu Tian",
                "Linjie Yang",
                "Heng Wang",
                "Xifeng Yan"
            ],
            "affiliations": [
                "Nvidia Research",
                "Seed Vision Team, ByteDance",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00595.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Open-Qwen2VL - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 29 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 442 Ñ‡Ğ°ÑĞ° GPU A100-40G. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Open-Qwen2VL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2-VL-2B Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL",
                    "desc": "The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Open-Qwen2VLï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰20äº¿å‚æ•°ï¼Œä½¿ç”¨2900ä¸‡å¯¹å›¾åƒ-æ–‡æœ¬æ•°æ®è¿›è¡Œé«˜æ•ˆé¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŠ¨æ€å›¾åƒåˆ†è¾¨ç‡å’Œå¤šæ¨¡æ€åºåˆ—æ‰“åŒ…æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨MLLMå’ŒCLIPçš„è¿‡æ»¤æŠ€æœ¯ï¼Œæå‡äº†æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚æœ€ç»ˆï¼ŒOpen-Qwen2VLåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†éƒ¨åˆ†å¼€æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00810",
            "title": "Z1: Efficient Test-time Scaling with Code",
            "url": "https://huggingface.co/papers/2504.00810",
            "abstract": "Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.",
            "score": 14,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "d982593a14ba7da9",
            "authors": [
                "Zhaojian Yu",
                "Yinghao Wu",
                "Yilun Zhao",
                "Arman Cohan",
                "Xiao-Ping Zhang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00810.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#dataset",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Z1-Code-Reasoning-107K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Shifted Thinking Window Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Z1-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Reasoning in Large Language Models",
                    "desc": "This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œç®€åŒ–æ€è€ƒï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç›¸å…³çš„æ¨ç†è½¨è¿¹ä¸Šï¼Œå‡å°‘å¤šä½™çš„æ€è€ƒæ ‡è®°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºZ1-Code-Reasoning-107Kçš„æ•°æ®é›†ï¼ŒåŒ…å«ç®€å•å’Œå¤æ‚çš„ç¼–ç é—®é¢˜åŠå…¶è§£å†³è½¨è¿¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç§»ä½æ€ç»´çª—å£ï¼Œé€šè¿‡å»é™¤ä¸Šä¸‹æ–‡åˆ†éš”æ ‡ç­¾å’Œé™åˆ¶æ¨ç†æ ‡è®°ï¼Œæ¥å‡è½»è¿‡åº¦æ€è€ƒçš„è´Ÿæ‹…ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹Z1-7Bèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è°ƒæ•´æ¨ç†æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01019",
            "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
            "url": "https://huggingface.co/papers/2504.01019",
            "abstract": "Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.",
            "score": 13,
            "issue_id": 3022,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "745108d1df40d1b4",
            "authors": [
                "Pablo Ruiz-Ponce",
                "German Barquero",
                "Cristina Palmero",
                "Sergio Escalera",
                "JosÃ© GarcÃ­a-RodrÃ­guez"
            ],
            "affiliations": [
                "Kings College London, UK",
                "Universidad de Alicante, Spain",
                "Universitat de Barcelona and Computer Vision Center, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01019.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#diffusion",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MixerMDM - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², MixerMDM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MixerMDM Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Dynamic Control of Human Motion Generation with MixerMDM",
                    "desc": "This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process."
                },
                "zh": {
                    "title": "åŠ¨æ€æ··åˆï¼Œç²¾ç»†æ§åˆ¶äººç±»è¿åŠ¨ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ç»„åˆæŠ€æœ¯ï¼Œç§°ä¸ºMixerMDMï¼Œç”¨äºç»“åˆé¢„è®­ç»ƒçš„æ–‡æœ¬æ¡ä»¶äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹ã€‚ä¸ä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼ŒMixerMDMé‡‡ç”¨åŠ¨æ€æ··åˆç­–ç•¥ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå­¦ä¹ å¦‚ä½•æ ¹æ®ç”Ÿæˆæ¡ä»¶ç»„åˆå»å™ªè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯¹å•äººå’Œå¤šäººè¿åŠ¨çš„ç²¾ç»†æ§åˆ¶ï¼Œæå‡äº†æ¯ä¸ªäººçš„åŠ¨æ€è¡¨ç°åŠæ•´ä½“äº’åŠ¨æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŠ€æœ¯ï¼Œé¦–æ¬¡é‡åŒ–äº†ç”Ÿæˆè¿åŠ¨ä¸æ¡ä»¶ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä»¥åŠMixerMDMåœ¨å»å™ªè¿‡ç¨‹ä¸­é€‚åº”æ··åˆçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00906",
            "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
            "url": "https://huggingface.co/papers/2504.00906",
            "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
            "score": 13,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "e51174b579a417e9",
            "authors": [
                "Saaket Agashe",
                "Kyle Wong",
                "Vincent Tu",
                "Jiachen Yang",
                "Ang Li",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Simular Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00906.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Agent S2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent S2 - Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Agent S2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding",
                    "desc": "This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems."
                },
                "zh": {
                    "title": "Agent S2ï¼šæ™ºèƒ½ä»£ç†çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent S2çš„æ–°å‹æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è®¤çŸ¥ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„é€šç”¨æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹æ¥æé«˜æ•°å­—ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå®šä½æŠ€æœ¯ï¼Œä»¥å®ç°ç²¾ç¡®çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å…ƒç´ å®šä½ï¼Œå¹¶å¼•å…¥äº†ä¸»åŠ¨å±‚æ¬¡è§„åˆ’ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸æ–­å˜åŒ–çš„è§‚å¯ŸåŠ¨æ€è°ƒæ•´è¡ŒåŠ¨è®¡åˆ’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAgent S2åœ¨ä¸‰ä¸ªä¸»è¦çš„è®¡ç®—æœºä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆä»£ç†ã€‚ç‰¹åˆ«æ˜¯åœ¨OSWorldè¯„ä¼°ä¸­ï¼ŒAgent S2ç›¸è¾ƒäºå…¶ä»–ä»£ç†å®ç°äº†18.9%å’Œ32.7%çš„ç›¸å¯¹æå‡ï¼Œå±•ç°äº†å…¶åœ¨ä¸åŒæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00509",
            "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
            "url": "https://huggingface.co/papers/2504.00509",
            "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.",
            "score": 11,
            "issue_id": 3018,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "c9697e67f23cfa4e",
            "authors": [
                "Kai Yan",
                "Yufei Xu",
                "Zhengyin Du",
                "Xuesong Yao",
                "Zheyu Wang",
                "Xiaowen Guo",
                "Jiecao Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00509.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑƒĞ¼Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ?",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RoR-Bench Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 60%) Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ñ€Ğ°Ğ·Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Reassessing LLM Intelligence: Are They Truly Reasoning?",
                    "desc": "This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance dropsâ€”up to 60%â€”when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence."
                },
                "zh": {
                    "title": "é‡æ–°å®¡è§†LLMçš„æ™ºèƒ½æ°´å¹³",
                    "desc": "è¿‘å¹´æ¥ï¼ŒLLMåŸºå‡†æµ‹è¯•çš„éš¾åº¦ä»å°å­¦æ°´å¹³è¿…é€Ÿä¸Šå‡åˆ°å‰æ²¿é—®é¢˜ï¼Œè¿™è®©ç ”ç©¶äººå‘˜æ„Ÿåˆ°æˆ‘ä»¬ç¦»è¶…è¶Šäººç±»æ™ºèƒ½åªæœ‰ä¸€æ­¥ä¹‹é¥ã€‚ç„¶è€Œï¼ŒLLMçš„æ¨ç†èƒ½åŠ›æ˜¯å¦çœŸçš„æ˜¯äººç±»æ ‡å‡†ä¸‹çš„çœŸæ­£æ™ºèƒ½ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨è®­ç»ƒä¸­è§è¿‡çš„è§£å†³æ–¹æ¡ˆçš„å¤è¿°ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoR-Benchï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºæ£€æµ‹LLMåœ¨ç®€å•æ¨ç†é—®é¢˜ä¸­æ˜¯å¦å­˜åœ¨å¤è¿°è¡Œä¸ºã€‚æˆ‘ä»¬çš„å®è¯åˆ†æå‘ç°ï¼Œç°æœ‰çš„é¡¶å°–LLMåœ¨æ¡ä»¶ç¨å¾®æ”¹å˜æ—¶ï¼Œè¡¨ç°å‡ºæå…¶ä¸¥é‡çš„å¤è¿°è¡Œä¸ºï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡æ–°è¯„ä¼°è¿™äº›æ¨¡å‹çš„çœŸå®æ™ºèƒ½æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24377",
            "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
            "url": "https://huggingface.co/papers/2503.24377",
            "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.",
            "score": 11,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "d18ba97a459aea36",
            "authors": [
                "Rui Wang",
                "Hongru Wang",
                "Boyang Xue",
                "Jianhui Pang",
                "Shudong Liu",
                "Yi Chen",
                "Jiahao Qiu",
                "Derek Fai Wong",
                "Heng Ji",
                "Kam-Fai Wong"
            ],
            "affiliations": [
                "Princeton University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "University of Illinois Urbana-Champaign",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24377.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#survey",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ğ°Ğ¿ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ĞµĞ¼ÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Balancing Performance and Cost in Language Model Reasoning",
                    "desc": "This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research."
                },
                "zh": {
                    "title": "æ¨ç†ç»æµï¼šå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬çš„å…³é”®",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å…¶æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¿«é€Ÿç›´è§‰æ€ç»´ï¼ˆç³»ç»Ÿ1ï¼‰ä¸ç¼“æ…¢æ·±åº¦æ¨ç†ï¼ˆç³»ç»Ÿ2ï¼‰ä¹‹é—´çš„è½¬å˜ã€‚è™½ç„¶ç³»ç»Ÿ2æ¨ç†æé«˜äº†ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œä½†ç”±äºå…¶æ€ç»´ç¼“æ…¢å’Œæ¨ç†è¡Œä¸ºä½æ•ˆï¼Œå¾€å¾€ä¼šå¸¦æ¥è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ç›¸å¯¹è€Œè¨€ï¼Œç³»ç»Ÿ1æ¨ç†è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œå½¢æˆäº†æ¨ç†ç»æµçš„æ¦‚å¿µï¼Œè¿™æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22952",
            "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts",
            "url": "https://huggingface.co/papers/2503.22952",
            "abstract": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.",
            "score": 11,
            "issue_id": 3024,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "8b78ccf427a5cdc0",
            "authors": [
                "Yuxuan Wang",
                "Yueqian Wang",
                "Bo Chen",
                "Tong Wu",
                "Dongyan Zhao",
                "Zilong Zheng"
            ],
            "affiliations": [
                "Beijing Institute for General Artificial Intelligence",
                "State Key Laboratory of General Artificial Intelligence",
                "Wangxuan Institute of Computer Technology, Peking University",
                "X-LANCE Lab, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22952.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#inference",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "OmniMMI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniMMI - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-modal Multiplexing Modeling (M4) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Interaction with OmniLLMs in Streaming Video",
                    "desc": "This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„äº’åŠ¨èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†OmniMMIï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºOmniè¯­è¨€æ¨¡å‹åœ¨æµåª’ä½“è§†é¢‘ç¯å¢ƒä¸­è®¾è®¡çš„å¤šæ¨¡æ€äº¤äº’åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡1121ä¸ªè§†é¢‘å’Œ2290ä¸ªé—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘åŸºå‡†ä¸­æµåª’ä½“è§†é¢‘ç†è§£å’Œä¸»åŠ¨æ¨ç†çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºå¤šæ¨¡æ€å¤ç”¨å»ºæ¨¡ï¼ˆM4ï¼‰ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆæ¨ç†çš„æµåª’ä½“æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶è¿›è¡Œè§†è§‰å’Œå¬è§‰å¤„ç†ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬å¸Œæœ›æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„äº’åŠ¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01017",
            "title": "Scaling Language-Free Visual Representation Learning",
            "url": "https://huggingface.co/papers/2504.01017",
            "abstract": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: \"Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?\" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.",
            "score": 9,
            "issue_id": 3023,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "9ab970f68b26c2ea",
            "authors": [
                "David Fan",
                "Shengbang Tong",
                "Jiachen Zhu",
                "Koustuv Sinha",
                "Zhuang Liu",
                "Xinlei Chen",
                "Michael Rabbat",
                "Nicolas Ballas",
                "Yann LeCun",
                "Amir Bar",
                "Saining Xie"
            ],
            "affiliations": [
                "FAIR, Meta",
                "New York University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01017.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SSL) Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… (CLIP) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SSL Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ CLIP Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (VQA). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ SSL Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Visual SSL: Bridging the Gap with Scale and Data",
                    "desc": "This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning."
                },
                "zh": {
                    "title": "è§†è§‰è‡ªç›‘ç£å­¦ä¹ çš„æ½œåŠ›ä¸CLIPç›¸åª²ç¾",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç›¸åŒçš„MetaCLIPæ•°æ®ä¸Šè®­ç»ƒè§†è§‰SSLå’ŒCLIPæ¨¡å‹ï¼Œæ¥åˆ†æè¯­è¨€ç›‘ç£çš„ç¼ºä¹æ˜¯å¦æ˜¯å¯¼è‡´è§†è§‰SSLè½åçš„åŸå› ã€‚ç»“æœæ˜¾ç¤ºï¼Œè§†è§‰SSLæ¨¡å‹åœ¨æ•°æ®å’Œæ¨¡å‹å®¹é‡æ–¹é¢çš„æ‰©å±•æ€§ä¼˜äºCLIPæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å‚æ•°è¾¾åˆ°70äº¿æ—¶æ€§èƒ½ä»æœªé¥±å’Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œçº¯è§†è§‰SSLåœ¨å¤§è§„æ¨¡ä¸‹å¯ä»¥è¾¾åˆ°ä¸è¯­è¨€ç›‘ç£è§†è§‰é¢„è®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºè§†è§‰ä¸­å¿ƒçš„è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°çš„æœºä¼šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01005",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.01005",
            "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "score": 9,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "ee8e4951bf6c7a18",
            "authors": [
                "Nishad Singhi",
                "Hritik Bansal",
                "Arian Hosseini",
                "Aditya Grover",
                "Kai-Wei Chang",
                "Marcus Rohrbach",
                "Anna Rohrbach"
            ],
            "affiliations": [
                "Google DeepMind",
                "Mila",
                "TU Darmstadt & hessian.AI",
                "University of California Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01005.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Consistency (SC), Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾Ğµ, Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Generative Reward Models (GenRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ SC Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ GenRM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Solution Generation and Verification for Efficient Reasoning in LLMs",
                    "desc": "This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†èƒ½åŠ›ï¼šè§£ç”Ÿæˆä¸éªŒè¯çš„å¹³è¡¡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¦‚ä½•é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„è‡ªä¸€è‡´æ€§ï¼ˆSelf-Consistency, SCï¼‰æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªè§£å¹¶é‡‡ç”¨å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆã€‚æœ€è¿‘çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenerative Reward Models, GenRMï¼‰åˆ™å°†éªŒè¯é‡æ„ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œä»è€Œåœ¨æ¨ç†æ—¶å¼•å…¥æ–°çš„æ‰©å±•æ–¹å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒSCåœ¨å¤§å¤šæ•°å®é™…æƒ…å†µä¸‹æ¯”GenRMæ›´å…·è®¡ç®—æ•ˆç‡ï¼Œæä¾›äº†åœ¨æµ‹è¯•æ—¶æ‰©å±•ä¸­ä¼˜åŒ–è§£ç”Ÿæˆä¸éªŒè¯çš„å®ç”¨æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00927",
            "title": "Multi-Token Attention",
            "url": "https://huggingface.co/papers/2504.00927",
            "abstract": "Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This \"single token attention\" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.",
            "score": 9,
            "issue_id": 3020,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "2af4c7adceecde31",
            "authors": [
                "Olga Golovneva",
                "Tianlu Wang",
                "Jason Weston",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00927.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Multi-Token Attention (MTA). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° soft attention, MTA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ ĞºĞ»ÑÑ‡Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MTA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Richer Context with Multi-Token Attention",
                    "desc": "This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval."
                },
                "zh": {
                    "title": "å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼šæå‡ä¸Šä¸‹æ–‡ç†è§£çš„å…³é”®",
                    "desc": "è½¯æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¸€ä¸ªé‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºåœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­å®šä½ç›¸å…³éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•ä¸ªä»¤ç‰Œæ³¨æ„åŠ›æ–¹æ³•ä»…ä¾èµ–äºå•ä¸ªæŸ¥è¯¢å’Œé”®å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œè¿™é™åˆ¶äº†ä¿¡æ¯çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æ–¹æ³•â€”â€”å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼ˆMTAï¼‰ï¼Œå®ƒå…è®¸LLMsåŒæ—¶åŸºäºå¤šä¸ªæŸ¥è¯¢å’Œé”®å‘é‡æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡å¯¹æŸ¥è¯¢ã€é”®å’Œå¤´éƒ¨åº”ç”¨å·ç§¯æ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä»è€Œåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ›´å‡†ç¡®åœ°å®šä½ç›¸å…³å†…å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23434",
            "title": "Towards Trustworthy GUI Agents: A Survey",
            "url": "https://huggingface.co/papers/2503.23434",
            "abstract": "GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.",
            "score": 9,
            "issue_id": 3024,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "e19e4d94bcea9cb0",
            "authors": [
                "Yucheng Shi",
                "Wenhao Yu",
                "Wenlin Yao",
                "Wenhu Chen",
                "Ninghao Liu"
            ],
            "affiliations": [
                "Amazon",
                "Tencent AI Seattle Lab",
                "University of Georgia",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23434.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security",
                    "#agents",
                    "#survey",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ ÑĞ±Ğ¾Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Ensuring Trust in Autonomous GUI Agents",
                    "desc": "This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent."
                },
                "zh": {
                    "title": "æ„å»ºå¯ä¿¡èµ–çš„GUIä»£ç†ï¼Œä¿éšœå®‰å…¨ä¸éšç§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„ä¿¡ä»»æ€§é—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†äº”ä¸ªå…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬å®‰å…¨æ¼æ´ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€é€æ˜æ€§ä¸å¯è§£é‡Šæ€§ã€ä¼¦ç†è€ƒé‡ä»¥åŠè¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºäº†ä¸»è¦æŒ‘æˆ˜ï¼Œå¦‚å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€åºåˆ—å†³ç­–ä¸­çš„çº§è”å¤±è´¥æ¨¡å¼ï¼Œä»¥åŠç¼ºä¹ç°å®çš„è¯„ä¼°åŸºå‡†ã€‚è¿™äº›é—®é¢˜ä¸ä»…é˜»ç¢äº†GUIä»£ç†çš„å®é™…åº”ç”¨ï¼Œè¿˜éœ€è¦è¶…è¶Šä»»åŠ¡æˆåŠŸçš„å…¨é¢ç¼“è§£ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23733",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
            "url": "https://huggingface.co/papers/2503.23733",
            "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.",
            "score": 8,
            "issue_id": 3017,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "ed45063868071c13",
            "authors": [
                "Yiyang Du",
                "Xiaochen Wang",
                "Chi Chen",
                "Jiabo Ye",
                "Yiru Wang",
                "Peng Li",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Zhifang Sui",
                "Maosong Sun",
                "Yang Liu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                "Institute of Intelligent Computing, Alibaba Group",
                "Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China",
                "ModelTC Open Source Organization, Beijing, China",
                "School of Software Microelectronics, Peking University, Beijing, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23733.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "AdaMMS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AdaMMS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ñ… ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². AdaMMS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AdaMMS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Merging Diverse Models with AdaMMS",
                    "desc": "This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods."
                },
                "zh": {
                    "title": "å¼‚è´¨æ¨¡å‹åˆå¹¶çš„æ–°çªç ´",
                    "desc": "æœ€è¿‘ï¼Œæ¨¡å‹åˆå¹¶æ–¹æ³•åœ¨ç»“åˆå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ä¼˜åŠ¿ã€‚ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆå¹¶å…·æœ‰ç›¸åŒæ¶æ„çš„åŒè´¨æ¨¡å‹ï¼Œä½†åœ¨å¤„ç†å…·æœ‰å›ºæœ‰å¼‚è´¨æ€§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AdaMMSï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¼‚è´¨MLLMsè®¾è®¡çš„æ–°å‹æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼Œé‡‡ç”¨æ˜ å°„ã€åˆå¹¶å’Œæœç´¢ä¸‰ä¸ªæ­¥éª¤æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡è®¾è®¡æ¨¡å‹ä¹‹é—´çš„æ˜ å°„å‡½æ•°ã€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œçº¿æ€§æ’å€¼ä»¥åŠæå‡ºæ— ç›‘ç£çš„è¶…å‚æ•°é€‰æ‹©æ–¹æ³•ï¼ŒAdaMMSåœ¨å„ç§è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00557",
            "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
            "url": "https://huggingface.co/papers/2504.00557",
            "abstract": "Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.",
            "score": 7,
            "issue_id": 3018,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "c5628fbf1a189a06",
            "authors": [
                "Jewon Lee",
                "Ki-Ung Song",
                "Seungmin Yang",
                "Donguk Lim",
                "Jaeyeon Kim",
                "Wooksu Shin",
                "Bo-Kyeong Kim",
                "Yong Jae Lee",
                "Tae-Ho Kim"
            ],
            "affiliations": [
                "Nota Inc.",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00557.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ñ€Ñ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Trimmed Llama: Efficient Visual Token Reduction for Faster Inference",
                    "desc": "This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks."
                },
                "zh": {
                    "title": "è§†è§‰ç‰¹å¾ä¿®å‰ªï¼Œæå‡æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰æ ‡è®°å‡å°‘çš„æ–¹æ³•ï¼Œä»¥é™ä½å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ¨ç†æ—¶çš„è®¡ç®—æˆæœ¬ã€‚ä¸ä»¥å¾€åªé’ˆå¯¹è‡ªæ³¨æ„åŠ›æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸“æ³¨äºåŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ›´ä¸ºä¼˜è¶Šã€‚æˆ‘ä»¬å‘ç°äº¤å‰æ³¨æ„åŠ›å±‚ä¸­å›¾åƒæ ‡è®°çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°è¿œå¤§äºè‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ–‡æœ¬æ ‡è®°ï¼Œæˆä¸ºè®¡ç®—ç“¶é¢ˆã€‚é€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å›¾çš„ç¨€ç–ç‰¹æ€§ï¼Œæˆ‘ä»¬é€‰æ‹©æ€§åœ°ä¿®å‰ªå†—ä½™çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘KVç¼“å­˜éœ€æ±‚ï¼Œé™ä½æ¨ç†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒåŸºå‡†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22165",
            "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
            "url": "https://huggingface.co/papers/2503.22165",
            "abstract": "Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.",
            "score": 6,
            "issue_id": 3034,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "76c9bb027c844f9c",
            "authors": [
                "Zhanke Zhou",
                "Zhaocheng Zhu",
                "Xuan Li",
                "Mikhail Galkin",
                "Xiao Feng",
                "Sanmi Koyejo",
                "Jian Tang",
                "Bo Han"
            ],
            "affiliations": [
                "HEC Montreal",
                "Intel AI Lab",
                "Mila - Quebec AI Institute",
                "Stanford University",
                "TMLR Group, Hong Kong Baptist University",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22165.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'landscape of thoughts' Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ t-SNE Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 'landscape of thoughts' ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Visualizing Reasoning Paths in Language Models",
                    "desc": "This paper introduces a new visualization tool called 'landscape of thoughts' that helps users understand how large language models (LLMs) reason through problems. It represents reasoning paths as feature vectors, which show how close each reasoning step is to possible answers. By using a technique called t-SNE, the tool creates two-dimensional plots that allow for easy comparison of different models and their reasoning effectiveness. The tool also identifies problematic reasoning patterns and can be adapted to evaluate the accuracy of reasoning paths in various tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è·¯å¾„",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ€ç»´æ™¯è§‚â€çš„å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†è·¯å¾„ã€‚è¯¥å·¥å…·é€šè¿‡å°†æ¨ç†è·¯å¾„ä¸­çš„çŠ¶æ€è¡¨ç¤ºä¸ºç‰¹å¾å‘é‡ï¼Œé‡åŒ–å®ƒä»¬ä¸æ‰€æœ‰ç­”æ¡ˆé€‰é¡¹çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨t-SNEè¿›è¡ŒäºŒç»´å¯è§†åŒ–ã€‚é€šè¿‡å¯¹æ€ç»´æ™¯è§‚çš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œå¯ä»¥æœ‰æ•ˆåŒºåˆ†å¼ºå¼±æ¨¡å‹ã€æ­£ç¡®ä¸é”™è¯¯ç­”æ¡ˆï¼Œä»¥åŠä¸åŒçš„æ¨ç†ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥å·¥å…·è¿˜èƒ½å¤Ÿæ­ç¤ºä¸ç†æƒ³çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚ä½ä¸€è‡´æ€§å’Œé«˜ä¸ç¡®å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00869",
            "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
            "url": "https://huggingface.co/papers/2504.00869",
            "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.",
            "score": 5,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "bd9586b08ce02a05",
            "authors": [
                "Xiaoke Huang",
                "Juncheng Wu",
                "Hui Liu",
                "Xianfeng Tang",
                "Yuyin Zhou"
            ],
            "affiliations": [
                "Amazon Research",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00869.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#healthcare",
                    "#science",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ m1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Medical Reasoning with Test-Time Scaling",
                    "desc": "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."
                },
                "zh": {
                    "title": "åŒ»å­¦æ¨ç†çš„æ–°çªç ´ï¼šæµ‹è¯•æ—¶ç¼©æ”¾çš„åŠ›é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯åœ¨åŒ»å­¦æ¨ç†ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºm1çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åŒ»å­¦èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸­å‡èƒ½æ˜¾è‘—æé«˜æ¨ç†æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºå‚æ•°å°‘äº10Bçš„è½»é‡çº§å¾®è°ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ€ä½³çš„æ¨ç†ä»¤ç‰Œé¢„ç®—çº¦ä¸º4Kï¼Œè¶…å‡ºæ­¤èŒƒå›´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¢åŠ æ•°æ®è§„æ¨¡ã€æé«˜æ•°æ®è´¨é‡å’Œæ‰©å±•æ¨¡å‹å®¹é‡æ˜¯æå‡åŒ»å­¦çŸ¥è¯†åŸºç¡€çš„å…³é”®ï¼Œå°¤å…¶æ˜¯åœ¨å°æ¨¡å‹æ€§èƒ½é¥±å’Œçš„æƒ…å†µä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00294",
            "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
            "url": "https://huggingface.co/papers/2504.00294",
            "abstract": "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.",
            "score": 5,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "b455a4adb4eae588",
            "authors": [
                "Vidhisha Balachandran",
                "Jingya Chen",
                "Lingjiao Chen",
                "Shivam Garg",
                "Neel Joshi",
                "Yash Lara",
                "John Langford",
                "Besmira Nushi",
                "Vibhav Vineet",
                "Yue Wu",
                "Safoora Yousefi"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00294.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸."
                },
                "en": {
                    "title": "Scaling Inference for Smarter Problem Solving in LLMs",
                    "desc": "This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements."
                },
                "zh": {
                    "title": "æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†ä¹ç§æœ€å…ˆè¿›æ¨¡å‹åœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œç©ºé—´æ¨ç†ç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ—¶é—´æ‰©å±•çš„ä¼˜åŠ¿å› ä»»åŠ¡è€Œå¼‚ï¼Œä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ä¼šå‡å¼±ã€‚å°½ç®¡ä½¿ç”¨æ›´å¤šçš„æ ‡è®°å¹¶ä¸æ€»èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†åœ¨æœ‰å¼ºåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23361",
            "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
            "url": "https://huggingface.co/papers/2503.23361",
            "abstract": "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.",
            "score": 4,
            "issue_id": 3017,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "9b957c49c958aea3",
            "authors": [
                "Linxin Song",
                "Xuwei Ding",
                "Jieyu Zhang",
                "Taiwei Shi",
                "Ryotaro Shimizu",
                "Rahul Gupta",
                "Yang Liu",
                "Jian Kang",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "AGI",
                "Amazon",
                "University of Rochester",
                "University of Southern California",
                "University of Washington",
                "University of Wisconsin-Madison",
                "ZOZO Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23361.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#optimization",
                    "#graphs",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SEA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SEA (ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SEA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² LLM."
                },
                "en": {
                    "title": "Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA",
                    "desc": "This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery."
                },
                "zh": {
                    "title": "å‘ç°LLMçŸ¥è¯†ç¼ºé™·çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸æ— æ³•å‡†ç¡®ä¿ç•™äº‹å®çŸ¥è¯†ï¼Œå¯¼è‡´å¹»è§‰å’Œä¸å¯é çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºéšæœºé”™è¯¯ä¸Šå‡ï¼ˆSEAï¼‰çš„æ¡†æ¶ï¼Œç”¨äºåœ¨ä¸¥æ ¼çš„æŸ¥è¯¢é¢„ç®—ä¸‹å‘ç°é—­åˆæƒé‡LLMsä¸­çš„çŸ¥è¯†ç¼ºé™·ã€‚SEAé€šè¿‡åˆ©ç”¨ä¸å…ˆå‰è§‚å¯Ÿåˆ°çš„å¤±è´¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿­ä»£æ£€ç´¢æ–°çš„é«˜é”™è¯¯å€™é€‰é¡¹ï¼Œä»è€Œå°†é”™è¯¯å‘ç°è¿‡ç¨‹å½¢å¼åŒ–ä¸ºéšæœºä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒSEAå‘ç°çš„çŸ¥è¯†é”™è¯¯æ•°é‡æ˜¾è‘—é«˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ¯ä¸ªé”™è¯¯çš„æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00072",
            "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
            "url": "https://huggingface.co/papers/2504.00072",
            "abstract": "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.",
            "score": 3,
            "issue_id": 3021,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "48f5266ddbfa7bca",
            "authors": [
                "Lucas Ventura",
                "Antoine Yang",
                "Cordelia Schmid",
                "GÃ¼l Varol"
            ],
            "affiliations": [
                "Google DeepMind",
                "Inria, Ecole normale superieure, CNRS, PSL Research University",
                "LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00072.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ“½ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ 'Chapter-Llama' Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. LLM Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ³Ğ»Ğ°Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VidChapters-7M, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ F1-score 45.3 Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 26.7."
                },
                "en": {
                    "title": "Efficient Video Chaptering with Chapter-Llama",
                    "desc": "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç« èŠ‚åˆ’åˆ†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç« èŠ‚åˆ’åˆ†çš„ä»»åŠ¡ï¼Œå³å°†é•¿è§†é¢‘æ—¶é—´çº¿åˆ’åˆ†ä¸ºè¯­ä¹‰å•å…ƒå¹¶ç”Ÿæˆç›¸åº”çš„ç« èŠ‚æ ‡é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†'Chapter-Llama'æ¡†æ¶ï¼Œé€šè¿‡é«˜æ•ˆå¤„ç†æ–‡æœ¬é¢†åŸŸçš„é—®é¢˜ï¼Œå®ç°äº†å¯¹é•¿è¾¾ä¸€å°æ—¶è§†é¢‘çš„å¼ºå¤§ç« èŠ‚åˆ’åˆ†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¾“å…¥åŒ…æ‹¬è¯­éŸ³è½¬å½•æ–‡æœ¬å’Œæè¿°è§†é¢‘å¸§çš„å­—å¹•ï¼Œä»¥åŠå®ƒä»¬å„è‡ªçš„æ—¶é—´æˆ³ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¯­éŸ³è½¬å½•å†…å®¹çš„è½»é‡çº§å¸§é€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ç« èŠ‚åˆ’åˆ†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24210",
            "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
            "url": "https://huggingface.co/papers/2503.24210",
            "abstract": "Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io",
            "score": 2,
            "issue_id": 3023,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "df1e0752d5790146",
            "authors": [
                "Seungjun Lee",
                "Gim Hee Lee"
            ],
            "affiliations": [
                "Department of Computer Science, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24210.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ§ĞµÑ‚ĞºĞ¾Ğµ 3D Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ 2D: DiET-GS Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiET-GS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ 3DGS Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiET-GS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing 3D Image Clarity with DiET-GS",
                    "desc": "This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets."
                },
                "zh": {
                    "title": "æ¸…æ™°ä¸‰ç»´é‡å»ºçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiET-GSçš„æ¡†æ¶ï¼Œç”¨äºä»æ¨¡ç³Šçš„å¤šè§†å›¾å›¾åƒä¸­é‡å»ºæ¸…æ™°çš„ä¸‰ç»´è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— æ¨¡ç³Šäº‹ä»¶æµå’Œæ‰©æ•£å…ˆéªŒï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æ¥æé«˜å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„çº¦æŸæ–¹æ³•ï¼Œåˆ©ç”¨äº‹ä»¶åŒé‡ç§¯åˆ†æ¥ç¡®ä¿é¢œè‰²å‡†ç¡®å’Œç»†èŠ‚æ¸…æ™°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„æŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›ä¸€æ­¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23157",
            "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL",
            "url": "https://huggingface.co/papers/2503.23157",
            "abstract": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.",
            "score": 2,
            "issue_id": 3029,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "083970087ba6180e",
            "authors": [
                "Mohammadreza Pourreza",
                "Shayan Talaei",
                "Ruoxi Sun",
                "Xingchen Wan",
                "Hailong Li",
                "Azalia Mirhoseini",
                "Amin Saberi",
                "Sercan \"O. Arik"
            ],
            "affiliations": [
                "Google Cloud",
                "Google DeepMind",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23157.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Text-to-SQL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Text-to-SQL, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² RL. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO), Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ supervised fine-tuning Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BIRD."
                },
                "en": {
                    "title": "Enhancing Text-to-SQL with Tailored Reinforcement Learning Rewards",
                    "desc": "This paper addresses the complex task of converting natural language into SQL queries, which requires understanding language, database structures, and formulating precise queries. The authors critique existing methods that use fixed reasoning paths, which can hinder performance, and propose a new approach that utilizes tailored partial rewards to improve reinforcement learning outcomes. By implementing group relative policy optimization (GRPO), their method encourages large language models to enhance their reasoning skills, leading to better SQL generation. The results show that their reinforcement learning approach outperforms traditional supervised fine-tuning, achieving higher accuracy on benchmark tests with a smaller model size."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°SQLçš„æ¨ç†èƒ½åŠ›ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°SQLçš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€é¡¹æ¶‰åŠè‡ªç„¶è¯­è¨€ç†è§£å’Œæ•°æ®åº“æ¶æ„ç†è§£çš„å¤æ‚ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨ç†è·¯å¾„ï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éƒ¨åˆ†å¥–åŠ±æœºåˆ¶ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ä¸Šéƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21860",
            "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
            "url": "https://huggingface.co/papers/2503.21860",
            "abstract": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.",
            "score": 2,
            "issue_id": 3017,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "0d9ea55946287027",
            "authors": [
                "Kailin Li",
                "Puhao Li",
                "Tengyu Liu",
                "Yuyang Li",
                "Siyuan Huang"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21860.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#robotics",
                    "#transfer_learning",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ManipTrans: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼",
                    "desc": "ManipTrans - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞº Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DexManipNet Ñ 3.3 Ñ‚Ñ‹Ñ. ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficiently Teaching Robots to Manipulate Like Humans",
                    "desc": "This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands."
                },
                "zh": {
                    "title": "é«˜æ•ˆè½¬ç§»äººç±»åŒæ‰‹æŠ€èƒ½çš„æœºå™¨äººæ‰‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºManipTransçš„æ–°æ–¹æ³•ï¼Œç”¨äºå°†äººç±»åŒæ‰‹çš„æŠ€èƒ½é«˜æ•ˆåœ°è½¬ç§»åˆ°çµå·§çš„æœºå™¨äººæ‰‹ä¸Šã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„è½¨è¿¹æ¨¡ä»¿å™¨æ¥æ¨¡æ‹Ÿæ‰‹éƒ¨åŠ¨ä½œï¼Œç„¶ååœ¨äº¤äº’çº¦æŸä¸‹å¾®è°ƒç‰¹å®šçš„æ®‹å·®æ¨¡å—ï¼Œä»è€Œå®ç°å¤æ‚åŒæ‰‹ä»»åŠ¡çš„é«˜æ•ˆå­¦ä¹ å’Œå‡†ç¡®æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManipTransåœ¨æˆåŠŸç‡ã€ä¿çœŸåº¦å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ManipTransï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºDexManipNetçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº†3.3Kä¸ªæœºå™¨äººæ“ä½œçš„å®ä¾‹ï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„ç­–ç•¥è®­ç»ƒå’Œå®é™…åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24219",
            "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing",
            "url": "https://huggingface.co/papers/2503.24219",
            "abstract": "We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.",
            "score": 1,
            "issue_id": 3027,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "c59d3238abf4164f",
            "authors": [
                "Karim Radouane",
                "Hanane Azzag",
                "Mustapha lebbah"
            ],
            "affiliations": [
                "University Paris-Saclay - DAVID Lab, UVSQ Versailles, France",
                "University Sorbonne Paris Nord - LIPN, Villetaneuse, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24219.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#graphs",
                    "#dataset"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OPT-RSVG Ğ¸ DIOR-RSVG."
                },
                "en": {
                    "title": "Integrating Object Detection and Visual Grounding for Enhanced Remote Sensing Analysis",
                    "desc": "This paper presents a new framework that combines object detection (OD) and visual grounding (VG) specifically for remote sensing imagery. The authors enhance a traditional open-set object detector by fine-tuning it with referring expression data, treating VG as a partially supervised OD task. They create a graph representation of images that includes object queries and class embeddings, which is then processed by a multi-branch network to generate proposals. The model outperforms existing methods on benchmark datasets while maintaining the capabilities of classical object detection."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶ï¼šç›®æ ‡æ£€æµ‹ä¸è§†è§‰å®šä½çš„ç»“åˆ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç›®æ ‡æ£€æµ‹ï¼ˆODï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰é›†æˆåˆ°é¥æ„Ÿå›¾åƒä¸­ã€‚ä¸ºäº†æ”¯æŒä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹å¹¶ä¸ºè§†è§‰å®šä½ä»»åŠ¡å»ºç«‹ç›´è§‚çš„å…ˆéªŒï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒè¡¨è¾¾æ•°æ®å¾®è°ƒäº†ä¸€ä¸ªå¼€æ”¾é›†ç›®æ ‡æ£€æµ‹å™¨ï¼Œå°†å…¶æ¡†å®šä¸ºéƒ¨åˆ†ç›‘ç£çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æ„å»ºäº†æ¯ä¸ªå›¾åƒçš„å›¾å½¢è¡¨ç¤ºï¼ŒåŒ…æ‹¬ç›®æ ‡æŸ¥è¯¢ã€ç±»åˆ«åµŒå…¥å’Œæè®®ä½ç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ„ŸçŸ¥æ¶æ„å¤„ç†è¿™ä¸ªå›¾å½¢ä»¥æ‰§è¡Œè§†è§‰å®šä½ä»»åŠ¡ï¼Œæœ€ç»ˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-01.html",
    "link_next": "2025-04-03.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "01.04",
        "en": "04/01",
        "zh": "4æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 16,
        "#agents": 5,
        "#cv": 5,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 10,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 16,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 12,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 12,
        "#survey": 2,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚",
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i Any2Caption de xÄ«n kuÃ ng jiÃ , zhÇ yÃº jiÄ› juÃ© dÄng qiÃ¡n shÃ¬ pÇn shÄ“ng chÃ©ng shÃ¨ qÅ« zhÅng zhÇ”n quÃ¨ lÇ jiÄ› yÃ²ng hÃ¹ de pÃ­ng yÇ wÃ¨n ti. gÇi kuÃ ng jiÃ  de hÃ© xÄ«n sÄ« xiÇng shÃ¬ jiÄng gÃ¨ zhÇ’ng tiÃ¡o jiÃ n jiÄ› shÃ¬ bÃ¹ zhÃ²u yÇ” shÃ¬ pÇn hÃ© chÃ©ng bÃ¹ zhÃ²u fÄ“n lÃ­. tÅng guÃ² lÃ¬ yÃ²ng xiÃ n dÃ i duÅ mÃ³ shÃ¬ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs), Any2Caption nÃ©ng gÃ²u jiÄng wÃ©n bÄ›n, tÃº xiÃ ng, shÃ¬ pÇn hÃ© tÃ¨ dÃ¬ng tÃ­ shÃ¬ (rÃº qÅ« yÃ¹, yÃ¹n dÃ²ng hÃ© shÃ¨ xiÃ ng tÃ³u zÄ« tÃ i) zhuÇn huÃ n wÃ¨i mÃ¬ jÄ«, jiÃ© gÃ²u huÃ  de zÃ¬ mÃ¹, cÃ³ng Ã©r wÃ©i shÃ¬ pÇn shÄ“ng chÃ©ng tÃ­ gÅng gÄ›ng hÇo de zhÇ dÇo. wÃ©n zhÄng hÃ¡i yÇn rÃ¹ le yÄ« gÃ¨ dÃ  guÄ« mÃ³ shÃ¹ jÃ¹ Any2CapIns, bÄo hÃ¡n 337K gÃ¨ shÃ­ lÃ¬ hÃ© 407K gÃ¨ tiÃ¡o jiÃ n, yÃ²ng yÃº rÃ¨n yÃ¬ tiÃ¡o jiÃ n dÃ o zÃ¬ mÃ¹ de zhÇ lÃ¬ng tiÃ¡o yÅu. zÃ²ng hÃ© pÃ­ng gÇ” biÇo mÃ­ng, gÇi xÃ¬ tÇ’ng zÃ i kÃ¨ kÃ²ng xÃ¬ng hÃ© shÃ¬ pÇn zhÃ¬ liÃ ng fÄng miÃ n xiÇn zhÃ¹ yÅu xiÃ n yÇ’u de shÃ¬ pÇn shÄ“ng chÃ©ng mÃ³ xÃ­ng.",
        "vocab": "[{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'ç“¶é¢ˆ', 'pinyin': 'pÃ­ng jÇng', 'trans': 'bottleneck'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'æ ¸å¿ƒ', 'pinyin': 'hÃ© xÄ«n', 'trans': 'core'},\n{'word': 'æ€æƒ³', 'pinyin': 'sÄ« xiÇng', 'trans': 'idea'},\n{'word': 'è§£é‡Š', 'pinyin': 'jiÄ› shÃ¬', 'trans': 'interpretation'},\n{'word': 'æ­¥éª¤', 'pinyin': 'bÃ¹ zhÃ²u', 'trans': 'step'},\n{'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesis'},\n{'word': 'åˆ†ç¦»', 'pinyin': 'fÄ“n lÃ­', 'trans': 'separation'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ shuÃ i', 'trans': 'multimodal'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è½¬æ¢', 'pinyin': 'zhuÇn huÃ n', 'trans': 'convert'},\n{'word': 'å¯†é›†', 'pinyin': 'mÃ¬ jÃ­', 'trans': 'dense'},\n{'word': 'ç»“æ„åŒ–', 'pinyin': 'jiÃ© gÃ²u huÃ ', 'trans': 'structured'},\n{'word': 'å­—å¹•', 'pinyin': 'zÃ¬ mÃ¹', 'trans': 'subtitle'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guidance'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å®ä¾‹', 'pinyin': 'shÃ­ lÃ¬', 'trans': 'instance'},\n{'word': 'è°ƒä¼˜', 'pinyin': 'tiÃ¡o yÅu', 'trans': 'tuning'},\n{'word': 'ç»¼åˆ', 'pinyin': 'zÃ²ng hÃ©', 'trans': 'comprehensive'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'å¯æ§æ€§', 'pinyin': 'kÄ› kÃ²ng xÃ¬ng', 'trans': 'controllability'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]",
        "trans": "This article introduces a new framework called Any2Caption, aimed at addressing the bottleneck issue of accurately understanding user intent in the current video generation community. The core idea of the framework is to separate various condition interpretation steps from video synthesis steps. By leveraging modern multimodal large language models (MLLMs), Any2Caption can convert text, images, videos, and specific prompts (such as regions, motion, and camera poses) into dense, structured captions, providing better guidance for video generation. The article also introduces a large-scale dataset, Any2CapIns, containing 337K instances and 407K conditions, for instruction tuning from any condition to captions. Comprehensive evaluations indicate that the system significantly outperforms existing video generation models in terms of controllability and video quality.",
        "update_ts": "2025-04-02 09:11"
    }
}