{
    "date": {
        "ru": "26 Ğ¸ÑĞ½Ñ",
        "en": "June 26",
        "zh": "6æœˆ26æ—¥"
    },
    "time_utc": "2025-06-26 09:13",
    "weekday": 3,
    "issue_id": 4500,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18095",
            "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
            "url": "https://huggingface.co/papers/2506.18095",
            "abstract": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.",
            "score": 43,
            "issue_id": 4496,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ½Ñ",
                "en": "June 22",
                "zh": "6æœˆ22æ—¥"
            },
            "hash": "ea0d767800ce404b",
            "authors": [
                "Junying Chen",
                "Zhenyang Cai",
                "Pengcheng Chen",
                "Shunian Chen",
                "Ke Ji",
                "Xidong Wang",
                "Yunjin Yang",
                "Benyou Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18095.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ShareGPT-4o-Image - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 45 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ 46 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4o. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Janus-4o, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Janus-4o Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼ Janus-Pro Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 91 Ñ‚Ñ‹ÑÑÑ‡Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ShareGPT-4o-Image Ğ¸ Janus-4o Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Democratizing Photorealistic Image Generation with Open Datasets and Models",
                    "desc": "This paper introduces ShareGPT-4o-Image, a comprehensive dataset designed to enhance photorealistic image generation aligned with user instructions. It includes 45,000 text-to-image and 46,000 text-and-image-to-image samples, all generated using the advanced capabilities of GPT-4o. The authors also present Janus-4o, a multimodal large language model that improves upon previous models by enabling both text-to-image and text-and-image-to-image generation. With only 91,000 synthetic samples and minimal training time, Janus-4o demonstrates significant advancements in generating high-quality images, promoting open research in this field."
                },
                "zh": {
                    "title": "å¼€æ”¾ç ”ç©¶ï¼ŒçœŸå®æ„Ÿå›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ShareGPT-4o-Imageæ•°æ®é›†å’ŒJanus-4oæ¨¡å‹ï¼Œæ—¨åœ¨æ¨åŠ¨å¼€æ”¾ç ”ç©¶åœ¨çœŸå®æ„Ÿå›¾åƒç”Ÿæˆé¢†åŸŸçš„å‘å±•ã€‚ShareGPT-4o-ImageåŒ…å«45Kæ–‡æœ¬åˆ°å›¾åƒå’Œ46Kæ–‡æœ¬ä¸å›¾åƒåˆ°å›¾åƒçš„æ•°æ®ï¼Œåˆ©ç”¨GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œåˆæˆã€‚Janus-4oæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬ä¸å›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆæœã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å·¥å…·èƒ½å¤Ÿä¿ƒè¿›çœŸå®æ„Ÿã€æŒ‡ä»¤å¯¹é½çš„å›¾åƒç”Ÿæˆç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19697",
            "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
            "url": "https://huggingface.co/papers/2506.19697",
            "abstract": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
            "score": 28,
            "issue_id": 4495,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "3a6d3af4578d26f6",
            "authors": [
                "Jungwoo Park",
                "Taewhoo Lee",
                "Chanwoong Yoon",
                "Hyeon Hwang",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19697.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Outlier-Safe Pre-Training (OSP). OSP Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon, Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RMSNorm Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OSP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "Preventing Outliers for Better LLM Performance",
                    "desc": "This paper presents Outlier-Safe Pre-Training (OSP), a novel approach to enhance the quantization performance of large language models (LLMs) by preventing extreme activation outliers during training. The authors identify that these outliers significantly impair the efficiency of LLMs when deployed on devices, and propose a proactive strategy rather than relying on post-training fixes. OSP incorporates three innovations: the Muon optimizer for efficient training, Single-Scale RMSNorm to control channel-wise amplification, and a learnable embedding projection to manage activation magnitudes. The results show that OSP-trained models achieve superior performance in quantization benchmarks while maintaining low training overhead, indicating that outliers can be effectively managed through improved training techniques."
                },
                "zh": {
                    "title": "åˆ›æ–°è®­ç»ƒæŠ€æœ¯ï¼Œæå‡æ¨¡å‹é‡åŒ–æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºOutlier-Safe Pre-Trainingï¼ˆOSPï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–æ€§èƒ½ã€‚OSPé€šè¿‡åˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œä¸»åŠ¨é˜²æ­¢æç«¯æ¿€æ´»å¼‚å¸¸å€¼çš„å½¢æˆï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šMuonä¼˜åŒ–å™¨ã€å•å°ºåº¦RMSNormå’Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¼‚å¸¸å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSPæ¨¡å‹åœ¨4ä½é‡åŒ–ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†è®­ç»ƒç­–ç•¥å¯¹å¼‚å¸¸å€¼çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16012",
            "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
            "url": "https://huggingface.co/papers/2506.16012",
            "abstract": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
            "score": 15,
            "issue_id": 4499,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "6a45067f4e822638",
            "authors": [
                "Boyu Li",
                "Siyuan He",
                "Hang Xu",
                "Haoqi Yuan",
                "Yu Zang",
                "Liwei Hu",
                "Junpeng Yue",
                "Zhenxiong Jiang",
                "Pengbo Hu",
                "BÃ¶rje F. Karlsson",
                "Yehui Tang",
                "Zongqing Lu"
            ],
            "affiliations": [
                "AgiBot",
                "Beijing Academy of Artificial Intelligence",
                "BeingBeyond",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16012.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#games",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "DualTHOR: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ",
                    "desc": "DualTHOR - ÑÑ‚Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¸Ñ… Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒÑ… Ñ€ÑƒĞº Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ²ÑƒÑ… Ñ€ÑƒĞº Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Dual-Arm Robots with DualTHOR Simulator",
                    "desc": "The paper introduces DualTHOR, a physics-based simulator designed for training dual-arm humanoid robots. It enhances Vision Language Models (VLMs) by integrating real-world assets and accounting for the complexities of low-level execution. This simulator addresses the limitations of existing platforms by incorporating a task suite for dual-arm collaboration and a contingency mechanism for potential failures. The findings indicate that current VLMs face challenges in dual-arm coordination and robustness, emphasizing the need for advanced simulation tools like DualTHOR to improve performance in real-world tasks."
                },
                "zh": {
                    "title": "DualTHORï¼šæå‡åŒè‡‚æœºå™¨äººä»»åŠ¡èƒ½åŠ›çš„ä»¿çœŸå¹³å°",
                    "desc": "DualTHORæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒåŒè‡‚ç±»äººæœºå™¨äººçš„ç‰©ç†ä»¿çœŸå¹³å°ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥å¹³å°ç»“åˆäº†çœŸå®ä¸–ç•Œçš„æœºå™¨äººèµ„äº§å’Œä»»åŠ¡å¥—ä»¶ï¼Œæ”¯æŒåŒè‡‚åä½œï¼Œå¹¶å¼•å…¥äº†é€†å‘è¿åŠ¨å­¦æ±‚è§£å™¨ã€‚é€šè¿‡æ¨¡æ‹Ÿä½çº§æ‰§è¡Œä¸­çš„æ½œåœ¨å¤±è´¥ï¼ŒDualTHORèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ç°å®åœºæ™¯ä¸­çš„å¤æ‚æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒè‡‚åè°ƒå’Œåº”å¯¹ç°å®ç¯å¢ƒä¸­çš„çªå‘æƒ…å†µæ—¶è¡¨ç°ä¸ä½³ï¼Œå› æ­¤ä½¿ç”¨DualTHORè¿›è¡Œè®­ç»ƒè‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18315",
            "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
            "url": "https://huggingface.co/papers/2506.18315",
            "abstract": "A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the \"cycle of self-deception\" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.",
            "score": 8,
            "issue_id": 4499,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "b53df58acbb9720b",
            "authors": [
                "Lehan He",
                "Zeren Chen",
                "Zhe Zhang",
                "Jing Shao",
                "Xiang Gao",
                "Lu Sheng"
            ],
            "affiliations": [
                "School of Software, Beihang University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18315.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Property-Based Testing Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Property-Generated Solver Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Property-Based Testing (PBT) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM - Generator Ğ¸ Tester - ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (TDD)."
                },
                "en": {
                    "title": "Enhancing Code Generation with Property-Based Testing and LLM Collaboration",
                    "desc": "This paper presents a new framework called Property-Generated Solver that enhances the correctness and generalization of code generated by Large Language Models (LLMs). It utilizes Property-Based Testing (PBT) to validate high-level program properties instead of relying solely on traditional test cases, which can be flawed or biased. The framework consists of two collaborative LLM agents: a Generator for creating and refining code, and a Tester that oversees the PBT process and provides feedback based on property violations. Experimental results show that this approach significantly improves the accuracy of code generation compared to conventional Test-Driven Development methods."
                },
                "zh": {
                    "title": "åŸºäºå±æ€§çš„æµ‹è¯•æå‡ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºå±æ€§çš„æµ‹è¯•ï¼ˆPBTï¼‰å’Œåä½œçš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ¥æé«˜ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„æµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰åœ¨å¤„ç†LLMç”Ÿæˆçš„ä»£ç æ—¶å¸¸å¸¸é¢ä¸´é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ç¨€ç¼ºçš„é—®é¢˜ï¼Œè€ŒPBTé€šè¿‡éªŒè¯é«˜å±‚ç¨‹åºå±æ€§æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªåä½œçš„LLMä»£ç†ï¼šä¸€ä¸ªç”¨äºä»£ç ç”Ÿæˆå’Œè¿­ä»£æ”¹è¿›ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç®¡ç†PBTç”Ÿå‘½å‘¨æœŸå¹¶æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†é€šè¿‡ç‡ï¼Œä¼˜äºä¼ ç»Ÿçš„TDDæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18088",
            "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
            "url": "https://huggingface.co/papers/2506.18088",
            "abstract": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.",
            "score": 7,
            "issue_id": 4499,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ½Ñ",
                "en": "June 22",
                "zh": "6æœˆ22æ—¥"
            },
            "hash": "92b73795da81e648",
            "authors": [
                "Tianxing Chen",
                "Zanxin Chen",
                "Baijun Chen",
                "Zijian Cai",
                "Yibin Liu",
                "Qiwei Liang",
                "Zixuan Li",
                "Xianliang Lin",
                "Yiheng Ge",
                "Zhenyu Gu",
                "Weiliang Deng",
                "Yubin Guo",
                "Tian Nian",
                "Xuanbing Xie",
                "Qiangyu Chen",
                "Kailun Su",
                "Tianling Xu",
                "Guodong Liu",
                "Mengkang Hu",
                "Huan-ang Gao",
                "Kaixuan Wang",
                "Zhixuan Liang",
                "Yusen Qin",
                "Xiaokang Yang",
                "Ping Luo",
                "Yao Mu"
            ],
            "affiliations": [
                "CSU",
                "D-Robotics",
                "FDU",
                "HKU MMLab",
                "HKU-SH ICRC",
                "Lumina EAI",
                "NEU",
                "NJU",
                "SJTU ScaleLab",
                "SUSTech",
                "SYSU",
                "SZU",
                "Shanghai AI Lab",
                "THU",
                "TeleAI",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18088.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#synthetic",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#data",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¸Ğ·Ğ½ĞµÑ† Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "RoboTwin 2.0 - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² RoboTwin-OD Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RoboTwin 2.0 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Bimanual Robot Manipulation with RoboTwin 2.0",
                    "desc": "RoboTwin 2.0 is a new simulation framework designed to improve how robots manipulate objects with both hands. It addresses the challenges of generating diverse and realistic synthetic data by using expert data synthesis and structured domain randomization. This framework creates a large-scale object library and employs advanced language models to automatically generate task execution code. The results show significant improvements in the robots' ability to perform tasks in real-world scenarios, demonstrating enhanced generalization and robustness."
                },
                "zh": {
                    "title": "RoboTwin 2.0ï¼šæå‡åŒæ‰‹æœºå™¨äººæ“ä½œçš„ä»¿çœŸæ¡†æ¶",
                    "desc": "RoboTwin 2.0 æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŒæ‰‹æœºå™¨äººæ“ä½œä»¿çœŸæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“å®¶æ•°æ®åˆæˆå’Œç»“æ„åŒ–é¢†åŸŸéšæœºåŒ–ç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„åˆæˆæ•°æ®ï¼Œä»è€Œæé«˜ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰åˆæˆæ•°æ®é›†åœ¨åŒæ‰‹æ“ä½œä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•å’Œå¤æ‚çš„ä»¿çœŸç¯å¢ƒã€‚RoboTwin 2.0 ç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œä»¿çœŸå¾ªç¯ä¼˜åŒ–ï¼Œè‡ªåŠ¨ç”Ÿæˆä»»åŠ¡æ‰§è¡Œä»£ç ï¼Œå¹¶é€šè¿‡äº”ä¸ªç»´åº¦çš„é¢†åŸŸéšæœºåŒ–å¢å¼ºæ•°æ®çš„å¤šæ ·æ€§å’Œç­–ç•¥çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»£ç ç”ŸæˆæˆåŠŸç‡å’Œå¯¹æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæ”¯æŒåŒæ‰‹æ“ä½œçš„å¯æ‰©å±•ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18674",
            "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
            "url": "https://huggingface.co/papers/2506.18674",
            "abstract": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.",
            "score": 5,
            "issue_id": 4497,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "8daec89854af9338",
            "authors": [
                "Raquel Ferrando",
                "Javier Conde",
                "Gonzalo MartÃ­nez",
                "Pedro Reviriego"
            ],
            "affiliations": [
                "ETSI de TelecomunicaciÃ³n Universidad PolitÃ©cnica de Madrid 28040 Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18674.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-10%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ­Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM)."
                },
                "en": {
                    "title": "Optimize Tokenizers, Save Energy!",
                    "desc": "This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency."
                },
                "zh": {
                    "title": "ä¼˜åŒ–åˆ†è¯å™¨ï¼ŒèŠ‚çœèƒ½æºä¸æˆæœ¬",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ºèŠå¤©æœºå™¨äººå¯¹è¯ä¼˜åŒ–åˆ†è¯å™¨çš„æ½œåœ¨å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè®¡ç®—å’Œèƒ½æºæˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œè€Œåˆ†è¯å™¨åœ¨æ¨¡å‹æ•ˆç‡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¯¹è¯ä¸­çš„æ ‡è®°æ•°é‡ï¼Œä»è€ŒèŠ‚çœ5%åˆ°10%çš„èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶å¯¹åŸå§‹è®­ç»ƒè¯­æ–™çš„æ ‡è®°åŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢æ•ˆæœã€‚é€šè¿‡é‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨èŠå¤©æœºå™¨äººé¢†åŸŸä¼˜åŒ–åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20544",
            "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
            "url": "https://huggingface.co/papers/2506.20544",
            "abstract": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.",
            "score": 4,
            "issue_id": 4497,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "8c4af7dcfe82a334",
            "authors": [
                "Ammar Khairi",
                "Daniel D'souza",
                "Ye Shen",
                "Julia Kreutzer",
                "Sara Hooker"
            ],
            "affiliations": [
                "Cohere",
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20544.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#inference",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Multilingual Performance in Large Language Models",
                    "desc": "This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs."
                },
                "zh": {
                    "title": "æå‡å¤šè¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°ç­–ç•¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨å¹¶æå‡ºäº†æ–°çš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œä»¥å¢å¼ºå¤šè¯­è¨€å’Œå¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥å¿…é¡»é€‚åº”ä¸åŒé¢†åŸŸå’Œè¯­è¨€ç¯å¢ƒã€‚é€šè¿‡è¯„ä¼°ç°æœ‰çš„é€‰æ‹©æ–¹æ³•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åœ¨è‹±è¯­ä¸­æœ‰æ•ˆçš„ç­–ç•¥å¾€å¾€æ— æ³•è·¨è¯­è¨€æ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºçš„åˆ›æ–°æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨m-ArenaHard-v2.0åŸºå‡†æµ‹è¯•ä¸­ï¼Œ8Bæ¨¡å‹çš„èƒœç‡å¹³å‡æé«˜äº†6.8ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20495",
            "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.20495",
            "abstract": "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.",
            "score": 4,
            "issue_id": 4500,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "22cd91ad69753ea2",
            "authors": [
                "Haoze Wu",
                "Yunzhi Yao",
                "Wenhao Yu",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Tencent AI, Seattle Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20495.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ReCode: ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ API Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ReCode Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ API Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ReCode Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ² Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ API. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° ÑÑ‚Ñ€Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ´Ğ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ API, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… CodeUpdateArena."
                },
                "en": {
                    "title": "Adapting Code Generation with ReCode: Reinforcement Learning for API Updates",
                    "desc": "ReCode is a rule-based reinforcement learning framework designed to improve large language models' (LLMs) ability to adapt to changes in external library APIs while maintaining their general code generation skills. The framework addresses the challenge that LLMs face due to outdated API knowledge, which hinders their performance in dynamic coding environments. By creating a dataset of around 2,000 entries for training and implementing a modified string similarity metric for evaluating code, ReCode effectively enhances the LLMs' adaptation capabilities. Experimental results show that ReCode significantly improves code generation performance in scenarios with frequent API updates, outperforming traditional supervised fine-tuning methods."
                },
                "zh": {
                    "title": "ReCodeï¼šæå‡ä»£ç ç”Ÿæˆä¸APIé€‚åº”èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ReCodeæ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹APIæ›´æ–°çš„é€‚åº”èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶ä»£ç ç”Ÿæˆçš„é€šç”¨æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«çº¦2000ä¸ªæ•°æ®æ¡ç›®çš„æ•°æ®é›†ï¼Œè®­ç»ƒLLMsè¿›è¡Œç‰ˆæœ¬è¿ç§»ï¼Œä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„APIå˜åŒ–ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¿®æ”¹è¿‡çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åº¦é‡ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ï¼Œä»¥è¯„ä¼°ä»£ç çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReCodeæ˜¾è‘—æå‡äº†LLMsåœ¨åŠ¨æ€APIåœºæ™¯ä¸‹çš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20452",
            "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
            "url": "https://huggingface.co/papers/2506.20452",
            "abstract": "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.",
            "score": 3,
            "issue_id": 4499,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "470af96a76c911d8",
            "authors": [
                "Tobias Vontobel",
                "Seyedmorteza Sadat",
                "Farnood Salehi",
                "Romann M. Weber"
            ],
            "affiliations": [
                "Disney Research Studios, Switzerland",
                "ETH Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20452.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "HiWave: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "HiWave - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ DDIM Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚Ğ¾Ğ². HiWave Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot."
                },
                "en": {
                    "title": "HiWave: Elevating Ultra-High-Resolution Image Synthesis with Pretrained Diffusion Models",
                    "desc": "HiWave is a novel approach for enhancing ultra-high-resolution image synthesis using pretrained diffusion models without the need for retraining. It employs a two-stage pipeline that first generates a base image and then applies DDIM inversion and a wavelet-based detail enhancement technique. This method improves visual fidelity by preserving global coherence and enriching fine details, effectively reducing common artifacts like object duplication. Evaluations show that HiWave outperforms existing methods, achieving superior perceptual quality in image synthesis."
                },
                "zh": {
                    "title": "HiWaveï¼šè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æ–°çªç ´",
                    "desc": "HiWaveæ˜¯ä¸€ç§å¢å¼ºè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æŠ€æœ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„æµç¨‹æ¥å®ç°ã€‚é¦–å…ˆï¼Œå®ƒä»é¢„è®­ç»ƒæ¨¡å‹ç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åè¿›è¡ŒåŸºäºæ³¢å½¢çš„å°å—DDIMåæ¼”æ­¥éª¤å’Œç»†èŠ‚å¢å¼ºã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§ï¼Œå‡å°‘äº†å¸¸è§çš„è§†è§‰ä¼ªå½±ã€‚é€šè¿‡å¯¹Stable Diffusion XLçš„å¹¿æ³›è¯„ä¼°ï¼ŒHiWaveåœ¨ç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡80%çš„æ¯”è¾ƒä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18403",
            "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
            "url": "https://huggingface.co/papers/2506.18403",
            "abstract": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.",
            "score": 2,
            "issue_id": 4494,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "30aa788d94afe45d",
            "authors": [
                "Muntasir Adnan",
                "Carlos C. N. Kuhn"
            ],
            "affiliations": [
                "Open Source Institute, University of Canberra, Bruce, Canberra, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18403.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ—Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ¸ (DDI), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ, ÑĞ»ĞµĞ´ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ. DDI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ²ĞµĞ¶ĞµĞ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¾ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Reviving AI Debugging with Strategic Interventions",
                    "desc": "The Debugging Decay Index (DDI) is a new method that measures how quickly AI debugging abilities decline over time. It shows that most AI models lose a significant portion of their debugging skills after just a few attempts, which is a major issue for systems that generate code. By using DDI, developers can identify the best moments to intervene and improve the debugging process, shifting from refining existing solutions to exploring new ones. This approach not only highlights a key limitation in current AI debugging methods but also offers a way to enhance the effectiveness of iterative code generation."
                },
                "zh": {
                    "title": "ä¼˜åŒ–AIè°ƒè¯•çš„å…³é”®ï¼šè°ƒè¯•è¡°å‡æŒ‡æ•°",
                    "desc": "è°ƒè¯•è¡°å‡æŒ‡æ•°ï¼ˆDDIï¼‰é‡åŒ–å¹¶ä¼˜åŒ–äº†è¿­ä»£AIè°ƒè¯•çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é¢„æµ‹å¹²é¢„ç‚¹æ¥æ¢å¤å’Œå¢å¼ºè°ƒè¯•èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAIè°ƒè¯•çš„æœ‰æ•ˆæ€§éµå¾ªå¯é¢„æµ‹çš„æŒ‡æ•°è¡°å‡æ¨¡å¼ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨ä»…ä»…2-3æ¬¡å°è¯•åå°±ä¼šå¤±å»60-80%çš„è°ƒè¯•èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„DDIæ¡†æ¶å¯ä»¥é‡åŒ–è°ƒè¯•å¤±æ•ˆçš„æ—¶æœºï¼Œå¹¶é¢„æµ‹å¹²é¢„ç‚¹ï¼Œä»è€Œåœ¨è°ƒè¯•è¿‡ç¨‹ä¸­å®ç°ä»åˆ©ç”¨åˆ°æ¢ç´¢çš„æˆ˜ç•¥è½¬å˜ã€‚DDIæ­ç¤ºäº†å½“å‰AIè°ƒè¯•çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶æä¾›äº†ä¼˜åŒ–è¿­ä»£ä»£ç ç”Ÿæˆç­–ç•¥çš„é¦–ä¸ªå®šé‡æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20512",
            "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
            "url": "https://huggingface.co/papers/2506.20512",
            "abstract": "Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  \t\t\t\t\tAI-generated summary \t\t\t\t Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).",
            "score": 1,
            "issue_id": 4500,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "67515964a17f77dc",
            "authors": [
                "Zengzhi Wang",
                "Fan Zhou",
                "Xuefeng Li",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University, SII, GAIR Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20512.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Llama Ğ¸ Qwen, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Stable-then-Decay, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ° Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ OctoThinker. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ñ€ÑƒĞ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğº RL ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing RL Performance with Strategic Mid-Training",
                    "desc": "This paper explores how mid-training strategies can improve reinforcement learning (RL) performance in language models, specifically focusing on the Qwen and Llama families. It finds that using high-quality mathematical datasets and well-structured chain-of-thought reasoning examples significantly enhances the models' reasoning capabilities. The authors introduce a two-stage mid-training approach called Stable-then-Decay, which optimizes learning rates to improve RL outcomes. The resulting model, OctoThinker, shows improved compatibility with RL tasks, bridging the performance gap with other models designed for RL."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ä¸­æœŸè®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ¨ç†å¯†é›†å‹ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°é«˜è´¨é‡çš„æ•°å­¦è¯­æ–™åº“å’Œæ ¼å¼è‰¯å¥½çš„é“¾å¼æ¨ç†ç¤ºä¾‹æ˜¾è‘—æå‡äº†æ¨¡å‹çš„RLæ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ç¨³å®š-è¡°å‡çš„ä¸¤é˜¶æ®µä¸­æœŸè®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¼€å‘äº†OctoThinkeræ¨¡å‹ç³»åˆ—ï¼Œå±•ç¤ºäº†ä¸å…¶ä»–RLå‹å¥½æ¨¡å‹çš„ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨ä¸ºåŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒç­–ç•¥æä¾›æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ æ—¶ä»£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19502",
            "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
            "url": "https://huggingface.co/papers/2506.19502",
            "abstract": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
            "score": 1,
            "issue_id": 4494,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "85d844ff6061cc93",
            "authors": [
                "Aleksandr Algazinov",
                "Matt Laing",
                "Paul Laban"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China",
                "Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19502.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#agents",
                    "#multimodal",
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "â™¿",
                "ru": {
                    "title": "MATE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "MATE - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ Ğ¸Ğ½Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸. MATE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² API Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ModCon-Task-Identifier - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering Accessibility Through Intelligent Data Conversion",
                    "desc": "MATE is a multimodal accessibility multi-agent system designed to convert data into formats that are understandable for users with disabilities. It addresses the limitations of existing multi-agent systems by providing customizable solutions that adapt to individual user needs. The system can perform tasks like converting images to audio descriptions, making digital content more accessible. Additionally, MATE integrates with institutional technologies and ensures user privacy by running locally, while its ModCon-Task-Identifier model excels in identifying specific modality conversion tasks."
                },
                "zh": {
                    "title": "MATEï¼šä¸ºæ¯ä¸ªäººæä¾›æ— éšœç¢çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "MATEæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ— éšœç¢å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·éœ€æ±‚å°†æ•°æ®è½¬æ¢ä¸ºå¯ç†è§£çš„æ ¼å¼ï¼Œä»¥æ”¯æŒå„ç§æ®‹ç–¾äººå£«ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ‰§è¡Œæ¨¡æ€è½¬æ¢ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ä¸æ•°å­—ç¯å¢ƒäº’åŠ¨ï¼Œä¾‹å¦‚å°†å›¾åƒè½¬æ¢ä¸ºéŸ³é¢‘æè¿°ï¼Œä»¥æ»¡è¶³è§†è§‰éšœç¢è€…çš„éœ€æ±‚ã€‚MATEå…·æœ‰çµæ´»æ€§ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œç¡¬ä»¶ï¼Œç¡®ä¿èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶ä¿æŠ¤æ•æ„Ÿä¿¡æ¯çš„éšç§å’Œå®‰å…¨ã€‚é€šè¿‡ä¸æœºæ„æŠ€æœ¯çš„æœ‰æ•ˆé›†æˆï¼ŒMATEèƒ½å¤Ÿæä¾›å®æ—¶çš„ç”¨æˆ·æ”¯æŒï¼Œæå‡æ— éšœç¢æœåŠ¡çš„è´¨é‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-25.html",
    "link_next": "2025-06-27.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 3,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}