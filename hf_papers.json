{
    "date": {
        "ru": "20 марта",
        "en": "March 20",
        "zh": "3月20日"
    },
    "time_utc": "2025-03-20 16:13",
    "weekday": 3,
    "issue_id": 2813,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.13288",
            "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
            "url": "https://huggingface.co/papers/2503.13288",
            "abstract": "Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.",
            "score": 37,
            "issue_id": 2805,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 марта",
                "en": "March 17",
                "zh": "3月17日"
            },
            "hash": "8a067ffdfadeb974",
            "authors": [
                "Fangzhi Xu",
                "Hang Yan",
                "Chang Ma",
                "Haiteng Zhao",
                "Jun Liu",
                "Qika Lin",
                "Zhiyong Wu"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13288.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#inference",
                    "#open_source",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация вывода языковых моделей с помощью предвидения и адаптивных вычислений",
                    "desc": "Статья представляет новую стратегию декодирования под названием phi-Decoding для оптимизации вывода языковых моделей. Метод использует выборку с предвидением, чтобы оценить оптимальные шаги генерации текста на основе симуляции будущих шагов. phi-Decoding аппроксимирует две распределения с помощью предвидения и кластеризации для точной оценки значимости шагов. Авторы также предлагают стратегии прунинга для адаптивного распределения вычислительных ресурсов, что повышает эффективность вывода."
                },
                "en": {
                    "title": "Optimizing Inference with phi-Decoding: Balancing Exploration and Exploitation",
                    "desc": "This paper introduces phi-Decoding, a new decoding strategy that optimizes inference-time computation in machine learning models. It addresses the limitations of previous search-based methods by balancing exploration and exploitation through foresight sampling, which simulates future steps for better decision-making. The approach uses clustering to approximate distributions, allowing for the selection of optimal steps during the decoding process. Extensive experiments demonstrate that phi-Decoding significantly improves both performance and efficiency across various benchmarks and large language models (LLMs)."
                },
                "zh": {
                    "title": "优化推理效率的phi-Decoding策略",
                    "desc": "本文提出了一种新的解码策略，称为phi-Decoding，旨在优化推理过程中的计算效率。通过前瞻采样，phi-Decoding能够利用模拟的未来步骤来获得全局最优的步骤估计，从而平衡探索与利用。该方法通过近似两个分布来提供精确的步骤价值估计，并通过联合分布进行采样以选择最佳步骤。实验结果表明，phi-Decoding在性能和效率上均优于现有的强基线，并且在不同的大型语言模型和计算预算下具有良好的可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15265",
            "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2503.15265",
            "abstract": "Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/",
            "score": 31,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "37236f5315cc8aef",
            "authors": [
                "Ruowen Zhao",
                "Junliang Ye",
                "Zhengyi Wang",
                "Guangce Liu",
                "Yiwen Chen",
                "Yikai Wang",
                "Jun Zhu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "ShengShu",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15265.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#alignment",
                    "#rlhf",
                    "#rl",
                    "#3d"
                ],
                "emoji": "🔷",
                "ru": {
                    "title": "DeepMesh: Революция в генерации 3D-сеток с учетом человеческих предпочтений",
                    "desc": "DeepMesh - это новый фреймворк для оптимизации генерации трехмерных сеток. Он использует эффективную стратегию предварительного обучения с новым алгоритмом токенизации и улучшенной обработкой данных. DeepMesh также внедряет обучение с подкреплением для создания 3D-сеток, соответствующих предпочтениям человека, с помощью Direct Preference Optimization. Система генерирует сетки с детальной структурой и точной топологией на основе облаков точек и изображений, превосходя современные методы по точности и качеству."
                },
                "en": {
                    "title": "DeepMesh: Elevating 3D Mesh Generation with Human-Centric Learning",
                    "desc": "This paper introduces DeepMesh, a new framework for generating 3D triangle meshes that enhances both quality and precision. It utilizes a unique pre-training strategy with an innovative tokenization method, improving how data is curated and processed. Additionally, it incorporates Reinforcement Learning to align mesh generation with human preferences through Direct Preference Optimization. By conditioning on point clouds and images, DeepMesh produces detailed and accurately structured meshes, surpassing existing methods in performance."
                },
                "zh": {
                    "title": "DeepMesh：优化3D网格生成的新方法",
                    "desc": "三角网格在3D应用中至关重要，能够高效地进行操作和渲染。传统的自回归方法通过预测离散的顶点标记生成结构化网格，但常常受到面数限制和网格不完整性的困扰。为了解决这些问题，我们提出了DeepMesh框架，通过两项关键创新来优化网格生成：一种高效的预训练策略和将强化学习引入3D网格生成。DeepMesh能够生成细节丰富、拓扑精确的网格，且在精度和质量上超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15485",
            "title": "TULIP: Towards Unified Language-Image Pretraining",
            "url": "https://huggingface.co/papers/2503.15485",
            "abstract": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io",
            "score": 24,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "d4b870742a020d5a",
            "authors": [
                "Zineng Tang",
                "Long Lian",
                "Seun Eisape",
                "XuDong Wang",
                "Roei Herzig",
                "Adam Yala",
                "Alane Suhr",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15485.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#architecture",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🌷",
                "ru": {
                    "title": "TULIP: Баланс между детальным зрением и языковым пониманием",
                    "desc": "Статья представляет TULIP - новую модель для задач компьютерного зрения и обработки естественного языка. TULIP улучшает понимание визуальных деталей изображений, сохраняя при этом способность к семантическому сопоставлению с текстом. Модель использует генеративное расширение данных, улучшенное контрастивное обучение и регуляризацию реконструкции для достижения баланса между визуальным и языковым пониманием. TULIP превосходит существующие модели на нескольких бенчмарках, устанавливая новый state-of-the-art в задачах zero-shot классификации и few-shot обучения."
                },
                "en": {
                    "title": "TULIP: Bridging Vision and Language for Enhanced Image Understanding",
                    "desc": "This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks."
                },
                "zh": {
                    "title": "TULIP：提升图像理解的新方法",
                    "desc": "尽管像CLIP和SigLIP这样的图像-文本对比模型取得了成功，但它们在需要高保真图像理解的视觉任务中表现不佳。本文提出了TULIP，这是一种开源的替代方案，旨在通过生成数据增强和对比学习来提高图像理解能力。TULIP能够学习细粒度的视觉特征，同时保持全局语义的一致性。我们的实验表明，TULIP在多个基准测试中超越了现有的最先进模型，显著提升了零-shot性能和少-shot分类的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15475",
            "title": "Cube: A Roblox View of 3D Intelligence",
            "url": "https://huggingface.co/papers/2503.15475",
            "abstract": "Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.",
            "score": 17,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "89037dc780448ff8",
            "authors": [
                "Foundation AI Team",
                "Kiran Bhat",
                "Nishchaie Khanna",
                "Karun Channa",
                "Tinghui Zhou",
                "Yiheng Zhu",
                "Xiaoxia Sun",
                "Charles Shang",
                "Anirudh Sudarshan",
                "Maurice Chu",
                "Daiqing Li",
                "Kangle Deng",
                "Jean-Philippe Fauconnier",
                "Tijmen Verhulsdonck",
                "Maneesh Agrawala",
                "Kayvon Fatahalian",
                "Alexander Weiss",
                "Christian Reiser",
                "Ravi Kiran Chirravuri",
                "Ravali Kandur",
                "Alejandro Pelaez",
                "Akash Garg",
                "Michael Palleschi",
                "Jessica Wang",
                "Skylar Litz",
                "Leon Liu",
                "Anying Li",
                "David Harmon",
                "Derek Liu",
                "Liangjun Feng",
                "Denis Goupil",
                "Lukas Kuczynski",
                "Jihyun Yoon",
                "Naveen Marri",
                "Peiye Zhuang",
                "Yinan Zhang",
                "Brian Yin",
                "Haomiao Jiang",
                "Marcel van Workum",
                "Thomas Lane",
                "Bryce Erickson",
                "Salil Pathare",
                "Kyle Price",
                "Anupam Singh",
                "David Baszucki"
            ],
            "affiliations": [
                "Foundation AI team, Roblox",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15475.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "3D-интеллект: фундаментальная модель для виртуальных миров",
                    "desc": "Статья описывает разработку фундаментальной модели для 3D-интеллекта в Roblox. Модель призвана помочь разработчикам в создании всех аспектов игрового опыта, включая генерацию 3D-объектов, анимацию персонажей и программные скрипты. Авторы представляют решение для токенизации 3D-форм и демонстрируют его применение в генерации форм из текста, текста из форм и сцен из текста. Также обсуждается интеграция с существующими языковыми моделями для анализа и рассуждений о сценах."
                },
                "en": {
                    "title": "Building the Future of 3D Intelligence in Roblox",
                    "desc": "This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation."
                },
                "zh": {
                    "title": "构建3D智能的基础模型",
                    "desc": "本论文探讨了如何为3D智能构建基础模型，该模型能够支持开发者在Roblox平台上生成3D对象、场景和动画角色。我们提出了三个关键设计要求，并介绍了构建3D形状标记器的初步步骤。我们的标记化方案可以应用于文本到形状生成、形状到文本生成和文本到场景生成等任务。最后，我们讨论了如何与现有的大型语言模型协作，以实现场景分析和推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15417",
            "title": "Temporal Regularization Makes Your Video Generator Stronger",
            "url": "https://huggingface.co/papers/2503.15417",
            "abstract": "Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.",
            "score": 17,
            "issue_id": 2801,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "8eb262eda880d162",
            "authors": [
                "Harold Haodong Chen",
                "Haojian Huang",
                "Xianfeng Wu",
                "Yexin Liu",
                "Yajing Bai",
                "Wen-Jie Shu",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "HKU",
                "HKUST",
                "UCF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15417.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "FluxFlow: Улучшение временного качества видео с помощью аугментации",
                    "desc": "В статье рассматривается проблема улучшения временного качества при генерации видео с помощью нейронных сетей. Авторы предлагают новый метод под названием FluxFlow, который применяет контролируемые временные возмущения на уровне данных. Эксперименты показывают, что FluxFlow значительно улучшает временную согласованность и разнообразие для различных архитектур генеративных моделей видео. Метод не требует изменений в архитектуре моделей и может быть легко интегрирован в существующие подходы."
                },
                "en": {
                    "title": "Enhancing Video Generation with Temporal Augmentation",
                    "desc": "This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity."
                },
                "zh": {
                    "title": "提升视频生成的时间质量",
                    "desc": "本研究探讨了视频生成中的时间质量问题，强调了在帧之间保持一致运动和真实动态的重要性。我们首次引入了时间增强技术，并提出了FluxFlow策略，以提高视频生成的时间质量。FluxFlow在数据层面进行操作，通过控制时间扰动来增强时间一致性和多样性，而无需修改模型架构。实验结果表明，FluxFlow在多个视频生成模型上显著改善了时间一致性和多样性，同时保持了空间保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14868",
            "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
            "url": "https://huggingface.co/papers/2503.14868",
            "abstract": "Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to 8.2times.",
            "score": 17,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "8555fb94242b412c",
            "authors": [
                "Hoigi Seo",
                "Wongi Jeong",
                "Kyungryeol Lee",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, INMC & IPAI Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14868.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Память под контролем: персонализация диффузионных моделей на краевых устройствах",
                    "desc": "Статья представляет новый метод для эффективной тонкой настройки диффузионных моделей с персонализацией на устройствах с ограниченной памятью. Авторы предлагают использовать квантизацию модели и оптимизацию нулевого порядка для токенов персонализации, что позволяет избежать хранения градиентов и активаций. Для улучшения оценки градиента предложен метод 'Subspace Gradient', проецирующий градиент на подпространство, построенное на основе истории токенов. Также введена техника 'Partial Uniform Timestep Sampling' для эффективного выбора временных шагов диффузии."
                },
                "en": {
                    "title": "Efficient Personalization of Diffusion Models with Subspace Gradient",
                    "desc": "This paper presents a method to improve the efficiency of training diffusion models for image synthesis, particularly for personalization on edge devices. It introduces a quantization technique that allows for fine-tuning without the need for dequantization, thus saving memory during gradient computation. The authors propose a novel approach called Subspace Gradient to reduce noise in gradient estimation by utilizing historical data from personalization tokens. Additionally, they explore the impact of text embeddings on image generation, leading to a new sampling method that optimizes diffusion timesteps, achieving significant memory savings while maintaining performance."
                },
                "zh": {
                    "title": "高效个性化扩散模型的内存优化",
                    "desc": "扩散模型在图像合成中表现出色，但训练和微调需要大量计算和内存资源。尽管先进的量化技术可以减少推理时的内存使用，但训练这些量化模型仍然需要大量内存。本文提出了一种通过文本反演对扩散模型进行量化的方法，并利用零阶优化在不去量化的情况下进行个性化微调，从而减少内存消耗。我们的方法在图像和文本对齐得分上与之前的方法相当，同时将训练内存需求降低了多达8.2倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14891",
            "title": "MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer",
            "url": "https://huggingface.co/papers/2503.14891",
            "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose MetaLadder, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the model's comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like \"learning from examples\" and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs' problem-solving accuracy, largely outperforming standard CoT-based methods (10.3\\% accuracy gain) and other methods. Our code and data has been released at https://github.com/LHL3341/MetaLadder.",
            "score": 14,
            "issue_id": 2811,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "0d8dbba5f4b7283d",
            "authors": [
                "Honglin Lin",
                "Zhuoshi Pan",
                "Yu Li",
                "Qizhi Pei",
                "Xin Gao",
                "Mengzhang Cai",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14891.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#math",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MetaLadder: Обучение ИИ решать задачи по аналогии с человеком",
                    "desc": "В статье представлен новый подход MetaLadder для улучшения способностей больших языковых моделей (LLM) в решении математических задач. Метод основан на имитации человеческого подхода к решению проблем путем вспоминания аналогичных задач и их решений. MetaLadder побуждает LLM вспоминать и анализировать мета-проблемы и их решения перед тем, как приступить к целевой задаче. Авторы также вводят механизм переформулирования задачи для улучшения понимания модели. Эксперименты показывают значительное повышение точности решения задач по сравнению с стандартными методами цепочки рассуждений."
                },
                "en": {
                    "title": "MetaLadder: Enhancing LLMs with Analogical Reasoning for Better Problem Solving",
                    "desc": "This paper introduces MetaLadder, a new framework designed to improve the mathematical reasoning abilities of Large Language Models (LLMs). It emphasizes the importance of recalling and reflecting on similar past problems, known as meta-problems, to enhance the model's problem-solving process. By incorporating a problem-restating mechanism, the framework helps the model better understand the target problem, leading to improved reasoning accuracy. Experimental results show that MetaLadder significantly outperforms traditional Chain-of-Thought methods, achieving a notable increase in accuracy on mathematical tasks."
                },
                "zh": {
                    "title": "借鉴类比问题，提升推理能力",
                    "desc": "大型语言模型（LLMs）在解决数学推理任务中展现了良好的能力，特别是利用链式思维（CoT）数据来指导答案生成。当前的方法通常直接为给定问题生成CoT和答案，这与人类的解题策略有所不同。人类通常通过回忆类似案例及其解决方案来推理当前任务。我们提出的MetaLadder框架，明确引导LLMs回忆和反思结构或语义上相似的元问题及其CoT解决方案，从而提高推理准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12532",
            "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
            "url": "https://huggingface.co/papers/2503.12532",
            "abstract": "Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.",
            "score": 12,
            "issue_id": 2800,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 марта",
                "en": "March 16",
                "zh": "3月16日"
            },
            "hash": "185728a70d3b80d0",
            "authors": [
                "Fanbin Lu",
                "Zhisheng Zhong",
                "Ziqin Wei",
                "Shu Liu",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12532.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#agents",
                    "#training",
                    "#cv"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "STEVE: эффективное обучение ИИ-агентов для работы с компьютерным интерфейсом",
                    "desc": "Статья представляет STEVE - новый метод обучения ИИ-агентов для автономного управления графическими интерфейсами. Авторы используют большой набор инструкций и субоптимальные траектории, которые затем верифицируются с помощью GPT-4. На основе бинарных оценок каждого шага применяется оптимизация Канемана-Тверски для улучшения агента. Эксперименты показывают, что данный подход превосходит обычное обучение с учителем и позволяет эффективно обучить модель размером 7 миллиардов параметров для работы в сложной среде рабочего стола."
                },
                "en": {
                    "title": "STEVE: Optimizing AI Agents for GUI Manipulation Efficiently",
                    "desc": "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."
                },
                "zh": {
                    "title": "智能代理训练的新突破：STEVE",
                    "desc": "本论文提出了一种名为STEVE的步骤验证管道，用于训练计算机使用代理。我们首先建立了一个大型指令集，并收集了一些次优代理的轨迹数据。通过使用GPT-4o验证每个步骤的正确性，并为每个步骤分配二元标签，最后采用卡尼曼和特沃斯优化方法来优化代理。实验结果表明，我们的代理在复杂的桌面环境中表现优于传统的监督微调方法，且训练效率高、成本低。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11557",
            "title": "VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity",
            "url": "https://huggingface.co/papers/2503.11557",
            "abstract": "Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).",
            "score": 12,
            "issue_id": 2812,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "468daf59bbaf8821",
            "authors": [
                "Jing Bi",
                "Junjia Guo",
                "Susan Liang",
                "Guangyu Sun",
                "Luchuan Song",
                "Yunlong Tang",
                "Jinxi He",
                "Jiarui Wu",
                "Ali Vosoughi",
                "Chen Chen",
                "Chenliang Xu"
            ],
            "affiliations": [
                "University of Central Florida",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11557.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VERIFY: Новый рубеж в оценке визуального мышления ИИ",
                    "desc": "Статья представляет новый бенчмарк VERIFY для оценки визуального мышления мультимодальных больших языковых моделей (MLLM). В отличие от существующих тестов, VERIFY фокусируется на способности моделей рассуждать на основе визуальной информации, минимизируя текстовый контекст. Бенчмарк включает аннотированные пути рассуждений и новые метрики для оценки качества визуального мышления. Тестирование ведущих MLLM с помощью VERIFY выявило значительные ограничения в их способностях к визуальному мышлению."
                },
                "en": {
                    "title": "VERIFY: Elevating Visual Reasoning in MLLMs",
                    "desc": "This paper introduces VERIFY, a new benchmark aimed at evaluating the visual reasoning abilities of Multimodal Large Language Models (MLLMs). Unlike existing benchmarks that focus on recognition tasks, VERIFY emphasizes reasoning from visual data with minimal textual support, reducing biases from language. Each task includes a human-annotated reasoning path, allowing for a deeper understanding of how models make decisions. The study also presents new metrics to assess visual reasoning fidelity, revealing significant limitations in current MLLMs and advocating for a more balanced approach to perception and reasoning."
                },
                "zh": {
                    "title": "视觉推理能力的全新评估",
                    "desc": "视觉推理是人类认知的重要部分，使人们能够理解和抽象地理解环境。尽管最近的多模态大型语言模型（MLLMs）在语言和视觉语言任务中表现出色，但现有基准主要测量识别能力，未能充分评估真正的视觉推理能力。为了解决这一问题，我们引入了VERIFY基准，专门设计用于严格评估最先进的MLLM的视觉推理能力。通过提供最小的文本上下文，VERIFY促使模型主要依赖视觉信息进行推理，从而减少对特定领域知识和语言偏见的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15354",
            "title": "Optimizing Decomposition for Optimal Claim Verification",
            "url": "https://huggingface.co/papers/2503.15354",
            "abstract": "Current research on the Decompose-Then-Verify paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.",
            "score": 10,
            "issue_id": 2811,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "1b88801be56f7c8f",
            "authors": [
                "Yining Lu",
                "Noah Ziems",
                "Hy Dang",
                "Meng Jiang"
            ],
            "affiliations": [
                "University of Notre Dame, South Bend, IN"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15354.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rlhf",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Динамическая декомпозиция для оптимальной верификации текста",
                    "desc": "Исследование посвящено улучшению метода Decompose-Then-Verify для оценки фактической точности длинных текстов. Авторы обнаружили, что существующие политики декомпозиции плохо согласуются с верификаторами по атомарности, что приводит к неоптимальным результатам проверки. Они предложили метод динамической декомпозиции, использующий обратную связь от верификатора для обучения политики декомпозиции утверждений. Эксперименты показали, что динамическая декомпозиция превосходит существующие подходы, улучшая точность верификации в среднем на 0.12."
                },
                "en": {
                    "title": "Optimizing Decomposition for Better Verification in Text Factuality",
                    "desc": "This paper addresses the challenges in the Decompose-Then-Verify approach for assessing the factuality of long-form text. It highlights that traditional decomposition methods do not effectively align with verification processes, particularly in terms of atomicity, which measures the density of information. The authors propose a bilevel optimization framework to find the best decomposition policy that enhances verification outcomes. They introduce a reinforcement learning method called dynamic decomposition, which adapts based on verifier feedback, resulting in improved verification confidence and accuracy across different scenarios."
                },
                "zh": {
                    "title": "动态分解：提升长文本验证的有效性",
                    "desc": "本研究探讨了在评估长文本事实性时，分解-验证范式的有效性。我们发现现有的分解策略与下游验证器之间存在不一致，导致验证结果不理想。为了解决这个问题，我们将寻找最佳分解策略视为一个双层优化问题，并提出了一种动态分解的强化学习框架。实验结果表明，动态分解在不同验证器和数据集上显著提高了验证信心和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15264",
            "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
            "url": "https://huggingface.co/papers/2503.15264",
            "abstract": "The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. The code, model, and dataset will be released.",
            "score": 8,
            "issue_id": 2805,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "4fb3a12e4b0a70e9",
            "authors": [
                "Hengrui Kang",
                "Siwei Wen",
                "Zichen Wen",
                "Junyan Ye",
                "Weijia Li",
                "Peilin Feng",
                "Baichuan Zhou",
                "Bin Wang",
                "Dahua Lin",
                "Linfeng Zhang",
                "Conghui He"
            ],
            "affiliations": [
                "Beihang University",
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15264.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#synthetic",
                    "#alignment",
                    "#cv",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "LEGION: Новый подход к обнаружению и анализу синтетических изображений",
                    "desc": "Статья представляет новый набор данных SynthScars, состоящий из 12,236 полностью синтетических изображений с экспертными аннотациями. Авторы также предлагают LEGION - мультимодальную модель на основе больших языковых моделей для анализа поддельных изображений. LEGION превосходит существующие методы по нескольким критериям, включая обнаружение артефактов, сегментацию и объяснение. Кроме того, модель может использоваться для улучшения качества генерируемых изображений."
                },
                "en": {
                    "title": "Enhancing Synthetic Image Detection with LEGION and SynthScars",
                    "desc": "This paper addresses the challenges in detecting synthetic images, which have become increasingly prevalent due to advancements in generative technology. It introduces SynthScars, a comprehensive dataset of 12,236 synthetic images with detailed annotations, including pixel-level segmentation and artifact categories. The authors propose LEGION, a multimodal large language model framework that enhances synthetic image detection by integrating artifact detection, segmentation, and explanation. Experimental results demonstrate that LEGION significantly outperforms existing methods, leading to improved image quality and alignment with human preferences."
                },
                "zh": {
                    "title": "提升合成图像检测的智能化与精确度",
                    "desc": "本论文介绍了SynthScars，这是一个包含12,236张完全合成图像的高质量多样化数据集，配有人工专家注释。该数据集涵盖了四种不同的图像内容类型和三类伪影，提供了像素级分割、详细文本解释和伪影类别标签的细粒度注释。我们还提出了LEGION，一个基于多模态大语言模型的图像伪造分析框架，能够整合伪影检测、分割和解释功能。实验结果表明，LEGION在多个基准测试中优于现有方法，生成的图像更符合人类偏好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14505",
            "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
            "url": "https://huggingface.co/papers/2503.14505",
            "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.",
            "score": 8,
            "issue_id": 2801,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "61ef56ac402491c0",
            "authors": [
                "Susung Hong",
                "Ira Kemelmacher-Shlizerman",
                "Brian Curless",
                "Steven M. Seitz"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.14505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "💃",
                "ru": {
                    "title": "Танцуй под музыку: ИИ-генерация видео в ритме мелодии",
                    "desc": "MusicInfuser - это новый подход к генерации высококачественных танцевальных видео, синхронизированных с заданной музыкальной дорожкой. Он адаптирует существующие модели диффузии видео для согласования с музыкальными входными данными, используя легковесное кросс-внимание между музыкой и видео и низкоранговый адаптер. В отличие от предыдущих работ, MusicInfuser не требует данных захвата движения и дообучается только на танцевальных видео. Для оценки качества генерации танцев по нескольким параметрам авторы представили систему оценки с использованием видео-LLM."
                },
                "en": {
                    "title": "Syncing Dance with Music: Introducing MusicInfuser",
                    "desc": "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."
                },
                "zh": {
                    "title": "音乐与舞蹈的完美融合",
                    "desc": "我们介绍了MusicInfuser，这是一种生成高质量舞蹈视频的方法，能够与指定的音乐轨道同步。我们通过引入轻量级的音乐-视频交叉注意力机制和低秩适配器，展示了如何调整现有的视频扩散模型以适应音乐输入。与之前需要运动捕捉数据的工作不同，我们的方法仅在舞蹈视频上进行微调。MusicInfuser在保持底层模型的灵活性和生成能力的同时，实现了高质量的音乐驱动视频生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12769",
            "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
            "url": "https://huggingface.co/papers/2503.12769",
            "abstract": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.",
            "score": 7,
            "issue_id": 2800,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 марта",
                "en": "March 17",
                "zh": "3月17日"
            },
            "hash": "0913c8e386f3aae5",
            "authors": [
                "Shenghao Fu",
                "Qize Yang",
                "Yuan-Ming Li",
                "Yi-Xing Peng",
                "Kun-Yu Lin",
                "Xihan Wei",
                "Jian-Fang Hu",
                "Xiaohua Xie",
                "Wei-Shi Zheng"
            ],
            "affiliations": [
                "Guangdong Province Key Laboratory of Information Security Technology, China",
                "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
                "Pazhou Laboratory (Huangpu), China",
                "Peng Cheng Laboratory, China",
                "School of Computer Science and Engineering, Sun Yat-sen University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12769.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#agents"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "ViSpeak: Революция в интерактивном понимании потокового видео",
                    "desc": "Статья представляет новую задачу под названием 'Визуальная инструктивная обратная связь' для потокового понимания видео. Авторы предлагают модель ViSpeak, способную распознавать визуальные инструкции и реагировать на них в режиме реального времени. Для обучения и оценки созданы наборы данных ViSpeak-Instruct и ViSpeak-Bench. Модель ViSpeak демонстрирует производительность на уровне GPT-4 в различных задачах потокового понимания видео."
                },
                "en": {
                    "title": "Enhancing User-Agent Interaction through Visual Instruction Feedback",
                    "desc": "This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions."
                },
                "zh": {
                    "title": "提升用户与代理互动的视觉指令反馈",
                    "desc": "近年来，大型多模态模型（LMM）在离线视频理解方面取得了显著进展。然而，流媒体视频理解由于其时间敏感性、全模态和交互特性，对现有模型提出了巨大挑战。本文提出了一种新任务，称为视觉指令反馈，模型需要理解视觉内容并从中提取指令，从而增强用户与代理之间的互动。我们定义了七个与视觉模态高度相关的子任务，并收集了ViSpeak-Instruct数据集用于训练，ViSpeak-Bench用于评估，同时提出了ViSpeak模型，展示了在流媒体视频理解基准上的先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11227",
            "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
            "url": "https://huggingface.co/papers/2503.11227",
            "abstract": "The construction of Generalized Knowledge Graph (GKG), including knowledge graph, event knowledge graph and commonsense knowledge graph, is fundamental for various natural language processing tasks. Current studies typically construct these types of graph separately, overlooking holistic insights and potential unification that could be beneficial in computing resources and usage perspectives. However, a key challenge in developing a unified framework for GKG is obstacles arising from task-specific differences. In this study, we propose a unified framework for constructing generalized knowledge graphs to address this challenge. First, we collect data from 15 sub-tasks in 29 datasets across the three types of graphs, categorizing them into in-sample, counter-task, and out-of-distribution (OOD) data. Then, we propose a three-stage curriculum learning fine-tuning framework, by iteratively injecting knowledge from the three types of graphs into the Large Language Models. Extensive experiments show that our proposed model improves the construction of all three graph types across in-domain, OOD and counter-task data.",
            "score": 7,
            "issue_id": 2805,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "2e48a3554a1036a6",
            "authors": [
                "Jian Zhang",
                "Bifan Wei",
                "Shihao Qi",
                "haiping Zhu",
                "Jun Liu",
                "Qika Lin"
            ],
            "affiliations": [
                "National University of Singapore",
                "School of Computer Science and Technology, Xian Jiaotong University, Xian, China",
                "School of Continuing Education, Xian Jiaotong University, Xian, China",
                "Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xian Jiaotong University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11227.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#transfer_learning",
                    "#data",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Единый фреймворк для построения обобщенных графов знаний с помощью LLM",
                    "desc": "Статья представляет унифицированный подход к построению обобщенных графов знаний (GKG), включающих графы знаний, графы событий и графы здравого смысла. Авторы предлагают трехэтапную структуру обучения с использованием курикулума для итеративного внедрения знаний в большие языковые модели (LLM). Эксперименты проводились на 15 подзадачах из 29 наборов данных, разделенных на внутривыборочные, контр-задачные и out-of-distribution данные. Результаты показывают улучшение в построении всех трех типов графов для различных видов данных."
                },
                "en": {
                    "title": "Unifying Knowledge Graphs for Enhanced NLP Performance",
                    "desc": "This paper presents a unified framework for constructing Generalized Knowledge Graphs (GKG) that integrates knowledge graphs, event knowledge graphs, and commonsense knowledge graphs. The authors identify the challenge of task-specific differences that hinder the unification of these graphs, which can lead to inefficiencies in resource usage. They propose a three-stage curriculum learning approach that fine-tunes Large Language Models by incorporating knowledge from all three graph types. Experimental results demonstrate that this framework enhances the performance of GKG construction across various data distributions, including in-sample, out-of-distribution, and counter-task scenarios."
                },
                "zh": {
                    "title": "统一构建广义知识图谱的框架",
                    "desc": "本文提出了一种统一的广义知识图谱（GKG）构建框架，旨在解决当前研究中各类知识图谱分开构建的问题。我们从29个数据集中收集了15个子任务的数据，并将其分类为样本内、对抗任务和分布外数据。通过三阶段的课程学习微调框架，我们将三种类型的知识逐步注入到大型语言模型中。实验结果表明，该模型在样本内、分布外和对抗任务数据上均提升了三种知识图谱的构建效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13360",
            "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
            "url": "https://huggingface.co/papers/2503.13360",
            "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.",
            "score": 5,
            "issue_id": 2804,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 марта",
                "en": "March 17",
                "zh": "3月17日"
            },
            "hash": "8877e2e1a13921d1",
            "authors": [
                "Hai-Long Sun",
                "Zhun Sun",
                "Houwen Peng",
                "Han-Jia Ye"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "School of Artificial Intelligence, Nanjing University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13360.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#math"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение мультимодальных рассуждений с помощью динамической обработки визуальной информации",
                    "desc": "Статья описывает проблему мультимодальных языковых моделей (MLLM), которые теряют фокус на визуальной информации при решении задач, требующих длинных цепочек рассуждений. Авторы предлагают метод Take-along Visual Conditioning (TVC), который перемещает обработку изображений на критические этапы рассуждения и сжимает избыточные визуальные токены. Этот подход помогает модели сохранять внимание к визуальным компонентам на протяжении всего процесса рассуждения. Применение TVC позволило достичь наилучших результатов на пяти эталонных тестах по математическим рассуждениям, превзойдя предыдущие показатели на 3.4%."
                },
                "en": {
                    "title": "Enhancing Visual Attention in Multimodal Reasoning with TVC",
                    "desc": "This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in maintaining attention to visual information during complex reasoning tasks. The authors found that MLLMs tend to rely more on text as reasoning progresses, leading to a decline in accuracy when visual inputs are removed. To address this issue, they propose a new method called Take-along Visual Conditioning (TVC), which strategically integrates visual inputs at critical stages of reasoning and reduces unnecessary visual data. Their approach significantly improves performance on mathematical reasoning tasks, achieving state-of-the-art results across multiple benchmarks."
                },
                "zh": {
                    "title": "提升多模态推理的视觉关注力",
                    "desc": "最近，大型语言模型（LLMs）在推理能力上取得了显著进展，尤其是从链式思维（CoT）提示演变到更先进的产品导向解决方案。在我们的模型重新实现过程中，我们发现多模态语言模型（MLLMs）在需要视觉输入的任务中（如几何问题）难以保持对视觉信息的关注，导致文本输出过于依赖。为了解决这个问题，我们提出了一种名为“随行视觉条件”（TVC）的策略，通过在关键推理阶段引入图像输入，并动态修剪冗余的视觉标记，帮助模型在整个推理过程中保持对视觉成分的关注。我们的研究在五个数学推理基准上实现了最新的最佳性能，证明了TVC在增强多模态推理系统中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12963",
            "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait",
            "url": "https://huggingface.co/papers/2503.12963",
            "abstract": "Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.",
            "score": 5,
            "issue_id": 2808,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 марта",
                "en": "March 17",
                "zh": "3月17日"
            },
            "hash": "2cf2c79f5c5a0177",
            "authors": [
                "Chaolong Yang",
                "Kai Yao",
                "Yuyao Yan",
                "Chenru Jiang",
                "Weiguang Zhao",
                "Jie Sun",
                "Guangliang Cheng",
                "Yifei Zhang",
                "Bin Dong",
                "Kaizhu Huang"
            ],
            "affiliations": [
                "Ant Group, Hangzhou, 310000, China",
                "Department of Computer Science, University of Liverpool, Liverpool, L69 7 ZX, UK",
                "Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China",
                "Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China",
                "Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China",
                "Ricoh Software Research Center, Beijing, 100027, China",
                "School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12963.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#audio",
                    "#diffusion",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "KDTalker: Революция в генерации говорящих портретов",
                    "desc": "KDTalker - это новый фреймворк для генерации анимированных портретов по аудио. Он сочетает неконтролируемые неявные 3D ключевые точки с пространственно-временной диффузионной моделью. KDTalker позволяет гибко моделировать разнообразные позы головы и мелкие детали лица. Благодаря специальному механизму пространственно-временного внимания, система обеспечивает точную синхронизацию губ и высокое качество анимации. Экспериментальные результаты показывают, что KDTalker достигает наилучших показателей по точности синхронизации губ, разнообразию поз головы и эффективности выполнения."
                },
                "en": {
                    "title": "KDTalker: Revolutionizing Talking Portraits with Audio and 3D Keypoints",
                    "desc": "This paper introduces KDTalker, a novel framework for generating talking portraits from audio inputs. It combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model to enhance facial detail and pose diversity. Unlike traditional methods, KDTalker effectively synchronizes lip movements with audio while maintaining character identity. The results show that KDTalker outperforms existing techniques in lip synchronization accuracy and computational efficiency."
                },
                "zh": {
                    "title": "KDTalker：音频驱动的高效说话肖像生成",
                    "desc": "本论文提出了一种名为KDTalker的框架，用于音频驱动的单图像说话肖像生成。该方法结合了无监督隐式3D关键点和时空扩散模型，能够灵活捕捉细腻的面部细节和多样的头部姿态。KDTalker通过自定义的时空注意机制，确保了准确的唇部同步，生成高质量且时间一致的动画。实验结果表明，KDTalker在唇部同步精度、头部姿态多样性和执行效率方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15055",
            "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
            "url": "https://huggingface.co/papers/2503.15055",
            "abstract": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework for generating high-quality synthetic training data in specialized domains. While Large Language Models (LLMs) have shown impressive general capabilities, their performance in specialized domains like cybersecurity remains limited by the scarcity of domain-specific training data. ELTEX addresses this challenge by systematically integrating explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge throughout the generation process. We demonstrate ELTEX's effectiveness in the context of blockchain-related cyberattack detection, where we fine-tune Gemma-2B using various combinations of real and ELTEX-generated data. Our results show that the ELTEX-enhanced model achieves performance competitive with GPT-4 across both standard classification metrics and uncertainty calibration, while requiring significantly fewer computational resources. We release a curated synthetic dataset of social media texts for cyberattack detection in blockchain. Our work demonstrates that domain-driven synthetic data generation can effectively bridge the performance gap between resource-efficient models and larger architectures in specialized domains.",
            "score": 1,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "785ffad4856eb286",
            "authors": [
                "Arina Razmyslovich",
                "Kseniia Murasheva",
                "Sofia Sedlova",
                "Julien Capitaine",
                "Eugene Dmitriev"
            ],
            "affiliations": [
                "Distributed Networks Institute (DNI)",
                "Technologies Mésozoïques"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15055.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "ELTEX: Синтетические данные для повышения эффективности малых ЯМ в специализированных областях",
                    "desc": "ELTEX - это фреймворк для генерации синтетических данных в специализированных областях, таких как кибербезопасность. Он объединяет извлечение индикаторов предметной области с динамическим промптингом для сохранения критических знаний. ELTEX был применен для обнаружения кибератак в блокчейне, где модель Gemma-2B, дообученная на сгенерированных данных, показала результаты, сравнимые с GPT-4. Этот подход демонстрирует, что синтетические данные могут эффективно улучшать производительность малых моделей в специализированных областях."
                },
                "en": {
                    "title": "Bridging the Data Gap in Cybersecurity with ELTEX",
                    "desc": "The paper introduces ELTEX, a framework designed to create high-quality synthetic training data specifically for specialized fields like cybersecurity. It tackles the issue of limited domain-specific data that affects the performance of Large Language Models (LLMs) in these areas. By combining domain indicator extraction with dynamic prompting, ELTEX ensures that essential domain knowledge is maintained during data generation. The framework is validated through its application in blockchain cyberattack detection, showing that it can enhance model performance while being more resource-efficient than larger models like GPT-4."
                },
                "zh": {
                    "title": "领域驱动的合成数据生成，提升模型性能！",
                    "desc": "ELTEX（高效LLM令牌提取）是一个针对特定领域生成高质量合成训练数据的框架。大型语言模型（LLM）在通用能力上表现出色，但在网络安全等专业领域的表现受到领域特定训练数据稀缺的限制。ELTEX通过系统地整合显式领域指示符提取和动态提示，确保在生成过程中保留关键的领域知识。我们的实验表明，ELTEX增强的模型在区块链相关的网络攻击检测中，性能与GPT-4相当，同时显著减少了计算资源的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14830",
            "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior",
            "url": "https://huggingface.co/papers/2503.14830",
            "abstract": "Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms SOTA methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/.",
            "score": 1,
            "issue_id": 2813,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "7263c31fc82817f1",
            "authors": [
                "Junfeng Ni",
                "Yu Liu",
                "Ruijie Lu",
                "Zirui Zhou",
                "Song-Chun Zhu",
                "Yixin Chen",
                "Siyuan Huang"
            ],
            "affiliations": [
                "Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14830.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DP-Recon: Реконструкция 3D-сцен с помощью диффузионных приоров",
                    "desc": "DP-Recon - это новый метод реконструкции 3D-сцен, использующий диффузионные приоры для оптимизации нейронного представления объектов. Он применяет Score Distillation Sampling (SDS) и подход с учетом видимости для улучшения восстановления геометрии и внешнего вида объектов при ограниченном количестве входных изображений. Метод значительно превосходит современные аналоги, особенно в слабо ограниченных и закрытых областях сцены. DP-Recon также позволяет редактировать геометрию и внешний вид объектов с помощью текстовых запросов, создавая детализированные 3D-модели для применения в визуальных эффектах."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Reconstruction with DP-Recon!",
                    "desc": "This paper presents DP-Recon, a method for reconstructing 3D scenes from sparse views while maintaining detailed shapes and textures of objects. The approach utilizes diffusion priors through Score Distillation Sampling (SDS) to fill in missing information in underconstrained areas, improving the reconstruction of occluded regions. To address potential conflicts between reconstruction accuracy and generative guidance, a visibility-guided mechanism is introduced to adjust loss weights dynamically. Experimental results show that DP-Recon outperforms state-of-the-art methods, achieving superior object reconstruction even with fewer input views, and enabling advanced text-based editing for visual effects."
                },
                "zh": {
                    "title": "DP-Recon：提升3D场景重建的创新方法",
                    "desc": "本文提出了一种名为DP-Recon的方法，用于从稀疏视图重建3D场景，旨在恢复所有物体的完整形状和细致纹理。该方法利用扩散先验和得分蒸馏采样（SDS）来优化每个物体的神经表示，从而为欠约束区域提供额外信息。为了避免重建与生成指导之间的冲突，本文还引入了一种可见性引导的方法，动态调整每个像素的SDS损失权重。通过在Replica和ScanNet++数据集上的广泛实验，DP-Recon在物体重建方面显著优于现有最先进的方法，尤其是在视图数量较少的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13517",
            "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning",
            "url": "https://huggingface.co/papers/2503.13517",
            "abstract": "Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie",
            "score": 1,
            "issue_id": 2810,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "56a7b2a9ca22936c",
            "authors": [
                "Hao Cui",
                "Zahra Shamsi",
                "Gowoon Cheon",
                "Xuejian Ma",
                "Shutong Li",
                "Maria Tikhanovskaya",
                "Peter Norgaard",
                "Nayantara Mudur",
                "Martyna Plomecka",
                "Paul Raccuglia",
                "Yasaman Bahri",
                "Victor V. Albert",
                "Pranesh Srinivasan",
                "Haining Pan",
                "Philippe Faist",
                "Brian Rohr",
                "Michael J. Statt",
                "Dan Morris",
                "Drew Purves",
                "Elise Kleeman",
                "Ruth Alcantara",
                "Matthew Abraham",
                "Muqthar Mohammad",
                "Ean Phing VanLee",
                "Chenfei Jiang",
                "Elizabeth Dorfman",
                "Eun-Ah Kim",
                "Michael P Brenner",
                "Viren Jain",
                "Sameera Ponda",
                "Subhashini Venugopalan"
            ],
            "affiliations": [
                "Cornell",
                "FU Berlin",
                "Google",
                "Harvard",
                "Modelyst",
                "NIST",
                "Rutgers",
                "UMD College Park",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13517.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#science",
                    "#dataset",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "CURIE: Новый рубеж для ИИ в науке",
                    "desc": "CURIE - это новый бенчмарк для оценки потенциала больших языковых моделей (LLM) в решении научных задач. Он включает 580 задач из 6 научных дисциплин, требующих экспертных знаний, понимания длинного контекста и многоступенчатых рассуждений. Тестирование различных LLM на CURIE показало, что даже лучшие модели достигают лишь 32% эффективности, оставляя большой простор для улучшений. Авторы надеются, что результаты CURIE помогут в дальнейшем развитии LLM для научных приложений."
                },
                "en": {
                    "title": "CURIE: Advancing LLMs for Scientific Problem-Solving",
                    "desc": "The paper presents CURIE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in scientific problem-solving. It includes ten complex tasks with 580 curated problems across various scientific fields, emphasizing the need for domain expertise and multi-step reasoning. The evaluation reveals that while some models like Gemini Flash 2.0 and Claude-3 perform well, others like GPT-4o struggle significantly, particularly in protein sequencing tasks. The findings suggest that there is substantial potential for improving LLMs to better assist scientists in their workflows."
                },
                "zh": {
                    "title": "CURIE：推动科学问题解决的语言模型基准",
                    "desc": "本文介绍了CURIE，一个用于评估大型语言模型（LLMs）在科学问题解决中的能力的基准。CURIE包含十个具有挑战性的任务，共580个问题和解决方案，涵盖材料科学、凝聚态物理、量子计算、地理空间分析、生物多样性和蛋白质等六个学科。我们评估了多种封闭和开放的LLMs，发现虽然Gemini Flash 2.0和Claude-3在各个领域表现出色，但GPT-4o和command-R+在蛋白质测序任务上表现不佳。CURIE的结果为未来LLMs在科学领域的发展提供了重要的指导。"
                }
            }
        }
    ],
    "link_prev": "2025-03-19.html",
    "link_next": "2025-03-21.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3月19日"
    },
    "short_date_next": {
        "ru": "21.03",
        "en": "03/21",
        "zh": "3月21日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 7,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 3,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 9,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "三角网格在3D应用中扮演重要角色，用于高效操作和渲染。自回归方法通过预测离散顶点标记生成结构化网格，但往往受到有限面数和网格不完整的限制。我们提出了DeepMesh框架，通过两项关键创新优化网格生成：(1) 高效的预训练策略，包含新的标记算法，以及数据整理和处理的改进；(2) 引入强化学习（RL）到3D网格生成，通过直接偏好优化（DPO）实现人类偏好对齐。我们设计了一个结合人类评估和3D度量的评分标准，收集DPO的偏好对，确保视觉吸引力和几何精度。基于点云和图像，DeepMesh生成具有复杂细节和精确拓扑的网格，在精度和质量上超越现有方法。项目页面：https://zhaorw02.github.io/DeepMesh/",
        "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nSān jiǎo wǎng gé zài 3D yìng yòng zhōng bàn yǎn zhòng yào jiǎo sè, yòng yú gāo xiào cāo zuò hé xuàn rán. Zì huí guī fāng fǎ tōng guò yù cè lì sàn dǐng biāo jì shēng chéng jié gòu huà wǎng gé, dàn wǎng wǎng shòu dào yǒu xiàn miàn shù hé wǎng gé bù wán zhěng de xiàn zhì. Wǒ men tí chū le DeepMesh kuàng jià, tōng guò liǎng xiàng guǎn jiàn chuàng xīn yǒu huà wǎng gé shēng chéng: (1) gāo xiào de yù xùn lüè, bāo hán xīn de biāo jì suàn fǎ, yǐ jiā dài shù zhěng lǐ hé chǔ lǐ de gǎi jìn; (2) yǐn rù qiáng huà xué xí (RL) dào 3D wǎng gé shēng chéng, tōng guò zhí jiē piàn hào yǒu huà (DPO) shí xiàn rén lèi piàn hào duì qǐ. Wǒ men shè jì le yī gè jié hé rén lèi píng jià hé 3D dù liàng de píng fēn biāo zhǔn, shōu jí DPO de piàn hào duì, què shì shí jué xī yǐn lì hé jǐ hé jīng dù. Jī yú diǎn yún hé tú xiàng, DeepMesh shēng chéng jù yǒu fù zá xì xiàng hé jīng xiào tuó pǔ de wǎng gé, zài jīng dù hé zhì lì shàng chāo yuè xiàn yǒu fāng fǎ. Xiàng mù yè miàn: https://zhaorw02.github.io/DeepMesh/\n\nPlease note that the URL at the end is not transcribed into pinyin as it is not Chinese text.",
        "vocab": "[\n    {\"word\": \"三角网格\", \"pinyin\": \"sānjiǎo wǎnggé\", \"trans\": \"triangular mesh\"},\n    {\"word\": \"扮演\", \"pinyin\": \"bànyǎn\", \"trans\": \"play a role\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāoxiào\", \"trans\": \"efficient\"},\n    {\"word\": \"渲染\", \"pinyin\": \"xuànrán\", \"trans\": \"rendering\"},\n    {\"word\": \"自回归\", \"pinyin\": \"zì huíguī\", \"trans\": \"autoregressive\"},\n    {\"word\": \"预测\", \"pinyin\": \"yùcè\", \"trans\": \"predict\"},\n    {\"word\": \"离散\", \"pinyin\": \"lísàn\", \"trans\": \"discrete\"},\n    {\"word\": \"顶点\", \"pinyin\": \"dǐngdiǎn\", \"trans\": \"vertex\"},\n    {\"word\": \"标记\", \"pinyin\": \"biāojì\", \"trans\": \"label\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"结构化\", \"pinyin\": \"jiégòuhuà\", \"trans\": \"structured\"},\n    {\"word\": \"往往\", \"pinyin\": \"wǎngwǎng\", \"trans\": \"often\"},\n    {\"word\": \"受到\", \"pinyin\": \"shòudào\", \"trans\": \"be subject to\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒuxiàn\", \"trans\": \"limited\"},\n    {\"word\": \"面数\", \"pinyin\": \"miànshù\", \"trans\": \"number of faces\"},\n    {\"word\": \"不完整\", \"pinyin\": \"bù wánzhěng\", \"trans\": \"incomplete\"},\n    {\"word\": \"提出\", \"pinyin\": \"tíchū\", \"trans\": \"propose\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōuhuà\", \"trans\": \"optimize\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovation\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"策略\", \"pinyin\": \"cèlüè\", \"trans\": \"strategy\"},\n    {\"word\": \"包含\", \"pinyin\": \"bāohán\", \"trans\": \"include\"},\n    {\"word\": \"算法\", \"pinyin\": \"suànfǎ\", \"trans\": \"algorithm\"},\n    {\"word\": \"数据\", \"pinyin\": \"shùjù\", \"trans\": \"data\"},\n    {\"word\": \"整理\", \"pinyin\": \"zhěnglǐ\", \"trans\": \"organize\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔlǐ\", \"trans\": \"process\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎijìn\", \"trans\": \"improve\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"偏好\", \"pinyin\": \"piānhào\", \"trans\": \"preference\"},\n    {\"word\": \"对齐\", \"pinyin\": \"duìqí\", \"trans\": \"align\"},\n    {\"word\": \"设计\", \"pinyin\": \"shèjì\", \"trans\": \"design\"},\n    {\"word\": \"结合\", \"pinyin\": \"jiéhé\", \"trans\": \"combine\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"度量\", \"pinyin\": \"dùliàng\", \"trans\": \"measure\"},\n    {\"word\": \"评分\", \"pinyin\": \"píngfēn\", \"trans\": \"score\"},\n    {\"word\": \"标准\", \"pinyin\": \"biāozhǔn\", \"trans\": \"standard\"},\n    {\"word\": \"收集\", \"pinyin\": \"shōují\", \"trans\": \"collect\"},\n    {\"word\": \"确保\", \"pinyin\": \"quèbǎo\", \"trans\": \"ensure\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shìjué\", \"trans\": \"visual\"},\n    {\"word\": \"吸引力\", \"pinyin\": \"xīyǐnlì\", \"trans\": \"attractiveness\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐhé\", \"trans\": \"geometric\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"precision\"},\n    {\"word\": \"基于\", \"pinyin\": \"jīyú\", \"trans\": \"based on\"},\n    {\"word\": \"点云\", \"pinyin\": \"diǎn yún\", \"trans\": \"point cloud\"},\n    {\"word\": \"图像\", \"pinyin\": \"túxiàng\", \"trans\": \"image\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fùzá\", \"trans\": \"complex\"},\n    {\"word\": \"细节\", \"pinyin\": \"xìjié\", \"trans\": \"detail\"},\n    {\"word\": \"精确\", \"pinyin\": \"jīngquè\", \"trans\": \"precise\"},\n    {\"word\": \"拓扑\", \"pinyin\": \"tuòpǔ\", \"trans\": \"topology\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"项目\", \"pinyin\": \"xiàngmù\", \"trans\": \"project\"},\n    {\"word\": \"页面\", \"pinyin\": \"yèmiàn\", \"trans\": \"page\"}\n]",
        "trans": "Triangular meshes play a crucial role in 3D applications, enabling efficient operations and rendering. Autoregressive methods generate structured meshes by predicting discrete vertex labels but are often constrained by limited face counts and incomplete meshes. We introduce the DeepMesh framework, which optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy that includes a new labeling algorithm, as well as improvements in data curation and processing; (2) the introduction of reinforcement learning (RL) into 3D mesh generation, achieving human preference alignment through direct preference optimization (DPO). We designed a scoring standard that combines human evaluation and 3D metrics to collect DPO preference pairs, ensuring visual appeal and geometric accuracy. Based on point clouds and images, DeepMesh generates meshes with complex details and precise topology, surpassing existing methods in both accuracy and quality. Project page: https://zhaorw02.github.io/DeepMesh/",
        "update_ts": "2025-03-20 09:11"
    }
}