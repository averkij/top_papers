{
    "date": {
        "ru": "24 Ğ¸ÑĞ½Ñ",
        "en": "June 24",
        "zh": "6æœˆ24æ—¥"
    },
    "time_utc": "2025-06-24 06:18",
    "weekday": 1,
    "issue_id": 4451,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18871",
            "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
            "url": "https://huggingface.co/papers/2506.18871",
            "abstract": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
            "score": 26,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "18382718ba53ccf7",
            "authors": [
                "Chenyuan Wu",
                "Pengfei Zheng",
                "Ruiran Yan",
                "Shitao Xiao",
                "Xin Luo",
                "Yueze Wang",
                "Wanli Li",
                "Xiyan Jiang",
                "Yexin Liu",
                "Junjie Zhou",
                "Ze Liu",
                "Ziyi Xia",
                "Chaofan Li",
                "Haoge Deng",
                "Jiahao Wang",
                "Kun Luo",
                "Bo Zhang",
                "Defu Lian",
                "Xinlong Wang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Zheng Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18871.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OmniGen2: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "OmniGen2 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², OmniGen2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OmniContext Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ."
                },
                "en": {
                    "title": "OmniGen2: Unifying Text and Image Generation with Dual Pathways",
                    "desc": "OmniGen2 is a generative model that enhances the creation of text and images through dual decoding pathways, allowing for specialized processing of each modality. It maintains the original text generation capabilities while introducing a new image tokenizer and reflection mechanism for improved image tasks. The model is trained using comprehensive data pipelines that support various generation tasks, including image editing and in-context generation. Despite its smaller size, OmniGen2 achieves competitive performance on benchmarks, particularly in subject-driven tasks, and aims to advance research in multimodal generation."
                },
                "zh": {
                    "title": "OmniGen2ï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ",
                    "desc": "OmniGen2æ˜¯ä¸€ç§å¤šåŠŸèƒ½çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆä»»åŠ¡æä¾›ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚ä¸OmniGen v1ä¸åŒï¼ŒOmniGen2é‡‡ç”¨äº†ä¸¤ä¸ªç‹¬ç«‹çš„è§£ç è·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿ç”¨äº†ä¸å…±äº«çš„å‚æ•°å’Œè§£è€¦çš„å›¾åƒæ ‡è®°å™¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—OmniGen2èƒ½å¤Ÿåœ¨ä¸é‡æ–°é€‚é…VAEè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œä¿ç•™åŸæœ‰çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å°½ç®¡å‚æ•°é‡ç›¸å¯¹è¾ƒå°ï¼ŒOmniGen2åœ¨å¤šä¸ªä»»åŠ¡åŸºå‡†ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18841",
            "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.18841",
            "abstract": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B",
            "score": 25,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "8589c05aae4b8258",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqiang Hu",
                "Roy Ka-Wei Lee",
                "Juanzi Li"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18841.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#rl",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: RL Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LongWriter-Zero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WritingBench Ğ¸ Arena-Write."
                },
                "en": {
                    "title": "Reinforcement Learning for Ultra-Long Text Generation Without Synthetic Data",
                    "desc": "This paper presents a novel approach to generating ultra-long, high-quality text using a large language model (LLM) without relying on synthetic data or supervised fine-tuning. The authors introduce an incentivization-based reinforcement learning (RL) method that allows the model to learn from scratch, enhancing its ability to produce coherent and structured long-form content. By employing specialized reward models, the LLM is guided to improve its writing quality, length control, and formatting during the generation process. Experimental results demonstrate that the proposed LongWriter-Zero model outperforms traditional methods, achieving state-of-the-art performance on long-form writing benchmarks."
                },
                "zh": {
                    "title": "æ¿€åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œè¶…é•¿æ–‡æœ¬ç”Ÿæˆæ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè¶…é•¿ä¸”é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œè€Œæ— éœ€åˆæˆæ•°æ®æˆ–ç›‘ç£å¾®è°ƒã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™éœ€è¦æ„å»ºåˆæˆçš„é•¿æ–‡æœ¬è¾“å‡ºï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»é›¶å¼€å§‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¨¡å‹ï¼Œä¿ƒè¿›è¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„å‡ºç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LongWriter-Zeroæ¨¡å‹åœ¨é•¿æ–‡æœ¬å†™ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18851",
            "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
            "url": "https://huggingface.co/papers/2506.18851",
            "abstract": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.",
            "score": 19,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "525a4c676b83f9a6",
            "authors": [
                "Zhuowei Chen",
                "Bingchuan Li",
                "Tianxiang Ma",
                "Lijie Liu",
                "Mingcong Liu",
                "Yi Zhang",
                "Gen Li",
                "Xinghui Li",
                "Siyu Zhou",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18851.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Phantom-Data: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñƒ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Phantom-Data Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñƒ. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Phantom-Data Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Phantom-Data: Enhancing Video Generation with Identity Consistency",
                    "desc": "The paper introduces Phantom-Data, a new dataset designed to enhance subject-to-video generation in machine learning. This dataset addresses the 'copy-paste problem' by providing identity-consistent pairs that are not tied to specific backgrounds or contexts. It is created through a three-stage process that includes subject detection, cross-context retrieval, and identity verification. Experiments demonstrate that using Phantom-Data leads to better alignment with prompts and improved visual quality while maintaining consistent subject identity."
                },
                "zh": {
                    "title": "Phantom-Dataï¼šæå‡è§†é¢‘ç”Ÿæˆçš„èº«ä»½ä¸€è‡´æ€§ä¸è§†è§‰è´¨é‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhantom-Dataçš„è·¨å¯¹æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæ–‡æœ¬ç”Ÿæˆè§†é¢‘çš„æ•ˆæœã€‚è¯¥æ•°æ®é›†é€šè¿‡å¢å¼ºæç¤ºå¯¹é½å’Œè§†è§‰è´¨é‡ï¼ŒåŒæ—¶ä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨éµå¾ªæ–‡æœ¬æŒ‡ä»¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚Phantom-DataåŒ…å«çº¦ä¸€ç™¾ä¸‡ä¸ªèº«ä»½ä¸€è‡´çš„é…å¯¹ï¼Œæ¶µç›–å¤šç§ç±»åˆ«ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µæµç¨‹æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Phantom-Dataè¿›è¡Œè®­ç»ƒæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18882",
            "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
            "url": "https://huggingface.co/papers/2506.18882",
            "abstract": "",
            "score": 12,
            "issue_id": 4450,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "39545287159810c0",
            "authors": [
                "Hong Li",
                "Houyuan Chen",
                "Chongjie Ye",
                "Zhaoxi Chen",
                "Bohan Li",
                "Shaocong Xu",
                "Xianda Guo",
                "Xuhui Liu",
                "Yikai Wang",
                "Baochang Zhang",
                "Satoshi Ikehata",
                "Boxin Shi",
                "Anyi Rao",
                "Hao Zhao"
            ],
            "affiliations": [
                "AIR, THU",
                "BAAI",
                "BNU",
                "BUAA",
                "FNii, CUHKSZ",
                "HKUST",
                "NII",
                "NJU",
                "PKU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18882.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18898",
            "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
            "url": "https://huggingface.co/papers/2506.18898",
            "abstract": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
            "score": 11,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "988f76bd08498ba9",
            "authors": [
                "Jiaming Han",
                "Hao Chen",
                "Yang Zhao",
                "Hanyu Wang",
                "Qi Zhao",
                "Ziyan Yang",
                "Hao He",
                "Xiangyu Yue",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18898.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Text-Aligned Tokenizer (TA-Tok), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ½Ğ¸Ğ³Ñƒ, ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Tar ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Vision and Text with TA-Tok for Enhanced Multimodal Learning",
                    "desc": "This paper introduces a multimodal framework that combines visual and textual data into a single representation using a Text-Aligned Tokenizer (TA-Tok). The TA-Tok transforms images into discrete tokens aligned with a large language model's vocabulary, allowing for seamless interaction between text and images. The framework employs a generative de-tokenizer that includes both autoregressive and diffusion-based models to generate high-quality visual outputs efficiently. Experimental results indicate that this approach not only enhances visual understanding and generation but also outperforms existing multimodal models in terms of training speed and efficiency."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰ä¸æ–‡æœ¬çš„å¤šæ¨¡æ€æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†è§‰ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åˆ°ä¸€ä¸ªå…±äº«çš„ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºä¸­ã€‚æ ¸å¿ƒæ˜¯æ–‡æœ¬å¯¹é½çš„æ ‡è®°å™¨ï¼ˆTA-Tokï¼‰ï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯æ±‡å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ã€‚é€šè¿‡æ‰©å±•è¯æ±‡ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹Tarå®ç°äº†è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œé¿å…äº†ç‰¹å®šæ¨¡æ€è®¾è®¡çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†è§„æ¨¡è‡ªé€‚åº”çš„ç¼–ç å’Œè§£ç æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œè§†è§‰ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆæ€§å»æ ‡è®°å™¨ç”Ÿæˆé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18896",
            "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2506.18896",
            "abstract": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux",
            "score": 8,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "6a30d79f40f7d98d",
            "authors": [
                "Jiaru Zou",
                "Ling Yang",
                "Jingwen Gu",
                "Jiahao Qiu",
                "Ke Shen",
                "Jingrui He",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Cornell University",
                "Princeton University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18896.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ReasonFlux-PRM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ†ĞµĞ»Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ReasonFlux-PRM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ReasonFlux-PRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Reasoning Evaluation with ReasonFlux-PRM",
                    "desc": "ReasonFlux-PRM is a new model that improves how we evaluate reasoning processes in large language models by focusing on both individual steps and overall trajectories. It addresses the limitations of previous Process Reward Models that mainly assessed final outputs, which often missed the nuances of intermediate reasoning. By using both step-level and trajectory-level supervision, it provides more accurate rewards that align with structured reasoning data. The model has shown significant performance improvements in tasks like model distillation, reinforcement learning, and test-time scaling, outperforming existing models and human-curated benchmarks."
                },
                "zh": {
                    "title": "æ¨ç†è½¨è¿¹çš„æ™ºèƒ½è¯„ä¼°",
                    "desc": "ReasonFlux-PRMæ˜¯ä¸€ç§æ–°é¢–çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨ç†è½¨è¿¹ï¼Œç»“åˆäº†é€æ­¥å’Œè½¨è¿¹çº§çš„ç›‘ç£ã€‚è¿™ç§æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡å‹è’¸é¦ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶æ‰©å±•ä¸­æå‡æ€§èƒ½ã€‚é€šè¿‡å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†è‡´å¥–åŠ±åˆ†é…ï¼ŒReasonFlux-PRMèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18631",
            "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
            "url": "https://huggingface.co/papers/2506.18631",
            "abstract": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",
            "score": 4,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "cc6b6162c9368cf4",
            "authors": [
                "Chenxing Wei",
                "Jiarui Yu",
                "Ying Tiffany He",
                "Hande Dong",
                "Yao Shu",
                "Fei Yu"
            ],
            "affiliations": [
                "College of Computer Science and Software Engineering, Shenzhen University, China",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China",
                "Hong Kong University of Science and Technology (Guangzhou), China",
                "Tencent, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18631.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "Ğ¨ÑƒĞ¼ Ğ²Ğ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾: ReDit ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ReDit - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ReDit Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ReDit Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ·Ğ° 10% ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ GRPO."
                },
                "en": {
                    "title": "ReDit: Smoother Rewards for Faster Learning",
                    "desc": "ReDit is a novel method designed to improve optimization in systems that use discrete rewards by adding random noise to the reward signal. This noise helps to create smoother gradient updates, which leads to faster convergence during training. By introducing stochasticity, ReDit encourages exploration of new policies, helping models avoid getting stuck in local optima. Experimental results show that ReDit not only reduces training time significantly but also enhances performance compared to traditional methods."
                },
                "zh": {
                    "title": "ReDitï¼šæå‡ç¦»æ•£å¥–åŠ±ç³»ç»Ÿçš„ä¼˜åŒ–æ•ˆç‡",
                    "desc": "ReDitæ˜¯ä¸€ç§å¥–åŠ±æŠ–åŠ¨æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¦»æ•£å¥–åŠ±ç³»ç»Ÿä¸­çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å™ªå£°ï¼ŒReDitä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ å¹³æ»‘ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ¯”æ ‡å‡†æ–¹æ³•æ›´å¿«ã€‚å®éªŒè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸å’Œä¸ç¨³å®šçš„ä¼˜åŒ–ï¼Œè€ŒReDité€šè¿‡æ·»åŠ éšæœºå™ªå£°æ¥æ”¹å–„è¿™ä¸€ç‚¹ã€‚æœ€ç»ˆï¼ŒReDitåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæ­¥éª¤ä»…ä¸ºä¼ ç»Ÿæ–¹æ³•çš„10%ï¼ŒåŒæ—¶åœ¨ç›¸ä¼¼è®­ç»ƒæ—¶é—´å†…æ€§èƒ½æå‡äº†4%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18903",
            "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
            "url": "https://huggingface.co/papers/2506.18903",
            "abstract": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control.",
            "score": 3,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "fe5d31c2b125d778",
            "authors": [
                "Runjia Li",
                "Philip Torr",
                "Andrea Vedaldi",
                "Tomas Jakab"
            ],
            "affiliations": [
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18903.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Surfel-Indexed View Memory (VMem) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. VMem Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ ÑÑ†ĞµĞ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (ÑÑƒÑ€Ñ„ĞµĞ»ĞµĞ¹). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Generation with Efficient Memory Retrieval",
                    "desc": "This paper introduces a new memory mechanism called Surfel-Indexed View Memory (VMem) that improves video generation by efficiently recalling relevant past views. Unlike traditional methods that either accumulate errors or have limited context, VMem uses geometric indexing based on 3D surface elements to enhance long-term scene coherence. By focusing on the most pertinent past views, it reduces computational costs while generating consistent and coherent video outputs. The approach is evaluated against challenging benchmarks, showing better performance in maintaining scene integrity and camera control compared to existing techniques."
                },
                "zh": {
                    "title": "é«˜æ•ˆè®°å¿†ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®°å¿†æœºåˆ¶ï¼Œç§°ä¸ºè¡¨é¢ç´¢å¼•è§†å›¾è®°å¿†ï¼ˆSurfel-Indexed View Memoryï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æœºåˆ¶é€šè¿‡å‡ ä½•ç´¢å¼•è¿‡å»çš„è§†å›¾ï¼ŒåŸºäºè§‚å¯Ÿåˆ°çš„ä¸‰ç»´è¡¨é¢å…ƒç´ ï¼ˆsurfelsï¼‰æ¥æœ‰æ•ˆåœ°è®°å¿†å’Œæ£€ç´¢ç›¸å…³çš„å†å²è§†å›¾ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVMemèƒ½å¤Ÿåœ¨ç”Ÿæˆæ–°è§†å›¾æ—¶é«˜æ•ˆåœ°æå–æœ€ç›¸å…³çš„è¿‡å»è§†å›¾ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé•¿æœŸåœºæ™¯çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨é•¿æœŸåœºæ™¯åˆæˆåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨åœºæ™¯ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18254",
            "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
            "url": "https://huggingface.co/papers/2506.18254",
            "abstract": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.",
            "score": 3,
            "issue_id": 4451,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "f96aa2817f2af792",
            "authors": [
                "Tianyu Yu",
                "Bo Ji",
                "Shouli Wang",
                "Shu Yao",
                "Zefan Wang",
                "Ganqu Cui",
                "Lifan Yuan",
                "Ning Ding",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Harbin Institute of Technology",
                "National University of Singapore",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18254.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RLPR: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "RLPR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. RLPR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Reinforcement Learning Without Verifiers: Unlocking LLM Reasoning",
                    "desc": "The paper introduces RLPR, a new framework that enhances the reasoning abilities of large language models (LLMs) without needing external verifiers. It leverages the token probability scores from LLMs as reward signals, allowing for a more scalable and efficient training process. By addressing the high variance in these probability rewards, the authors implement methods to stabilize the reward signal, leading to improved performance in both general and mathematical reasoning tasks. Experimental results show that RLPR significantly outperforms existing methods, demonstrating its effectiveness across various benchmarks."
                },
                "zh": {
                    "title": "RLPRï¼šæ— éœ€éªŒè¯å™¨çš„æ¨ç†èƒ½åŠ›æå‡æ¡†æ¶",
                    "desc": "RLPRæ˜¯ä¸€ç§æ— éœ€éªŒè¯å™¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ ‡è®°æ¦‚ç‡åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¢å¼ºäº†å…¶åœ¨ä¸€èˆ¬å’Œæ•°å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸéªŒè¯å™¨ä¾èµ–æ€§å¸¦æ¥çš„å¤æ‚æ€§å’Œå¯æ‰©å±•æ€§é™åˆ¶ã€‚é€šè¿‡ä½¿ç”¨LLMè‡ªèº«çš„æ ‡è®°æ¦‚ç‡åˆ†æ•°ä½œä¸ºå‚è€ƒç­”æ¡ˆçš„å¥–åŠ±ä¿¡å·ï¼ŒRLPRåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLPRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18527",
            "title": "Auto-Regressively Generating Multi-View Consistent Images",
            "url": "https://huggingface.co/papers/2506.18527",
            "abstract": "",
            "score": 2,
            "issue_id": 4450,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "5b7e755fbaf18d79",
            "authors": [
                "JiaKui Hu",
                "Yuxiao Yang",
                "Jialun Liu",
                "Jinbo Wu",
                "Chen Zhao",
                "Yanye Lu"
            ],
            "affiliations": [
                "Baidu VIS",
                "Biomedical Engineering Department, College of Future Technology, Peking University",
                "Institute of Medical Technology, Peking University Health Science Center, Peking University",
                "National Biomedical Imaging Center, Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18527.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Hybrid Models: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18309",
            "title": "LettinGo: Explore User Profile Generation for Recommendation System",
            "url": "https://huggingface.co/papers/2506.18309",
            "abstract": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.  \t\t\t\t\tAI-generated summary \t\t\t\t User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",
            "score": 2,
            "issue_id": 4450,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "6f2beb5a53c301ed",
            "authors": [
                "Lu Wang",
                "Di Zhang",
                "Fangkai Yang",
                "Pu Zhao",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Qingwei Lin",
                "Weiwei Deng",
                "Dongmei Zhang",
                "Feng Sun",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft Corporation Beijing, China",
                "Peking University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18309.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "LettinGo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LettinGo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "LettinGo: Adaptive User Profiles for Smarter Recommendations",
                    "desc": "LettinGo is a new framework designed to improve user profiling for recommendation systems by creating diverse and adaptive profiles. It utilizes large language models (LLMs) to generate richer, text-based profiles that are more interpretable than traditional methods. The framework employs Direct Preference Optimization (DPO) to ensure that the profiles are aligned with specific recommendation tasks, allowing for greater flexibility and effectiveness. By exploring various user profiles and evaluating their impact on recommendations, LettinGo significantly enhances the accuracy and contextual awareness of personalized suggestions."
                },
                "zh": {
                    "title": "LettinGoï¼šæå‡æ¨èç³»ç»Ÿçš„ç”¨æˆ·ç”»åƒç”Ÿæˆ",
                    "desc": "LettinGo æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–å’Œè‡ªé€‚åº”çš„ç”¨æˆ·ç”»åƒï¼Œä»¥æé«˜æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å›ºå®šæ ¼å¼çš„é™åˆ¶ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„æ“ä½œï¼ŒLettinGo èƒ½å¤Ÿæ¢ç´¢å¤šæ ·çš„ç”¨æˆ·ç”»åƒã€è¯„ä¼°å…¶åœ¨æ¨èç³»ç»Ÿä¸­çš„è´¨é‡ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ€§èƒ½è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨èçš„å‡†ç¡®æ€§ã€çµæ´»æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16962",
            "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
            "url": "https://huggingface.co/papers/2506.16962",
            "abstract": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
            "score": 2,
            "issue_id": 4449,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "c1531ab3106ca207",
            "authors": [
                "Haoran Sun",
                "Yankai Jiang",
                "Wenjie Lou",
                "Yujie Zhang",
                "Wenjie Li",
                "Lilong Wang",
                "Mianxin Liu",
                "Lei Liu",
                "Xiaosong Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16962.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#training",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MICS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. MICS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑ‚Ğ°Ğ¶ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ MLLM Chiron-o1, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMRP. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Chiron-o1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Medical Reasoning with MICS: A New Path to Better Diagnosis",
                    "desc": "This paper introduces MICS, a new method for improving reasoning in medical multimodal large language models (MLLMs) like Chiron-o1. MICS generates high-quality chain-of-thought (CoT) data by using mentor models to guide the reasoning process and intern models to explore these paths. The effectiveness of the reasoning paths is evaluated using an MICS-Score, which helps in selecting the best paths for medical diagnosis. The results show that Chiron-o1, trained with MICS-generated data, outperforms existing models in medical visual question answering and reasoning tasks."
                },
                "zh": {
                    "title": "MICSï¼šæå‡åŒ»ç–—æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ¡ˆ",
                    "desc": "MICSæ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†è·¯å¾„æœç´¢æ–¹æ¡ˆï¼Œæ—¨åœ¨å¢å¼ºåŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆå…¨é¢çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ï¼ŒMICSèƒ½å¤Ÿæœ‰æ•ˆåœ°æ„å»ºåŒ»ç–—é¢†åŸŸçš„æ¨ç†æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯¼å¸ˆæ¨¡å‹é€æ­¥åˆå§‹åŒ–æ¨ç†ï¼Œç„¶åè®©å®ä¹ æ¨¡å‹æ²¿ç€è¿™äº›è·¯å¾„ç»§ç»­æ€è€ƒï¼Œæœ€ç»ˆé€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºMICSæ„å»ºçš„Chiron-o1æ¨¡å‹åœ¨åŒ»ç–—è§†è§‰é—®ç­”å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18879",
            "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
            "url": "https://huggingface.co/papers/2506.18879",
            "abstract": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.",
            "score": 1,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "1d442a58c0e72d5c",
            "authors": [
                "Junyan Li",
                "Yang Zhang",
                "Muhammad Yusuf Hassan",
                "Talha Chafekar",
                "Tianle Cai",
                "Zhile Ren",
                "Pengsheng Guo",
                "Foroozan Karimzadeh",
                "Colorado Reed",
                "Chong Wang",
                "Chuang Gan"
            ],
            "affiliations": [
                "Apple Inc.",
                "Massachusetts Institute of Technology",
                "Princeton University",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18879.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#open_source",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Commutative Vector Quantization (CommVQ) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. CommVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Rotary Position Embedding Ğ´Ğ»Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºÑÑˆĞ° Ğ½Ğ° 87.5% Ğ¿Ñ€Ğ¸ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA-3.1 8B Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ 128K Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU RTX 4090."
                },
                "en": {
                    "title": "Efficient Memory Management for Long-Context LLMs with CommVQ",
                    "desc": "This paper introduces Commutative Vector Quantization (CommVQ), a method designed to reduce memory usage in long-context inference for Large Language Models (LLMs). By employing additive quantization and a commutative codebook integrated with Rotary Position Embedding (RoPE), the method compresses the key-value (KV) cache effectively. The approach allows for efficient decoding through simple matrix multiplication, significantly lowering computational costs. Experiments demonstrate that CommVQ can reduce the KV cache size by 87.5% while maintaining high accuracy, enabling LLMs to handle longer contexts on standard GPUs."
                },
                "zh": {
                    "title": "å¯äº¤æ¢å‘é‡é‡åŒ–ï¼šä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å†…å­˜ä½¿ç”¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯äº¤æ¢å‘é‡é‡åŒ–ï¼ˆCommVQï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡å¼•å…¥åŠ æ³•é‡åŒ–å’Œè½»é‡çº§ç¼–ç å™¨ï¼ŒCommVQèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ï¼Œå¹¶é€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•è¿›è¡Œè§£ç ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å…¼å®¹çš„ä»£ç æœ¬ï¼Œå¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å®ç°é«˜æ•ˆè§£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†FP16 KVç¼“å­˜å¤§å°å‡å°‘87.5%ï¼Œå¹¶åœ¨1ä½é‡åŒ–ä¸‹å®ç°æœ€å°çš„å‡†ç¡®æ€§æŸå¤±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18349",
            "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
            "url": "https://huggingface.co/papers/2506.18349",
            "abstract": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
            "score": 1,
            "issue_id": 4449,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "ae21c466bdfe4727",
            "authors": [
                "Zichong Li",
                "Chen Liang",
                "Zixuan Zhang",
                "Ilgee Hong",
                "Young Jin Kim",
                "Weizhu Chen",
                "Tuo Zhao"
            ],
            "affiliations": [
                "Georgia Tech",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18349.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#open_source",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "SlimMoE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "SlimMoE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture of Experts (MoE) Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¶Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Phi 3.5-MoE Ğ´Ğ¾ Phi-mini-MoE Ğ¸ Phi-tiny-MoE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Compression of MoE Models for Resource-Constrained Environments",
                    "desc": "SlimMoE is a novel framework designed to compress large Mixture of Experts (MoE) models into smaller, more efficient versions without the need for extensive retraining. It employs a multi-stage compression approach that reduces the number of parameters while preserving model performance, making it feasible to deploy in environments with limited resources. By systematically slimming down experts and transferring knowledge through intermediate stages, SlimMoE mitigates the performance loss typically associated with one-shot pruning methods. The resulting models, Phi-mini-MoE and Phi-tiny-MoE, demonstrate competitive performance with significantly fewer activated parameters, making them ideal for academic and resource-constrained applications."
                },
                "zh": {
                    "title": "SlimMoEï¼šé«˜æ•ˆå‹ç¼©å¤§å‹MoEæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "SlimMoEæ˜¯ä¸€ç§å¤šé˜¶æ®µå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤§å‹æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å‹ç¼©ä¸ºæ›´å°ã€æ›´é«˜æ•ˆçš„å˜ä½“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾ç®€ä¸“å®¶å’Œåœ¨ä¸­é—´é˜¶æ®µè½¬ç§»çŸ¥è¯†ï¼Œæœ‰æ•ˆå‡å°‘å‚æ•°æ•°é‡ï¼Œé¿å…äº†ä¸€æ¬¡æ€§å‰ªææ–¹æ³•å¸¸è§çš„æ€§èƒ½ä¸‹é™ã€‚å®éªŒè¡¨æ˜ï¼Œå‹ç¼©åçš„æ¨¡å‹åœ¨ç›¸ä¼¼è§„æ¨¡ä¸‹è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è¾ƒä½å»¶è¿Ÿä¸‹ä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚SlimMoEä¸ºåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ä½¿ç”¨MoEæ¶æ„æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17538",
            "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
            "url": "https://huggingface.co/papers/2506.17538",
            "abstract": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent shift in Generative AI (GenAI) applications from cloud-only environments to end-user devices introduces new challenges in resource management, system efficiency, and user experience. This paper presents ConsumerBench, a comprehensive benchmarking framework designed to evaluate the system efficiency and response time of GenAI models running on end-user devices. Unlike existing benchmarks that assume exclusive model access on dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios executing concurrently on constrained hardware. Furthermore, ConsumerBench supports customizable workflows that simulate complex tasks requiring coordination among multiple applications. ConsumerBench captures both application-level metrics, including latency and Service Level Objective (SLO) attainment, and system-level metrics like CPU/GPU utilization and memory bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies in resource sharing, unfair scheduling under greedy allocation, and performance pitfalls of static model server configurations. The paper also provides practical insights for model developers and system designers, highlighting the benefits of custom kernels tailored to consumer-grade GPU architectures and the value of implementing SLO-aware scheduling strategies.",
            "score": 1,
            "issue_id": 4451,
            "pub_date": "2025-06-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ½Ñ",
                "en": "June 21",
                "zh": "6æœˆ21æ—¥"
            },
            "hash": "77484dbfc212d862",
            "authors": [
                "Yile Gu",
                "Rohan Kadekodi",
                "Hoang Nguyen",
                "Keisuke Kamahori",
                "Yiyu Liu",
                "Baris Kasikci"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17538.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ConsumerBench: ĞÑ†ĞµĞ½ĞºĞ° GenAI Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "ConsumerBench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ConsumerBench Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing GenAI Performance on Everyday Devices",
                    "desc": "ConsumerBench is a benchmarking framework that assesses the efficiency and response time of Generative AI (GenAI) systems on end-user devices. It addresses the challenges posed by running multiple applications simultaneously on limited hardware, unlike traditional benchmarks that focus on dedicated GPUs. The framework measures both application-level metrics, such as latency and Service Level Objectives (SLO), and system-level metrics like CPU/GPU utilization. The findings highlight issues in resource sharing and scheduling, offering insights for developers to optimize performance on consumer-grade devices."
                },
                "zh": {
                    "title": "ConsumerBenchï¼šè¯„ä¼°ç»ˆç«¯è®¾å¤‡ä¸ŠGenAIç³»ç»Ÿçš„æ•ˆç‡ä¸å“åº”æ—¶é—´",
                    "desc": "ConsumerBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åœ¨ç»ˆç«¯è®¾å¤‡ä¸Šè¿è¡Œçš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”æ—¶é—´ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒConsumerBenchæ¨¡æ‹Ÿäº†åœ¨å—é™ç¡¬ä»¶ä¸ŠåŒæ—¶æ‰§è¡Œçš„å¤šåº”ç”¨åœºæ™¯ï¼Œå¼ºè°ƒäº†èµ„æºç®¡ç†å’Œç”¨æˆ·ä½“éªŒçš„æ–°æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ”¯æŒå¯å®šåˆ¶çš„å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿéœ€è¦å¤šä¸ªåº”ç”¨åè°ƒçš„å¤æ‚ä»»åŠ¡ã€‚é€šè¿‡å®éªŒï¼ŒConsumerBenchæ­ç¤ºäº†èµ„æºå…±äº«ä¸­çš„ä½æ•ˆã€è´ªå©ªåˆ†é…ä¸‹çš„ä¸å…¬å¹³è°ƒåº¦ä»¥åŠé™æ€æ¨¡å‹æœåŠ¡å™¨é…ç½®çš„æ€§èƒ½é™·é˜±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10597",
            "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
            "url": "https://huggingface.co/papers/2506.10597",
            "abstract": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety mechanisms. Guardrails--external defense mechanisms that monitor and control LLM interaction--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, explore their universality across attack types, and provide insights into optimizing defense combinations. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.",
            "score": 0,
            "issue_id": 4448,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "4122cc84dd4333e8",
            "authors": [
                "Xunguang Wang",
                "Zhenlan Ji",
                "Wenxuan Wang",
                "Zongjie Li",
                "Daoyuan Wu",
                "Shuai Wang"
            ],
            "affiliations": [
                "Renmin University of China",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10597.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#security",
                    "#optimization"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ LLM Ğ¾Ñ‚ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² (Ğ³Ğ°Ñ€Ğ´Ñ€ĞµĞ¹Ğ»Ğ¾Ğ²) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ‚Ğ¸Ğ¿Ğ° jailbreak. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ³Ğ°Ñ€Ğ´Ñ€ĞµĞ¹Ğ»Ñ‹ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. Ğ’Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Security-Efficiency-Utility Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€."
                },
                "en": {
                    "title": "Strengthening LLMs: A New Framework for Jailbreak Guardrails",
                    "desc": "This paper presents a comprehensive framework for analyzing and evaluating guardrails designed to protect Large Language Models (LLMs) from jailbreak attacks. It introduces a multi-dimensional taxonomy that categorizes these guardrails based on six important aspects, helping to clarify their roles and effectiveness. Additionally, the authors propose a new evaluation framework that balances security, efficiency, and utility, allowing for a thorough assessment of guardrail performance. By identifying the strengths and weaknesses of current approaches, this work aims to enhance the development of more effective defenses for LLMs against potential vulnerabilities."
                },
                "zh": {
                    "title": "ç³»ç»ŸåŒ–è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„è¶Šç‹±é˜²æŠ¤æœºåˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„åˆ†æå’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¶Šç‹±é˜²æŠ¤æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡LLMså–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æš´éœ²äº†å…³é”®çš„å®‰å…¨æ¼æ´ï¼Œå°¤å…¶æ˜¯è¶Šç‹±æ”»å‡»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šç»´åˆ†ç±»æ³•ï¼Œå°†é˜²æŠ¤æœºåˆ¶åˆ†ä¸ºå…­ä¸ªå…³é”®ç»´åº¦ï¼Œå¹¶å¼•å…¥äº†å®‰å…¨æ€§ã€æ•ˆç‡å’Œå®ç”¨æ€§è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å…¶å®é™…æ•ˆæœã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æå’Œå®éªŒï¼Œæˆ‘ä»¬è¯†åˆ«äº†ç°æœ‰é˜²æŠ¤æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†ç»“æ„åŒ–çš„åŸºç¡€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-23.html",
    "link_next": "2025-06-25.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 5,
        "#benchmark": 9,
        "#agents": 0,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    }
}