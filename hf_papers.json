{
    "date": {
        "ru": "13 Ğ¸ÑĞ½Ñ",
        "en": "June 13",
        "zh": "6æœˆ13æ—¥"
    },
    "time_utc": "2025-06-13 08:16",
    "weekday": 4,
    "issue_id": 4278,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.09513",
            "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.09513",
            "abstract": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
            "score": 38,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "7c6aa342a51b1d59",
            "authors": [
                "Yu Sun",
                "Xingyu Qian",
                "Weiwen Xu",
                "Hao Zhang",
                "Chenghao Xiao",
                "Long Li",
                "Yu Rong",
                "Wenbing Huang",
                "Qifeng Bai",
                "Tingyang Xu"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "BeÄ³ing Key Laboratory of Research on Large Models",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation",
                "Gaoling School of",
                "Hupan Lab",
                "Renmin University of China",
                "School of Basic Medical Sciences, Lanzhou University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09513.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#healthcare",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ReasonMed: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜",
                    "desc": "ReasonMed - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 370 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Error Refiner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Chain-of-Thought Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ´ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReasonMed-7B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.17% Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° LLaMA3.1-70B Ğ½Ğ° 4.60% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ PubMedQA."
                },
                "en": {
                    "title": "Enhancing Medical AI with ReasonMed: A New Benchmark in Reasoning",
                    "desc": "ReasonMed is a comprehensive medical reasoning dataset designed to improve the performance of medical question answering models. It consists of 370,000 high-quality examples derived from 1.7 million initial reasoning paths created by various large language models (LLMs). The dataset is refined through a multi-agent process that includes an Error Refiner to correct mistakes in reasoning paths. By combining detailed Chain-of-Thought reasoning with concise summaries, ReasonMed-7B achieves superior results, surpassing previous benchmarks for smaller models and even outperforming larger models on specific tasks."
                },
                "zh": {
                    "title": "ReasonMedï¼šæå‡åŒ»å­¦é—®ç­”æ¨¡å‹çš„æ–°åŸºå‡†",
                    "desc": "ReasonMedæ˜¯ä¸€ä¸ªå¤§å‹åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦é—®ç­”æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å®ƒç»“åˆäº†è¯¦ç»†çš„æ¨ç†è·¯å¾„å’Œç®€æ´çš„æ€»ç»“ï¼Œåˆ›é€ äº†æ–°çš„æ¨¡å‹æ€§èƒ½åŸºå‡†ã€‚è¯¥æ•°æ®é›†åŒ…å«370,000ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œç»è¿‡å¤šä»£ç†éªŒè¯å’Œç²¾ç‚¼è¿‡ç¨‹æ„å»ºè€Œæˆã€‚é€šè¿‡ç»“åˆè¯¦ç»†çš„æ€ç»´é“¾æ¨ç†å’Œç®€æ´çš„ç­”æ¡ˆæ€»ç»“ï¼ŒReasonMed-7Bæ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10954",
            "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
            "url": "https://huggingface.co/papers/2506.10954",
            "abstract": "An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.",
            "score": 27,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "dfb4cf3e253468bd",
            "authors": [
                "Lianghong Guo",
                "Yanlin Wang",
                "Caihua Li",
                "Pengyu Yang",
                "Jiachi Chen",
                "Wei Tao",
                "Yingtian Zou",
                "Duyu Tang",
                "Zibin Zheng"
            ],
            "affiliations": [
                "Huawei",
                "Independent Researcher",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10954.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#dataset",
                    "#agents",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "SWE-Factory: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ LLM Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "SWE-Factory - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ SWE-Builder - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ fail2pass. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑĞ±Ğ¾Ñ€ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ…, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Automating Dataset Creation for LLMs in GitHub Issue Resolution",
                    "desc": "The paper presents SWE-Factory, an automated pipeline designed to streamline the creation of large-scale datasets for training and evaluating Large Language Models (LLMs) in GitHub issue resolution tasks. It addresses the challenges of environment setup, grading, and validation by integrating three automated components: SWE-Builder for environment construction, a standardized exit-code-based grading system, and an automated fail2pass validation process. Experiments demonstrate that SWE-Factory can efficiently generate valid task instances at a low cost while achieving high accuracy in grading and validation. This innovation aims to enhance the quality and speed of dataset collection for LLM training and evaluation."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–ç®¡é“åŠ é€ŸGitHubé—®é¢˜è§£å†³æ•°æ®é›†æ„å»º",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSWE-Factoryçš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œæ—¨åœ¨ç®€åŒ–å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºï¼Œä»¥è¯„ä¼°å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨GitHubé—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„æ•°æ®é›†æ„å»ºè¿‡ç¨‹ç¹çä¸”è€—æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨ç¯å¢ƒæ­å»ºã€ç»“æœè¯„åˆ†å’Œä»»åŠ¡éªŒè¯é˜¶æ®µã€‚SWE-Factoryé€šè¿‡é›†æˆä¸‰ä¸ªæ ¸å¿ƒè‡ªåŠ¨åŒ–ç»„ä»¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºçš„å¤šä»£ç†ç³»ç»ŸSWE-Builderã€åŸºäºé€€å‡ºä»£ç çš„æ ‡å‡†åŒ–è¯„åˆ†æ–¹æ³•ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„fail2passéªŒè¯è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿæœ‰æ•ˆæ„å»ºæœ‰æ•ˆçš„ä»»åŠ¡å®ä¾‹ï¼Œå¹¶åœ¨è¯„åˆ†å’ŒéªŒè¯æ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’Œé«˜ç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09993",
            "title": "Text-Aware Image Restoration with Diffusion Models",
            "url": "https://huggingface.co/papers/2506.09993",
            "abstract": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
            "score": 27,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "91f1bb97ef062632",
            "authors": [
                "Jaewon Min",
                "Jin Hyeon Kim",
                "Paul Hyunbin Cho",
                "Jaeeun Lee",
                "Jihye Park",
                "Minkyu Park",
                "Sangpil Kim",
                "Hyunhee Park",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "Samsung Electronics",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09993.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° (TAIR) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SA-Text - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TeReDiff Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Restoring Images with Textual Precision",
                    "desc": "The Text-Aware Image Restoration (TAIR) system addresses the challenge of restoring images while maintaining the accuracy of textual information. Traditional diffusion-based methods often produce incorrect text patterns, leading to what is known as text-image hallucination. TAIR introduces a multi-task diffusion framework, TeReDiff, which combines image restoration with a text-spotting module to improve both visual and textual fidelity. By leveraging a large-scale dataset of annotated images, TAIR significantly enhances text recognition accuracy compared to existing methods."
                },
                "zh": {
                    "title": "æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼šæå‡å›¾åƒä¸æ–‡æœ¬çš„åŒé‡æ¢å¤",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰çš„ç³»ç»Ÿï¼Œæ—¨åœ¨åŒæ—¶æ¢å¤å›¾åƒå†…å®¹å’Œæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„æ‰©æ•£åŸºç¡€ä¿®å¤æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒä¿®å¤æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸæ—¶å¸¸å¸¸å‡ºç°é”™è¯¯çš„æ–‡æœ¬æ¨¡å¼ã€‚TAIRç³»ç»Ÿç»“åˆäº†å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶å’Œæ–‡æœ¬æ£€æµ‹æ¨¡å—ï¼Œé€šè¿‡è”åˆè®­ç»ƒæé«˜äº†æ–‡æœ¬è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAIRåœ¨å›¾åƒä¿®å¤å’Œæ–‡æœ¬ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ä¿®å¤æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10857",
            "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
            "url": "https://huggingface.co/papers/2506.10857",
            "abstract": "VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.",
            "score": 21,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "7eabd83f69c00df7",
            "authors": [
                "Jiashuo Yu",
                "Yue Wu",
                "Meng Chu",
                "Zhifei Ren",
                "Zizheng Huang",
                "Pei Chu",
                "Ruijie Zhang",
                "Yinan He",
                "Qirui Li",
                "Songze Li",
                "Zhenxiang Li",
                "Zhongying Tu",
                "Conghui He",
                "Yu Qiao",
                "Yali Wang",
                "Yi Wang",
                "Limin Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10857.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VRBench: ĞÑ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "VRBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 1010 Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 1,6 Ñ‡Ğ°ÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 9468 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ 30292 ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸. VRBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° 12 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ 16 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM), Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "VRBench: Advancing Multi-Step Reasoning in Long Video Understanding",
                    "desc": "VRBench is a new benchmark designed to evaluate how well large models understand long videos through multi-step reasoning. It includes 1,010 long videos and thousands of human-labeled question-answering pairs, focusing on both temporal reasoning and procedural validity. The framework allows for the generation of coherent reasoning chains that require multiple steps, assessing models not just on final answers but also on the reasoning process. By testing various large language models (LLMs) and vision-language models (VLMs), VRBench aims to provide insights that enhance the understanding of multi-step reasoning in video comprehension."
                },
                "zh": {
                    "title": "VRBenchï¼šé•¿è§†é¢‘ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "VRBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿è§†é¢‘ç†è§£çš„åŸºå‡†ï¼Œä¸“æ³¨äºå¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯æ—¶é—´æ¨ç†å’Œç¨‹åºæœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†åŒ…å«1010ä¸ªé•¿è§†é¢‘ï¼Œå¹³å‡æ—¶é•¿ä¸º1.6å°æ—¶ï¼Œä»¥åŠ9468ä¸ªäººå·¥æ ‡æ³¨çš„å¤šæ­¥éª¤é—®ç­”å¯¹å’Œ30292ä¸ªå¸¦æ—¶é—´æˆ³çš„æ¨ç†æ­¥éª¤ã€‚é€šè¿‡å¤šé˜¶æ®µç­›é€‰è¿‡ç¨‹ï¼Œç¡®ä¿è§†é¢‘æƒ…èŠ‚è¿è´¯æ€§ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªäººæœºåä½œæ¡†æ¶ï¼Œç”Ÿæˆéœ€è¦å¤šä¸ªæ—¶é—´åŸºç¡€æ­¥éª¤çš„è¿è´¯æ¨ç†é“¾ã€‚VRBenchè®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç»¼åˆè¯„ä¼°æ¨¡å‹çš„ç»“æœå’Œè¿‡ç¨‹ï¼Œæ¨åŠ¨äº†å¤šæ­¥éª¤æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10540",
            "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
            "url": "https://huggingface.co/papers/2506.10540",
            "abstract": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.",
            "score": 21,
            "issue_id": 4275,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "5c11232a01ad90bb",
            "authors": [
                "Haoyuan Shi",
                "Yunxin Li",
                "Xinyu Chen",
                "Longyue Wang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce, Hangzhou, China",
                "Harbin Institute of Technology, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10540.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#multimodal",
                    "#story_generation",
                    "#agents"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "AniMaker: ÑƒĞ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "AniMaker - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS-Gen) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AniEval Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². AniMaker ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€Ğ°, Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AniMaker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹."
                },
                "en": {
                    "title": "AniMaker: Crafting Coherent Stories from Text with AI",
                    "desc": "AniMaker is a multi-agent framework designed to generate coherent storytelling videos from text input, addressing challenges in video generation. It utilizes specialized agents for different tasks, such as storyboard creation and video clip generation, ensuring a consistent narrative flow. The framework incorporates MCTS-Gen for efficient clip generation and AniEval for evaluating animation quality, focusing on story coherence and visual continuity. Experiments show that AniMaker outperforms existing models in both quality and efficiency, making AI-generated storytelling animation more viable for production."
                },
                "zh": {
                    "title": "AniMakerï¼šé«˜æ•ˆç”Ÿæˆè¿è´¯æ•…äº‹è§†é¢‘çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "AniMakeræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨MCTS-Genå’ŒAniEvalï¼Œä»æ–‡æœ¬è¾“å…¥ç”Ÿæˆè¿è´¯çš„æ•…äº‹è§†é¢‘ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„è´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œå¦‚å¯¼æ¼”æ™ºèƒ½ä½“ã€æ‘„å½±æ™ºèƒ½ä½“ã€è¯„å®¡æ™ºèƒ½ä½“å’ŒåæœŸåˆ¶ä½œæ™ºèƒ½ä½“ï¼Œæ¥å®ç°é«˜æ•ˆçš„å¤šå€™é€‰ç‰‡æ®µç”Ÿæˆå’Œæ•…äº‹æ„è¯†ç‰‡æ®µé€‰æ‹©ã€‚AniMakerçš„æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬MCTS-Genï¼Œå®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å¯¼èˆªå€™é€‰ç©ºé—´ï¼Œç”Ÿæˆé«˜æ½œåŠ›çš„ç‰‡æ®µï¼ŒåŒæ—¶ä¼˜åŒ–èµ„æºä½¿ç”¨ï¼›ä»¥åŠAniEvalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå¤šé•œå¤´åŠ¨ç”»è¯„ä¼°è®¾è®¡çš„æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAniMakeråœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†AIç”Ÿæˆçš„æ•…äº‹åŠ¨ç”»æ›´æ¥è¿‘ç”Ÿäº§æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10974",
            "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
            "url": "https://huggingface.co/papers/2506.10974",
            "abstract": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.",
            "score": 9,
            "issue_id": 4276,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "50bcb63544ac7586",
            "authors": [
                "Yixin Ou",
                "Yujie Luo",
                "Jingsheng Zheng",
                "Lanning Wei",
                "Shuofei Qiao",
                "Jintian Zhang",
                "Da Zheng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10974.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "AutoMind: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "AutoMind - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. AutoMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "AutoMind: Revolutionizing Automated Data Science with Expert Knowledge and Adaptability",
                    "desc": "AutoMind is a new framework designed to enhance automated data science by integrating expert knowledge and adapting its approach based on the complexity of tasks. It utilizes a curated knowledge base to inform its decisions, allowing it to tackle more complex problems than traditional systems. The framework employs a knowledgeable tree search algorithm to explore various solutions strategically, improving its problem-solving capabilities. Evaluations show that AutoMind outperforms existing methods, making it a significant advancement in the field of automated machine learning."
                },
                "zh": {
                    "title": "AutoMindï¼šè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„æ–°çªç ´",
                    "desc": "AutoMindæ˜¯ä¸€ä¸ªçµæ´»ä¸”çŸ¥è¯†ä¸°å¯Œçš„LLMä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆä¸“å®¶çŸ¥è¯†ã€æˆ˜ç•¥æ€§è§£å†³æ–¹æ¡ˆæ¢ç´¢å’Œè‡ªé€‚åº”ç¼–ç æ¥æå‡è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„èƒ½åŠ›ã€‚ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒAutoMindåœ¨å¤„ç†å¤æ‚å’Œåˆ›æ–°ä»»åŠ¡æ—¶è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¡†æ¶çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å»ºç«‹ä¸€ä¸ªç»è¿‡ç­›é€‰çš„ä¸“å®¶çŸ¥è¯†åº“ã€é‡‡ç”¨æ™ºèƒ½çš„çŸ¥è¯†æ ‘æœç´¢ç®—æ³•ä»¥åŠåŠ¨æ€è°ƒæ•´ç¼–ç ç­–ç•¥ï¼Œæ¥é€‚åº”ä¸åŒä»»åŠ¡çš„å¤æ‚æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAutoMindåœ¨è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°å‡ºé«˜æ•ˆå’Œç¨³å¥çš„ç‰¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10960",
            "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
            "url": "https://huggingface.co/papers/2506.10960",
            "abstract": "A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.",
            "score": 9,
            "issue_id": 4276,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "db6960a49f9467ee",
            "authors": [
                "Kangwei Liu",
                "Siyuan Cheng",
                "Bozhong Tian",
                "Xiaozhuan Liang",
                "Yuyang Yin",
                "Meng Han",
                "Ningyu Zhang",
                "Bryan Hooi",
                "Xi Chen",
                "Shumin Deng"
            ],
            "affiliations": [
                "National University of Singapore",
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10960.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#small_models",
                    "#low_resource",
                    "#ethics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞµÑÑ‚ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Small Models for Chinese Harm Detection",
                    "desc": "This paper introduces a new benchmark for detecting harmful content in Chinese, addressing the lack of resources in this area compared to English. It features a dataset that is professionally annotated and covers six categories of harmful content, using real-world examples. The authors also develop a knowledge-augmented baseline that combines expert knowledge with insights from large language models, allowing smaller models to perform effectively without needing extensive resources. This approach enhances the accuracy of harmful content detection in Chinese, making it more accessible for various applications."
                },
                "zh": {
                    "title": "æå‡ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹çš„åŸºå‡†ä¸æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å…­ä¸ªä»£è¡¨æ€§ç±»åˆ«ï¼Œå¹¶å®Œå…¨åŸºäºçœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œä¸“ä¸šæ ‡æ³¨ã€‚ç°æœ‰çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†ç›¸å¯¹ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¢å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œç»“åˆäº†äººå·¥æ ‡æ³¨çš„çŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæ€§çŸ¥è¯†ï¼Œä½¿å¾—è¾ƒå°çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶ä¸ºä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹æä¾›äº†é‡è¦çš„èµ„æºå’Œæ–¹æ³•ï¼Œæå‡äº†å†…å®¹å®¡æ ¸çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10821",
            "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
            "url": "https://huggingface.co/papers/2506.10821",
            "abstract": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.",
            "score": 9,
            "issue_id": 4273,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "e1e5003f31573e97",
            "authors": [
                "Huaying Yuan",
                "Zheng Liu",
                "Junjie Zhou",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10821.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#long_context",
                    "#video",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "VideoDeepResearch - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑÑŒ Ğº Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with Text-Only Reasoning",
                    "desc": "VideoDeepResearch is a novel framework designed to improve long video understanding (LVU) tasks without relying on extended context windows or advanced visual perception capabilities. It utilizes a text-only large reasoning model (LRM) in conjunction with a modular toolkit that includes multimodal retrievers and visual perceivers. This system formulates problem-solving strategies through reasoning and selectively accesses relevant video content as needed. Experimental results show that VideoDeepResearch significantly outperforms existing multi-modal large language models (MLLMs) on various LVU benchmarks, demonstrating its effectiveness in tackling complex video understanding challenges."
                },
                "zh": {
                    "title": "çªç ´é•¿è§†é¢‘ç†è§£çš„å…¨æ–°æ¡†æ¶",
                    "desc": "VideoDeepResearchæ˜¯ä¸€ç§æ–°å‹çš„é•¿è§†é¢‘ç†è§£æ¡†æ¶ï¼Œå®ƒä»…ä¾èµ–æ–‡æœ¬æ¨ç†æ¨¡å‹å’Œæ¨¡å—åŒ–å·¥å…·ï¼Œè€Œä¸éœ€è¦æ‰©å±•ä¸Šä¸‹æ–‡çª—å£æˆ–å¢å¼ºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ¨ç†åˆ¶å®šé—®é¢˜è§£å†³ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å·¥å…·é€‰æ‹©æ€§åœ°è®¿é—®å’Œä½¿ç”¨è§†é¢‘å†…å®¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºVideoDeepResearchåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†ç³»ç»Ÿåœ¨è§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10890",
            "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
            "url": "https://huggingface.co/papers/2506.10890",
            "abstract": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter",
            "score": 7,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "22fffd0088d280e0",
            "authors": [
                "Zhao Zhang",
                "Yutao Cheng",
                "Dexiang Hong",
                "Maoke Yang",
                "Gonglei Shi",
                "Lei Ma",
                "Hui Zhang",
                "Jie Shao",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance, Fudan University",
                "ByteDance, Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10890.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "CreatiPoster: Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ",
                    "desc": "CreatiPoster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ JSON-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°. CreatiPoster Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ appeal. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ¾Ğ»ÑÑ‚Ğ°, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Graphic Design with AI-Generated Custom Compositions",
                    "desc": "CreatiPoster is a novel framework that generates high-quality, editable graphic designs from user inputs like text or images. It utilizes a protocol model to create a detailed JSON specification for each design layer, ensuring precise layout and style. A conditional background model then generates a cohesive background that complements the foreground elements. This approach not only enhances the editability and visual appeal of designs but also outperforms existing tools and templates in the market."
                },
                "zh": {
                    "title": "CreatiPosterï¼šè®©å›¾å½¢è®¾è®¡æ›´ç®€å•",
                    "desc": "CreatiPoster æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜è´¨é‡ã€å¯ç¼–è¾‘å’Œå¯å®šåˆ¶å›¾å½¢ä½œå“çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æˆ–èµ„äº§ä¸­åˆ›å»ºå¤šå±‚æ¬¡çš„å›¾å½¢è®¾è®¡ã€‚ä¸ç°æœ‰å·¥å…·ç›¸æ¯”ï¼Œå®ƒåœ¨ç”¨æˆ·æä¾›çš„èµ„äº§æ•´åˆã€å¯ç¼–è¾‘æ€§å’Œè§†è§‰å¸å¼•åŠ›æ–¹é¢è¡¨ç°æ›´ä½³ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åè®®æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„ JSON è§„èŒƒï¼Œæè¿°æ¯ä¸€å±‚çš„å¸ƒå±€ã€å±‚æ¬¡ã€å†…å®¹å’Œé£æ ¼ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæ— ç‰ˆæƒçš„ 100,000 ä¸ªå¤šå±‚è®¾è®¡çš„è¯­æ–™åº“ï¼ŒCreatiPoster ä¿ƒè¿›äº† AI è¾…åŠ©å›¾å½¢è®¾è®¡çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10741",
            "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
            "url": "https://huggingface.co/papers/2506.10741",
            "abstract": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft",
            "score": 7,
            "issue_id": 4276,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "5c43d96d0a604ef8",
            "authors": [
                "SiXiang Chen",
                "Jianyu Lai",
                "Jialin Gao",
                "Tian Ye",
                "Haoyu Chen",
                "Hengyu Shi",
                "Shitong Shao",
                "Yunlong Lin",
                "Song Fei",
                "Zhaohu Xing",
                "Yeying Jin",
                "Junfeng Luo",
                "Xiaoming Wei",
                "Lei Zhu"
            ],
            "affiliations": [
                "Meituan",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10741.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#architecture",
                    "#open_source",
                    "#dataset",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ",
                    "desc": "PosterCraft - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. PosterCraft Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Elevating Poster Design with PosterCraft's Unified Framework",
                    "desc": "PosterCraft is a novel framework designed to enhance the generation of aesthetic posters by integrating advanced techniques in text rendering and layout optimization. It utilizes a modular pipeline that includes region-aware fine-tuning and aesthetic reinforcement learning to improve the visual quality of the generated images. The framework operates on a cascaded workflow, leveraging large-scale datasets for training and optimizing each component for better performance. Experimental results show that PosterCraft surpasses existing open-source models in rendering accuracy and overall aesthetic appeal, making it competitive with state-of-the-art commercial systems."
                },
                "zh": {
                    "title": "PosterCraftï¼šç¾å­¦æµ·æŠ¥ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "PosterCraft æ˜¯ä¸€ä¸ªæ”¹è¿›ç¾å­¦æµ·æŠ¥ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å—åŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡å¢å¼ºçš„æ–‡æœ¬æ¸²æŸ“ã€åŒºåŸŸæ„ŸçŸ¥å¾®è°ƒã€ç¾å­¦å¼ºåŒ–å­¦ä¹ å’Œè”åˆè§†è§‰-è¯­è¨€ä¼˜åŒ–ï¼Œæå‡äº†æµ·æŠ¥çš„ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹è‡ªç”±æ¢ç´¢è§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„ç»„åˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨¡å—åŒ–ç®¡é“çš„å±€é™æ€§ã€‚ç»è¿‡å¤šé¡¹å®éªŒè¯„ä¼°ï¼ŒPosterCraft åœ¨æ¸²æŸ“ç²¾åº¦ã€å¸ƒå±€ä¸€è‡´æ€§å’Œæ•´ä½“è§†è§‰å¸å¼•åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºå¼€æºåŸºçº¿ï¼Œæ¥è¿‘æœ€å…ˆè¿›çš„å•†ä¸šç³»ç»Ÿçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10952",
            "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
            "url": "https://huggingface.co/papers/2506.10952",
            "abstract": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%.",
            "score": 6,
            "issue_id": 4273,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "f9cd5a16ae7e2cc3",
            "authors": [
                "Mozhi Zhang",
                "Howe Tissue",
                "Lu Wang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Ritzz-AI",
                "School of Computer Science, Fudan University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10952.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Domain2Vec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Domain2Vec Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° Pile-CC, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 51.5% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… The Pile."
                },
                "en": {
                    "title": "Optimize Language Models with Domain2Vec!",
                    "desc": "Domain2Vec is a new method that breaks down datasets into smaller parts called meta-domains to improve the training of language models. It uses a classifier to create a domain vector that represents the dataset's features, allowing for better alignment between training and validation data distributions. This approach helps in selecting the best combination of data for pretraining language models while using less computational power. Experiments show that Domain2Vec can achieve similar performance with significantly reduced computational costs, enhancing efficiency in machine learning tasks."
                },
                "zh": {
                    "title": "Domain2Vecï¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ•°æ®åˆ†è§£æ–¹æ³•",
                    "desc": "Domain2Vecæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒå°†æ•°æ®é›†åˆ†è§£ä¸ºå¤šä¸ªå…ƒåŸŸçš„çº¿æ€§ç»„åˆï¼Œä»¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•ç»´æŠ¤ä¸€ä¸ªå…ƒåŸŸè¯æ±‡è¡¨ï¼Œå¹¶ä½¿ç”¨åˆ†ç±»å™¨å°†ç»™å®šæ•°æ®é›†åˆ†è§£ä¸ºå¯¹åº”äºè¯¥è¯æ±‡è¡¨çš„åŸŸå‘é‡ã€‚è¿™äº›åŸŸå‘é‡èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ ¹æ®åˆ†å¸ƒå¯¹é½å‡è®¾ï¼ˆDAÂ²ï¼‰è¯†åˆ«å‡ºæœ€ä½³çš„æ•°æ®æ··åˆï¼Œä»è€Œé™ä½éªŒè¯æŸå¤±ã€‚æ­¤å¤–ï¼ŒDomain2Vecå¯ä»¥æ— ç¼é›†æˆåˆ°ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œå»ºæ¨¡åŸŸå‘é‡ä¸è¯­è¨€æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09967",
            "title": "Resa: Transparent Reasoning Models via SAEs",
            "url": "https://huggingface.co/papers/2506.09967",
            "abstract": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.",
            "score": 6,
            "issue_id": 4274,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "03a45cae6d11be64",
            "authors": [
                "Shangshang Wang",
                "Julian Asilis",
                "Ã–mer Faruk AkgÃ¼l",
                "Enes Burak Bilgin",
                "Ollie Liu",
                "Deqing Fu",
                "Willie Neiswanger"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.09967.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAE-Tuning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SAE-Tuning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Reasoning Enhancement in Language Models with SAE-Tuning",
                    "desc": "The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼šç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜çš„åŠ›é‡",
                    "desc": "SAE-Tuningæ˜¯ä¸€ç§é«˜æ•ˆçš„ç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å‘å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œä»¥ä»æºæ¨¡å‹ä¸­æ•æ‰æ¨ç†èƒ½åŠ›ï¼Œç„¶ååˆ©ç”¨è®­ç»ƒå¥½çš„SAEæŒ‡å¯¼æ ‡å‡†çš„ç›‘ç£å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œåœ¨ç›®æ ‡æ¨¡å‹ä¸­å¼•å‘è¿™äº›èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSAE-Tuningåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¯æ³›åŒ–å’Œæ¨¡å—åŒ–çš„ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ä¹‹é—´çµæ´»åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09344",
            "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
            "url": "https://huggingface.co/papers/2506.09344",
            "abstract": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.",
            "score": 4,
            "issue_id": 4273,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "b9166301d93dd2bb",
            "authors": [
                "Inclusion AI",
                "Biao Gong",
                "Cheng Zou",
                "Chuanyang Zheng",
                "Chunluan Zhou",
                "Canxiang Yan",
                "Chunxiang Jin",
                "Chunjie Shen",
                "Dandan Zheng",
                "Fudong Wang",
                "Furong Xu",
                "GuangMing Yao",
                "Jun Zhou",
                "Jingdong Chen",
                "Jianxin Sun",
                "Jiajia Liu",
                "Jianjiang Zhu",
                "Jun Peng",
                "Kaixiang Ji",
                "Kaiyou Song",
                "Kaimeng Ren",
                "Libin Wang",
                "Lixiang Ru",
                "Lele Xie",
                "Longhua Tan",
                "Lyuxin Xue",
                "Lan Wang",
                "Mochen Bai",
                "Ning Gao",
                "Pei Chen",
                "Qingpei Guo",
                "Qinglong Zhang",
                "Qiang Xu",
                "Rui Liu",
                "Ruijie Xiong",
                "Sirui Gao",
                "Tinghao Liu",
                "Taisong Li",
                "Weilong Chai",
                "Xinyu Xiao",
                "Xiaomei Wang",
                "Xiaoxue Chen",
                "Xiao Lu",
                "Xiaoyu Li",
                "Xingning Dong",
                "Xuzheng Yu",
                "Yi Yuan",
                "Yuting Gao",
                "Yunxiao Sun",
                "Yipeng Chen",
                "Yifei Wu",
                "Yongjie Lyu",
                "Ziping Ma",
                "Zipeng Feng",
                "Zhijiang Fang",
                "Zhihao Qiu",
                "Ziyuan Huang",
                "Zhengyu He"
            ],
            "affiliations": [
                "Ant Group",
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09344.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#open_source",
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ming-Omni - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ MoE Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ming-Omni Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ming-Omni Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Ming-Omni: One Model, Many Modalities!",
                    "desc": "Ming-Omni is a cutting-edge multimodal model designed to handle various types of data, including images, text, audio, and video. It utilizes dedicated encoders to extract information from these different modalities and employs a mixture of experts (MoE) architecture with modality-specific routers for efficient processing. This innovative design allows Ming-Omni to perform a wide range of tasks, such as generating speech and images, engaging in context-aware conversations, and editing images, all within a single framework. By being open-source and matching the capabilities of advanced models like GPT-4o, Ming-Omni aims to foster further research and development in the field of multimodal AI."
                },
                "zh": {
                    "title": "Ming-Omniï¼šç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆ",
                    "desc": "Ming-Omniæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å®ƒä½¿ç”¨ä¸“ç”¨ç¼–ç å™¨æå–ä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–°æå‡ºçš„æ¨¡æ€ç‰¹å®šè·¯ç”±å™¨è¿›è¡Œå¤„ç†ã€‚è¯¥æ¨¡å‹æ”¯æŒè¯­éŸ³å’Œå›¾åƒç”Ÿæˆï¼Œèƒ½å¤Ÿè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å’Œå¤šåŠŸèƒ½çš„å›¾åƒç¼–è¾‘ã€‚Ming-Omniæ˜¯é¦–ä¸ªå¼€æºæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ”¯æŒä¸Šä¸GPT-4oç›¸åª²ç¾ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08060",
            "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
            "url": "https://huggingface.co/papers/2506.08060",
            "abstract": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.",
            "score": 4,
            "issue_id": 4272,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "5c9cde8c4bcbdc6e",
            "authors": [
                "Asankhaya Sharma"
            ],
            "affiliations": [
                "Patched Codes, Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08060.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#rag",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Transformers: Fine-Tuning Efficiency through In-Context Learning",
                    "desc": "This paper explores how transformers can mimic the performance of supervised fine-tuning (SFT) through a method called in-context learning (ICL) without changing the model's parameters. It provides theoretical proofs that under certain ideal conditions, a base transformer can achieve results similar to those obtained through SFT. The authors extend their findings to practical situations, showing that smaller datasets can still approximate fine-tuned behavior effectively. This research highlights the potential for more efficient use of large language models in real-world applications by leveraging retrieval-augmented generation techniques."
                },
                "zh": {
                    "title": "å˜æ¢å™¨æ¨¡å‹ï¼šé«˜æ•ˆè¿‘ä¼¼ç›‘ç£å¾®è°ƒçš„æœªæ¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å˜æ¢å™¨æ¨¡å‹å¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨ä¸æ”¹å˜æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œè¿‘ä¼¼ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå˜æ¢å™¨æ¨¡å‹å¯ä»¥åˆ©ç”¨æ¨ç†æ—¶çš„æŠ€æœ¯æ¥æ¨¡æ‹ŸSFTçš„æ•ˆæœã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†è¿™äº›ç»“æœåˆ°å®é™…åœºæ™¯ï¼Œè€ƒè™‘æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œéƒ¨åˆ†æ•°æ®é›†è®¿é—®ã€‚é€šè¿‡ç†è®ºè¯æ˜ï¼Œè¿™ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„èµ„æºé«˜æ•ˆéƒ¨ç½²æä¾›äº†åŸºç¡€ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆç­‰å®ç”¨æŠ€æœ¯ï¼Œå°†ç†è®ºä¸å®é™…åº”ç”¨ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10910",
            "title": "Magistral",
            "url": "https://huggingface.co/papers/2506.10910",
            "abstract": "A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.",
            "score": 3,
            "issue_id": 4276,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "59d3abdb994da001",
            "authors": [
                "Mistral-AI",
                ":",
                "Abhinav Rastogi",
                "Albert Q. Jiang",
                "Andy Lo",
                "Gabrielle Berrada",
                "Guillaume Lample",
                "Jason Rute",
                "Joep Barmentlo",
                "Karmesh Yadav",
                "Kartik Khandelwal",
                "Khyathi Raghavi Chandu",
                "LÃ©onard Blier",
                "Lucile Saulnier",
                "Matthieu Dinot",
                "Maxime Darrin",
                "Neha Gupta",
                "Roman Soletskyi",
                "Sagar Vaze",
                "Teven Le Scao",
                "Yihan Wang",
                "Adam Yang",
                "Alexander H. Liu",
                "Alexandre Sablayrolles",
                "AmÃ©lie HÃ©liou",
                "AmÃ©lie Martin",
                "Andy Ehrenberg",
                "Anmol Agarwal",
                "Antoine Roux",
                "Arthur Darcet",
                "Arthur Mensch",
                "Baptiste Bout",
                "Baptiste RoziÃ¨re",
                "Baudouin De Monicault",
                "Chris Bamford",
                "Christian Wallenwein",
                "Christophe Renaudin",
                "ClÃ©mence Lanfranchi",
                "Darius Dabert",
                "Devon Mizelle",
                "Diego de las Casas",
                "Elliot Chane-Sane",
                "Emilien Fugier",
                "Emma Bou Hanna",
                "Gauthier Delerce",
                "Gauthier Guinet",
                "Georgii Novikov",
                "Guillaume Martin",
                "Himanshu Jaju",
                "Jan Ludziejewski",
                "Jean-Hadrien Chabran",
                "Jean-Malo Delignon",
                "Joachim Studnia",
                "Jonas Amar",
                "Josselin Somerville Roberts",
                "Julien Denize",
                "Karan Saxena",
                "Kush Jain",
                "Lingxiao Zhao",
                "Louis Martin",
                "Luyu Gao",
                "LÃ©lio Renard Lavaud",
                "Marie Pellat",
                "Mathilde Guillaumin",
                "Mathis Felardos",
                "Maximilian Augustin",
                "MickaÃ«l Seznec",
                "Nikhil Raghuraman",
                "Olivier Duchenne",
                "Patricia Wang",
                "Patrick von Platen",
                "Patryk Saffer",
                "Paul Jacob",
                "Paul Wambergue",
                "Paula Kurylowicz",
                "Pavankumar Reddy Muddireddy",
                "PhilomÃ¨ne Chagniot",
                "Pierre Stock",
                "Pravesh Agrawal",
                "Romain Sauvestre",
                "RÃ©mi Delacourt",
                "Sanchit Gandhi",
                "Sandeep Subramanian",
                "Shashwat Dalal",
                "Siddharth Gandhi",
                "Soham Ghosh",
                "Srijan Mishra",
                "Sumukh Aithal",
                "Szymon Antoniak",
                "Thibault Schueller",
                "Thibaut Lavril",
                "Thomas Robert",
                "Thomas Wang",
                "TimothÃ©e Lacroix",
                "Valeriia Nemychnikova",
                "Victor Paltz",
                "Virgile Richard",
                "Wen-Ding Li",
                "William Marshall",
                "Xuanyu Zhang",
                "Yunhao Tang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.10910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Magistral - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RL Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Magistral Small Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Magistral Medium Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with Pure Reinforcement Learning",
                    "desc": "This paper presents Magistral, a new reasoning model developed using a scalable reinforcement learning (RL) pipeline. The authors emphasize a novel approach that does not depend on previous implementations or RL traces from other models, focusing instead on their own infrastructure. They demonstrate that training with pure RL on text data can preserve or enhance capabilities in multimodal understanding, instruction following, and function calling. Additionally, they introduce Magistral Medium, which is built on Mistral Medium 3, and make Magistral Small available as open-source software."
                },
                "zh": {
                    "title": "å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œæå‡æ¨ç†æ¨¡å‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Magistralï¼Œè¿™æ˜¯Mistralçš„ç¬¬ä¸€ä¸ªæ¨ç†æ¨¡å‹ï¼Œä»¥åŠæˆ‘ä»¬è‡ªå·±çš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®¡é“ã€‚æˆ‘ä»¬é‡‡ç”¨è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼Œå®Œå…¨ä¾èµ–äºè‡ªå·±çš„æ¨¡å‹å’ŒåŸºç¡€è®¾æ–½ï¼Œè€Œä¸æ˜¯ç°æœ‰çš„å®ç°å’Œä»å…ˆå‰æ¨¡å‹ä¸­æå–çš„RLè½¨è¿¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œçº¯æ–‡æœ¬æ•°æ®çš„RLè®­ç»ƒèƒ½å¤Ÿä¿æŒæˆ–æ”¹å–„å¤šæ¨¡æ€ç†è§£ã€æŒ‡ä»¤è·Ÿéšå’ŒåŠŸèƒ½è°ƒç”¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Magistral Mediumå’Œå¼€æºçš„Magistral Smallï¼Œè¿›ä¸€æ­¥æ”¯æŒæ¨ç†è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10357",
            "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
            "url": "https://huggingface.co/papers/2506.10357",
            "abstract": "Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/",
            "score": 3,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "145045d8e634c76b",
            "authors": [
                "Zaijing Li",
                "Yuquan Xie",
                "Rui Shao",
                "Gongwei Chen",
                "Weili Guan",
                "Dongmei Jiang",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Peng Cheng Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10357.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#agents",
                    "#rag",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Optimus-3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Minecraft",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Optimus-3 - Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ Minecraft, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Optimus-3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Minecraft. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Optimus-3: Mastering Minecraft with Advanced AI Techniques",
                    "desc": "Optimus-3 is a general-purpose agent designed for the open-world environment of Minecraft, utilizing advanced techniques in machine learning. It incorporates a knowledge-enhanced data generation pipeline to create high-quality training data, addressing the challenge of insufficient domain-specific data. The agent employs a Mixture-of-Experts (MoE) architecture to effectively manage interference among diverse tasks, allowing for better performance. Additionally, it uses Multimodal Reasoning-Augmented Reinforcement Learning to improve its reasoning capabilities, enabling it to handle the visual diversity present in Minecraft."
                },
                "zh": {
                    "title": "Optimus-3ï¼šåœ¨Minecraftä¸­è¶…è¶Šæé™çš„æ™ºèƒ½ä½“",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Optimus-3ï¼Œä¸€ä¸ªåˆ©ç”¨çŸ¥è¯†å¢å¼ºæ•°æ®ç”Ÿæˆã€ä¸“å®¶æ··åˆè·¯ç”±å’Œå¤šæ¨¡æ€æ¨ç†å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“ã€‚è¯¥æ™ºèƒ½ä½“åœ¨Minecraftç­‰å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè§£å†³äº†é¢†åŸŸç‰¹å®šæ•°æ®ä¸è¶³ã€å¼‚æ„ä»»åŠ¡å¹²æ‰°å’Œè§†è§‰å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†å¢å¼ºçš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»¥æä¾›å¯æ‰©å±•çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥äº†ä»»åŠ¡çº§è·¯ç”±çš„ä¸“å®¶æ··åˆæ¶æ„æ¥å‡è½»ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šæ¨¡æ€æ¨ç†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æå‡æ™ºèƒ½ä½“åœ¨è§†è§‰å¤šæ ·æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09942",
            "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
            "url": "https://huggingface.co/papers/2506.09942",
            "abstract": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.",
            "score": 3,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "5430c32ec46dccaa",
            "authors": [
                "Hao Peng",
                "Yunjia Qi",
                "Xiaozhi Wang",
                "Bin Xu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09942.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "VerIF: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RL Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerIF - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VerInstruct Ñ Ğ¾ĞºĞ¾Ğ»Ğ¾ 22 000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ RL Ñ VerIF Ğº Ğ´Ğ²ÑƒĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ RL Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "VerIF: Boosting Instruction-Following RL with Hybrid Verification",
                    "desc": "This paper introduces VerIF, a novel hybrid verification method that merges rule-based and large language model (LLM) approaches to improve reinforcement learning (RL) in instruction-following tasks. The authors highlight the importance of verification engineering in enhancing LLMs through reinforcement learning with verifiable rewards (RLVR). They present a new dataset, VerInstruct, which contains around 22,000 instruction-following instances with verification signals to support their method. The results show that models trained with VerIF achieve state-of-the-art performance and maintain strong generalization capabilities, indicating that this approach can effectively enhance existing RL frameworks."
                },
                "zh": {
                    "title": "VerIFï¼šæå‡æŒ‡ä»¤è·Ÿéšçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerIFçš„æ··åˆéªŒè¯æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºè§„åˆ™çš„éªŒè¯å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éªŒè¯ï¼Œæ˜¾è‘—æå‡äº†æŒ‡ä»¤è·Ÿéšçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤è·Ÿéšæ•°æ®é›†VerInstructï¼ŒåŒ…å«çº¦22,000ä¸ªå®ä¾‹åŠå…¶éªŒè¯ä¿¡å·ï¼Œä»¥æ”¯æŒè¿™ä¸€æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨VerIFè¿›è¡ŒRLè®­ç»ƒï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»£è¡¨æ€§çš„æŒ‡ä»¤è·ŸéšåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶ä¸”å¯¹æœªè§çº¦æŸå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒVerIFå¯ä»¥ä¸ç°æœ‰çš„RLæ–¹æ³•ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10953",
            "title": "Build the web for agents, not agents for the web",
            "url": "https://huggingface.co/papers/2506.10953",
            "abstract": "A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.",
            "score": 2,
            "issue_id": 4274,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "77dd06867c379745",
            "authors": [
                "Xing Han LÃ¹",
                "Gaurav Kamath",
                "Marius Mosbach",
                "Siva Reddy"
            ],
            "affiliations": [
                "Equal Advising",
                "McGill University",
                "Mila",
                "Quebec AI Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10953.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ²Ğ¾Ğ´Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² (AWI). AWI Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° AWI, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ ML-ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Redefining Web Interaction for AI Agents with AWIs",
                    "desc": "This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community."
                },
                "zh": {
                    "title": "ä¸ºä»£ç†è®¾è®¡ä¼˜åŒ–ç½‘ç»œäº¤äº’ç•Œé¢",
                    "desc": "æœ¬æ–‡æå‡ºäº†ç½‘ç»œä»£ç†ç ”ç©¶çš„èŒƒå¼è½¬å˜ï¼Œå€¡å¯¼å¼€å‘ä»£ç†ç½‘ç»œæ¥å£ï¼ˆAWIï¼‰ï¼Œä»¥ä¼˜åŒ–äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç½‘ç»œç¯å¢ƒä¸­çš„äº¤äº’ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¼€å‘èƒ½å¤Ÿè‡ªä¸»å¯¼èˆªå’Œå®Œæˆä»»åŠ¡çš„ç½‘ç»œä»£ç†å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´ç€äººç±»è®¾è®¡çš„ç•Œé¢ä¸LLMèƒ½åŠ›ä¹‹é—´çš„æ ¹æœ¬ä¸åŒ¹é…é—®é¢˜ï¼Œå¯¼è‡´å¤„ç†å¤æ‚ç½‘ç»œè¾“å…¥æ—¶çš„å›°éš¾ã€‚æœ¬æ–‡æå‡ºçš„AWIæ¦‚å¿µæ—¨åœ¨ä¸ºä»£ç†è®¾è®¡ä¸“é—¨çš„äº¤äº’ç•Œé¢ï¼Œä»¥æé«˜å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ ‡å‡†åŒ–ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆã€å¯é å’Œé€æ˜çš„ç½‘ç»œä»£ç†è®¾è®¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06694",
            "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
            "url": "https://huggingface.co/papers/2506.06694",
            "abstract": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.",
            "score": 2,
            "issue_id": 4275,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ½Ñ",
                "en": "June 7",
                "zh": "6æœˆ7æ—¥"
            },
            "hash": "f878d8e46d64a439",
            "authors": [
                "Yuan Yuan",
                "Yukun Liu",
                "Chonghua Han",
                "Jie Feng",
                "Yong Li"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06694.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#data",
                    "#training",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸš¶",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "MoveGCL - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. MoveGCL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MoveGCL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Mobility Models with Privacy-Preserving Learning",
                    "desc": "MoveGCL is a framework designed to train mobility foundation models while ensuring data privacy. It uses generative continual learning to create synthetic data from a teacher model, allowing for model updates without sharing sensitive raw data. The framework employs a Mixture-of-Experts Transformer to adapt to various mobility patterns and includes strategies to prevent catastrophic forgetting during training. Experiments show that MoveGCL performs well compared to traditional methods while maintaining strong privacy protections."
                },
                "zh": {
                    "title": "MoveGCLï¼šéšç§ä¿æŠ¤çš„ç§»åŠ¨åŸºç¡€æ¨¡å‹è®­ç»ƒæ¡†æ¶",
                    "desc": "MoveGCLæ˜¯ä¸€ä¸ªä¿æŠ¤éšç§çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç”ŸæˆæŒç»­å­¦ä¹ å’Œæ··åˆä¸“å®¶Transformeræ¥è®­ç»ƒç§»åŠ¨åŸºç¡€æ¨¡å‹ï¼Œè€Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ”¾ä»å†»ç»“æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆè½¨è¿¹ï¼Œå®ç°å»ä¸­å¿ƒåŒ–å’Œæ¸è¿›å¼æ¨¡å‹æ¼”åŒ–ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„è’¸é¦ç­–ç•¥å¢å¼ºçŸ¥è¯†ä¿ç•™ï¼Œå‡å°‘ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†åº”å¯¹ç§»åŠ¨æ¨¡å¼çš„å¼‚è´¨æ€§ï¼ŒMoveGCLç»“åˆäº†ç§»åŠ¨æ„ŸçŸ¥çš„ä¸“å®¶è·¯ç”±æœºåˆ¶å’Œé€å±‚æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œä»¥ç¨³å®šæŒç»­æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoveGCLåœ¨å…­ä¸ªçœŸå®åŸå¸‚æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸è”åˆè®­ç»ƒç›¸å½“ï¼Œæ˜¾è‘—ä¼˜äºè”é‚¦å­¦ä¹ åŸºçº¿ï¼ŒåŒæ—¶æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿æŠ¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10036",
            "title": "Token Perturbation Guidance for Diffusion Models",
            "url": "https://huggingface.co/papers/2506.10036",
            "abstract": "Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance",
            "score": 1,
            "issue_id": 4277,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "3637011ee12cd77a",
            "authors": [
                "Javad Rajabi",
                "Soroush Mehraban",
                "Seyedmorteza Sadat",
                "Babak Taati"
            ],
            "affiliations": [
                "ETH ZÃ¼rich",
                "KITE Research Institute",
                "University of Toronto",
                "Vector Institute for Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10036.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Token Perturbation Guidance (TPG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TPG Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ² ÑĞµÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Classifier-free guidance (CFG). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDXL Ğ¸ Stable Diffusion 2.1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ FID Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Token Perturbation Guidance",
                    "desc": "Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts."
                },
                "zh": {
                    "title": "ä»¤ç‰Œæ‰°åŠ¨å¼•å¯¼ï¼šæ— è®­ç»ƒçš„ç”Ÿæˆè´¨é‡æå‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºä»¤ç‰Œæ‰°åŠ¨å¼•å¯¼ï¼ˆTPGï¼‰ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚TPGé€šè¿‡å¯¹æ‰©æ•£ç½‘ç»œä¸­é—´ä»¤ç‰Œè¡¨ç¤ºæ–½åŠ æ‰°åŠ¨çŸ©é˜µï¼Œæä¾›æœ‰æ•ˆä¸”ç¨³å®šçš„å¼•å¯¼ä¿¡å·ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ç›¸æ¯”ï¼ŒTPGåœ¨æ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘åˆ†ç±»å™¨æ— å…³å¼•å¯¼çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPGåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸”é€‚ç”¨äºæ¡ä»¶å’Œæ— æ¡ä»¶ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08373",
            "title": "Draft-based Approximate Inference for LLMs",
            "url": "https://huggingface.co/papers/2506.08373",
            "abstract": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "02a9a3f798ba1509",
            "authors": [
                "Kevin Galim",
                "Ethan Ewer",
                "Wonjun Kang",
                "Minjae Lee",
                "Hyung Il Koo",
                "Kangwook Lee"
            ],
            "affiliations": [
                "Ajou University",
                "FuriosaAI",
                "Seoul National University",
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08373.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ñ€ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: SpecKV Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ SpecPC Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Inference with Draft Models for Efficiency and Accuracy",
                    "desc": "This paper introduces a new framework that uses draft models to enhance approximate inference in long-context Large Language Models (LLMs). By accurately predicting the importance of tokens and key-value (KV) pairs, the framework improves the accuracy of LLMs while keeping memory and computational efficiency in check. The authors present two specific implementations: SpecKV for effective KV cache dropping and SpecPC for identifying unimportant prompt tokens. Their experiments demonstrate that this approach outperforms existing methods in accuracy while maintaining low resource usage."
                },
                "zh": {
                    "title": "åˆ©ç”¨è‰ç¨¿æ¨¡å‹æå‡é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨è‰ç¨¿æ¨¡å‹æ¥å¢å¼ºé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘ä¼¼æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°é¢„æµ‹ä»¤ç‰Œå’Œé”®å€¼å¯¹çš„é‡è¦æ€§ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…·ä½“å®ç°ï¼šSpecKVå’ŒSpecPCï¼Œåˆ†åˆ«ç”¨äºä¼˜åŒ–é”®å€¼ç¼“å­˜å’Œæç¤ºä»¤ç‰Œçš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€å†…å­˜ä½¿ç”¨ã€å»¶è¿Ÿå’Œååé‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07795",
            "title": "LLM Unlearning Should Be Form-Independent",
            "url": "https://huggingface.co/papers/2506.07795",
            "abstract": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.",
            "score": 1,
            "issue_id": 4278,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "7314028832938fb2",
            "authors": [
                "Xiaotian Ye",
                "Mengqi Zhang",
                "Shu Wu"
            ],
            "affiliations": [
                "New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences",
                "School of Computer Science, Beijing University of Posts and Telecommunications",
                "Shandong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07795.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06950",
            "title": "What Makes a Good Natural Language Prompt?",
            "url": "https://huggingface.co/papers/2506.06950",
            "abstract": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.",
            "score": 1,
            "issue_id": 4275,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ½Ñ",
                "en": "June 7",
                "zh": "6æœˆ7æ—¥"
            },
            "hash": "265555e63c6771ba",
            "authors": [
                "Do Xuan Long",
                "Duy Dinh",
                "Ngoc-Hai Nguyen",
                "Kenji Kawaguchi",
                "Nancy F. Chen",
                "Shafiq Joty",
                "Min-Yen Kan"
            ],
            "affiliations": [
                "Institute for Infocomm Research (I2R), A*STAR",
                "National University of Singapore",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06950.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#survey",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 150 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ 21 ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 6 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing Prompts for Smarter AI Reasoning",
                    "desc": "This paper presents a framework for evaluating and optimizing natural language prompts used in large language models (LLMs). It identifies 21 properties of prompts, organized into six dimensions, that influence their effectiveness in reasoning tasks. The authors conducted a meta-analysis of over 150 studies to highlight the inconsistencies in how prompt quality is assessed across different models and tasks. Their findings suggest that enhancing prompts based on specific properties can significantly improve LLM performance, particularly through instruction-tuning techniques."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æç¤ºï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡ªç„¶è¯­è¨€æç¤ºçš„æ¡†æ¶ï¼Œæ­ç¤ºäº†æç¤ºå±æ€§ä¸æ¨ç†ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¯¹2022è‡³2025å¹´é—´150å¤šç¯‡ä¸æç¤ºç›¸å…³çš„è®ºæ–‡è¿›è¡Œå…ƒåˆ†æï¼Œæ¢è®¨äº†è‡ªç„¶è¯­è¨€æç¤ºçš„é‡åŒ–æ ‡å‡†ã€‚è¯¥æ¡†æ¶åŒ…å«21ä¸ªå±æ€§ï¼Œåˆ†ä¸ºå…­ä¸ªç»´åº¦ï¼Œæ—¨åœ¨è¯„ä¼°æç¤ºè´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œå•ä¸€å±æ€§çš„å¢å¼ºå¯¹æ¨ç†ä»»åŠ¡çš„å½±å“æœ€å¤§ï¼Œè€ŒåŸºäºå±æ€§å¢å¼ºçš„æŒ‡ä»¤è°ƒä¼˜å¯ä»¥æå‡æ¨ç†æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06561",
            "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
            "url": "https://huggingface.co/papers/2506.06561",
            "abstract": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "12942392f309be51",
            "authors": [
                "Ho Yin 'Sam' Ng",
                "Ting-Yao Hsu",
                "Aashish Anantha Ramakrishnan",
                "Branislav Kveton",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Dongwon Lee",
                "Tong Yu",
                "Sungchul Kim",
                "Ryan A. Rossi",
                "Ting-Hao 'Kenneth' Huang"
            ],
            "affiliations": [
                "Adobe Research",
                "Pennsylvania State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06561.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´",
                    "desc": "LaMP-Cap Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğº Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹, Ñ‡ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ñ‹, ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Personalized Captions Through Multimodal Contexts",
                    "desc": "LaMP-Cap is a new dataset designed to enhance the generation of personalized figure captions by utilizing multimodal profiles. It provides not only the target figure images but also additional contextual figures and their associated captions and paragraphs. This approach allows AI models to create captions that better reflect the author's style and the specific domain. Experiments demonstrate that incorporating profile information, especially images, significantly improves the quality of AI-generated captions compared to traditional text-only methods."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "LaMP-Capæ˜¯ä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€èµ„æ–™æé«˜AIç”Ÿæˆæ ‡é¢˜çš„è´¨é‡ã€‚å›¾å½¢æ ‡é¢˜å¯¹äºå¸®åŠ©è¯»è€…ç†è§£å’Œè®°ä½å›¾å½¢çš„å…³é”®ä¿¡æ¯è‡³å…³é‡è¦ã€‚å°½ç®¡å·²æœ‰è®¸å¤šæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿™äº›æ ‡é¢˜ï¼Œä½†ä½œè€…é€šå¸¸éœ€è¦ä¿®æ”¹é€šç”¨çš„AIç”Ÿæˆæ ‡é¢˜ä»¥åŒ¹é…ä»–ä»¬çš„å†™ä½œé£æ ¼ã€‚LaMP-Capæä¾›äº†å›¾åƒå’Œç›¸å…³å›¾å½¢çš„ä¸Šä¸‹æ–‡èµ„æ–™ï¼Œå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›å¤šæ¨¡æ€èµ„æ–™å¯ä»¥ç”Ÿæˆæ›´æ¥è¿‘ä½œè€…åŸå§‹å†™ä½œçš„æ ‡é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05982",
            "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
            "url": "https://huggingface.co/papers/2506.05982",
            "abstract": "MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "6cf7938ff751b2ff",
            "authors": [
                "Zonglin Wu",
                "Yule Xue",
                "Xin Wei",
                "Yiren Song"
            ],
            "affiliations": [
                "National University of Singapore",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05982.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#security",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA",
                    "desc": "MCA-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ CAPTCHA Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. MCA-Bench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ CAPTCHA, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MCA-Bench ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… CAPTCHA Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Strengthening CAPTCHA Security with MCA-Bench",
                    "desc": "MCA-Bench is a new tool designed to evaluate the security of different types of CAPTCHAs against automated attacks. It combines various CAPTCHA formats, such as text, images, and interactive puzzles, into one comprehensive testing framework. By using a shared vision-language model, it fine-tunes specific agents to crack each type of CAPTCHA, allowing for consistent comparisons across different modalities. The results provide insights into how the complexity and interaction of CAPTCHAs affect their vulnerability, helping to improve their design and security."
                },
                "zh": {
                    "title": "MCA-Benchï¼šCAPTCHAå®‰å…¨è¯„ä¼°çš„æ–°åŸºå‡†",
                    "desc": "MCA-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°CAPTCHAçš„å®‰å…¨æ€§ã€‚å®ƒé€šè¿‡å…±äº«çš„è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒä¸“é—¨çš„ç ´è§£ä»£ç†ï¼Œä»¥ä¾¿å¯¹ä¸åŒç±»å‹çš„CAPTCHAè¿›è¡Œä¸€è‡´çš„è¯„ä¼°ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†ç°æœ‰CAPTCHAè¯„ä¼°ä¸­ç¼ºä¹ç»Ÿä¸€å¤§è§„æ¨¡åŸºå‡†çš„ç©ºç™½ï¼Œæä¾›äº†å¯¹ç°ä»£CAPTCHAè®¾è®¡è„†å¼±æ€§çš„å®šé‡åˆ†æã€‚åŸºäºå®éªŒç»“æœï¼Œæå‡ºäº†ä¸‰ä¸ªå¯è¡Œçš„è®¾è®¡åŸåˆ™ï¼Œå¹¶è¯†åˆ«äº†å…³é”®çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºCAPTCHAçš„ç³»ç»Ÿæ€§å¼ºåŒ–å’Œå…¬å¹³åŸºå‡†æµ‹è¯•å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10378",
            "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
            "url": "https://huggingface.co/papers/2506.10378",
            "abstract": "The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.",
            "score": 0,
            "issue_id": 4275,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "50915ea9038be86d",
            "authors": [
                "Jikai Jin",
                "Vasilis Syrgkanis",
                "Sham Kakade",
                "Hanlin Zhang"
            ],
            "affiliations": [
                "Harvard University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10378.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑƒĞ·Ğ»Ğ¾Ğ²ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‰ÑƒÑ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ‚ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Uncovering Causal Relationships in Language Model Performance",
                    "desc": "This paper introduces a causal representation learning framework designed to assess the capabilities of language models by examining latent factors. It highlights the necessity of controlling for variations in base models to accurately identify causal relationships. The authors analyze a dataset of over 1500 models across six benchmarks, revealing a three-node linear causal structure that explains performance differences. Their findings emphasize the importance of understanding the causal pathways from general problem-solving to specific abilities like instruction-following and mathematical reasoning."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹èƒ½åŠ›çš„å› æœå…³ç³»",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé€šè¿‡æ½œåœ¨å› ç´ è¯„ä¼°è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼ºè°ƒæ§åˆ¶åŸºç¡€æ¨¡å‹å˜å¼‚çš„é‡è¦æ€§ï¼Œä»¥æ­ç¤ºæ½œåœ¨çš„å› æœå…³ç³»ã€‚é€šè¿‡å¯¹è¶…è¿‡1500ä¸ªæ¨¡å‹åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸€ä¸ªç®€æ´çš„ä¸‰èŠ‚ç‚¹çº¿æ€§å› æœç»“æ„ï¼Œèƒ½å¤Ÿå¯é åœ°è§£é‡Šè§‚å¯Ÿåˆ°çš„æ€§èƒ½å˜åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä»”ç»†æ§åˆ¶åŸºç¡€æ¨¡å‹çš„å˜å¼‚æ˜¯æ­ç¤ºæ½œåœ¨æ¨¡å‹èƒ½åŠ›ä¹‹é—´å› æœå…³ç³»çš„å…³é”®æ­¥éª¤ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-12.html",
    "link_next": "2025-06-16.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "16.06",
        "en": "06/16",
        "zh": "6æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 4,
        "#benchmark": 13,
        "#agents": 7,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 14,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 2,
        "#science": 3,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Seedance 1.0ï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®ƒç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€é«˜æ•ˆçš„æ¶æ„è®¾è®¡ã€è®­ç»ƒåä¼˜åŒ–å’Œæ¨¡å‹åŠ é€Ÿã€‚Seedance 1.0 èƒ½å¤Ÿåœ¨1080påˆ†è¾¨ç‡ä¸‹ç”Ÿæˆ5ç§’çš„è§†é¢‘ï¼Œåªéœ€41.4ç§’ã€‚ä¸å…¶ä»–é¡¶å°–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒSeedance 1.0 åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ—¶ç©ºæµç•…æ€§å’Œç»“æ„ç¨³å®šæ€§ã€‚",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Seedance 1.0ï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®ƒç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€é«˜æ•ˆçš„æ¶æ„è®¾è®¡ã€è®­ç»ƒåä¼˜åŒ–å’Œæ¨¡å‹åŠ é€Ÿã€‚Seedance 1.0 èƒ½å¤Ÿåœ¨1080påˆ†è¾¨ç‡ä¸‹ç”Ÿæˆ5ç§’çš„è§†é¢‘ï¼Œåªéœ€41.4ç§’ã€‚ä¸å…¶ä»–é¡¶å°–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒSeedance 1.0 åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ—¶ç©ºæµç•…æ€§å’Œç»“æ„ç¨³å®šæ€§ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le Seedance 1.0, yÄ«gÃ¨ gÄo xÃ¬ngnÃ©ng de shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng. TÄ jiÃ©hÃ© le xiÄnjÃ¬n de shÃ¹jÃ¹ zhÄ›nglÇ, gÄoxiÃ o de jiÃ gÃ²u shÃ¨jÃ¬, xÃ¹nliÃ n hÃ²u yÅuhuÃ  hÃ© mÃ³xÃ­ng jiÄsÃ¹. Seedance 1.0 nÃ©nggÃ²u zÃ i 1080p fÄ“nbiÄnlÇœ xiÃ  shÄ“ngchÃ©ng 5 miÇo de shÃ¬pÃ­n, zhÇ xÅ« 41.4 miÇo. YÇ” qÃ­tÄ dÇngjiÄn de shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng xiÄngbÇ, Seedance 1.0 zÃ i zhÃ¬liÃ ng hÃ© sÃ¹dÃ¹ shÃ ng biÇoxiÃ n chÅ«sÃ¨, jÃ¹yÇ’u yÅuyuÃ¨ de shÃ­kÅng liÃºchÃ ngxÃ¬ng hÃ© jiÃ©gÃ²u wÄ›ndÃ¬ngxÃ¬ng.",
        "vocab": "[\n    {\"word\": \"Seedance\", \"pinyin\": \"SÄ«dÃ nsÃ¬\", \"trans\": \"Seedance\"},\n    {\"word\": \"é«˜æ€§èƒ½\", \"pinyin\": \"gÄo xÃ¬ngnÃ©ng\", \"trans\": \"high performance\"},\n    {\"word\": \"è§†é¢‘ç”Ÿæˆæ¨¡å‹\", \"pinyin\": \"shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng\", \"trans\": \"video generation model\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄnjÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"æ•°æ®æ•´ç†\", \"pinyin\": \"shÃ¹jÃ¹ zhÄ›nglÇ\", \"trans\": \"data processing\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ¶æ„è®¾è®¡\", \"pinyin\": \"jiÃ gÃ²u shÃ¨jÃ¬\", \"trans\": \"architecture design\"},\n    {\"word\": \"è®­ç»ƒåä¼˜åŒ–\", \"pinyin\": \"xÃ¹nliÃ n hÃ²u yÅuhuÃ \", \"trans\": \"post-training optimization\"},\n    {\"word\": \"æ¨¡å‹åŠ é€Ÿ\", \"pinyin\": \"mÃ³xÃ­ng jiÄsÃ¹\", \"trans\": \"model acceleration\"},\n    {\"word\": \"1080påˆ†è¾¨ç‡\", \"pinyin\": \"1080p fÄ“nbiÄnlÇœ\", \"trans\": \"1080p resolution\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"åªéœ€\", \"pinyin\": \"zhÇ xÅ«\", \"trans\": \"only need\"},\n    {\"word\": \"é¡¶å°–\", \"pinyin\": \"dÇngjiÄn\", \"trans\": \"top-notch\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"é€Ÿåº¦\", \"pinyin\": \"sÃ¹dÃ¹\", \"trans\": \"speed\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ä¼˜è¶Š\", \"pinyin\": \"yÅuyuÃ¨\", \"trans\": \"superior\"},\n    {\"word\": \"æ—¶ç©ºæµç•…æ€§\", \"pinyin\": \"shÃ­kÅng liÃºchÃ ngxÃ¬ng\", \"trans\": \"spatiotemporal smoothness\"},\n    {\"word\": \"ç»“æ„ç¨³å®šæ€§\", \"pinyin\": \"jiÃ©gÃ²u wÄ›ndÃ¬ngxÃ¬ng\", \"trans\": \"structural stability\"}\n]",
        "trans": "This article introduces Seedance 1.0, a high-performance video generation model. It combines advanced data curation, efficient architectural design, post-training optimization, and model acceleration. Seedance 1.0 can generate a 5-second video at 1080p resolution in just 41.4 seconds. Compared to other leading video generation models, Seedance 1.0 excels in both quality and speed, demonstrating superior spatiotemporal smoothness and structural stability.",
        "update_ts": "2025-06-12 11:10"
    }
}