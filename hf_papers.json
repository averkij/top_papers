{
    "date": {
        "ru": "4 декабря",
        "en": "December 4",
        "zh": "12月4日"
    },
    "time_utc": "2024-12-04 21:10",
    "weekday": 2,
    "issue_id": 952,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.02259",
            "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
            "url": "https://huggingface.co/papers/2412.02259",
            "abstract": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.",
            "score": 43,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "5a007f38be3e3ba7",
            "authors": [
                "Mingzhe Zheng",
                "Yongqi Xu",
                "Haojian Huang",
                "Xuran Ma",
                "Yexin Liu",
                "Wenjie Shu",
                "Yatian Pang",
                "Feilong Tang",
                "Qifeng Chen",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "Peking University",
                "University of Central Florida",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02259.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#story_generation",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VGoT: Новый уровень в генерации многокадровых видео с сохранением логики повествования",
                    "desc": "Статья представляет новый подход к генерации многокадровых видео под названием VideoGen-of-Thought (VGoT). Эта архитектура разделяет процесс на несколько этапов: генерация сценария, создание ключевых кадров, генерация отдельных сцен и механизм сглаживания. VGoT использует встраивания для сохранения идентичности персонажей между сценами и применяет межкадровое сглаживание для обеспечения визуальной согласованности. Эксперименты показывают, что VGoT превосходит существующие методы в создании качественных и связных многокадровых видео."
                },
                "en": {
                    "title": "Revolutionizing Multi-Shot Video Generation with VGoT",
                    "desc": "The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots."
                },
                "zh": {
                    "title": "多镜头视频生成的新突破",
                    "desc": "当前的视频生成模型在生成短片方面表现出色，但在创建多镜头、电影般的视频时仍然存在困难。现有模型通常只针对单镜头目标进行训练，因此在保持逻辑故事线和视觉一致性方面显得不足。为此，我们提出了VideoGen-of-Thought（VGoT），这是一种专门为多镜头视频生成设计的协作和无训练架构。VGoT通过脚本生成、关键帧生成和镜头级视频生成等模块化步骤，确保了合理的叙事设计和跨镜头的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19943",
            "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability",
            "url": "https://huggingface.co/papers/2411.19943",
            "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.",
            "score": 33,
            "issue_id": 933,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "aaf523f6bd9412e3",
            "authors": [
                "Zicheng Lin",
                "Tian Liang",
                "Jiahao Xu",
                "Xing Wang",
                "Ruilin Luo",
                "Chufan Shi",
                "Siheng Li",
                "Yujiu Yang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Tsinghua University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19943.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности рассуждений LLM путем выявления критических токенов",
                    "desc": "Исследование посвящено влиянию отдельных токенов на результаты рассуждений в больших языковых моделях (LLM). Авторы обнаружили существование 'критических токенов', которые приводят к неправильным траекториям рассуждений. Они предложили новый метод cDPO для автоматического распознавания и обучения с учетом критических токенов в процессе выравнивания модели. Экспериментальные результаты на бенчмарках GSM8K и MATH500 с использованием моделей Llama-3 и deepseek-math продемонстрировали эффективность предложенного подхода."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs by Identifying Critical Tokens",
                    "desc": "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."
                },
                "zh": {
                    "title": "识别关键token，提升推理准确性",
                    "desc": "大型语言模型（LLMs）在推理任务中表现出色，能够通过自回归的方式生成推理过程。本文探讨了单个token对推理任务最终结果的影响，发现存在“关键token”，这些token会导致错误的推理轨迹。我们提出了一种新方法cDPO，旨在自动识别关键token并在对齐过程中进行token级奖励。通过对比正负模型的生成可能性，我们能够识别出在错误轨迹中导致错误结果的关键token，从而提高模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01928",
            "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
            "url": "https://huggingface.co/papers/2412.01928",
            "abstract": "Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward \"Multi-agent LLM training\" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.",
            "score": 19,
            "issue_id": 943,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "980ef49924f6a484",
            "authors": [
                "Sumeet Ramesh Motwani",
                "Chandler Smith",
                "Rocktim Jyoti Das",
                "Markian Rybchuk",
                "Philip H. S. Torr",
                "Ivan Laptev",
                "Fabio Pizzati",
                "Ronald Clark",
                "Christian Schroeder de Witt"
            ],
            "affiliations": [
                "University of Oxford",
                "Cooperative AI Foundation",
                "MBZUAI",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01928.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#synthetic",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Совместное обучение LLM для решения сложных задач рассуждения",
                    "desc": "Статья представляет новый подход к обучению больших языковых моделей (LLM) для совместной работы над сложными задачами рассуждения. Авторы предлагают метод 'Мульти-агентного обучения LLM' (MALT), использующий последовательную setup с гетерогенными LLM в специализированных ролях: генератор, верификатор и модель уточнения. Процесс включает генерацию синтетических данных на основе расширения траекторий и стратегию распределения кредита, управляемую совместными результатами. Эксперименты показывают значительные улучшения производительности на задачах математического и здравого рассуждения по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "Unlocking Collaborative Intelligence in LLMs",
                    "desc": "This paper introduces a novel approach called Multi-agent LLM training (MALT) aimed at enhancing collaboration among large language models (LLMs) for solving complex reasoning tasks. The authors propose a structured setup where different LLMs take on specialized roles—such as generator, verifier, and refinement model—to iteratively tackle problems. They implement a synthetic data generation process and a credit assignment strategy that rewards models based on their joint performance, allowing them to learn from both successful and unsuccessful attempts. The results show significant performance improvements on various reasoning benchmarks, highlighting the potential of cooperative multi-agent systems in advancing LLM capabilities."
                },
                "zh": {
                    "title": "多智能体协作训练，提升推理能力！",
                    "desc": "本文探讨了多智能体大语言模型（LLM）训练的潜力，旨在提高模型在复杂问题上的协作能力。我们提出了一种多智能体设置，模型分为生成器、验证器和精炼模型，协同解决推理问题。通过轨迹扩展的合成数据生成和基于联合结果的奖励策略，我们的模型能够自主提升各自的专业能力。实验结果表明，使用MALT方法的Llama 3.1 8B模型在数学和常识推理任务上取得了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01981",
            "title": "Free Process Rewards without Process Labels",
            "url": "https://huggingface.co/papers/2412.01981",
            "abstract": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.",
            "score": 18,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13434e4f301a0d88",
            "authors": [
                "Lifan Yuan",
                "Wendi Li",
                "Huayu Chen",
                "Ganqu Cui",
                "Ning Ding",
                "Kaiyan Zhang",
                "Bowen Zhou",
                "Zhiyuan Liu",
                "Hao Peng"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01981.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение PRM без пошаговой разметки",
                    "desc": "Статья представляет новый подход к обучению процессуальных моделей вознаграждения (PRM) в машинном обучении. Авторы показывают, что неявную PRM можно получить без дополнительных затрат, просто обучая модель вознаграждения за результат (ORM) на более дешевых метках уровня ответов. Эксперименты на датасете MATH демонстрируют, что предложенный метод превосходит сильный бейзлайн на основе MCTS, используя менее 1/38 обучающих данных. Исследование также выявляет, что масштабирование инструкций и ответов улучшает производительность неявной PRM."
                },
                "en": {
                    "title": "Unlocking Efficient Training for Process Reward Models",
                    "desc": "This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples."
                },
                "zh": {
                    "title": "隐式过程奖励模型：高效训练的新思路",
                    "desc": "本文提出了一种新的过程奖励模型（PRM），与传统的结果奖励模型（ORM）不同，PRM能够逐步评估推理过程，提供更细致的奖励。然而，训练PRM需要在每个中间步骤都有标注，这在数据收集上面临挑战。我们展示了通过训练ORM并使用响应级别的标签，可以在没有额外成本的情况下获得隐式PRM。实验结果表明，隐式PRM在数据效率和性能上优于传统方法，尤其是在数据稀缺的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02611",
            "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?",
            "url": "https://huggingface.co/papers/2412.02611",
            "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.",
            "score": 17,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "f63565048b4948b4",
            "authors": [
                "Kaixiong Gong",
                "Kaituo Feng",
                "Bohao Li",
                "Yibing Wang",
                "Mofan Cheng",
                "Shijia Yang",
                "Jiaming Han",
                "Benyou Wang",
                "Yutong Bai",
                "Zhuoran Yang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "CUHK MMLab",
                "Stanford University",
                "UC Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02611.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Слышат ли ИИ-модели то, что видят?",
                    "desc": "Статья представляет новый тест DeafTest и бенчмарк AV-Odyssey Bench для оценки мультимодальных больших языковых моделей (MLLM) в задачах аудио-визуального понимания. Исследователи обнаружили, что современные MLLM часто не справляются с простыми задачами определения громкости и высоты звуков. Бенчмарк включает 4,555 задач с текстом, изображениями и аудио, требующих эффективного использования обоих модальностей. Авторы провели тестирование ряда закрытых и открытых моделей, выявив ограничения текущих систем для дальнейшего улучшения сбора данных и разработки моделей."
                },
                "en": {
                    "title": "Unveiling the Limits of Multimodal Models with AV-Odyssey Bench",
                    "desc": "This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation."
                },
                "zh": {
                    "title": "揭示多模态模型的局限性",
                    "desc": "最近，多模态大型语言模型（MLLMs）如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其在视觉和音频方面的能力。尽管这些模型在多种音频-视觉应用中表现出色，但我们的DeafTest显示，MLLMs在一些人类认为简单的任务上常常表现不佳，例如判断两个声音哪个更响和哪个音调更高。为此，我们提出了AV-Odyssey Bench，这是一个全面的音频-视觉基准，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准包含4555个精心设计的问题，要求模型有效利用视觉和音频输入中的线索，以准确推断答案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02114",
            "title": "OmniCreator: Self-Supervised Unified Generation with Universal Editing",
            "url": "https://huggingface.co/papers/2412.02114",
            "abstract": "We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.",
            "score": 12,
            "issue_id": 939,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "62bf26709baf7f97",
            "authors": [
                "Haodong Chen",
                "Lan Wang",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "HKUST",
                "MSU",
                "UCF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02114.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Универсальный генератор и редактор медиа по текстовому запросу",
                    "desc": "OmniCreator - это новая система для генерации и редактирования изображений и видео на основе текстовых запросов. Она обучается самостоятельно, используя пары текст-видео, и устанавливает семантические связи между видео и текстом. OmniCreator может как генерировать новый контент по текстовому описанию, так и редактировать существующие видео и изображения без ограничений. Авторы также представили новый набор данных OmniBench-99 для оценки моделей генеративного редактирования видео."
                },
                "en": {
                    "title": "OmniCreator: Unified Text-Prompted Image and Video Generation and Editing",
                    "desc": "OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models."
                },
                "zh": {
                    "title": "OmniCreator：统一的图像与视频生成与编辑框架",
                    "desc": "OmniCreator是一个新颖的框架，能够在一个平台上进行文本提示的统一生成（图像+视频）和编辑。它通过自监督学习，利用原始的文本-视频对作为条件，同时使用相同的视频作为去噪目标，学习视频与文本之间的语义对应关系。在推理阶段，OmniCreator能够根据文本提示和视频生成忠实于两者的目标，实现无约束的通用编辑效果。我们还引入了OmniBench-99数据集，以全面评估生成视频编辑模型的性能，实验结果表明OmniCreator在所有模型中表现出显著的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19655",
            "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS",
            "url": "https://huggingface.co/papers/2411.19655",
            "abstract": "After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.",
            "score": 11,
            "issue_id": 947,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "d1255811f49c640e",
            "authors": [
                "Alessandro Scirè",
                "Andrei Stefan Bejgu",
                "Simone Tedeschi",
                "Karim Ghonim",
                "Federico Martelli",
                "Roberto Navigli"
            ],
            "affiliations": [
                "Babelscape, Italy",
                "Sapienza University of Rome"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19655.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#multimodal",
                    "#machine_translation",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "LLM-Oasis: Новый стандарт для оценки фактологической точности языковых моделей",
                    "desc": "Эта статья представляет LLM-Oasis - крупнейший ресурс для обучения оценщиков фактологической точности больших языковых моделей (LLM). LLM-Oasis создан путем извлечения утверждений из Википедии, фальсификации части этих утверждений и генерации пар фактических и нефактических текстов. Эксперименты показывают, что LLM-Oasis представляет значительную сложность для современных LLM, при этом GPT-4 достигает точности до 60% в предложенной задаче оценки фактологической точности. Ресурс имеет потенциал для стимулирования будущих исследований в этой области."
                },
                "en": {
                    "title": "LLM-Oasis: A New Frontier for Factuality Evaluation in Language Models",
                    "desc": "This paper discusses the advancements in Natural Language Generation (NLG) tasks due to Large Language Models (LLMs), while also addressing the issue of hallucinations, which are inaccuracies in generated content. It introduces LLM-Oasis, a comprehensive resource designed to train evaluators for assessing the factuality of LLM outputs. LLM-Oasis is created by extracting claims from Wikipedia, falsifying some, and generating pairs of factual and unfactual texts, validated by human annotators. The experiments show that LLM-Oasis poses a significant challenge to current LLMs, indicating its potential to enhance future research in factuality evaluation."
                },
                "zh": {
                    "title": "推动事实性评估的未来研究",
                    "desc": "本文介绍了大型语言模型（LLMs）在自然语言生成任务中的应用，尤其是在文本摘要和机器翻译方面的进展。然而，LLMs 仍然会产生虚假信息，即与事实不符的内容，因此评估 LLMs 的事实性变得非常重要。为了解决这一问题，本文提出了 LLM-Oasis，这是一个用于训练端到端事实性评估器的最大资源。通过从维基百科提取声明并生成真实与虚假的文本对，LLM-Oasis 为事实性评估系统的基准测试提供了重要的数据集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02592",
            "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2412.02592",
            "abstract": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench",
            "score": 8,
            "issue_id": 937,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "91dbac114744b1e9",
            "authors": [
                "Junyuan Zhang",
                "Qintong Zhang",
                "Bin Wang",
                "Linke Ouyang",
                "Zichen Wen",
                "Ying Li",
                "Ka-Ho Chow",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The University of HongKong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02592.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#rag",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "OHRBench: раскрывая влияние OCR на системы RAG",
                    "desc": "OHRBench - это первый бенчмарк для оценки влияния оптического распознавания символов (OCR) на системы генерации с извлечением информации (RAG). Он включает 350 неструктурированных PDF-документов из шести реальных областей применения RAG, а также вопросы и ответы, созданные на основе мультимодальных элементов документов. Исследователи выделяют два основных типа шума OCR: семантический и форматный, и применяют возмущения для создания структурированных данных с различной степенью каждого типа шума. Результаты показывают уязвимость систем RAG к ошибкам OCR и потенциал использования мультимодальных языковых моделей (VLM) без OCR в системах RAG."
                },
                "en": {
                    "title": "Enhancing RAG: Understanding OCR's Impact with OHRBench",
                    "desc": "This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems."
                },
                "zh": {
                    "title": "揭示OCR对RAG系统的影响",
                    "desc": "本论文介绍了OHRBench，这是第一个用于理解光学字符识别（OCR）对检索增强生成（RAG）系统影响的基准。研究发现，OCR在处理非结构化PDF文档时会引入语义噪声和格式噪声，导致知识库质量下降。通过对350个真实世界应用领域的文档进行评估，结果显示现有的OCR解决方案无法有效构建高质量的知识库。最后，论文探讨了在RAG系统中使用视觉语言模型（VLMs）而不依赖OCR的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02632",
            "title": "Scaling Image Tokenizers with Grouped Spherical Quantization",
            "url": "https://huggingface.co/papers/2412.02632",
            "abstract": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.",
            "score": 8,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "60eda94a31cded90",
            "authors": [
                "Jiangtao Wang",
                "Zhen Qin",
                "Yifan Zhang",
                "Vincent Tao Hu",
                "Björn Ommer",
                "Rania Briq",
                "Stefan Kesselheim"
            ],
            "affiliations": [
                "CompVis @ LMU Munich",
                "Jülich Supercomputing Centre",
                "TapTap",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02632.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#cv",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "GSQ: Эффективная токенизация изображений на сферической поверхности",
                    "desc": "В статье представлен новый метод токенизации изображений - Групповая Сферическая Квантизация (GSQ). GSQ использует сферическую инициализацию кодовой книги и регуляризацию поиска для ограничения латентного пространства кодовой книги сферической поверхностью. Авторы провели эмпирический анализ стратегий обучения токенизаторов изображений и показали, что GSQ-GAN превосходит современные методы по качеству реконструкции при меньшем количестве итераций обучения. Исследование масштабируемости GSQ выявило различное поведение при высоких и низких уровнях пространственного сжатия, подчеркивая сложности представления высокоразмерных латентных пространств."
                },
                "en": {
                    "title": "Efficient Image Processing with Grouped Spherical Quantization",
                    "desc": "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."
                },
                "zh": {
                    "title": "分组球面量化：高效的视觉标记器新方法",
                    "desc": "本文介绍了一种新的视觉标记器方法，称为分组球面量化（GSQ），旨在解决传统方法中的一些问题。GSQ通过球面代码本初始化和查找正则化，限制了代码本潜在空间在球面上的分布。我们的实证分析表明，GSQ-GAN在重建质量上优于现有的最先进方法，并且训练迭代次数更少。研究还系统地考察了GSQ在潜在维度、代码本大小和压缩比等方面的扩展行为，揭示了高维潜在空间表示的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01292",
            "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
            "url": "https://huggingface.co/papers/2412.01292",
            "abstract": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
            "score": 7,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "e8f8ddd05e13e9ef",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Junyan Li",
                "Shuailei Ma",
                "Xinyu Sun",
                "Tianhang Xiang",
                "Yinjie Lei",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "Northeastern University",
                "Pazhou Laboratory",
                "Sichuan University",
                "South China University of Technology",
                "Tencent Robotics X",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01292.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Умное понимание 3D-сцен с LSceneLLM",
                    "desc": "Исследование 3D Vision-Language Models (3D-VLMs) важно для развития воплощенного AI в 3D-сценах, таких как визуальная навигация и ответы на вопросы. Из-за высокой плотности визуальных признаков в больших 3D-сценах сложно точно определить важную для задачи информацию. Предлагается адаптивная структура LSceneLLM, которая автоматически определяет важные области, используя визуальные предпочтения LLM для разных задач. Эксперименты показывают, что наш метод превосходит существующие методы в понимании больших сцен и улучшает результаты при интеграции в существующие 3D-VLMs."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with LSceneLLM",
                    "desc": "This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods."
                },
                "zh": {
                    "title": "自适应3D视觉语言模型，提升场景理解能力",
                    "desc": "3D视觉语言模型（3D-VLMs）的研究越来越受到关注，这对在3D场景中发展具身人工智能至关重要，如视觉导航和具身问答。由于3D场景中视觉特征的高密度，准确定位与任务相关的视觉信息变得具有挑战性。现有方法尝试对所有对象进行分割，并将其特征视为场景表示，但这些与任务无关的对象特征包含大量冗余信息和缺失的细节。为了解决这些问题，我们提出了LSceneLLM，一个自适应框架，通过利用大语言模型（LLM）对不同任务的视觉偏好，自动识别与任务相关的区域，并通过可插拔的场景放大模块捕捉聚焦区域的细粒度细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02700",
            "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
            "url": "https://huggingface.co/papers/2412.02700",
            "abstract": "Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/",
            "score": 5,
            "issue_id": 949,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "6adffbc375f9f4f5",
            "authors": [
                "Daniel Geng",
                "Charles Herrmann",
                "Junhwa Hur",
                "Forrester Cole",
                "Serena Zhang",
                "Tobias Pfaff",
                "Tatiana Lopez-Guevara",
                "Carl Doersch",
                "Yusuf Aytar",
                "Michael Rubinstein",
                "Chen Sun",
                "Oliver Wang",
                "Andrew Owens",
                "Deqing Sun"
            ],
            "affiliations": [
                "Brown University",
                "Google DeepMind",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02700.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Управление движением открывает новые горизонты в генерации видео",
                    "desc": "Статья представляет новый подход к генерации видео с использованием управления движением. Авторы разработали модель, которая может генерировать видео на основе разреженных или плотных траекторий движения, называемых 'motion prompts'. Эта гибкая система позволяет контролировать движение камеры, объектов и общую динамику сцены. Модель демонстрирует впечатляющие результаты в различных приложениях, включая перенос движения и редактирование изображений, а также проявляет способность к реалистичной физике."
                },
                "en": {
                    "title": "Empowering Video Generation with Flexible Motion Prompts",
                    "desc": "This paper presents a novel approach to video generation by using motion prompts, which are flexible representations of motion trajectories. Unlike traditional models that rely solely on text prompts, this method allows for the encoding of various types of motion, including object-specific and global scene movements. The authors introduce a technique called motion prompt expansion, enabling users to convert high-level requests into detailed motion trajectories. The results indicate that this approach not only enhances video generation but also allows for realistic interactions and behaviors within the generated content."
                },
                "zh": {
                    "title": "运动提示：视频生成的新方式",
                    "desc": "本论文提出了一种新的视频生成模型，利用运动轨迹作为控制手段，克服了传统文本提示在动态动作和时间组合上的局限性。我们引入了一种灵活的运动提示表示，可以编码任意数量的轨迹，包括特定物体或全局场景的运动。用户可以直接指定稀疏轨迹，我们还展示了如何将高层次的用户请求转化为详细的半稀疏运动提示。通过多种应用展示了我们方法的多样性，包括相机和物体运动控制、与图像的交互、运动转移和图像编辑。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19542",
            "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
            "url": "https://huggingface.co/papers/2411.19542",
            "abstract": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.",
            "score": 5,
            "issue_id": 936,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "27226211eddf71d4",
            "authors": [
                "Luo Yu",
                "Liu Yucheng",
                "Shen Haihao"
            ],
            "affiliations": [
                "Intel Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19542.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение инференса ИИ на гибридных CPU: балансировка для максимальной производительности",
                    "desc": "Статья представляет динамический метод параллельных вычислений для гибридных процессоров, оптимизирующий инференс больших языковых моделей (LLM). Авторы обнаружили, что существующие фреймворки для инференса ИИ не учитывают неравномерные возможности ядер в гибридных CPU, что приводит к низкой производительности. Предложенный метод балансирует нагрузку между ядрами перед началом параллельной работы, что значительно повышает эффективность инференса LLM. В результате, инструмент Neural Speed достиг более 90% использования пропускной способности памяти на двух гибридных процессорах Intel."
                },
                "en": {
                    "title": "Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs",
                    "desc": "The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs."
                },
                "zh": {
                    "title": "动态平衡，提升AI推理性能！",
                    "desc": "AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，目前的AI推理框架忽视了混合CPU的不平衡硬件能力，导致推理性能低下。为了解决这个问题，我们提出了一种动态并行方法，显著提高了混合CPU的LLM推理性能，通过在并行工作开始之前平衡每个核心的工作负载。该方法使Neural Speed在两款混合Intel CPU上实现了超过90%的内存带宽。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19067",
            "title": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation",
            "url": "https://huggingface.co/papers/2411.19067",
            "abstract": "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.",
            "score": 5,
            "issue_id": 934,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "74d4a17af3574a5d",
            "authors": [
                "Minhyun Lee",
                "Seungho Lee",
                "Song Park",
                "Dongyoon Han",
                "Byeongho Heo",
                "Hyunjung Shim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19067.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#survey",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Маскирование для улучшения сегментации изображений по описанию",
                    "desc": "Статья посвящена задаче сегментации изображений по текстовому описанию (RIS). Авторы предлагают новый метод обучения под названием MaskRIS, который использует маскирование изображений и текста. Этот подход улучшает устойчивость модели к окклюзиям, неполной информации и языковым сложностям. Эксперименты показывают, что MaskRIS превосходит существующие методы как при полностью контролируемом, так и при слабо контролируемом обучении."
                },
                "en": {
                    "title": "Enhancing Referring Image Segmentation with Masking Techniques",
                    "desc": "Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets."
                },
                "zh": {
                    "title": "Masked Referring Image Segmentation：提升图像分割性能的新方法",
                    "desc": "引用图像分割（RIS）是一种先进的视觉-语言任务，旨在根据自由形式的文本描述识别和分割图像中的对象。本文探讨了有效的数据增强技术，并提出了一种新的训练框架，称为Masked Referring Image Segmentation（MaskRIS）。研究表明，传统的图像增强方法在RIS中效果不佳，而简单的随机遮罩显著提升了RIS的性能。MaskRIS结合了图像和文本遮罩，并采用了失真感知上下文学习（DCL），从而提高了模型对遮挡、不完整信息和语言复杂性的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00239",
            "title": "Generating a Low-code Complete Workflow via Task Decomposition and RAG",
            "url": "https://huggingface.co/papers/2412.00239",
            "abstract": "AI technologies are moving rapidly from research to production. With the popularity of Foundation Models (FMs) that generate text, images, and video, AI-based systems are increasing their complexity. Compared to traditional AI-based software, systems employing FMs, or GenAI-based systems, are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across GenAI applications. Our first contribution is to formalize two techniques, Task Decomposition and Retrieval-Augmented Generation (RAG), as design patterns for GenAI-based systems. We discuss their trade-offs in terms of software quality attributes and comment on alternative approaches. We recommend to AI practitioners to consider these techniques not only from a scientific perspective but also from the standpoint of desired engineering properties such as flexibility, maintainability, safety, and security. As a second contribution, we describe our industry experience applying Task Decomposition and RAG to build a complex real-world GenAI application for enterprise users: Workflow Generation. The task of generating workflows entails generating a specific plan using data from the system environment, taking as input a user requirement. As these two patterns affect the entire AI development cycle, we explain how they impacted the dataset creation, model training, model evaluation, and deployment phases.",
            "score": 2,
            "issue_id": 948,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "44bf29d0fbeafbc3",
            "authors": [
                "Orlando Marquez Ayala",
                "Patrice Béchard"
            ],
            "affiliations": [
                "ServiceNow"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00239.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rag",
                    "#dataset",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Паттерны проектирования для систем генеративного ИИ: от теории к практике",
                    "desc": "Статья посвящена формализации двух техник - декомпозиции задач и генерации с дополнением извлечением информации (RAG) - как паттернов проектирования для систем на основе генеративного ИИ. Авторы обсуждают компромиссы этих паттернов с точки зрения атрибутов качества программного обеспечения и рекомендуют рассматривать их не только с научной точки зрения, но и с позиции желаемых инженерных свойств. В статье также описывается опыт применения этих паттернов для создания сложного приложения генеративного ИИ для корпоративных пользователей - генерации рабочих процессов. Авторы объясняют, как эти паттерны повлияли на весь цикл разработки ИИ, включая создание датасетов, обучение и оценку моделей, а также развертывание."
                },
                "en": {
                    "title": "Streamlining GenAI Development with Design Patterns",
                    "desc": "This paper discusses the challenges of designing AI systems that use Foundation Models (FMs) due to their complexity and versatility. It introduces two design patterns, Task Decomposition and Retrieval-Augmented Generation (RAG), which can help streamline the development of GenAI applications. The authors analyze the trade-offs of these techniques in relation to software quality attributes like flexibility and security. Additionally, they share their practical experience in applying these patterns to create a Workflow Generation application for enterprise users, highlighting their influence on various stages of the AI development cycle."
                },
                "zh": {
                    "title": "设计模式助力生成AI系统的开发",
                    "desc": "随着基础模型（FMs）在文本、图像和视频生成中的普及，AI系统的复杂性不断增加。与传统的AI软件相比，基于生成AI（GenAI）的系统在设计上更具挑战性，因此需要记录最佳实践，称为设计模式。本文首次将任务分解和检索增强生成（RAG）形式化为GenAI系统的设计模式，并讨论它们在软件质量属性方面的权衡。我们还分享了在企业用户的复杂GenAI应用——工作流生成中的实际经验，说明这些模式如何影响整个AI开发周期。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01558",
            "title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval",
            "url": "https://huggingface.co/papers/2412.01558",
            "abstract": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .",
            "score": 2,
            "issue_id": 935,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "12235c4ebf26fe4a",
            "authors": [
                "Dhiman Paul",
                "Md Rizwan Parvez",
                "Nabeel Mohammed",
                "Shafin Rahman"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh",
                "Qatar Computing Research Institute (QCRI), Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01558.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#synthetic",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VideoLights: Новый подход к анализу ключевых моментов видео с помощью продвинутых нейросетевых архитектур",
                    "desc": "Статья представляет VideoLights - новую систему для обнаружения ключевых моментов видео и поиска по ним. Авторы предлагают улучшенные методы выравнивания видео и текста, двунаправленное слияние модальностей и механизм обратной связи между задачами. Они также вводят адаптивные функции потерь и используют большие мультимодальные модели для улучшения представления данных. Эксперименты показывают, что VideoLights превосходит существующие методы на нескольких наборах данных."
                },
                "en": {
                    "title": "Enhancing Video-Text Integration with VideoLights",
                    "desc": "This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis."
                },
                "zh": {
                    "title": "VideoLights：提升视频与文本分析的全新框架",
                    "desc": "本论文提出了一种名为VideoLights的视频高亮检测和时刻检索框架，旨在解决现有模型在视频与文本对齐和跨任务动态方面的不足。我们引入了卷积投影和特征精炼模块，以提高视频和文本特征的对齐效果，并采用双向跨模态融合网络来增强查询感知的片段表示。通过单向联合任务反馈机制，我们能够提升两个任务之间的相关性，同时引入硬正负损失以改善学习效果。实验结果表明，VideoLights在多个基准数据集上表现出色，达到了最先进的性能。"
                }
            }
        }
    ],
    "link_prev": "2024-12-03.html",
    "link_next": "2024-12-05.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12月3日"
    },
    "short_date_next": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12月5日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "当前的视频生成模型擅长生成短片，但在创建多镜头、电影般的视频时仍然面临挑战。现有模型通常以单镜头为目标进行训练，难以维持连贯的故事情节和视觉一致性。我们提出了VideoGen-of-Thought (VGoT)，一种专为多镜头视频生成设计的协作且无需训练的架构。VGoT通过结构化、模块化的过程生成视频，包括脚本生成、关键帧生成、镜头级视频生成和平滑机制，确保视频的连贯性和一致性。实验证明，VGoT在生成高质量、连贯的多镜头视频方面超越了现有方法。",
        "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
        "pinyin": "当前的视频生成模型擅长生成短片，但在创建多镜头、电影般的视频时仍然面临挑战。现有模型通常以单镜头为目标进行训练，难以维持连贯的故事情节和视觉一致性。我们提出了VideoGen-of-Thought (VGoT)，一种专为多镜头视频生成设计的协作且无需训练的架构。VGoT通过结构化、模块化的过程生成视频，包括脚本生成、关键帧生成、镜头级视频生成和平滑机制，确保视频的连贯性和一致性。实验证明，VGoT在生成高质量、连贯的多镜头视频方面超越了现有方法。\n\ndāng qián de shì pín shēng chéng mó xíng shàn cháng shēng chéng duǎn piàn, dàn zài chuàng jiàn duō jìng tóu, diàn yǐng bān de shì pín shí réng rán miàn línn tiǎo zhàn. xiàn yǒu mó xíng tōng cháng yǐ dān jìng tóu wéi mù biāo jìn xíng xùn liàn, nán yǐ wéi chí lián de gù shi qíng jié hé shì jué yī zhì xìng. wǒ men tí chū le VideoGen-of-Thought (VGoT), yī zhǒng zhuān wéi duō jìng tóu shì pín shēng chéng shè jì de xié zuò qiě wú xū xùn liàn de jià gòu. VGoT tōng guò jié gòu huà, mó kuài huà de guò chéng shēng chéng shì pín, bāo kuò jiǎo běn shēng chéng, guǎn jiàn zhēn shēng chéng, jìng tóu jí shì pín shēng chéng hé píng huá jī zhì, què bǎo shì pín de lián hé xìng hé yī zhì xìng. shí yàn zhèng míng,VGoT zài shēng chéng gāo zhì liàng, lián hé de duō jìng tóu shì pín fāng miàn chāo yuè le xiàn yǒu fāng fǎ.",
        "vocab": "[{'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '擅长', 'pinyin': 'shàn cháng', 'trans': 'be good at'}, {'word': '短片', 'pinyin': 'duǎn piàn', 'trans': 'short film'}, {'word': '但在', 'pinyin': 'dàn zài', 'trans': 'but in'}, {'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'}, {'word': '多镜头', 'pinyin': 'duō jìng tóu', 'trans': 'multi-shot'}, {'word': '电影般', 'pinyin': 'diàn yǐng bān', 'trans': 'cinematic'}, {'word': '面临', 'pinyin': 'miàn lín', 'trans': 'face'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'}, {'word': '通常', 'pinyin': 'tōng cháng', 'trans': 'usually'}, {'word': '以...为目标', 'pinyin': 'yǐ ... wéi mù biāo', 'trans': 'aim at'}, {'word': '进行', 'pinyin': 'jìn xíng', 'trans': 'carry out'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'hardly'}, {'word': '维持', 'pinyin': 'wéi chí', 'trans': 'maintain'}, {'word': '连贯', 'pinyin': 'lián guàn', 'trans': 'coherent'}, {'word': '故事情节', 'pinyin': 'gù shi qíng jié', 'trans': 'storyline'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '我们', 'pinyin': 'wǒ men', 'trans': 'we'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': 'VideoGen-of-Thought', 'pinyin': 'VideoGen-of-Thought', 'trans': 'VideoGen-of-Thought'}, {'word': 'VGoT', 'pinyin': 'VGoT', 'trans': 'VGoT'}, {'word': '专为', 'pinyin': 'zhuān wéi', 'trans': 'specifically for'}, {'word': '协作', 'pinyin': 'xié zuò', 'trans': 'collaborative'}, {'word': '且', 'pinyin': 'qiě', 'trans': 'and'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'without needing'}, {'word': '训练的', 'pinyin': 'xùn liàn de', 'trans': 'training'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '结构化', 'pinyin': 'jié gòu huà', 'trans': 'structured'}, {'word': '模块化', 'pinyin': 'mó kuài huà', 'trans': 'modularized'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '包括', 'pinyin': 'bāo kuò', 'trans': 'include'}, {'word': '脚本', 'pinyin': 'jiǎo běn', 'trans': 'script'}, {'word': '关键帧', 'pinyin': 'guǎn jiàn zhēn', 'trans': 'keyframe'}, {'word': '镜头级', 'pinyin': 'jìng tóu jí', 'trans': 'shot-level'}, {'word': '平滑', 'pinyin': 'píng huá', 'trans': 'smooth'}, {'word': '机制', 'pinyin': 'jī zhì', 'trans': 'mechanism'}, {'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '超越', 'pinyin': 'chāo yuè', 'trans': 'surpass'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]",
        "trans": "The current video generation models excel at producing short clips but still face challenges in creating multi-shot, cinematic videos. Existing models are typically trained with a single-shot target, making it difficult to maintain coherent storylines and visual consistency. We propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture specifically designed for multi-shot video generation. VGoT generates videos through a structured, modular process that includes script generation, keyframe generation, shot-level video generation, and smoothing mechanisms, ensuring the coherence and consistency of the video. Experiments demonstrate that VGoT surpasses existing methods in generating high-quality, coherent multi-shot videos.",
        "update_ts": "2024-12-04 09:11"
    }
}