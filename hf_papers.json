{
    "date": {
        "ru": "19 мая",
        "en": "May 19",
        "zh": "5月19日"
    },
    "time_utc": "2025-05-19 04:20",
    "weekday": 0,
    "issue_id": 3824,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09388",
            "title": "Qwen3 Technical Report",
            "url": "https://huggingface.co/papers/2505.09388",
            "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
            "score": 36,
            "issue_id": 3823,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "69a0f87bb5460e8d",
            "authors": [
                "An Yang",
                "Anfeng Li",
                "Baosong Yang",
                "Beichen Zhang",
                "Binyuan Hui",
                "Bo Zheng",
                "Bowen Yu",
                "Chang Gao",
                "Chengen Huang",
                "Chenxu Lv",
                "Chujie Zheng",
                "Dayiheng Liu",
                "Fan Zhou",
                "Fei Huang",
                "Feng Hu",
                "Hao Ge",
                "Haoran Wei",
                "Huan Lin",
                "Jialong Tang",
                "Jian Yang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jianxin Yang",
                "Jiaxi Yang",
                "Jing Zhou",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Keqin Bao",
                "Kexin Yang",
                "Le Yu",
                "Lianghao Deng",
                "Mei Li",
                "Mingfeng Xue",
                "Mingze Li",
                "Pei Zhang",
                "Peng Wang",
                "Qin Zhu",
                "Rui Men",
                "Ruize Gao",
                "Shixuan Liu",
                "Shuang Luo",
                "Tianhao Li",
                "Tianyi Tang",
                "Wenbiao Yin",
                "Xingzhang Ren",
                "Xinyu Wang",
                "Xinyu Zhang",
                "Xuancheng Ren",
                "Yang Fan",
                "Yang Su",
                "Yichang Zhang",
                "Yinger Zhang",
                "Yu Wan",
                "Yuqiong Liu",
                "Zekun Wang",
                "Zeyu Cui",
                "Zhenru Zhang",
                "Zhipeng Zhou",
                "Zihan Qiu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.09388.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#agi",
                    "#reasoning",
                    "#multilingual",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Qwen3: Единая модель для мышления и быстрых ответов",
                    "desc": "Qwen3 - это новое семейство больших языковых моделей (LLM), разработанное для улучшения производительности, эффективности и многоязычных возможностей. Ключевой инновацией Qwen3 является интеграция режима мышления и режима без мышления в единую структуру, что позволяет динамически переключаться между ними. Модель вводит механизм бюджета мышления, позволяющий адаптивно распределять вычислительные ресурсы во время вывода. Qwen3 достигает передовых результатов в различных бенчмарках и поддерживает 119 языков и диалектов."
                },
                "en": {
                    "title": "Qwen3: Unifying Thinking and Efficiency in Language Models",
                    "desc": "Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference."
                },
                "zh": {
                    "title": "Qwen3：统一思维与响应的智能语言模型",
                    "desc": "本文介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包含一系列大型语言模型，旨在提高性能、效率和多语言能力。其创新之处在于将思维模式和非思维模式整合到一个统一框架中，实现动态模式切换，适应用户查询。Qwen3还引入了思维预算机制，允许用户在推理过程中自适应分配计算资源，从而在任务复杂性基础上平衡延迟和性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11152",
            "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
            "url": "https://huggingface.co/papers/2505.11152",
            "abstract": "Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.",
            "score": 2,
            "issue_id": 3822,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "caa702fa71c24606",
            "authors": [
                "Daniel Sungho Jung",
                "Kyoung Mu Lee"
            ],
            "affiliations": [
                "IPAI, Dept. of ECE & ASRI, Seoul National University, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11152.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🖐️",
                "ru": {
                    "title": "Точная оценка контактов рук: преодоление дисбаланса данных",
                    "desc": "Эта статья представляет новый подход к оценке плотного контакта рук с окружающей средой, что важно для понимания взаимодействия человека с миром. Авторы разработали фреймворк HACO для обучения на несбалансированных данных, решая проблемы классового и пространственного дисбаланса в наборах данных о контактах рук. Они предложили метод сбалансированной выборки контактов и функцию потерь VCB, учитывающую пространственное распределение контактов на поверхности руки. Результаты показывают эффективность предложенного подхода для точного предсказания плотных контактов рук на основе крупномасштабных данных."
                },
                "en": {
                    "title": "Enhancing Hand Contact Estimation with Balanced Learning Techniques",
                    "desc": "This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation."
                },
                "zh": {
                    "title": "提升手部接触估计的准确性",
                    "desc": "这篇论文探讨了手部接触估计的重要性，尤其是在与物体、其他手、场景和身体的互动中。尽管已有大量高质量的数据集，但如何有效学习密集的手部接触估计仍然是一个未被充分研究的问题。论文提出了一种新的框架，称为HACO，旨在解决类不平衡和空间不平衡的问题。通过引入平衡接触采样和顶点级类平衡损失，研究者们成功地提高了手部接触估计的准确性。"
                }
            }
        }
    ],
    "link_prev": "2025-05-16.html",
    "link_next": "2025-05-20.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "short_date_next": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。",
        "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
        "pinyin": "这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。\n\nZhè piān wénzhāng tǎolùn le dàxíng tuīlǐ móxíng (LRMs) de qiánzài chánglián tuīlǐ nénglì. Zhīqián de yánjiū biǎomíng, jiéguǒ dǎoxiàng de qiángzhì xuéxí (RL) kěyǐ ǒurán yǐnfā zìwǒ jiàozhèng, huítuì hé yànzhèng děng gāojí tuīlǐ xíngwéi. Rán'ér, zhèxiē xíngwéi de shíjiān hé yīzhìxìng nán yǐ yùcè hé kòngzhì, xiànzhì le LRMs de tuīlǐ nénglì. Wèile jiějué zhèxiē wèntí, zuòzhě tíchū le xiǎnshì de jiāng móxíng yǔ sān zhǒng yuán nénglì (yǎnyì, guīnà hé sùyīn) duìqí de fāngfǎ. Tōngguò sān jiēduàn liúshuǐxiàn, tíshēng le xìngnéng, bìng zài shùxué, biānchéng hé kēxué jīzhǔn cèshì zhōng xiǎnzhù tīgāo le biǎoxiàn. Dàimǎ kě zài GitHub shàng zhǎo dào.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"潜在\", \"pinyin\": \"qián zài\", \"trans\": \"potential\"},\n    {\"word\": \"长链\", \"pinyin\": \"cháng liàn\", \"trans\": \"long-chain\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"之前\", \"pinyin\": \"zhī qián\", \"trans\": \"before\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"导向\", \"pinyin\": \"dǎo xiàng\", \"trans\": \"oriented\"},\n    {\"word\": \"强化\", \"pinyin\": \"qiáng huà\", \"trans\": \"reinforcement\"},\n    {\"word\": \"学习\", \"pinyin\": \"xué xí\", \"trans\": \"learning\"},\n    {\"word\": \"偶然\", \"pinyin\": \"ǒu rán\", \"trans\": \"occasionally\"},\n    {\"word\": \"引发\", \"pinyin\": \"yǐn fā\", \"trans\": \"trigger\"},\n    {\"word\": \"自我\", \"pinyin\": \"zì wǒ\", \"trans\": \"self\"},\n    {\"word\": \"校正\", \"pinyin\": \"jiào zhèng\", \"trans\": \"correct\"},\n    {\"word\": \"回溯\", \"pinyin\": \"huí sù\", \"trans\": \"backtrack\"},\n    {\"word\": \"验证\", \"pinyin\": \"yàn zhèng\", \"trans\": \"verify\"},\n    {\"word\": \"行为\", \"pinyin\": \"xíng wéi\", \"trans\": \"behavior\"},\n    {\"word\": \"一致性\", \"pinyin\": \"yī zhì xìng\", \"trans\": \"consistency\"},\n    {\"word\": \"预测\", \"pinyin\": \"yù cè\", \"trans\": \"predict\"},\n    {\"word\": \"控制\", \"pinyin\": \"kòng zhì\", \"trans\": \"control\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"显式\", \"pinyin\": \"xiǎn shì\", \"trans\": \"explicit\"},\n    {\"word\": \"对齐\", \"pinyin\": \"duì qí\", \"trans\": \"align\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"元\", \"pinyin\": \"yuán\", \"trans\": \"meta\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"演绎\", \"pinyin\": \"yǎn yì\", \"trans\": \"deduction\"},\n    {\"word\": \"归纳\", \"pinyin\": \"guī nà\", \"trans\": \"induction\"},\n    {\"word\": \"溯因\", \"pinyin\": \"sù yīn\", \"trans\": \"abduction\"},\n    {\"word\": \"三阶段\", \"pinyin\": \"sān jiē duàn\", \"trans\": \"three-stage\"},\n    {\"word\": \"流水线\", \"pinyin\": \"liú shuǐ xiàn\", \"trans\": \"pipeline\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"enhance\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"代码\", \"pinyin\": \"dài mǎ\", \"trans\": \"code\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article discusses the potential long-chain reasoning capabilities of Large Reasoning Models (LRMs). Previous research has shown that result-oriented reinforcement learning (RL) can occasionally trigger advanced reasoning behaviors such as self-correction, backtracking, and verification. However, the timing and consistency of these behaviors are difficult to predict and control, limiting the reasoning capabilities of LRMs. To address these issues, the authors propose a method that explicitly aligns the model with three meta-abilities: deduction, induction, and abduction. Through a three-stage pipeline, performance is enhanced, and significant improvements are achieved in mathematical, programming, and scientific benchmark tests. The code can be found on GitHub.",
        "update_ts": "2025-05-18 12:44"
    }
}