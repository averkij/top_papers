{
    "date": {
        "ru": "20 –º–∞—è",
        "en": "May 20",
        "zh": "5Êúà20Êó•"
    },
    "time_utc": "2025-05-20 02:38",
    "weekday": 1,
    "issue_id": 3845,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.12081",
            "title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12081",
            "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).",
            "score": 5,
            "issue_id": 3845,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 –º–∞—è",
                "en": "May 17",
                "zh": "5Êúà17Êó•"
            },
            "hash": "9b7953f88ae7653d",
            "authors": [
                "Yuqi Liu",
                "Tianyuan Qu",
                "Zhisheng Zhong",
                "Bohao Peng",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12081.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VisionReasoner - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. VisionReasoner –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –≤—ã–¥–∞—á–µ–π –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ VisionReasoner –Ω–∞–¥ Qwen2.5VL –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø–æ–¥—Å—á–µ—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤."
                },
                "en": {
                    "title": "VisionReasoner: Unifying Visual Perception with Advanced Reasoning",
                    "desc": "This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception."
                },
                "zh": {
                    "title": "Áªü‰∏ÄËßÜËßâÊÑüÁü•ÁöÑÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VisionReasonerÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®ÂÖ±‰∫´Ê®°Âûã‰∏≠Â§ÑÁêÜÂ§öÁßçËßÜËßâÊÑüÁü•‰ªªÂä°„ÄÇÈÄöËøáËÆæËÆ°Êñ∞È¢ñÁöÑÂ§öÂØπË±°ËÆ§Áü•Â≠¶‰π†Á≠ñÁï•ÂíåÁ≥ªÁªüÁöÑ‰ªªÂä°ÈáçÊûÑÔºåVisionReasonerÂ¢ûÂº∫‰∫ÜÂÖ∂Êé®ÁêÜËÉΩÂäõÔºå‰ª•ÂàÜÊûêËßÜËßâËæìÂÖ•Âπ∂Ëß£ÂÜ≥Â§öÊ†∑ÁöÑÊÑüÁü•‰ªªÂä°„ÄÇËØ•Ê®°ÂûãÂú®ÁîüÊàêÊâÄÈúÄËæìÂá∫‰πãÂâçÔºå‰ºöÂÖàËøõË°åÁªìÊûÑÂåñÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰ª•ÂìçÂ∫îÁî®Êà∑Êü•ËØ¢„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisionReasonerÂú®Ê£ÄÊµã„ÄÅÂàÜÂâ≤ÂíåËÆ°Êï∞Á≠â‰∏â‰∏™ÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑÂçÅ‰∏™‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜQwen2.5VL„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13417",
            "title": "AdaptThink: Reasoning Models Can Learn When to Think",
            "url": "https://huggingface.co/papers/2505.13417",
            "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.",
            "score": 3,
            "issue_id": 3845,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 –º–∞—è",
                "en": "May 19",
                "zh": "5Êúà19Êó•"
            },
            "hash": "edd33223d8d833a7",
            "authors": [
                "Jiajie Zhang",
                "Nianyi Lin",
                "Lei Hou",
                "Ling Feng",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13417.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º AdaptThink, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏. –ê–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ AdaptThink –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø–æ–≤—ã—à–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –ù–∞ —Ç—Ä–µ—Ö –Ω–∞–±–æ—Ä–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º —Å–æ–∫—Ä–∞—Ç–∏–ª —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ DeepSeek-R1-Distill-Qwen-1.5B –Ω–∞ 53% –∏ –ø–æ–≤—ã—Å–∏–ª –µ–µ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 2.4%."
                },
                "en": {
                    "title": "Optimize Reasoning with Adaptive Thinking Modes!",
                    "desc": "This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÊÄùËÄÉÊ®°ÂºèÈÄâÊã©ÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá‰∏éË¥®Èáè",
                    "desc": "ÊúÄËøëÔºåÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ÂÜóÈïøÁöÑÊÄùËÄÉËøáÁ®ãÊòæËëóÂ¢ûÂä†‰∫ÜÊé®ÁêÜÂºÄÈîÄÔºåÂØºËá¥ÊïàÁéáÊàê‰∏∫Áì∂È¢à„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫NoThinkingÁöÑÊñπÊ≥ïÔºåÈºìÂä±Êé®ÁêÜÊ®°ÂûãË∑≥ËøáÊÄùËÄÉÔºåÁõ¥Êé•ÁîüÊàêÊúÄÁªàËß£ÂÜ≥ÊñπÊ°àÔºåÈÄÇÁî®‰∫éÁõ∏ÂØπÁÆÄÂçïÁöÑ‰ªªÂä°„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAdaptThinkÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®Ê†πÊçÆÈóÆÈ¢òÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÈÄâÊã©ÊúÄ‰Ω≥ÊÄùËÄÉÊ®°Âºè„ÄÇÂÆûÈ™åË°®ÊòéÔºåAdaptThinkÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊàêÊú¨ÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-05-19.html",
    "link_next": "2025-05-21.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "19.05",
        "en": "05/19",
        "zh": "5Êúà19Êó•"
    },
    "short_date_next": {
        "ru": "21.05",
        "en": "05/21",
        "zh": "5Êúà21Êó•"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫ÜQwen3ÔºåËøôÊòØQwenÊ®°ÂûãÁ≥ªÂàóÁöÑÊúÄÊñ∞ÁâàÊú¨„ÄÇQwen3ÂåÖÊã¨Â§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÊÄßËÉΩ„ÄÅÊïàÁéáÂíåÂ§öËØ≠Ë®ÄËÉΩÂäõ„ÄÇÂÆÉÂåÖÂê´ÂØÜÈõÜÂíåÊ∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÔºåÂèÇÊï∞ËßÑÊ®°‰ªé0.6Âà∞2350‰∫ø‰∏çÁ≠â„ÄÇQwen3ÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂ∞ÜÊÄùËÄÉÊ®°ÂºèÂíåÈùûÊÄùËÄÉÊ®°ÂºèÁªìÂêàÂú®‰∏Ä‰∏™Ê°ÜÊû∂‰∏≠ÔºåÊ∂àÈô§‰∫ÜÂàáÊç¢Ê®°ÂûãÁöÑÈúÄË¶Å„ÄÇÂÆÉËøòÂºïÂÖ•‰∫ÜÊÄùËÄÉÈ¢ÑÁÆóÊú∫Âà∂ÔºåÂÖÅËÆ∏Áî®Êà∑Ê†πÊçÆ‰ªªÂä°Â§çÊùÇÊÄßÂä®ÊÄÅÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫ê„ÄÇ",
        "title": "Qwen3 Technical Report",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫ÜQwen3ÔºåËøôÊòØQwenÊ®°ÂûãÁ≥ªÂàóÁöÑÊúÄÊñ∞ÁâàÊú¨„ÄÇ\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le Qwen3, zh√® sh√¨ Qwen m√≥x√≠ng x√¨li√® de zu√¨xƒ´n b«énbƒõn.\n\nQwen3ÂåÖÊã¨Â§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÊÄßËÉΩ„ÄÅÊïàÁéáÂíåÂ§öËØ≠Ë®ÄËÉΩÂäõ„ÄÇ\nQwen3 bƒÅoku√≤ du≈çzh«íng d√†x√≠ng y«îy√°n m√≥x√≠ng, zh«ê z√†i t√≠gƒÅo x√¨ngn√©ng, xi√†ol«ú h√© du≈çy«îy√°n n√©ngl√¨.\n\nÂÆÉÂåÖÂê´ÂØÜÈõÜÂíåÊ∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÔºåÂèÇÊï∞ËßÑÊ®°‰ªé0.6Âà∞2350‰∫ø‰∏çÁ≠â„ÄÇ\nTƒÅ bƒÅoh√°n m√¨j√≠ h√© h√πnh√© zhuƒÅnjiƒÅ ji√†g√≤u, cƒÅnsh√π guƒ´m√≥ c√≥ng 0.6 d√†o 2350 y√¨ b√πdƒõng.\n\nQwen3ÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂ∞ÜÊÄùËÄÉÊ®°ÂºèÂíåÈùûÊÄùËÄÉÊ®°ÂºèÁªìÂêàÂú®‰∏Ä‰∏™Ê°ÜÊû∂‰∏≠ÔºåÊ∂àÈô§‰∫ÜÂàáÊç¢Ê®°ÂûãÁöÑÈúÄË¶Å„ÄÇ\nQwen3 de chu√†ngxƒ´n zhƒ´ ch√π z√†i y√∫ jiƒÅng sƒ´k«éo m√≥sh√¨ h√© fƒìi sƒ´k«éo m√≥sh√¨ ji√©h√© z√†i yƒ´g√® ku√†ngji√† zh≈çng, xiƒÅoch√∫ le qiƒìhu√†n m√≥x√≠ng de x≈´y√†o.\n\nÂÆÉËøòÂºïÂÖ•‰∫ÜÊÄùËÄÉÈ¢ÑÁÆóÊú∫Âà∂ÔºåÂÖÅËÆ∏Áî®Êà∑Ê†πÊçÆ‰ªªÂä°Â§çÊùÇÊÄßÂä®ÊÄÅÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫ê„ÄÇ\nTƒÅ h√°i y«ênr√π le sƒ´k«éo y√πsu√†n jƒ´zh√¨, y«înx«î y√≤ngh√π gƒìnj√π r√®nw√π f√πz√°x√¨ng d√≤ngt√†i fƒìnp√®i j√¨su√†n zƒ´yu√°n.",
        "vocab": "[\n    {\"word\": \"Á≥ªÂàó\", \"pinyin\": \"x√¨li√®\", \"trans\": \"series\"},\n    {\"word\": \"ÁâàÊú¨\", \"pinyin\": \"b«énbƒõn\", \"trans\": \"version\"},\n    {\"word\": \"Êó®Âú®\", \"pinyin\": \"zh«êz√†i\", \"trans\": \"aim to\"},\n    {\"word\": \"ÊïàÁéá\", \"pinyin\": \"xi√†ol«ú\", \"trans\": \"efficiency\"},\n    {\"word\": \"Â§öËØ≠Ë®Ä\", \"pinyin\": \"du≈çy«îy√°n\", \"trans\": \"multilingual\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ngl√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÂåÖÂê´\", \"pinyin\": \"bƒÅoh√°n\", \"trans\": \"contain\"},\n    {\"word\": \"ÂØÜÈõÜ\", \"pinyin\": \"m√¨j√≠\", \"trans\": \"dense\"},\n    {\"word\": \"Ê∑∑Âêà\", \"pinyin\": \"h√πnh√©\", \"trans\": \"hybrid\"},\n    {\"word\": \"‰∏ìÂÆ∂\", \"pinyin\": \"zhuƒÅnjiƒÅ\", \"trans\": \"expert\"},\n    {\"word\": \"Êû∂ÊûÑ\", \"pinyin\": \"ji√†g√≤u\", \"trans\": \"architecture\"},\n    {\"word\": \"ÂèÇÊï∞\", \"pinyin\": \"cƒÅnsh«î\", \"trans\": \"parameter\"},\n    {\"word\": \"ËßÑÊ®°\", \"pinyin\": \"guƒ´m√≥\", \"trans\": \"scale\"},\n    {\"word\": \"ÂàõÊñ∞\", \"pinyin\": \"chu√†ngxƒ´n\", \"trans\": \"innovation\"},\n    {\"word\": \"‰πãÂ§Ñ\", \"pinyin\": \"zhƒ´ch√π\", \"trans\": \"place\"},\n    {\"word\": \"ÊÄùËÄÉ\", \"pinyin\": \"sƒ´k«éo\", \"trans\": \"think\"},\n    {\"word\": \"Ê®°Âºè\", \"pinyin\": \"m√≥sh√¨\", \"trans\": \"mode\"},\n    {\"word\": \"ÁªìÂêà\", \"pinyin\": \"ji√©h√©\", \"trans\": \"combine\"},\n    {\"word\": \"Ê°ÜÊû∂\", \"pinyin\": \"ku√†ngji√†\", \"trans\": \"framework\"},\n    {\"word\": \"Ê∂àÈô§\", \"pinyin\": \"xiƒÅoch√∫\", \"trans\": \"eliminate\"},\n    {\"word\": \"ÂàáÊç¢\", \"pinyin\": \"qiƒìhu√†n\", \"trans\": \"switch\"},\n    {\"word\": \"ÈúÄË¶Å\", \"pinyin\": \"x≈´y√†o\", \"trans\": \"need\"},\n    {\"word\": \"ÂºïÂÖ•\", \"pinyin\": \"y«ênr√π\", \"trans\": \"introduce\"},\n    {\"word\": \"È¢ÑÁÆó\", \"pinyin\": \"y√πsu√†n\", \"trans\": \"budget\"},\n    {\"word\": \"Êú∫Âà∂\", \"pinyin\": \"jƒ´zh√¨\", \"trans\": \"mechanism\"},\n    {\"word\": \"ÂÖÅËÆ∏\", \"pinyin\": \"y«înx«î\", \"trans\": \"allow\"},\n    {\"word\": \"Ê†πÊçÆ\", \"pinyin\": \"gƒìnj√π\", \"trans\": \"according to\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®nw√π\", \"trans\": \"task\"},\n    {\"word\": \"Â§çÊùÇÊÄß\", \"pinyin\": \"f√πz√°x√¨ng\", \"trans\": \"complexity\"},\n    {\"word\": \"Âä®ÊÄÅ\", \"pinyin\": \"d√≤ngt√†i\", \"trans\": \"dynamic\"},\n    {\"word\": \"ÂàÜÈÖç\", \"pinyin\": \"fƒìnp√®i\", \"trans\": \"allocate\"},\n    {\"word\": \"ËÆ°ÁÆó\", \"pinyin\": \"j√¨su√†n\", \"trans\": \"compute\"},\n    {\"word\": \"ËµÑÊ∫ê\", \"pinyin\": \"zƒ´yu√°n\", \"trans\": \"resources\"}\n]",
        "trans": "This article introduces Qwen3, the latest version in the Qwen model series. Qwen3 includes a variety of large language models aimed at enhancing performance, efficiency, and multilingual capabilities. It features dense and mixture-of-experts architectures, with parameter sizes ranging from 0.6 to 2350 billion. The innovation of Qwen3 lies in combining thinking and non-thinking modes within a single framework, eliminating the need to switch models. It also introduces a thinking budget mechanism, allowing users to dynamically allocate computational resources based on the complexity of the task.",
        "update_ts": "2025-05-19 09:13"
    }
}