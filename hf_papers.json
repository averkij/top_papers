{
    "date": {
        "ru": "11 ноября",
        "en": "November 11",
        "zh": "11月11日"
    },
    "time_utc": "2024-11-11 04:14",
    "weekday": 0,
    "issue_id": 508,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 2,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "38beaabd86eeaa88",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Эффективная настройка языковых моделей для генерации модульных тестов",
                    "desc": "Эта статья исследует применение методов эффективной настройки параметров (PEFT) для больших языковых моделей (LLM) в задаче генерации модульных тестов. Авторы сравнивают полную тонкую настройку с различными методами PEFT, включая LoRA, (IA)^3 и настройку промптов, на разных архитектурах и размерах моделей. Результаты показывают, что методы PEFT могут обеспечить производительность, сравнимую с полной тонкой настройкой, при этом значительно снижая вычислительные затраты. Особенно эффективной оказалась настройка промптов, а LoRA в некоторых случаях приближается к эффективности полной тонкой настройки."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "参数高效微调：提升单元测试生成的经济性与有效性",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在单元测试生成中的应用，特别是参数高效微调（PEFT）方法。传统的全量微调虽然有效，但成本高昂，PEFT方法通过只微调部分参数来降低计算开销。研究表明，PEFT方法在单元测试生成中能够达到与全量微调相当的性能，尤其是提示微调在成本和资源利用上最为有效。论文还比较了不同模型架构和大小下的多种PEFT方法，展示了其在特定任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 1,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 ноября",
                "en": "November 8",
                "zh": "11月8日"
            },
            "hash": "b23d3650ace21f86",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "StdGEN: Революция в создании 3D-персонажей с семантическим разделением",
                    "desc": "StdGEN - это инновационный конвейер для генерации семантически декомпозированных трехмерных персонажей высокого качества из одиночных изображений. В основе StdGEN лежит предложенная авторами Semantic-aware Large Reconstruction Model (S-LRM), трансформер-модель, которая реконструирует геометрию, цвет и семантику из многоракурсных изображений. Конвейер включает дифференцируемую схему извлечения многослойной семантической поверхности, специализированную эффективную многоракурсную диффузионную модель и модуль итеративного уточнения многослойной поверхности. Эксперименты показывают значительное превосходство StdGEN над существующими методами в генерации 3D-персонажей аниме по качеству геометрии, текстур и возможностям декомпозиции."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGEN：高效生成可分解3D角色的创新管道",
                    "desc": "StdGEN是一种创新的管道，能够从单张图像生成语义分解的高质量3D角色，广泛应用于虚拟现实、游戏和电影制作等领域。与以往方法相比，StdGEN在可分解性、有效性和效率上表现出色，能够在三分钟内生成细致的3D角色，且各个语义组件如身体、衣服和头发分离。其核心是语义感知的大型重建模型（S-LRM），该模型基于变换器，能够从多视图图像中联合重建几何、颜色和语义。通过引入可微分的多层语义表面提取方案，StdGEN实现了高质量、可分解的3D角色生成，实验结果显示其在3D动漫角色生成方面的性能超越了现有基准。"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11月8日"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11月12日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大型语言模型（LLMs）在代码生成、推理任务和代理系统中的重要性。虽然开放访问的代码LLMs性能接近专有模型，但适合严格科学研究的高质量模型仍然有限。为了填补这一空白，作者介绍了OpenCoder，一个顶尖的代码LLM，性能媲美领先模型，并提供详细的训练数据和协议。通过这种开放性，作者希望加速代码AI的研究和可重复的进展。",
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "pinyin": "Zhè piān wénzhāng tǎolùn le dàxíng yǔyán móxíng (LLMs) zài dàimǎ shēngchéng, tuīlǐ rènwù hé dàilǐ xìtǒng zhōng de zhòngyàoxìng. Suīrán kāifàng fǎngwèn de dàimǎ LLMs xìngnéng jìnkè zhuānyǒu móxíng, dàn shìhé yánge kēxué yánjiū de gāo zhìliàng móxíng réngrán yǒuxiàn. Wèile tiánbǔ zhè yī kòngbái, zuòzhě jièshào le OpenCoder, yīgè dǐngjiān de dàimǎ LLM, xìngnéng jìmǐ lǐngxiān móxíng, bìng tígōng xiángxì de xùnliàn shùjù hé xiéyì. Tōngguò zhè zhǒng kāifàngxìng, zuòzhě xīwàng jiāsù dàimǎ AI de yánjiū hé kě chóngfù de jìnbù.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"代码生成\", \"pinyin\": \"dài mǎ shēng chéng\", \"trans\": \"code generation\"},\n    {\"word\": \"推理任务\", \"pinyin\": \"tuī lǐ rèn wù\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"代理系统\", \"pinyin\": \"dài lǐ xì tǒng\", \"trans\": \"proxy system\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòng yào xìng\", \"trans\": \"importance\"},\n    {\"word\": \"开放访问\", \"pinyin\": \"kāi fàng fǎng wèn\", \"trans\": \"open access\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"接近\", \"pinyin\": \"jiē jìn\", \"trans\": \"close to\"},\n    {\"word\": \"专有模型\", \"pinyin\": \"zhuān yǒu mó xíng\", \"trans\": \"proprietary model\"},\n    {\"word\": \"适合\", \"pinyin\": \"shì hé\", \"trans\": \"suitable\"},\n    {\"word\": \"严格\", \"pinyin\": \"yán gé\", \"trans\": \"strict\"},\n    {\"word\": \"科学研究\", \"pinyin\": \"kē xué yán jiū\", \"trans\": \"scientific research\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒu xiàn\", \"trans\": \"limited\"},\n    {\"word\": \"填补\", \"pinyin\": \"tián bǔ\", \"trans\": \"fill\"},\n    {\"word\": \"空白\", \"pinyin\": \"kòng bái\", \"trans\": \"gap\"},\n    {\"word\": \"作者\", \"pinyin\": \"zuò zhě\", \"trans\": \"author\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"OpenCoder\", \"pinyin\": \"OpenCoder\", \"trans\": \"OpenCoder\"},\n    {\"word\": \"顶尖\", \"pinyin\": \"dǐng jiān\", \"trans\": \"top-notch\"},\n    {\"word\": \"媲美\", \"pinyin\": \"pì měi\", \"trans\": \"rival\"},\n    {\"word\": \"领先模型\", \"pinyin\": \"lǐng xiān mó xíng\", \"trans\": \"leading model\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiáng xì\", \"trans\": \"detailed\"},\n    {\"word\": \"训练数据\", \"pinyin\": \"xùn liàn shù jù\", \"trans\": \"training data\"},\n    {\"word\": \"协议\", \"pinyin\": \"xié yì\", \"trans\": \"protocol\"},\n    {\"word\": \"开放性\", \"pinyin\": \"kāi fàng xìng\", \"trans\": \"openness\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"可重复\", \"pinyin\": \"kě chóng fù\", \"trans\": \"reproducible\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìn zhǎn\", \"trans\": \"progress\"}\n]",
        "trans": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While open-access code LLMs perform nearly as well as proprietary models, high-quality models suitable for rigorous scientific research remain limited. To fill this gap, the authors introduce OpenCoder, a top-tier code LLM that matches the performance of leading models and provides detailed training data and protocols. Through this openness, the authors aim to accelerate research and reproducible progress in code AI.",
        "update_ts": "2024-11-10 10:11"
    }
}