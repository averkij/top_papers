{
    "date": {
        "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 29",
        "zh": "4æœˆ29æ—¥"
    },
    "time_utc": "2025-04-29 05:12",
    "weekday": 1,
    "issue_id": 3482,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 12,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "LLM Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‰‹æœºè‡ªåŠ¨åŒ–å˜é©",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ‰‹æœºè‡ªåŠ¨åŒ–å‘ç”Ÿäº†å˜é©æ€§å˜åŒ–ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†åŸºäºLLMçš„æ‰‹æœºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œå¼ºè°ƒäº†å®ƒä»¬ä»åŸºäºè„šæœ¬çš„è‡ªåŠ¨åŒ–åˆ°æ™ºèƒ½è‡ªé€‚åº”ç³»ç»Ÿçš„æ¼”å˜ã€‚æˆ‘ä»¬é¦–å…ˆé˜æ˜äº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„é€šç”¨æ€§ã€é«˜ç»´æŠ¤æˆæœ¬å’Œæ„å›¾ç†è§£è–„å¼±ï¼Œå¹¶å±•ç¤ºäº†LLMå¦‚ä½•é€šè¿‡å…ˆè¿›çš„è¯­è¨€ç†è§£ã€å¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¼ºå¤§çš„å†³ç­–èƒ½åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡ç«¯éƒ¨ç½²æ•ˆç‡ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ç­‰å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸæä¾›å‰ç»æ€§è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19724",
            "title": "RepText: Rendering Visual Text via Replicating",
            "url": "https://huggingface.co/papers/2504.19724",
            "abstract": "Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.",
            "score": 6,
            "issue_id": 3481,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "daf4c90a7de01768",
            "authors": [
                "Haofan Wang",
                "Yujia Xu",
                "Yimeng Li",
                "Junchen Li",
                "Chaowei Zhang",
                "Jing Wang",
                "Kejia Yang",
                "Zhibo Chen"
            ],
            "affiliations": [
                "Liblib AI",
                "Shakker Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19724.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#multilingual",
                    "#video",
                    "#open_source",
                    "#low_resource",
                    "#multimodal"
                ],
                "emoji": "ğŸ–‹ï¸",
                "ru": {
                    "title": "RepText: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RepText - Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆÑ€Ğ¸Ñ„Ñ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ControlNet, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»Ğ¸Ñ„Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RepText Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Multilingual Typography",
                    "desc": "This paper introduces RepText, a novel approach to enhance text-to-image generation models by enabling them to accurately replicate multilingual text in various fonts without needing to understand the text. The method leverages a combination of language agnostic glyphs and position information to create visually coherent text that users can customize. To improve the rendering accuracy, the authors implement a text perceptual loss alongside diffusion loss and utilize a specific initialization strategy during inference. Extensive experiments demonstrate that RepText outperforms existing open-source models and achieves results comparable to advanced closed-source systems, while also addressing its limitations."
                },
                "zh": {
                    "title": "èµ‹èƒ½æ–‡æœ¬ç”Ÿæˆï¼Œè¶…è¶Šè¯­è¨€é™åˆ¶",
                    "desc": "å°½ç®¡ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ ï¼Œå°¤å…¶æ˜¯éæ‹‰ä¸å­—æ¯æ–¹é¢ä»ç„¶å­˜åœ¨é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RepTextï¼Œå®ƒæ—¨åœ¨èµ‹äºˆé¢„è®­ç»ƒçš„å•è¯­æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶æ•´åˆäº†ä¸è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä»¥ä¾¿ç”Ÿæˆå’Œè°çš„è§†è§‰æ–‡æœ¬ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†RepTextçš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æºæ¨¡å‹ä¸­è¡¨ç°æ›´ä¼˜ï¼Œå¹¶ä¸æœ¬åœ°å¤šè¯­è¨€é—­æºæ¨¡å‹çš„ç»“æœç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15780",
            "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
            "url": "https://huggingface.co/papers/2504.15780",
            "abstract": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen",
            "score": 2,
            "issue_id": 3481,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "e736decf98fa2805",
            "authors": [
                "Daocheng Fu",
                "Zijun Chen",
                "Renqiu Xia",
                "Qi Liu",
                "Yuan Feng",
                "Hongbin Zhou",
                "Renrui Zhang",
                "Shiyang Feng",
                "Peng Gao",
                "Junchi Yan",
                "Botian Shi",
                "Bo Zhang",
                "Yu Qiao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15780.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#math",
                    "#data",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "TrustGeoGen: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TrustGeoGen - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. TrustGeoGen ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GeoTrust-200K Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ GeoTrust-test, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 49.17%. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° GeoTrust, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ OpenAI-o1."
                },
                "en": {
                    "title": "TrustGeoGen: Ensuring Reliable Geometric Problem Solving",
                    "desc": "This paper addresses the challenges in mathematical geometric problem solving (GPS) by introducing TrustGeoGen, a scalable data engine designed for generating reliable problem sets. It emphasizes the importance of integrating multimodal information and ensuring logical coherence through formal verification. The engine innovatively creates geometric data with aligned diagrams, textual descriptions, and solutions while maintaining rule compliance. The resulting GeoTrust-200K dataset and GeoTrust-test benchmark demonstrate improved evaluation standards and out-of-distribution generalization for models trained on this data."
                },
                "zh": {
                    "title": "ä¿¡ä»»å‡ ä½•ï¼šæå‡å‡ ä½•é—®é¢˜è§£å†³çš„å¯é æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTrustGeoGençš„æ•°æ®å¼•æ“ï¼Œç”¨äºç”Ÿæˆæ•°å­¦å‡ ä½•é—®é¢˜ï¼Œå¹¶é€šè¿‡å½¢å¼éªŒè¯æä¾›å¯é çš„åŸºå‡†ã€‚è¯¥å¼•æ“é€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°å‡ ä½•æ•°æ®çš„åˆæˆï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¯¹é½ç”Ÿæˆã€å½¢å¼éªŒè¯ã€é€’å½’çŠ¶æ€ç”Ÿæˆçš„å¤æ‚æ€§å‡çº§æœºåˆ¶ï¼Œä»¥åŠåŒæ—¶ç”Ÿæˆå¤šè§£å˜ä½“çš„ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨GeoTrust-testä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º49.17%ï¼Œæ˜¾ç¤ºå‡ºè¯„ä¼°çš„ä¸¥æ ¼æ€§ã€‚é€šè¿‡åœ¨GeoTrustä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨GeoQAä¸Šå®ç°äº†OODæ³›åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†é€»è¾‘ä¸ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18919",
            "title": "Clinical knowledge in LLMs does not translate to human interactions",
            "url": "https://huggingface.co/papers/2504.18919",
            "abstract": "Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.",
            "score": 1,
            "issue_id": 3480,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 26",
                "zh": "4æœˆ26æ—¥"
            },
            "hash": "25dc210bebba9c94",
            "authors": [
                "Andrew M. Bean",
                "Rebecca Payne",
                "Guy Parsons",
                "Hannah Rose Kirk",
                "Juan Ciro",
                "Rafael Mosquera",
                "Sara HincapiÃ© Monsalve",
                "Aruna S. Ekanayaka",
                "Lionel Tarassenko",
                "Luc Rocher",
                "Adam Mahdi"
            ],
            "affiliations": [
                "Birmingham Womens and Childrens NHS Foundation Trust, Birmingham, UK",
                "Contextual AI, Mountain View, USA",
                "Factored AI, Palo Alto, USA",
                "Institute of Biomedical Engineering, University of Oxford, Oxford, UK",
                "MLCommons, San Francisco, USA",
                "North Wales Medical School, Bangor University, Bangor, UK",
                "Nuffield Department of Primary Care, University of Oxford, Oxford, UK",
                "Oxford Internet Institute, University of Oxford, Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18919.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#data",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "LLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ: Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½Ğ¾ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ’ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ 1298 ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ LLM ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞ»Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² 94.9% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ°Ğ´Ğ°Ğ» Ğ´Ğ¾ 34.5%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ LLM Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs in Healthcare Need User Testing!",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in providing medical advice to the public. Although LLMs like GPT-4o and Llama 3 perform well on medical licensing exams, their real-world application shows significant discrepancies. In a study with 1,298 participants, LLMs accurately identified medical conditions in 94.9% of cases but failed to assist users effectively, with only 34.5% correctly identifying conditions. The findings highlight the importance of user interaction and suggest that standard testing does not adequately predict real-world performance, emphasizing the need for thorough human user testing before deploying LLMs in healthcare settings."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å»ºè®®ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—å»ºè®®ä¸­çš„åº”ç”¨ã€‚å°½ç®¡LLMsåœ¨åŒ»å­¦æ‰§ç…§è€ƒè¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ã€‚æˆ‘ä»¬åœ¨ä¸€é¡¹åŒ…å«1298åå‚ä¸è€…çš„æ§åˆ¶ç ”ç©¶ä¸­å‘ç°ï¼Œä½¿ç”¨LLMsçš„å‚ä¸è€…åœ¨è¯†åˆ«ç›¸å…³ç—…ç—‡å’Œé€‰æ‹©å¤„ç†æ–¹æ¡ˆæ—¶çš„å‡†ç¡®ç‡è¿œä½äºé¢„æœŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”¨æˆ·äº¤äº’æ˜¯LLMsåœ¨åŒ»ç–—å»ºè®®ä¸­åº”ç”¨çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œæœªæ¥éœ€è¦è¿›è¡Œç³»ç»Ÿçš„äººæœºäº¤äº’æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17258",
            "title": "Group Downsampling with Equivariant Anti-aliasing",
            "url": "https://huggingface.co/papers/2504.17258",
            "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature maps) on general finite groups with anti-aliasing. This involves the following: (a) Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup. (b) Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into G-equivariant networks",
            "score": 1,
            "issue_id": 3481,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "c8bf8ac7bd71ca11",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…: Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ½Ñ‚Ğ¸Ğ°Ğ»Ğ¸Ğ°ÑĞ¸Ğ½Ğ³",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (G-CNN). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğµ Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ñ‚Ğ¸Ğ°Ğ»Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°ÑƒĞ½ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² G-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing G-CNNs with Advanced Downsampling Techniques",
                    "desc": "This paper explores the role of downsampling layers in convolutional neural networks (CNNs), particularly focusing on group equivariant architectures like G-CNNs. The authors propose a new algorithm for downsampling feature maps on finite groups while ensuring anti-aliasing, which helps maintain the integrity of the signals. They introduce concepts such as bandlimited-ness and demonstrate how their method aligns with classical sampling theory, especially for periodic signals. Experimental results show that their approach enhances accuracy, preserves equivariance, and reduces the overall size of the model in image classification tasks."
                },
                "zh": {
                    "title": "æå‡CNNæ€§èƒ½çš„ä¸‹é‡‡æ ·æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨ç¾¤ç­‰å˜æ¶æ„ä¸­å‡åŒ€ä¸‹é‡‡æ ·å±‚çš„æ¨å¹¿ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹G-CNNsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šæœ‰é™ç¾¤å’Œä¸‹é‡‡æ ·ç‡çš„æƒ…å†µä¸‹é€‰æ‹©åˆé€‚çš„å­ç¾¤ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å¸¦é™æ€§æ¦‚å¿µï¼Œå¹¶æå‡ºäº†å¦‚ä½•è¿›è¡ŒæŠ—æ··å å¤„ç†çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ä¸‹é‡‡æ ·æ“ä½œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œæ›´å¥½åœ°ä¿æŒäº†ç­‰å˜æ€§ï¼Œå¹¶å‡å°‘äº†æ¨¡å‹å¤§å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16083",
            "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
            "url": "https://huggingface.co/papers/2504.16083",
            "abstract": "The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.",
            "score": 1,
            "issue_id": 3482,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "8ba72cd8f5463bbe",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16083.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² VLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMInference - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ğº Ğ´Ğ»Ñ ĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. MMInference Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° GPU Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 8,3 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Accelerating Vision Language Models with Dynamic Sparse Attention",
                    "desc": "This paper presents MMInference, a novel method designed to enhance Vision Language Models (VLMs) by addressing the challenge of quadratic attention complexity during the pre-filling phase. By analyzing the unique sparse patterns in video inputs, specifically the Grid pattern, the authors develop a dynamic sparse attention mechanism that optimizes the processing of long-context multi-modal data. MMInference allows for efficient computation by dynamically constructing sparse distributions based on input characteristics, without requiring any modifications to existing VLM architectures. Experimental results demonstrate that this approach significantly accelerates the pre-filling stage, achieving up to 8.3 times faster processing while preserving accuracy across various multi-modal benchmarks."
                },
                "zh": {
                    "title": "åŠ é€Ÿå¤šæ¨¡æ€è¾“å…¥çš„é¢„å¡«å……ï¼Œé‡Šæ”¾è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMMInferenceçš„åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€è¾“å…¥çš„é¢„å¡«å……é˜¶æ®µã€‚é€šè¿‡åˆ†æè§†é¢‘è¾“å…¥çš„æ—¶é—´å’Œç©ºé—´å±€éƒ¨æ€§ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§ç‹¬ç‰¹çš„ç¨€ç–æ¨¡å¼ï¼Œå³ç½‘æ ¼æ¨¡å¼ã€‚åŒæ—¶ï¼Œä¸åŒæ¨¡æ€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±•ç°å‡ºæ˜¾è‘—ä¸åŒçš„ç¨€ç–åˆ†å¸ƒã€‚MMInferenceèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€æ„å»ºç¨€ç–åˆ†å¸ƒï¼Œä»è€Œåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜é¢„å¡«å……é€Ÿåº¦ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† CameraBenchï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›æ‘„åƒæœºè¿åŠ¨ç†è§£ã€‚CameraBench åŒ…å«çº¦ 3,000 ä¸ªå¤šæ ·åŒ–çš„ç½‘ç»œè§†é¢‘ï¼Œç”±ä¸“å®¶é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿‡ç¨‹è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬ä¸æ‘„å½±å¸ˆåˆä½œï¼Œè®¾è®¡äº†æ‘„åƒæœºè¿åŠ¨åŸºå…ƒçš„åˆ†ç±»æ³•ã€‚ä¾‹å¦‚ï¼ŒæŸäº›è¿åŠ¨å¦‚â€œè·Ÿéšâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹ï¼Œå¦‚ç§»åŠ¨çš„ä¸»ä½“ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œä»¥é‡åŒ–äººç±»æ ‡æ³¨æ€§èƒ½ï¼Œå‘ç°é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’ŒåŸºäºæ•™ç¨‹çš„åŸ¹è®­å¯æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚ä½¿ç”¨ CameraBenchï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å’Œè§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå‘ç° SfM æ¨¡å‹éš¾ä»¥æ•æ‰ä¾èµ–åœºæ™¯å†…å®¹çš„è¯­ä¹‰åŸºå…ƒï¼Œè€Œ VLMs éš¾ä»¥æ•æ‰éœ€è¦ç²¾ç¡®è½¨è¿¹ä¼°è®¡çš„å‡ ä½•åŸºå…ƒã€‚æˆ‘ä»¬éšååœ¨ CameraBench ä¸Šå¾®è°ƒäº†ä¸€ä¸ªç”Ÿæˆ VLMï¼Œä»¥å®ç°ä¸¤è€…çš„ä¼˜åŠ¿ç»“åˆï¼Œå¹¶å±•ç¤ºå…¶åº”ç”¨ï¼ŒåŒ…æ‹¬è¿åŠ¨å¢å¼ºçš„å­—å¹•ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘æ–‡æœ¬æ£€ç´¢ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†ç±»æ³•ã€åŸºå‡†å’Œæ•™ç¨‹èƒ½æ¨åŠ¨æœªæ¥åŠªåŠ›ï¼Œå®ç°ç†è§£ä»»ä½•è§†é¢‘ä¸­æ‘„åƒæœºè¿åŠ¨çš„æœ€ç»ˆç›®æ ‡ã€‚",
        "title": "Towards Understanding Camera Motions in Any Video",
        "pinyin": "WÇ’men jiÃ¨shÃ o le CameraBench, yÄ«gÃ¨ dÃ guÄ«mÃ³ de shÃ¹jÃ¹jÃ­ hÃ© jÄ«zhÇ”n, zhÇyÇn pÃ­nggÇ” hÃ© gÇijÃ¬n shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng lÇjiÄ›. CameraBench bÄohÃ¡n yuÄ“ 3,000 gÃ¨ duÅyÃ nghuÃ  de wÇngluÃ² shÃ¬pÇn, yÃ³u zhuÄnjiÄ tÅngguÃ² yÃ¡nzhÃ²ng de duÅ jiÄ“duÃ n zhÃ¬liÃ ng kÃ²ngzhÃ¬ guÃ²chÃ©ng jÃ¬nxiÃ ng biÄozhÃ¹. WÇ’men yÇ” shÃ¨yÇngshÄ« hÃ©zuÃ², shÃ¨jÃ¬ le shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng jÄ«yuÇn de fÄ“nlÃ¨i fÇ. LÃ¬rÃº, mÇ’uxiÄ“ yÃ¹ndÃ²ng rÃº â€œgÄ“nsuÃ­â€ xÅ«yÃ o lÇjiÄ› chÇngjÄ«ng nÃ¨irÃ³ng, rÃº yÃ­dÃ²ng de zhÇ”tÇ. WÇ’men jÃ¬nxÃ­ng le dÃ guÄ«mÃ³ de rÃ©nlÃ¨i yÃ¡njiÅ«, yÇ liÃ ngzhÃ¬ rÃ©nlÃ¨i biÄozhÃ¹ xÃ¬ngnÃ©ng, fÄxiÃ n lÇngyÃ¹ zhuÄnjÃ¬ zhÄ«shi hÃ© jÄ«yÃº jiÃ oxuÃ© de pÃ©ixÃ¹n kÄ› xiÇnzhÃ¹ tÄ«gÄo zhÃ¹nquÃ¨xÃ¬ng. ShÇyÃ²ng CameraBench, wÇ’men pÃ­nggÇ” le jiÃ©gÃ²u cÃ³ng yÃ¹ndÃ²ng (SfM) hÃ© shÃ¬pÇn yÇ”yÃ¡n mÃ³xÃ­ng (VLMs), fÄxiÃ n SfM mÃ³xÃ­ng nÃ¡n yÇ bÇ”zhÃ²u yÄ«lÃ i chÇngjÄ«ng nÃ¨irÃ³ng de yÇ”yÃ¡n jÄ«yuÇn, Ã©r VLMs nÃ¡n yÇ bÇ”zhÃ²u xÅ«yÃ o jÄ«ngquÃ¨ guÇjÃ¬ gÅ«jÃ¬ de jÇhÃ© jÄ«yuÇn. WÇ’men shÃ¹hÃ²u zÃ i CameraBench shÃ ng wÄ“itiÃ¡o le yÄ«gÃ¨ shÄ“ngchÃ©ng VLM, yÇ shÃ­xiÃ n liÇng zhÄ› de yÅushÃ¬ jiÄ“hÃ©, bÃ¬ng zhÇnshÃ¬ qÃ­ yÃ¬ngyÃ²ng, bÄokuÃ² yÃ¹ndÃ²ng zÄ“ngqiÃ¡ng de zÃ¬mÇ”, shÃ¬pÇn wÃ¨ndÃ¡ hÃ© shÃ¬pÇn wÃ©nbÄ›n chÃ¡xÃºn. WÇ’men xÄ«wÃ ng wÇ’men de fÄ“nlÃ¨i fÇ, jÄ«zhÇ”n hÃ© jiÃ oxuÃ© nÃ©ng tuÄ«dÃ²ng wÃ¨ilÃ¡i nÇ”lÃ¬, shÃ­xiÃ n lÇjiÄ› rÃ¨nhÃ© shÃ¬pÇn zhÅng shÃ¨xiÃ ngjÄ« yÃ¹ndÃ²ng de zhÃ²ngdiÇn mÃ¹biÄo.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"æ‘„åƒæœº\", \"pinyin\": \"shÃ¨ xiÃ ng jÄ«\", \"trans\": \"camera\"},\n    {\"word\": \"è¿åŠ¨\", \"pinyin\": \"yÃ¹n dÃ²ng\", \"trans\": \"motion\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understand\"},\n    {\"word\": \"å¤šæ ·åŒ–\", \"pinyin\": \"duÅ yÃ ng huÃ \", \"trans\": \"diverse\"},\n    {\"word\": \"ç½‘ç»œè§†é¢‘\", \"pinyin\": \"wÇng luÃ² shÃ¬ pÃ­n\", \"trans\": \"online video\"},\n    {\"word\": \"ä¸“å®¶\", \"pinyin\": \"zhuÄn jiÄ\", \"trans\": \"expert\"},\n    {\"word\": \"ä¸¥æ ¼\", \"pinyin\": \"yÃ¡n gÃ©\", \"trans\": \"strict\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“ duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"è´¨é‡æ§åˆ¶\", \"pinyin\": \"zhÃ¬ liÃ ng kÃ²ng zhÃ¬\", \"trans\": \"quality control\"},\n    {\"word\": \"è¿‡ç¨‹\", \"pinyin\": \"guÃ² chÃ©ng\", \"trans\": \"process\"},\n    {\"word\": \"æ ‡æ³¨\", \"pinyin\": \"biÄo zhÃ¹\", \"trans\": \"annotate\"},\n    {\"word\": \"æ‘„å½±å¸ˆ\", \"pinyin\": \"shÃ¨ yÇng shÄ«\", \"trans\": \"photographer\"},\n    {\"word\": \"è®¾è®¡\", \"pinyin\": \"shÃ¨ jÃ¬\", \"trans\": \"design\"},\n    {\"word\": \"åŸºå…ƒ\", \"pinyin\": \"jÄ« yuÃ¡n\", \"trans\": \"primitive\"},\n    {\"word\": \"åˆ†ç±»æ³•\", \"pinyin\": \"fÄ“n lÃ¨i fÇ\", \"trans\": \"classification method\"},\n    {\"word\": \"ä¾‹å¦‚\", \"pinyin\": \"lÃ¬ rÃº\", \"trans\": \"for example\"},\n    {\"word\": \"æŸäº›\", \"pinyin\": \"mÇ’u xiÄ“\", \"trans\": \"some\"},\n    {\"word\": \"è·Ÿéš\", \"pinyin\": \"gÄ“n suÃ­\", \"trans\": \"follow\"},\n    {\"word\": \"éœ€è¦\", \"pinyin\": \"xÅ« yÃ o\", \"trans\": \"need\"},\n    {\"word\": \"åœºæ™¯\", \"pinyin\": \"chÇng jÇng\", \"trans\": \"scene\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"ä¸»ä½“\", \"pinyin\": \"zhÇ” tÇ\", \"trans\": \"subject\"},\n    {\"word\": \"ç§»åŠ¨\", \"pinyin\": \"yÃ­ dÃ²ng\", \"trans\": \"move\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"study\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ ng huÃ \", \"trans\": \"quantify\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"ä¸“ä¸šçŸ¥è¯†\", \"pinyin\": \"zhuÄn yÃ¨ zhÄ« shi\", \"trans\": \"professional knowledge\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ•™ç¨‹\", \"pinyin\": \"jiÃ o chÃ©ng\", \"trans\": \"tutorial\"},\n    {\"word\": \"åŸ¹è®­\", \"pinyin\": \"pÃ©i xÃ¹n\", \"trans\": \"training\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”n quÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"ç»“æ„ä»è¿åŠ¨\", \"pinyin\": \"jiÃ© gÃ²u cÃ³ng yÃ¹n dÃ²ng\", \"trans\": \"Structure from Motion (SfM)\"},\n    {\"word\": \"è§†é¢‘è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ pÃ­n yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"Video Language Model (VLM)\"},\n    {\"word\": \"æ•æ‰\", \"pinyin\": \"bÇ” zhuÅ\", \"trans\": \"capture\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ” yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇ hÃ©\", \"trans\": \"geometric\"},\n    {\"word\": \"è½¨è¿¹\", \"pinyin\": \"guÇ jÄ«\", \"trans\": \"trajectory\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ« jÃ¬\", \"trans\": \"estimate\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅu shÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"åŒ…æ‹¬\", \"pinyin\": \"bÄo kuÃ²\", \"trans\": \"include\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"å­—å¹•\", \"pinyin\": \"zÃ¬ mÃ¹\", \"trans\": \"subtitle\"},\n    {\"word\": \"é—®ç­”\", \"pinyin\": \"wÃ¨n dÃ¡\", \"trans\": \"question and answer\"},\n    {\"word\": \"æ–‡æœ¬æ£€ç´¢\", \"pinyin\": \"wÃ©n bÄ›n jiÇn suÇ’\", \"trans\": \"text retrieval\"},\n    {\"word\": \"å¸Œæœ›\", \"pinyin\": \"xÄ« wÃ ng\", \"trans\": \"hope\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ« dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"åŠªåŠ›\", \"pinyin\": \"nÇ” lÃ¬\", \"trans\": \"effort\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€ç»ˆ\", \"pinyin\": \"zuÃ¬ zhÅng\", \"trans\": \"ultimate\"},\n    {\"word\": \"ç›®æ ‡\", \"pinyin\": \"mÃ¹ biÄo\", \"trans\": \"goal\"}\n]",
        "trans": "We introduce CameraBench, a large-scale dataset and benchmark aimed at evaluating and improving the understanding of camera motion. CameraBench contains approximately 3,000 diverse web videos, annotated by experts through a rigorous multi-stage quality control process. We collaborated with photographers to design a classification scheme for camera motion primitives. For instance, certain motions like \"following\" require an understanding of scene content, such as moving subjects. We conducted large-scale human studies to quantify human annotation performance and found that domain expertise and tutorial-based training can significantly improve accuracy. Using CameraBench, we evaluated Structure-from-Motion (SfM) and Video Language Models (VLMs), finding that SfM models struggle to capture semantic primitives dependent on scene content, while VLMs struggle with geometric primitives that require precise trajectory estimation. Subsequently, we fine-tuned a generative VLM on CameraBench to combine the strengths of both, demonstrating its applications, including motion-enhanced captioning, video question answering, and video-text retrieval. We hope that our classification scheme, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motion in any video.",
        "update_ts": "2025-04-28 10:45"
    }
}