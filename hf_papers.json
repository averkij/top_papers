{
    "date": {
        "ru": "18 апреля",
        "en": "April 18",
        "zh": "4月18日"
    },
    "time_utc": "2025-04-18 02:20",
    "weekday": 4,
    "issue_id": 3303,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.12626",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "url": "https://huggingface.co/papers/2504.12626",
            "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.",
            "score": 2,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "fd1688a4e26dbb32",
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12626.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "FramePack: эффективное предсказание кадров для генерации видео",
                    "desc": "FramePack - это новая структура нейронной сети для предсказания следующего кадра в видео. Она сжимает входные кадры, что позволяет обрабатывать большое количество кадров с вычислительной сложностью, сравнимой с диффузией изображений. Авторы также предлагают метод сэмплирования, предотвращающий накопление ошибок. FramePack может быть использован для дообучения существующих моделей видеодиффузии, потенциально улучшая их визуальное качество."
                },
                "en": {
                    "title": "FramePack: Efficient Video Generation with Next-Frame Prediction",
                    "desc": "The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning."
                },
                "zh": {
                    "title": "FramePack：提升视频生成的下一帧预测能力",
                    "desc": "我们提出了一种神经网络结构，FramePack，用于训练视频生成的下一帧预测模型。FramePack通过压缩输入帧，使得变换器的上下文长度固定，无论视频长度如何。这样，我们能够使用与图像扩散相似的计算瓶颈处理大量帧，从而显著提高视频训练的批量大小。我们还提出了一种反漂移采样方法，以避免迭代过程中的曝光偏差，从而提高生成帧的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05506",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "url": "https://huggingface.co/papers/2504.05506",
            "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "score": 2,
            "issue_id": 3303,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "a727d08eac22e920",
            "authors": [
                "Ahmed Masry",
                "Mohammed Saidul Islam",
                "Mahir Ahmed",
                "Aayush Bajaj",
                "Firoz Kabir",
                "Aaryaman Kartha",
                "Md Tahmid Rahman Laskar",
                "Mizanur Rahman",
                "Shadikur Rahman",
                "Mehrad Shahmohammadi",
                "Megh Thakkar",
                "Md Rizwan Parvez",
                "Enamul Hoque",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Dialpad Inc., Canada",
                "MILA - Quebec AI Institute, Canada",
                "Nanyang Technological University, Singapore",
                "Qatar Computing Research Institute (QCRI)",
                "RBC, Canada",
                "Salesforce Research, USA",
                "York University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ChartQAPro: новый вызов для ИИ в понимании графиков",
                    "desc": "ChartQAPro - новый набор данных для задачи ответов на вопросы по графикам. Он включает более 1300 разнообразных графиков из реальных источников и около 2000 вопросов различных типов. Тестирование показало, что современные мультимодальные языковые модели значительно хуже справляются с ChartQAPro по сравнению с предыдущими наборами данных. Авторы провели детальный анализ ошибок и выявили ключевые проблемы в понимании и рассуждении о графиках для языковых моделей."
                },
                "en": {
                    "title": "ChartQAPro: Elevating Chart Understanding for AI",
                    "desc": "This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs."
                },
                "zh": {
                    "title": "提升图表问答系统的挑战与机遇",
                    "desc": "本论文介绍了ChartQAPro，这是一个新的基准测试，旨在提高图表问答系统的性能。它包含来自157个不同来源的1,341个图表，涵盖多种图表类型，并提供1,948个多样化的问题。通过对21个模型的评估，我们发现现代大型视觉语言模型在ChartQAPro上的表现显著下降，显示出图表推理的复杂性。我们的研究还包括详细的错误分析和消融研究，以识别图表理解和推理中的关键挑战和机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13122",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "url": "https://huggingface.co/papers/2504.13122",
            "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.",
            "score": 1,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "efbc3b240498ce70",
            "authors": [
                "Haojian Huang",
                "Haodong Chen",
                "Shengqiong Wu",
                "Meng Luo",
                "Jinlan Fu",
                "Xinya Du",
                "Hanwang Zhang",
                "Hao Fei"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13122.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VistaDPO: Точное согласование видео и языка на всех уровнях",
                    "desc": "VistaDPO - это новая система для оптимизации предпочтений в видео с использованием иерархического пространственно-временного подхода. Она улучшает согласование текста и видео на трех уровнях: общем содержании, временной семантике и пространственных объектах. Авторы создали датасет VistaDPO-7k с 7200 парами вопросов-ответов, аннотированными предпочтительными и отвергнутыми ответами, а также временными метками и ограничивающими рамками. Эксперименты показали, что VistaDPO значительно улучшает работу существующих больших видеомоделей, эффективно снижая рассогласование видео и языка и галлюцинации."
                },
                "en": {
                    "title": "Aligning Video and Language: Introducing VistaDPO",
                    "desc": "This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition."
                },
                "zh": {
                    "title": "VistaDPO：提升视频理解的偏好对齐",
                    "desc": "本文介绍了一种新框架VistaDPO，用于视频层次空间-时间直接偏好优化，旨在解决大型视频模型（LVMs）在视频理解中的人类直觉不一致和视频幻觉问题。VistaDPO通过三个层次增强文本-视频偏好对齐：实例层、时间层和感知层，分别对齐视频内容、时间语义和空间对象。为了支持细粒度视频-语言偏好对齐，研究团队构建了VistaDPO-7k数据集，包含7200个问答对及其空间-时间信息。实验结果表明，VistaDPO显著提升了现有LVMs的性能，有效减轻了视频-语言的不一致性和幻觉现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12369",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "url": "https://huggingface.co/papers/2504.12369",
            "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
            "score": 1,
            "issue_id": 3303,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "79cf162a1b60f887",
            "authors": [
                "Zeqi Xiao",
                "Yushi Lan",
                "Yifan Zhou",
                "Wenqi Ouyang",
                "Shuai Yang",
                "Yanhong Zeng",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "WorldMem: Улучшение долгосрочной согласованности в симуляции миров с помощью памяти",
                    "desc": "WorldMem - это фреймворк для улучшения симуляции виртуальных миров с помощью банка памяти. Он использует механизм внимания для извлечения релевантной информации из кадров памяти на основе их состояний. Это позволяет точно реконструировать ранее наблюдаемые сцены даже при значительных изменениях ракурса или временных промежутках. Включение временных меток в состояния позволяет моделировать не только статичный мир, но и его динамическое развитие во времени."
                },
                "en": {
                    "title": "Enhancing World Simulation with Memory-Driven Consistency",
                    "desc": "This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds."
                },
                "zh": {
                    "title": "增强世界模拟的一致性与动态性",
                    "desc": "世界模拟因其建模虚拟环境和预测行为后果的能力而越来越受欢迎。然而，有限的时间上下文窗口常常导致长期一致性维护的失败，特别是在保持三维空间一致性方面。在这项工作中，我们提出了WorldMem框架，通过一个包含记忆单元的记忆库来增强场景生成，这些记忆单元存储记忆帧和状态（例如，姿势和时间戳）。通过采用记忆注意机制，我们的方法能够准确重建先前观察到的场景，即使在显著的视角或时间间隔下也能有效提取相关信息。"
                }
            }
        }
    ],
    "link_prev": "2025-04-17.html",
    "link_next": "2025-04-21.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4月17日"
    },
    "short_date_next": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4月21日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZhè wénzhāng tǎolùn zhè zhǒng yǎnsè zài rénlèi hé shìyǎn yǔyán móxíng (VLMs) zhījiàn hé lǐjiě shìjiè zhōng de zuòyòng. Tā jièshào Yǎnsè Běnch, yīgè xīn de běnchmark yǐ cèshì VLMs de yǎnsè lǐjiě jìnéng. Yánjiū fāxiàn zhǐyǒu dàxíng móxíng biǎoxiàn gèng hǎo, dāngqián VLMs yībān shū huǎng yǎnsè lǐjiě. Tā yě shuōmíng yòng Lián de Sīxiǎng (CoT) tuǐlǐ néng gǎishàn yǎnsè rènwù de zhǔnquèdu. Zhèxiē zuòzhě xīwàng Yǎnsè Běnch huì bāngzhù tuījìn zhè ge lǐngyù de yánjiū.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"pərˈsiːv\", \"trans\": \"感知\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"ˈvɪʒən ˈlæŋɡwɪdʒ mɒdəlz\", \"trans\": \"视觉-语言模型\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"ˈbɛnʧmɑːk\", \"trans\": \"基准\"},\n    {\"word\": \"neglect\", \"pinyin\": \"nɪˈɡlɛkt\", \"trans\": \"忽视\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \"ʧeɪn ɒv θɔːt\", \"trans\": \"思维链\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ˈriːz(ə)nɪŋ\", \"trans\": \"推理\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"ˈækjərəsi\", \"trans\": \"准确性\"},\n    {\"word\": \"advance\", \"pinyin\": \"ædˈvɑːns\", \"trans\": \"推进\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}