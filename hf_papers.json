{
    "date": {
        "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 7",
        "zh": "10æœˆ7æ—¥"
    },
    "time_utc": "2025-10-07 02:16",
    "weekday": 1,
    "issue_id": 6275,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.00263",
            "title": "Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions",
            "url": "https://huggingface.co/papers/2510.00263",
            "abstract": "A framework for calibrating probabilistic autoraters to preference distributions using supervised fine-tuning and reinforcement learning improves alignment with human values and reduces bias.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.",
            "score": 5,
            "issue_id": 6275,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "96cee62eae60ad82",
            "authors": [
                "Zhuohang Li",
                "Xiaowei Li",
                "Chengyu Huang",
                "Guowang Li",
                "Katayoon Goshvadi",
                "Bo Dai",
                "Dale Schuurmans",
                "Paul Zhou",
                "Hamid Palangi",
                "Yiwen Song",
                "Palash Goyal",
                "Murat Kantarcioglu",
                "Bradley A. Malin",
                "Yuan Xue"
            ],
            "affiliations": [
                "Cornell University",
                "Google",
                "Google DeepMind",
                "Scale AI",
                "University of Alberta",
                "Vanderbilt University",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00263.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#ethics",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ framework Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² (autoraters) - LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‡Ğ°Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: supervised fine-tuning Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ reinforcement learning Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ bias Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞµ alignment Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Aligning Autoraters with Human Preferences through Advanced Calibration",
                    "desc": "This paper presents a framework for improving the accuracy of automated judges, known as autoraters, which evaluate preferences in a way that aligns better with human values. The authors highlight the limitations of traditional autoraters that rely on fixed preference labels, which can oversimplify complex human judgments. They propose two methods for training these autoraters: one using supervised fine-tuning for detailed preference data and another using reinforcement learning for simpler binary data. The results demonstrate that their approach enhances the alignment of predictions with actual human preferences, reduces bias, and maintains performance on objective tasks."
                },
                "zh": {
                    "title": "æ ¡å‡†è‡ªåŠ¨è¯„åˆ†å™¨ä»¥å¯¹é½äººç±»ä»·å€¼è§‚",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¥æ ¡å‡†æ¦‚ç‡è‡ªåŠ¨è¯„åˆ†å™¨ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»ä»·å€¼è§‚å¯¹é½å¹¶å‡å°‘åè§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå¯é çš„è‡ªåŠ¨è¯„åˆ†å™¨å¿…é¡»å­¦ä¹ å»ºæ¨¡ç›®æ ‡äººç¾¤å®šä¹‰çš„å®Œæ•´åå¥½åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–äºç¦»æ•£çš„åå¥½æ ‡ç­¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å­¦ä¹ æ–¹æ³•ï¼Œåˆ†åˆ«é€‚ç”¨äºä¸åŒçš„æ•°æ®æ¡ä»¶ï¼šä¸€ç§æ˜¯é’ˆå¯¹å¯†é›†æ¦‚ç‡æ ‡ç­¾çš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼Œå¦ä¸€ç§æ˜¯é’ˆå¯¹ç¨€ç–äºŒå…ƒæ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨åˆ†å¸ƒåŒ¹é…ç›®æ ‡å¾®è°ƒè‡ªåŠ¨è¯„åˆ†å™¨å¯ä»¥æé«˜å…¶é¢„æµ‹çš„æ¦‚ç‡ä¸ç›®æ ‡åå¥½åˆ†å¸ƒçš„å¯¹é½ç¨‹åº¦ï¼ŒåŒæ—¶é™ä½ä½ç½®åè§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03264",
            "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
            "url": "https://huggingface.co/papers/2510.03264",
            "abstract": "Introducing reasoning data during pretraining significantly enhances LLM performance compared to post-training, with pretraining benefiting more from diverse data patterns while SFT benefits more from high-quality data.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
            "score": 2,
            "issue_id": 6275,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "4ab12dcfe1afbbf7",
            "authors": [
                "Syeda Nahida Akter",
                "Shrimai Prabhumoye",
                "Eric Nyberg",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Yejin Choi",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "Boston University",
                "Carnegie Mellon University",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03264.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ (Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ 19%), Ñ‡ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ post-training, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ fine-tuning. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿: pretraining Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ 11%), Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº supervised fine-tuning Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ 15%). Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining Ğ¸Ğ¼ĞµÑÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»Ğµ SFT, Ğ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ SFT-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Front-Load Reasoning for Stronger LLMs!",
                    "desc": "This paper investigates the impact of introducing reasoning data during the pretraining phase of large language models (LLMs) compared to post-training. The authors find that incorporating diverse reasoning data early in pretraining leads to significant performance improvements, establishing foundational reasoning capabilities that are not fully recoverable through later fine-tuning. They highlight that pretraining benefits from a variety of reasoning patterns, while fine-tuning is more effective with high-quality data. The study challenges traditional views on language modeling and reasoning, offering insights on optimal data allocation throughout the training process."
                },
                "zh": {
                    "title": "æå‰å¼•å…¥æ¨ç†æ•°æ®ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥æ¨ç†æ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæå‰åœ¨é¢„è®­ç»ƒä¸­åŠ å…¥æ¨ç†æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æå‡19%ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒé˜¶æ®µæ›´ä¾èµ–äºæ¨ç†æ¨¡å¼çš„å¤šæ ·æ€§ï¼Œè€Œå¾®è°ƒé˜¶æ®µåˆ™æ›´æ³¨é‡æ•°æ®çš„è´¨é‡ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‘æˆ˜äº†è¯­è¨€å»ºæ¨¡ä¸æ¨ç†çš„ä¼ ç»Ÿåˆ†ç¦»ï¼Œä¸ºæ•°æ®åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„åˆç†åˆ†é…æä¾›äº†æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04399",
            "title": "Utility-Learning Tension in Self-Modifying Agents",
            "url": "https://huggingface.co/papers/2510.04399",
            "abstract": "Self-improving systems face a utility-learning tension that can degrade their ability to learn and generalize, requiring capacity bounds to ensure safe self-modification.  \t\t\t\t\tAI-generated summary \t\t\t\t As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
            "score": 1,
            "issue_id": 6275,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "9fa188fee82ece5c",
            "authors": [
                "Charles L. Wang",
                "Keir Dorchen",
                "Peter Jin"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DeepMind",
                "ETH Zurich",
                "Google Brain",
                "IDSIA (Istituto Dalle Molle di Studi sullâ€™Intelligenza Artificiale)",
                "Max Planck Institute for Intelligent Systems",
                "New York University",
                "SingularityNET",
                "Stanford University",
                "Technische UniversitÃ¤t MÃ¼nchen",
                "University of Amsterdam",
                "University of Bath",
                "University of California, Berkeley",
                "University of Cambridge",
                "University of Edinburgh",
                "University of Freiburg",
                "University of Montreal (MILA)",
                "University of Oxford",
                "University of Toronto",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04399.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#alignment",
                    "#agi",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ğº AI Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ AI-ÑĞ¸ÑÑ‚ĞµĞ¼, ÑÑ‚Ñ€ĞµĞ¼ÑÑ‰Ğ¸Ñ…ÑÑ Ğº ÑĞ²ĞµÑ€Ñ…Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ: Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ¸Ñ‚ÑŒ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼. ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ (capacity) Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±ĞµĞ· Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Balancing Improvement and Learning in Self-Modifying AI Systems",
                    "desc": "This paper discusses the challenges faced by self-improving AI systems, particularly the conflict between improving performance (utility) and maintaining the ability to learn effectively. It introduces a framework that separates different aspects of self-modification, allowing for a clearer analysis of how changes can impact learning. The authors identify a critical tension where beneficial changes can lead to a decline in the system's ability to generalize from data. They propose that to ensure safe self-modification, the system's capacity for change must be limited, and they validate their findings through numerical experiments comparing different modification strategies."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ”¹è¿›ç³»ç»Ÿçš„æ•ˆç”¨ä¸å­¦ä¹ çš„å¹³è¡¡",
                    "desc": "è‡ªæˆ‘æ”¹è¿›ç³»ç»Ÿé¢ä¸´æ•ˆç”¨å­¦ä¹ çš„ç´§å¼ å…³ç³»ï¼Œè¿™å¯èƒ½ä¼šé™ä½å…¶å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡äº”ä¸ªç»´åº¦çš„åˆ†è§£å’Œå†³ç­–å±‚çš„å½¢å¼åŒ–ï¼Œåˆ†æäº†æ¿€åŠ±ä¸å­¦ä¹ è¡Œä¸ºçš„åˆ†ç¦»ã€‚æˆ‘ä»¬çš„ä¸»è¦ç»“æœæ­ç¤ºäº†æ•ˆç”¨ä¸å­¦ä¹ ä¹‹é—´çš„ç»“æ„æ€§å†²çªï¼Œè¡¨æ˜æ•ˆç”¨é©±åŠ¨çš„å˜åŒ–å¯èƒ½ä¼šç ´åå¯é å­¦ä¹ å’Œæ³›åŒ–çš„ç»Ÿè®¡å‰æã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹çš„å®¹é‡æ— é™å¢é•¿æ—¶ï¼Œæ•ˆç”¨ç†æ€§çš„è‡ªæˆ‘å˜åŒ–å¯èƒ½ä½¿å¯å­¦ä¹ çš„ä»»åŠ¡å˜å¾—ä¸å¯å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03561",
            "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
            "url": "https://huggingface.co/papers/2510.03561",
            "abstract": "The Reactive Transformer (RxT) addresses the limitations of stateless Transformers in conversational AI by using an event-driven paradigm with a fixed-size Short-Term Memory (STM) system, achieving linear scaling and low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
            "score": 1,
            "issue_id": 6275,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "b213f271f5c52cec",
            "authors": [
                "Adam Filipek"
            ],
            "affiliations": [
                "Reactive AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03561.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Transformer: Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Reactive Transformer (RxT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ² conversational AI. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Transformer Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²ÑÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, RxT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ event-driven Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ (STM). Ğ­Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ O(NÂ²Â·T) Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ O(NÂ·T) Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³."
                },
                "en": {
                    "title": "Revolutionizing Conversational AI with Reactive Transformers",
                    "desc": "The Reactive Transformer (RxT) is a new architecture designed to improve conversational AI by addressing the limitations of traditional stateless Transformers. It uses an event-driven approach combined with a fixed-size Short-Term Memory (STM) system, which allows for linear scaling and reduced latency during interactions. By processing each conversational turn as a discrete event, RxT maintains context efficiently and updates memory asynchronously, leading to faster response times. Experimental results show that RxT outperforms stateless models in terms of performance and inference speed, making it suitable for real-time, long-form conversations."
                },
                "zh": {
                    "title": "ååº”å¼å˜æ¢å™¨ï¼šå®ç°å®æ—¶å¯¹è¯çš„åˆ›æ–°æ¶æ„",
                    "desc": "ååº”å¼å˜æ¢å™¨ï¼ˆRxTï¼‰é€šè¿‡ä½¿ç”¨äº‹ä»¶é©±åŠ¨çš„èŒƒå¼å’Œå›ºå®šå¤§å°çš„çŸ­æœŸè®°å¿†ï¼ˆSTMï¼‰ç³»ç»Ÿï¼Œè§£å†³äº†æ— çŠ¶æ€å˜æ¢å™¨åœ¨å¯¹è¯AIä¸­çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼ŒRxTèƒ½å¤Ÿä»¥çº¿æ€§æ–¹å¼æ‰©å±•ï¼Œå¹¶æ˜¾è‘—é™ä½å»¶è¿Ÿã€‚è¯¥æ¶æ„å°†æ¯ä¸ªå¯¹è¯è½®æ¬¡è§†ä¸ºå®æ—¶çš„ç¦»æ•£äº‹ä»¶ï¼Œä¿æŒä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œä¼˜åŒ–äº†å†…å­˜æ›´æ–°è¿‡ç¨‹ã€‚é€šè¿‡å°†å“åº”ç”Ÿæˆä¸å†…å­˜æ›´æ–°è§£è€¦ï¼ŒRxTå®ç°äº†çœŸæ­£çš„å®æ—¶å¯¹è¯ï¼Œé€‚ç”¨äºé•¿æ—¶é—´çš„äº¤äº’ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-06.html",
    "link_next": "2025-10-08.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "06.10",
        "en": "10/06",
        "zh": "10æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.10",
        "en": "10/08",
        "zh": "10æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}