{
    "date": {
        "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 24",
        "zh": "12æœˆ24æ—¥"
    },
    "time_utc": "2024-12-24 05:10",
    "weekday": 1,
    "issue_id": 1283,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17256",
            "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
            "url": "https://huggingface.co/papers/2412.17256",
            "abstract": "In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.",
            "score": 15,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "1a1ee4818597feae",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Lulu Zhao",
                "Yijun Wang",
                "Zifei Shan",
                "Junxian He"
            ],
            "affiliations": [
                "BAAI",
                "Tencent",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17256.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ (Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² (ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ). ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº B-STaR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ B-STaR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ°, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°."
                },
                "en": {
                    "title": "Balancing Exploration and Exploitation for Self-Improvement in ML Models",
                    "desc": "This paper addresses the challenge of improving machine learning models in the absence of large human-annotated datasets, focusing on self-improvement through iterative training on their own outputs. It identifies two key factors that influence this process: the model's ability to explore diverse responses and the effectiveness of external rewards in selecting high-quality outputs. The authors introduce B-STaR, a framework that dynamically adjusts training configurations to maintain a balance between exploration and exploitation. Experiments show that B-STaR enhances exploratory capabilities and improves overall model performance in reasoning tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ”¹è¿›ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„å…³é”®",
                    "desc": "åœ¨ç¼ºä¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œè‡ªæˆ‘æ”¹è¿›æˆä¸ºæå‡æ¨¡å‹æ€§èƒ½çš„ä¸»è¦æ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªå…³é”®å› ç´ ï¼šæ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–å“åº”çš„èƒ½åŠ›ï¼ˆæ¢ç´¢ï¼‰å’Œå¤–éƒ¨å¥–åŠ±åœ¨åŒºåˆ†é«˜è´¨é‡å€™é€‰é¡¹ä¸ä½è´¨é‡å€™é€‰é¡¹ä¸­çš„æœ‰æ•ˆæ€§ï¼ˆåˆ©ç”¨ï¼‰ã€‚é€šè¿‡å¯¹æ•°å­¦æ¨ç†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›åœ¨è¿­ä»£è¿‡ç¨‹ä¸­è¿…é€Ÿä¸‹é™ï¼Œè€Œå¤–éƒ¨å¥–åŠ±çš„æœ‰æ•ˆæ€§ä¹Ÿéšä¹‹å‡å¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†B-STaRæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¿­ä»£ä¸­è‡ªæˆ‘è°ƒæ•´é…ç½®ï¼Œä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä»è€Œä¼˜åŒ–è‡ªæˆ‘æ”¹è¿›çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17451",
            "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2412.17451",
            "abstract": "Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.",
            "score": 14,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "2866001b23a585e1",
            "authors": [
                "Wei Liu",
                "Junlong Li",
                "Xiwen Zhang",
                "Fan Zhou",
                "Yu Cheng",
                "Junxian He"
            ],
            "affiliations": [
                "Helixon Research",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17451.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MSTaR Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with Self-Evolving Training",
                    "desc": "This paper focuses on improving reasoning abilities in Large Multimodal Models (LMMs) through a method called self-evolving training, which allows models to learn from their own outputs. The authors identify three critical factors that influence the effectiveness of this training: the Training Method, Reward Model, and Prompt Variation. They provide a detailed analysis of how different configurations of these factors can optimize multimodal reasoning performance. The study culminates in the development of a framework named MSTaR, which demonstrates significant improvements in reasoning tasks across various model sizes and benchmarks without requiring additional human annotations."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–è®­ç»ƒï¼šæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘è¿›åŒ–è®­ç»ƒåœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¯†åˆ«äº†å½±å“è®­ç»ƒæ•ˆæœçš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šè®­ç»ƒæ–¹æ³•ã€å¥–åŠ±æ¨¡å‹å’Œæç¤ºå˜ä½“ï¼Œå¹¶ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›å› ç´ çš„ä¸åŒé…ç½®ã€‚ç ”ç©¶ç»“æœæä¾›äº†ä¸€å¥—æœ€ä½³å®è·µï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€æ¨ç†çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†MSTaRæ¡†æ¶ï¼Œå±•ç¤ºäº†è‡ªæˆ‘è¿›åŒ–è®­ç»ƒåœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šçš„æ™®éæœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†é¢„å…ˆè¿›åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14922",
            "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
            "url": "https://huggingface.co/papers/2412.14922",
            "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
            "score": 12,
            "issue_id": 1281,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "9cc4a703e686ea87",
            "authors": [
                "Junyu Luo",
                "Xiao Luo",
                "Kaize Ding",
                "Jingyang Yuan",
                "Zhiping Xiao",
                "Ming Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Peking University",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14922.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§¼",
                "ru": {
                    "title": "Ğ§Ğ¸ÑÑ‚Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞºĞ°: RobustFT Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RobustFT Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RobustFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RobustFT Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Language Models with Robust Fine-Tuning",
                    "desc": "This paper presents a new framework called RobustFT for supervised fine-tuning (SFT) of large language models (LLMs) that addresses the issue of noisy data in training. The framework includes a multi-expert system for detecting noise in the data, which helps improve the quality of the training process. It also features a context-enhanced strategy for relabeling data, ensuring that only the most relevant and reliable information is used for fine-tuning. Experiments show that RobustFT significantly enhances model performance on downstream tasks, even in the presence of noise."
                },
                "zh": {
                    "title": "æ„å»ºæŠ—å™ªå£°çš„å¾®è°ƒæ¡†æ¶ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­æ”¶é›†çš„æ•°æ®ä¸å¯é¿å…åœ°åŒ…å«å™ªå£°ï¼Œè¿™å¯¹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°é€ æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§æŠ—å™ªå£°çš„SFTæ¡†æ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„SFTæ¡†æ¶ï¼ˆRobustFTï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ä¸Šæ‰§è¡Œå™ªå£°æ£€æµ‹å’Œé‡æ–°æ ‡æ³¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16926",
            "title": "Revisiting In-Context Learning with Long Context Language Models",
            "url": "https://huggingface.co/papers/2412.16926",
            "abstract": "In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.",
            "score": 8,
            "issue_id": 1281,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "764c013157322e0d",
            "authors": [
                "Jinheon Baek",
                "Sun Jae Lee",
                "Prakhar Gupta",
                "Geunseob",
                "Oh",
                "Siddharth Dalmia",
                "Prateek Kolhar"
            ],
            "affiliations": [
                "Google DeepMind",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16926.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² - Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (LCLM) Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 18 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğµ Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ² ÑĞ±Ğ¾Ñ€Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°. ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ICL Ğ½Ğ° 5% Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Maximizing ICL Performance with Long Contexts: Simplicity Over Sophistication",
                    "desc": "In-Context Learning (ICL) allows language models to make predictions based on examples in their input. With the introduction of Long Context Language Models (LCLMs), the number of examples that can be included has increased, prompting a reevaluation of example selection methods. The study finds that complex selection techniques do not significantly outperform simple random sampling in many-shot scenarios. Instead, the focus shifts to ensuring enough examples fill the context window, and using data augmentation can enhance ICL performance by 5%."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä¸‹çš„ç¤ºä¾‹é€‰æ‹©æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰ä¸­ï¼Œç¤ºä¾‹é€‰æ‹©å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å¤æ‚çš„ç¤ºä¾‹é€‰æ‹©æŠ€æœ¯å¹¶æœªæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†ç®€å•çš„éšæœºé€‰æ‹©æ–¹æ³•åœ¨è®¸å¤šæƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚éšç€LCLMsçš„å‡ºç°ï¼ŒICLçš„æŒ‘æˆ˜å·²ä»é€‰æ‹©æœ€æœ‰æ•ˆçš„ç¤ºä¾‹è½¬å˜ä¸ºæ”¶é›†è¶³å¤Ÿçš„ç¤ºä¾‹ä»¥å¡«å……ä¸Šä¸‹æ–‡çª—å£ã€‚é€šè¿‡ç®€å•çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æŸäº›æ•°æ®é›†ä¸Šæé«˜äº†ICLæ€§èƒ½ï¼Œæå‡å¹…åº¦è¾¾åˆ°5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17153",
            "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
            "url": "https://huggingface.co/papers/2412.17153",
            "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3times speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.",
            "score": 3,
            "issue_id": 1283,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "b1968a6263a19386",
            "authors": [
                "Enshu Liu",
                "Xuefei Ning",
                "Yu Wang",
                "Zinan Lin"
            ],
            "affiliations": [
                "Department of EE, Tsinghua University",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17153.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Distilled Decoding (DD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… (AR) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ flow matching Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ AR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° ÑˆĞ°Ğ³Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ AR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DD Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 6-217 Ñ€Ğ°Ğ· Ñ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¼ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID."
                },
                "en": {
                    "title": "Revolutionizing Autoregressive Generation: Fast and Efficient with Distilled Decoding!",
                    "desc": "This paper introduces Distilled Decoding (DD), a novel approach to enhance the efficiency of autoregressive (AR) models in generating text and images. Traditional AR models generate outputs token-by-token, which can be slow, but DD aims to enable generation in just one or two steps by creating a deterministic mapping from a Gaussian distribution to the AR model's output distribution. By training a network to distill this mapping, DD allows for rapid generation without requiring the original training data, making it more practical for real-world applications. The results show significant speed-ups in generation times while maintaining acceptable quality, challenging the belief that AR models are inherently slow."
                },
                "zh": {
                    "title": "è’¸é¦è§£ç ï¼šåŠ é€Ÿè‡ªå›å½’æ¨¡å‹ç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºé€ä¸ªç”Ÿæˆçš„è¿‡ç¨‹ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè’¸é¦è§£ç ï¼ˆDDï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„ARæ¨¡å‹é€‚åº”ä¸ºä»…éœ€ä¸€æ­¥æˆ–ä¸¤æ­¥ç”Ÿæˆè¾“å‡ºã€‚DDé€šè¿‡æµåŒ¹é…åˆ›å»ºä»é«˜æ–¯åˆ†å¸ƒåˆ°ARæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„ç¡®å®šæ€§æ˜ å°„ï¼Œä»è€Œå®ç°å¿«é€Ÿç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDåœ¨å¤šä¸ªå›¾åƒARæ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—çš„ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16686",
            "title": "NILE: Internal Consistency Alignment in Large Language Models",
            "url": "https://huggingface.co/papers/2412.16686",
            "abstract": "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.",
            "score": 3,
            "issue_id": 1282,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "0d1e640729e131e5",
            "authors": [
                "Minda Hu",
                "Qiyuan Zhang",
                "Yufei Wang",
                "Bowei He",
                "Hongru Wang",
                "Jingyan Zhou",
                "Liangyou Li",
                "Yasheng Wang",
                "Chen Ma",
                "Irwin King"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16686.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#data",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº NILE Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (IFT) ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. NILE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IFT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Aligning IFT Datasets for Enhanced LLM Performance",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human intentions through Instruction Fine-Tuning (IFT). It highlights the problem of existing IFT datasets containing inconsistent knowledge that conflicts with the LLMs' pre-trained knowledge, which can hinder their performance. To solve this, the authors introduce the NILE framework, which optimizes IFT datasets by aligning them with the internal knowledge of the LLMs. The framework includes a method called Internal Consistency Filtering (ICF) to ensure high consistency, leading to significant performance improvements in LLM evaluations."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ•°æ®é›†ï¼Œæå‡LLMæ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºNILEçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰æ•°æ®é›†ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»æ„å›¾çš„ä¸€è‡´æ€§ã€‚ç°æœ‰çš„IFTæ•°æ®é›†å¸¸å¸¸åŒ…å«ä¸LLMsé¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ä¸ä¸€è‡´çš„ä¿¡æ¯ï¼Œè¿™ä¼šå½±å“IFTçš„æ•ˆæœã€‚NILEé€šè¿‡æå–ç›®æ ‡é¢„è®­ç»ƒLLMçš„å†…éƒ¨çŸ¥è¯†æ¥ä¿®æ­£IFTæ•°æ®é›†ä¸­çš„ç­”æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å†…éƒ¨ä¸€è‡´æ€§è¿‡æ»¤ï¼ˆICFï¼‰æ–¹æ³•ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ ·æœ¬ä¸LLMçš„å†…éƒ¨çŸ¥è¯†é«˜åº¦ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNILEå¯¹IFTæ•°æ®é›†çš„ä¼˜åŒ–æ˜¾è‘—æå‡äº†LLMåœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜äº†æ•°æ®é›†ä¸é¢„è®­ç»ƒå†…éƒ¨çŸ¥è¯†ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17747",
            "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
            "url": "https://huggingface.co/papers/2412.17747",
            "abstract": "Techniques enabling large language models (LLMs) to \"think more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",
            "score": 1,
            "issue_id": 1283,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "b3ee8264ebbee41e",
            "authors": [
                "Luyang Liu",
                "Jonas Pfeiffer",
                "Jiaxing Wu",
                "Jun Xie",
                "Arthur Szlam"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17747.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ĞºÑÑˆĞµĞ¼ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ² Ğ½ĞµĞ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "Enhancing LLMs with Offline Cache Augmentation for Better Reasoning",
                    "desc": "This paper presents a method to enhance large language models (LLMs) by using an offline coprocessor that improves the model's key-value (kv) cache. The coprocessor adds latent embeddings to the cache, which helps the model generate better responses by refining its reasoning process. By training the coprocessor with language modeling loss while keeping the main decoder unchanged, the system can learn to optimize its computations without increasing latency. Experimental results show that this cache augmentation leads to lower perplexity and better performance on various reasoning tasks, even without specific training for those tasks."
                },
                "zh": {
                    "title": "å¢å¼ºç¼“å­˜ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªç¦»çº¿åå¤„ç†å™¨ï¼Œè¯¥åå¤„ç†å™¨åœ¨æ¨¡å‹çš„é”®å€¼ç¼“å­˜ä¸Šæ“ä½œï¼Œä»è€Œæé«˜åç»­è§£ç çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ¨¡å‹ä»¥ç«¯åˆ°ç«¯å¯å¾®åˆ†çš„æ–¹å¼å­¦ä¹ å¦‚ä½•å°†é¢å¤–çš„è®¡ç®—æç‚¼åˆ°å…¶ç¼“å­˜ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºç¼“å­˜åï¼Œè§£ç å™¨åœ¨å¤šä¸ªåç»­æ ‡è®°ä¸Šè¡¨ç°å‡ºæ›´ä½çš„å›°æƒ‘åº¦ï¼Œä¸”åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-23.html",
    "link_next": "2024-12-25.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚",
        "title": "Parallelized Autoregressive Visual Generation",
        "pinyin": "ZÃ¬ huÃ­guÄ« mÃ³xÃ­ng zÃ i shÃ¬juÃ© shÄ“ngchÃ©ng zhÅng biÇoxiÃ n chÅ« qiÃ¡ngdÃ  de xÃ¬ngnÃ©ng, dÃ n yÃ³uyÃº qÃ­ shÃ¹nxÃ¹ zhÃº biÄojÃ¬ yÃ¹cÃ¨ de guÃ²chÃ©ng, tuÄ«lÇ sÃ¹dÃ¹ jiÃ o mÃ n. BÄ›nwÃ©n tÃ­chÅ« le yÄ«zhÇ’ng jiÇndÄn yÇ’uxiÃ o de bÃ¬ngxÃ­ng zÃ¬huÃ­guÄ« shÃ¬juÃ© shÄ“ngchÃ©ng fÄngfÇ, tÃ­gÄo shÄ“ngchÃ©ng xiÃ olÇœ, tÃ³ngshÃ­ bÇoliÃº zÃ¬huÃ­guÄ« jiÃ nmÃ³ de yÅushÃ¬. WÇ’men de guÇnjiÃ n dÃ²ngjiÃ n shÃ¬ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng yÄ«lÃ i yÃº shÃ¬juÃ© biÄojÃ¬ de yÄ«lÃ i guÄnxÃ¬: ruÃ² yÄ«lÃ i de biÄojÃ¬ kÄ›yÇ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng, Ã©r qiÃ¡ng yÄ«lÃ i de xiÄnglÃ­n biÄojÃ¬ nÃ¡n yÇ yÄ«qÇ shÄ“ngchÃ©ng, yÄ«nwÃ¨i tÄmen de dÃºlÃ¬ cÇiyÇng kÄ›nÃ©ng dÇozhÃ¬ bÃ¹ yÄ«zhÃ¬. JÄ«yÃº cÇ guÄnchÃ¡, wÇ’men kÄifÄ le yÄ«zhÇ’ng bÃ¬ngxÃ­ng shÄ“ngchÃ©ng cÃ¨lÃ¼Ã¨, bÃ¬ngxÃ­ng shÄ“ngchÃ©ng ruÃ² yÄ«lÃ i de yuÇn jÃ¹lÃ­ biÄojÃ¬, tÃ³ngshÃ­ bÇochÃ­ qiÃ¡ng yÄ«lÃ i de jÃºbÃ¹ biÄojÃ¬ de shÃ¹nxÃ¹ shÄ“ngchÃ©ng. WÇ’men de fÄngfÇ kÄ›yÇ wÃºfÃ¨ng jÃ­chÃ©ng dÃ o biÄozhÇ”n zÃ¬huÃ­guÄ« mÃ³xÃ­ng zhÅng, wÃºxÅ« xiÅ«gÇi jiÃ gÃ²u huÃ² biÄojÃ¬qÃ¬. ZÃ i ImageNet hÃ© UCF-101 shÃ ng de shÃ­yÃ n biÇomÃ­ng, wÇ’men de fÄngfÇ zÃ i tÃºxiÃ ng hÃ© shÃ¬pÇn shÄ“ngchÃ©ng rÃ¨nwÃ¹ zhÅng shÃ­xiÃ n le zuÃ¬gÄo 9.5 bÃ¨i de jiÄsÃ¹, tÃ³ngshÃ­ zhÃ¬liÃ ng sÇ”nshÄ« zuÃ¬shÇo. WÇ’men xÄ«wÃ ng zhÃ¨ jiÃ n gÅngzuÃ² nÃ©ng jÄ«fÄ wÃ¨ilÃ¡i zÃ i gÄoxiÃ o shÃ¬juÃ© shÄ“ngchÃ©ng hÃ© tÇ’ngyÄ« zÃ¬huÃ­guÄ« jiÃ nmÃ³ fÄngmiÃ n de yÃ¡njiÅ«. XiÃ ngmÃ¹ yÃ¨miÃ n: https://epiphqny.github.io/PAR-project.",
        "vocab": "[{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ng dÃ ', 'trans': 'powerful'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'é¡ºåº', 'pinyin': 'shÃ¹n xÃ¹', 'trans': 'sequential'},\n{'word': 'é€', 'pinyin': 'zhÃº', 'trans': 'gradual'},\n{'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'},\n{'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'å¹¶è¡Œ', 'pinyin': 'bÃ¬ng xÃ­ng', 'trans': 'parallel'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'},\n{'word': 'ä¿ç•™', 'pinyin': 'bÇo liÃº', 'trans': 'retain'},\n{'word': 'ä¼˜åŠ¿', 'pinyin': 'yÅu shÃ¬', 'trans': 'advantage'},\n{'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'},\n{'word': 'æ´è§', 'pinyin': 'dÃ²ng jiÃ n', 'trans': 'insight'},\n{'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'dependency'},\n{'word': 'å…³ç³»', 'pinyin': 'guÄn xÃ¬', 'trans': 'relationship'},\n{'word': 'å¼±', 'pinyin': 'ruÃ²', 'trans': 'weak'},\n{'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'},\n{'word': 'ç›¸é‚»', 'pinyin': 'xiÄng lÃ­n', 'trans': 'adjacent'},\n{'word': 'éš¾ä»¥', 'pinyin': 'nÃ¡n yÇ', 'trans': 'difficult'},\n{'word': 'ç‹¬ç«‹', 'pinyin': 'dÃº lÃ¬', 'trans': 'independent'},\n{'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'},\n{'word': 'ä¸ä¸€è‡´', 'pinyin': 'bÃ¹ yÄ« zhÃ¬', 'trans': 'inconsistency'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'è¿œè·ç¦»', 'pinyin': 'yuÇn jÃ¹ lÃ­', 'trans': 'long-distance'},\n{'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'},\n{'word': 'æ— ç¼', 'pinyin': 'wÃº fÃ¨ng', 'trans': 'seamless'},\n{'word': 'é›†æˆ', 'pinyin': 'jÃ­ chÃ©ng', 'trans': 'integrate'},\n{'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'},\n{'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'},\n{'word': 'ä¿®æ”¹', 'pinyin': 'xiÅ« gÇi', 'trans': 'modify'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'},\n{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'acceleration'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},\n{'word': 'æŸå¤±', 'pinyin': 'sÇ”n shÄ«', 'trans': 'loss'},\n{'word': 'æ¿€å‘', 'pinyin': 'jÄ« fÄ', 'trans': 'inspire'},\n{'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'},\n{'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},\n{'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'},\n{'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]",
        "trans": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
        "update_ts": "2024-12-23 09:11"
    }
}