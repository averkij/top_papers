{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2025-01-22 05:10",
    "weekday": 2,
    "issue_id": 1797,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12380",
            "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
            "url": "https://huggingface.co/papers/2501.12380",
            "abstract": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains.",
            "score": 17,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "dcb04aaca349cc32",
            "authors": [
                "Yilun Zhao",
                "Lujing Xie",
                "Haowei Zhang",
                "Guo Gan",
                "Yitao Long",
                "Zhiyuan Hu",
                "Tongyan Hu",
                "Weiyuan Chen",
                "Chuhan Li",
                "Junyang Song",
                "Zhijian Xu",
                "Chengye Wang",
                "Weifeng Pan",
                "Ziyao Shangguan",
                "Xiangru Tang",
                "Zhenwen Liang",
                "Yixin Liu",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "affiliations": [
                "Yale NLP"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12380.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#benchmark",
                    "#video",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMVU - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. MMVU Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 27 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞÑ†ĞµĞ½ĞºĞ° 32 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MMVU Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ."
                },
                "en": {
                    "title": "MMVU: Elevating Video Understanding to Expert Levels",
                    "desc": "The paper presents MMVU, a new benchmark designed to evaluate foundation models specifically in video understanding across various expert domains. It includes 3,000 questions that require advanced reasoning and domain-specific knowledge, moving beyond simple visual recognition tasks. Each question is meticulously annotated by human experts, ensuring high data quality and providing reasoning rationales to enhance analysis. The evaluation of 32 advanced multimodal models reveals that while some perform well, they still do not reach the level of human expertise, highlighting areas for future improvement in this field."
                },
                "zh": {
                    "title": "MMVUï¼šè§†é¢‘ç†è§£çš„æ–°æ ‡å‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MMVUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸“å®¶çº§å¤šå­¦ç§‘åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„è¡¨ç°ã€‚MMVUåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ç§‘å­¦ã€åŒ»ç–—ã€äººæ–‡å­¦ç§‘ä¸ç¤¾ä¼šç§‘å­¦å’Œå·¥ç¨‹å››ä¸ªæ ¸å¿ƒå­¦ç§‘ã€‚ä¸ä¹‹å‰çš„åŸºå‡†ç›¸æ¯”ï¼ŒMMVUåœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼ŒåŒ…æ‹¬è¦æ±‚æ¨¡å‹åº”ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†è¿›è¡Œä¸“å®¶çº§æ¨ç†ï¼Œç¡®ä¿æ•°æ®é›†çš„é«˜è´¨é‡ï¼Œä»¥åŠä¸ºæ¯ä¸ªç¤ºä¾‹æä¾›ä¸“å®¶æ³¨é‡Šçš„æ¨ç†ä¾æ®å’Œç›¸å…³é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬å¯¹32ä¸ªå‰æ²¿å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨MMVUä¸Šçš„è¡¨ç°è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°æœ€æ–°çš„ç³»ç»Ÿ2èƒ½åŠ›æ¨¡å‹o1å’ŒGemini 2.0 Flash Thinkingåœ¨æµ‹è¯•æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†ä»æœªèƒ½è¾¾åˆ°äººç±»ä¸“å®¶çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12273",
            "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
            "url": "https://huggingface.co/papers/2501.12273",
            "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.",
            "score": 6,
            "issue_id": 1796,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "10499c8b820d5368",
            "authors": [
                "Maosong Cao",
                "Taolin Zhang",
                "Mo Li",
                "Chuyu Zhang",
                "Yunxin Liu",
                "Haodong Duan",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12273.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Condor: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Condor - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 20 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Condor Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Synthetic Data Generation",
                    "desc": "This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡å¯¹è¯èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®çš„è´¨é‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è¯èƒ½åŠ›çš„é‡è¦æ€§ã€‚éšç€LLMsçš„è¿›æ­¥ï¼Œé«˜è´¨é‡çš„äººç±»æ ‡æ³¨SFTæ•°æ®å˜å¾—ç¨€ç¼ºï¼Œå› æ­¤éœ€è¦æ›´å¤šä¾èµ–åˆæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCondorçš„ä¸¤é˜¶æ®µåˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆäº†ä¸–ç•ŒçŸ¥è¯†æ ‘å’Œè‡ªæˆ‘åæ€ç²¾ç‚¼ï¼Œä»¥å¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„SFTæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ç”¨20Kä¸ªCondorç”Ÿæˆçš„æ ·æœ¬å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12390",
            "title": "GPS as a Control Signal for Image Generation",
            "url": "https://huggingface.co/papers/2501.12390",
            "abstract": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.",
            "score": 3,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "11d289e8a895bedd",
            "authors": [
                "Chao Feng",
                "Ziyang Chen",
                "Aleksander Holynski",
                "Alexei A. Efros",
                "Andrew Owens"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12390.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "GPS-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº GPS-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ² Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPS-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ¹Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· 2D GPS-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ score distillation sampling. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GPS-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Harnessing GPS Data for Location-Aware Image Generation",
                    "desc": "This paper explores the use of GPS data embedded in photo metadata as a control signal for generating images. The authors develop GPS-to-image models, particularly a diffusion model, that can create images based on both GPS coordinates and textual descriptions. The model effectively captures the unique characteristics of various urban environments, such as neighborhoods and landmarks. Additionally, they demonstrate the ability to extract 3D models from these images, enhancing the accuracy of 3D reconstructions by using GPS information to guide the process."
                },
                "zh": {
                    "title": "åˆ©ç”¨GPSæ ‡ç­¾ç”ŸæˆåŸå¸‚å›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†ç…§ç‰‡å…ƒæ•°æ®ä¸­çš„GPSæ ‡ç­¾å¯ä»¥ä½œä¸ºå›¾åƒç”Ÿæˆçš„æœ‰ç”¨æ§åˆ¶ä¿¡å·ã€‚æˆ‘ä»¬è®­ç»ƒäº†GPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºéœ€è¦ç»†è‡´ç†è§£åŸå¸‚ä¸­å›¾åƒå˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”ŸæˆåŒæ—¶ä¾èµ–äºGPSå’Œæ–‡æœ¬çš„å›¾åƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¹ äº†åŸºäºä½ç½®ç”Ÿæˆå˜åŒ–å›¾åƒï¼Œå¹¶ä¸”GPSæ¡ä»¶æ”¹å–„äº†ä¼°è®¡çš„3Dç»“æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11873",
            "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
            "url": "https://huggingface.co/papers/2501.11873",
            "abstract": "This paper revisits the implementation of Load-balancing Loss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i represents the frequency of expert i being selected, and p_i denotes the average gating score of the expert i. Existing MoE training frameworks usually employ the parallel training strategy so that f_i and the LBL are calculated within a micro-batch and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence (e.g., code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a global-batch to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize f_i across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to 42.8B total parameters and 400B tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
            "score": 1,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "370d057fec504963",
            "authors": [
                "Zihan Qiu",
                "Zeyu Huang",
                "Bo Zheng",
                "Kaiyue Wen",
                "Zekun Wang",
                "Rui Men",
                "Ivan Titov",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "Stanford University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11873.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ (LBL) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ LBL Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ğ° Ğ½Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑĞ»Ğ°Ğ±Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… downstream. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Expert Specialization with Global-Batch Load-Balancing",
                    "desc": "This paper focuses on improving the Load-balancing Loss (LBL) in training Mixture-of-Experts (MoEs) models. The authors highlight that traditional methods use micro-batches, which limit the diversity of sequences and hinder expert specialization. They propose a new approach that utilizes global-batches, allowing for a broader range of sequences and better load balancing across the entire dataset. Experimental results show that this global-batch LBL method significantly enhances model performance and expert specialization in large language models."
                },
                "zh": {
                    "title": "å…¨å±€æ‰¹æ¬¡æå‡æ··åˆä¸“å®¶æ¨¡å‹çš„è´Ÿè½½å‡è¡¡ä¸ä¸“ä¸šåŒ–",
                    "desc": "æœ¬æ–‡é‡æ–°å®¡è§†äº†åœ¨è®­ç»ƒæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEsï¼‰æ—¶çš„è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆLBLï¼‰å®ç°ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨å…¨å±€æ‰¹æ¬¡æ¥è®¡ç®—LBLï¼Œä»¥æ‰“ç ´å¾®æ‰¹æ¬¡çš„ä¸¥æ ¼çº¦æŸï¼Œä»è€Œåœ¨è¯­æ–™åº“å±‚é¢ä¸Šä¿ƒè¿›è´Ÿè½½å‡è¡¡ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥é¢å¤–çš„é€šä¿¡æ­¥éª¤æ¥åŒæ­¥ä¸“å®¶é€‰æ‹©é¢‘ç‡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¨å±€æ‰¹æ¬¡LBLç­–ç•¥åœ¨é¢„è®­ç»ƒå›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­å‡æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå…¨å±€æ‰¹æ¬¡LBLè¿˜å¤§å¤§æ”¹å–„äº†MoEä¸“å®¶çš„é¢†åŸŸä¸“ä¸šåŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12326",
            "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
            "url": "https://huggingface.co/papers/2501.12326",
            "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
            "score": 0,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "1f98d8f49b073983",
            "authors": [
                "Yujia Qin",
                "Yining Ye",
                "Junjie Fang",
                "Haoming Wang",
                "Shihao Liang",
                "Shizuo Tian",
                "Junda Zhang",
                "Jiahao Li",
                "Yunxin Li",
                "Shijue Huang",
                "Wanjun Zhong",
                "Kuanye Li",
                "Jiale Yang",
                "Yu Miao",
                "Woyu Lin",
                "Longxiang Liu",
                "Xu Jiang",
                "Qianli Ma",
                "Jingyu Li",
                "Xiaojun Xiao",
                "Kai Cai",
                "Chuang Li",
                "Yaowei Zheng",
                "Chaolin Jin",
                "Chen Li",
                "Xiao Zhou",
                "Minchao Wang",
                "Haoli Chen",
                "Zhaojian Li",
                "Haihua Yang",
                "Haifeng Liu",
                "Feng Lin",
                "Tao Peng",
                "Xin Liu",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "UI-TARS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¸Ñ€Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UI-TARS - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼. UI-TARS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ-2 Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ‚Ñ€Ğ°ÑÑĞ°Ğ¼Ğ¸. UI-TARS Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with UI-TARS: The End-to-End Agent Model",
                    "desc": "UI-TARS is a novel GUI agent model that processes screenshots to perform tasks like a human would, using keyboard and mouse actions. Unlike existing models that rely on complex commercial frameworks and pre-defined prompts, UI-TARS operates end-to-end and shows superior performance in various benchmarks. It achieves state-of-the-art results in GUI task execution by utilizing enhanced perception, unified action modeling, and system-2 reasoning for better decision-making. Additionally, its iterative training approach allows it to learn from past interactions, improving its adaptability with minimal human input."
                },
                "zh": {
                    "title": "UI-TARSï¼šé©æ–°å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†çš„å…¨æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†UI-TARSï¼Œè¿™æ˜¯ä¸€ç§åŸç”Ÿçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿä»…é€šè¿‡å±å¹•æˆªå›¾è¿›è¡Œäººç±»èˆ¬çš„äº¤äº’ã€‚ä¸ä¾èµ–å¤æ‚å•†ä¸šæ¨¡å‹çš„ç°æœ‰ä»£ç†æ¡†æ¶ä¸åŒï¼ŒUI-TARSæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªGUIä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ„ŸçŸ¥ã€å®šä½å’Œä»»åŠ¡æ‰§è¡Œæ–¹é¢ã€‚UI-TARSé€šè¿‡å¢å¼ºæ„ŸçŸ¥ã€ç»Ÿä¸€åŠ¨ä½œå»ºæ¨¡ã€ç³»ç»Ÿ-2æ¨ç†å’Œåæ€åœ¨çº¿è¿½è¸ªç­‰åˆ›æ–°ï¼Œæ˜¾è‘—æé«˜äº†å…¶æ€§èƒ½ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒå’Œåæ€è°ƒä¼˜ï¼ŒUI-TARSèƒ½å¤Ÿä¸æ–­å­¦ä¹ å¹¶é€‚åº”æ–°çš„æƒ…å†µï¼Œå‡å°‘å¯¹äººç±»å¹²é¢„çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11223",
            "title": "Reasoning Language Models: A Blueprint",
            "url": "https://huggingface.co/papers/2501.11223",
            "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and experimentation.",
            "score": 0,
            "issue_id": 1797,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "f554416ad9af3344",
            "authors": [
                "Maciej Besta",
                "Julia Barth",
                "Eric Schreiber",
                "Ales Kubicek",
                "Afonso Catarino",
                "Robert Gerstenberger",
                "Piotr Nyczyk",
                "Patrick Iff",
                "Yueling Li",
                "Sam Houliston",
                "Tomasz Sternal",
                "Marcin Copik",
                "Grzegorz KwaÅ›niewski",
                "JÃ¼rgen MÃ¼ller",
                "Åukasz Flis",
                "Hannes Eberhard",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "affiliations": [
                "BASF SE",
                "Cledar",
                "Cyfronet AGH",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11223.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#math",
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RLM), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ…ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ x1 - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ RLM."
                },
                "en": {
                    "title": "Democratizing Advanced Reasoning in AI",
                    "desc": "This paper introduces a modular framework for Reasoning Language Models (RLMs), which enhance traditional Large Language Models (LLMs) with advanced reasoning capabilities. The authors address the challenges of high costs and complex architectures by organizing RLM components into a comprehensive blueprint that includes various reasoning structures and strategies. They provide mathematical formulations and algorithmic specifications to facilitate easier implementation of RLMs. Additionally, the paper presents x1, a tool for rapid prototyping, and discusses how RLMs can be integrated into the larger LLM ecosystem to promote accessibility and innovation in AI development."
                },
                "zh": {
                    "title": "ç®€åŒ–æ¨ç†è¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›AIåˆ›æ–°",
                    "desc": "æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ã€æœç´¢å¯å‘å¼å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‡æ–°å®šä¹‰äº†äººå·¥æ™ºèƒ½çš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰å¼ºå¤§çš„æ¨ç†æœºåˆ¶ï¼Œä½†é«˜æˆæœ¬å’Œå¤æ‚æ¶æ„ä½¿å¾—å…¶å¯è®¿é—®æ€§å’Œå¯æ‰©å±•æ€§é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œç»„ç»‡RLMç»„ä»¶ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ•°å­¦å…¬å¼å’Œç®—æ³•è§„èŒƒï¼Œä»¥ç®€åŒ–RLMçš„å®ç°ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨é™ä½RLMå¼€å‘å’Œå®éªŒçš„é—¨æ§›ï¼Œä¿ƒè¿›åˆ›æ–°ï¼Œç¼©å°â€œå¯Œæœ‰AIâ€å’Œâ€œè´«ç©·AIâ€ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i GameFactory de kuÃ ng jiÃ , zhÇ zÃ i tÅng guÃ² shÄ“ng chÃ©ng yÃ²u xÃ­ yÇn qÃ­ng lÃ¡i gÃ© xÄ«n yÃ²u xÃ­ kÄi fÄ. tÄ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n de shÃ¬ pÃ­n kuÃ² sÃ n mÃ³ xÃ­ng, nÃ©ng gÃ²u chuÃ ng jiÃ n quÃ¡n xÄ«n qiÄ› duÅ yÃ ng huÃ  de yÃ²u xÃ­. wÃ¨i le jiÄ› juÃ© xiÃ n yÇ’u fÄng fÇ zÃ i chÇng jÄ«ng shÄ“ng chÃ©ng shÃ ng de jÃº xiÃ n, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng duÅ jiÄ“ duÃ n xÃ¹n liÃ n cÃ¨ lÃ¼Ã¨. tÄ men hÃ¡i fÄ bÃ¹ le yÄ« gÃ¨ jÄ« yÃº Minecraft de gÄo zhÃ¬ liÃ ng shÃ¬ pÃ­n shÃ¹ jÃ¹ jÃ­, bÃ¬ng zhÃ n shÃ¬ le kuÃ ng jiÃ  nÃ©ng gÃ²u shÄ“ng chÃ©ng kÄi fÃ ng yÃ¹, duÅ yÃ ng huÃ  hÃ© kÄ› kÃ²ng de yÃ²u xÃ­ shÃ¬ pÃ­n.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'é©æ–°', 'pinyin': 'gÃ©xÄ«n', 'trans': 'innovate'},\n{'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅyÃ nghuÃ ', 'trans': 'diversified'},\n{'word': 'å±€é™', 'pinyin': 'jÃºxiÃ n', 'trans': 'limitation'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'},\n{'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open domain'},\n{'word': 'å¯æ§', 'pinyin': 'kÄ›kÃ²ng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}