{
    "date": {
        "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 3",
        "zh": "4æœˆ3æ—¥"
    },
    "time_utc": "2025-04-03 03:27",
    "weekday": 3,
    "issue_id": 3041,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.00883",
            "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
            "url": "https://huggingface.co/papers/2504.00883",
            "abstract": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.",
            "score": 28,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "43fba84cc49adfad",
            "authors": [
                "Zhenyi Liao",
                "Qingsong Xie",
                "Yanhao Zhang",
                "Zijian Kong",
                "Haonan Lu",
                "Zhenyu Yang",
                "Zhijie Deng"
            ],
            "affiliations": [
                "OPPO AI Center",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00883.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#video",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ R1-Zero. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2-VL Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GRPO Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VSI-100k Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vsGRPO-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2-VL-7B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ LLaVA-NeXT-Video-72B."
                },
                "en": {
                    "title": "Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training",
                    "desc": "This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶èšç„¦äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åŸºç¡€çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰æ–¹é¢ã€‚æˆ‘ä»¬å‘ç°å°åˆ°ä¸­å‹çš„Qwen2-VLæ¨¡å‹æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»å…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤å¼•å…¥äº†GRPOè®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„VSI-100kæ•°æ®é›†è¿›è¡Œæ”¹è¿›ã€‚ç»è¿‡120ä¸ªGPUå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„vsGRPO-2Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºç¡€æ¨¡å‹12.1%ï¼Œå¹¶ä¸”vsGRPO-7Bæ¨¡å‹çš„è¡¨ç°ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¿æŒKLæƒ©ç½šåœ¨GRPOä¸­æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”vsGRPOåœ¨ä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00999",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "url": "https://huggingface.co/papers/2504.00999",
            "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.",
            "score": 21,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "bb6506ffd72aed19",
            "authors": [
                "Siyuan Li",
                "Luyuan Zhang",
                "Zedong Wang",
                "Juanxi Tian",
                "Cheng Tan",
                "Zicheng Liu",
                "Chang Yu",
                "Qingsong Xie",
                "Haonan Lu",
                "Haoqian Wang",
                "Zhen Lei"
            ],
            "affiliations": [
                "CAIR, HKISI-CAS",
                "MAIS CASIA",
                "OPPO AI Center",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (VQ) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. MergeVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MergeAR, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MergeVQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "MergeVQ: Bridging Image Generation and Representation Learning Efficiently",
                    "desc": "This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed."
                },
                "zh": {
                    "title": "MergeVQï¼šæå‡å›¾åƒç”Ÿæˆä¸è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MergeVQï¼Œæ—¨åœ¨æ”¹å–„åŸºäºå‘é‡é‡åŒ–çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨ç¼–ç å™¨ä¸­å¼•å…¥ä»¤ç‰Œåˆå¹¶æŠ€æœ¯ï¼ŒMergeVQèƒ½å¤Ÿåœ¨è‡ªæ³¨æ„åŠ›å—åè§£è€¦æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¢å¤ç»†èŠ‚ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨KVç¼“å­˜å‹ç¼©æ¥æé«˜é¢„æµ‹æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeVQåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01014",
            "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
            "url": "https://huggingface.co/papers/2504.01014",
            "abstract": "Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.",
            "score": 11,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "98efa783105c3173",
            "authors": [
                "Junhao Cheng",
                "Yuying Ge",
                "Yixiao Ge",
                "Jing Liao",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01014.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "AnimeGamer: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
                    "desc": "AnimeGamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². AnimeGamer Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Anime into Interactive Gaming with Dynamic AI",
                    "desc": "This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations."
                },
                "zh": {
                    "title": "AnimeGamerï¼šåŠ¨æ€äº’åŠ¨çš„æ— é™åŠ¨æ¼«æ¸¸æˆä½“éªŒ",
                    "desc": "æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢çš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆå¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAnimeGamerçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œä»¥å¢å¼ºæ¸¸æˆçš„äº’åŠ¨æ€§å’Œæ²‰æµ¸æ„Ÿã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒAnimeGamerèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€å˜åŒ–çš„æ¸¸æˆçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00824",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
            "url": "https://huggingface.co/papers/2504.00824",
            "abstract": "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.",
            "score": 7,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "b135f3f003dcaaff",
            "authors": [
                "Yubo Wang",
                "Xueguang Ma",
                "Ping Nie",
                "Huaye Zeng",
                "Zhiheng Lyu",
                "Yuxuan Zhang",
                "Benjamin Schneider",
                "Yi Lu",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#rag",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°",
                    "desc": "ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½ [RET], Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScholarCopilot ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· arXiv, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ top-1 Ğ² 40.1% Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Academic Writing with ScholarCopilot",
                    "desc": "This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience."
                },
                "zh": {
                    "title": "ScholarCopilotï¼šæå‡å­¦æœ¯å†™ä½œçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ScholarCopilotï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸“ä¸šå­¦æœ¯æ–‡ç« æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ£€ç´¢æ ‡è®°[RET]ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨å…¶è¡¨ç¤ºä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸å…³å¼•ç”¨ã€‚ScholarCopilotåœ¨ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡ã€‚ç»è¿‡åœ¨500Kç¯‡arXivè®ºæ–‡ä¸Šçš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†40.1%çš„é¡¶çº§æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬çš„ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01308",
            "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
            "url": "https://huggingface.co/papers/2504.01308",
            "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
            "score": 5,
            "issue_id": 3040,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "315938d70f25095e",
            "authors": [
                "Jiawei Wang",
                "Yushen Zuo",
                "Yuanjun Chai",
                "Zhendong Liu",
                "Yichen Fu",
                "Yichun Feng",
                "Kin-man Lam"
            ],
            "affiliations": [
                "Nanjing University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China",
                "University of Washington",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01308.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robust-VLGuard, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiffPure-VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº."
                },
                "en": {
                    "title": "Strengthening Vision-Language Models Against Noise Attacks",
                    "desc": "This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality."
                },
                "zh": {
                    "title": "å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å¯¹å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„è„†å¼±æ€§å´æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œç»“åˆäº†å¯¹é½å’Œä¸å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºçš„å¾®è°ƒæ¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»ç‰¹æ€§ä¸æˆ‘ä»¬å¾®è°ƒçš„VLMså¾ˆå¥½åœ°å¯¹é½ï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ‰°åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23135",
            "title": "LSNet: See Large, Focus Small",
            "url": "https://huggingface.co/papers/2503.23135",
            "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.",
            "score": 2,
            "issue_id": 3040,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "d2ac65a2356c89c3",
            "authors": [
                "Ao Wang",
                "Hui Chen",
                "Zijia Lin",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist, Tsinghua University",
                "Department of Automation, Tsinghua University",
                "School of Software, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23135.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾' Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ LS-ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LS-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LSNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSNet Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "See Large, Focus Small: Efficient Vision Networks",
                    "desc": "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."
                },
                "zh": {
                    "title": "è½»é‡çº§è§†è§‰ç½‘ç»œçš„æ–°ç­–ç•¥ï¼šå¤§è§†é‡ï¼Œå°èšç„¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§è§†è§‰ç½‘ç»œè®¾è®¡ç­–ç•¥ï¼Œç§°ä¸ºâ€œSee Large, Focus Smallâ€ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤§æ ¸æ„ŸçŸ¥å’Œå°æ ¸èšåˆçš„LSå·ç§¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¹¿æ³›çš„æ„ŸçŸ¥ä¿¡æ¯å¹¶å®ç°ç²¾ç¡®çš„ç‰¹å¾èšåˆã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLSNetåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…‹æœäº†ç°æœ‰è½»é‡çº§æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™æƒ…å†µä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSNetåœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01848",
            "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
            "url": "https://huggingface.co/papers/2504.01848",
            "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.",
            "score": 1,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "60923777325e85cc",
            "authors": [
                "Giulio Starace",
                "Oliver Jaffe",
                "Dane Sherburn",
                "James Aung",
                "Jun Shern Chan",
                "Leon Maksin",
                "Rachel Dias",
                "Evan Mays",
                "Benjamin Kinsella",
                "Wyatt Thompson",
                "Johannes Heidecke",
                "Amelia Glaese",
                "Tejal Patwardhan"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01848.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PaperBench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "PaperBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 20 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· ICML 2024, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Claude 3.5 Sonnet (New) Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ 21.0%."
                },
                "en": {
                    "title": "Evaluating AI's Research Replication Skills with PaperBench",
                    "desc": "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."
                },
                "zh": {
                    "title": "PaperBenchï¼šè¯„ä¼°AIå¤åˆ¶ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†å¤åˆ¶æœ€å…ˆè¿›AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†ã€‚ä»£ç†éœ€è¦ä»å¤´å¼€å§‹å¤åˆ¶20ç¯‡ICML 2024çš„äº®ç‚¹å’Œå£å¤´è®ºæ–‡ï¼ŒåŒ…æ‹¬ç†è§£è®ºæ–‡è´¡çŒ®ã€å¼€å‘ä»£ç åº“å’ŒæˆåŠŸæ‰§è¡Œå®éªŒã€‚ä¸ºäº†è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†åˆ†å±‚çš„è¯„åˆ†æ ‡å‡†ï¼Œå°†æ¯ä¸ªå¤åˆ¶ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶è®¾å®šæ˜ç¡®çš„è¯„åˆ†æ ‡å‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜åŒ…æ‹¬ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„å®¡è€…è‡ªåŠ¨è¯„åˆ†ï¼Œå¹¶ä¸é¡¶å°–çš„æœºå™¨å­¦ä¹ åšå£«è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°ç›®å‰çš„æ¨¡å‹å°šæœªè¶…è¶Šäººç±»åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01724",
            "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
            "url": "https://huggingface.co/papers/2504.01724",
            "abstract": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.",
            "score": 1,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "d59102a274145730",
            "authors": [
                "Yuxuan Luo",
                "Zhengkun Rong",
                "Lizhen Wang",
                "Longhao Zhang",
                "Tianshu Hu",
                "Yongming Zhu"
            ],
            "affiliations": [
                "Bytedance Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01724.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DreamActor-M1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, 3D-ÑÑ„ĞµÑ€Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ 3D-ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼. DreamActor-M1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency",
                    "desc": "The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations."
                },
                "zh": {
                    "title": "çªç ´åŠ¨ç”»ç”Ÿæˆçš„å±€é™æ€§ï¼ŒDreamActor-M1å¼•é¢†æ–°æ½®æµï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶DreamActor-M1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåŸºç¡€çš„äººä½“åŠ¨ç”»æ–¹æ³•åœ¨ç»†ç²’åº¦æ•´ä½“å¯æ§æ€§ã€å¤šå°ºåº¦é€‚åº”æ€§å’Œé•¿æœŸæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡æ··åˆæ§åˆ¶ä¿¡å·ï¼Œç»“åˆéšå¼é¢éƒ¨è¡¨ç¤ºã€3Då¤´éƒ¨çƒä½“å’Œ3Dèº«ä½“éª¨æ¶ï¼Œå®ç°äº†å¯¹é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œçš„å¼ºå¤§æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒåŠ¨ç”»çš„è¡¨ç°åŠ›å’Œèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„èº«ä½“å§¿åŠ¿å’Œå›¾åƒå°ºåº¦ï¼Œé‡‡ç”¨äº†æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡å’Œå°ºåº¦çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚–åƒã€ä¸ŠåŠèº«å’Œå…¨èº«ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æœŸä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01204",
            "title": "Articulated Kinematics Distillation from Video Diffusion Models",
            "url": "https://huggingface.co/papers/2504.01204",
            "abstract": "We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/",
            "score": 1,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "c8630e7ca691cef3",
            "authors": [
                "Xuan Li",
                "Qianli Ma",
                "Tsung-Yi Lin",
                "Yongxin Chen",
                "Chenfanfu Jiang",
                "Ming-Yu Liu",
                "Donglai Xiang"
            ],
            "affiliations": [
                "NVIDIA",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01204.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Articulated Kinematics Distillation (AKD) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AKD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. AKD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 4D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with AKD",
                    "desc": "Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible."
                },
                "zh": {
                    "title": "å…³èŠ‚è¿åŠ¨è’¸é¦ï¼šé«˜ä¿çœŸåŠ¨ç”»çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå…³èŠ‚è¿åŠ¨è’¸é¦ï¼ˆAKDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸè§’è‰²åŠ¨ç”»ã€‚AKDé€šè¿‡ä½¿ç”¨åŸºäºéª¨éª¼çš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡å°‘äº†è‡ªç”±åº¦ï¼Œä¸“æ³¨äºå…³èŠ‚çº§æ§åˆ¶ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ä¸€è‡´çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç»“åˆçš„å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰ï¼ŒAKDèƒ½å¤Ÿè’¸é¦å¤æ‚çš„å…³èŠ‚è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒç»“æ„å®Œæ•´æ€§ï¼Œå…‹æœäº†4Dç¥ç»å˜å½¢åœºåœ¨ä¿æŒå½¢çŠ¶ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒAKDåœ¨3Dä¸€è‡´æ€§å’Œè¿åŠ¨è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ°4Dç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-02.html",
    "link_next": "2025-04-04.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "02.04",
        "en": "04/02",
        "zh": "4æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚",
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i Any2Caption de xÄ«n kuÃ ng jiÃ , zhÇ yÃº jiÄ› juÃ© dÄng qiÃ¡n shÃ¬ pÇn shÄ“ng chÃ©ng shÃ¨ qÅ« zhÅng zhÇ”n quÃ¨ lÇ jiÄ› yÃ²ng hÃ¹ de pÃ­ng yÇ wÃ¨n ti. gÇi kuÃ ng jiÃ  de hÃ© xÄ«n sÄ« xiÇng shÃ¬ jiÄng gÃ¨ zhÇ’ng tiÃ¡o jiÃ n jiÄ› shÃ¬ bÃ¹ zhÃ²u yÇ” shÃ¬ pÇn hÃ© chÃ©ng bÃ¹ zhÃ²u fÄ“n lÃ­. tÅng guÃ² lÃ¬ yÃ²ng xiÃ n dÃ i duÅ mÃ³ shÃ¬ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs), Any2Caption nÃ©ng gÃ²u jiÄng wÃ©n bÄ›n, tÃº xiÃ ng, shÃ¬ pÇn hÃ© tÃ¨ dÃ¬ng tÃ­ shÃ¬ (rÃº qÅ« yÃ¹, yÃ¹n dÃ²ng hÃ© shÃ¨ xiÃ ng tÃ³u zÄ« tÃ i) zhuÇn huÃ n wÃ¨i mÃ¬ jÄ«, jiÃ© gÃ²u huÃ  de zÃ¬ mÃ¹, cÃ³ng Ã©r wÃ©i shÃ¬ pÇn shÄ“ng chÃ©ng tÃ­ gÅng gÄ›ng hÇo de zhÇ dÇo. wÃ©n zhÄng hÃ¡i yÇn rÃ¹ le yÄ« gÃ¨ dÃ  guÄ« mÃ³ shÃ¹ jÃ¹ Any2CapIns, bÄo hÃ¡n 337K gÃ¨ shÃ­ lÃ¬ hÃ© 407K gÃ¨ tiÃ¡o jiÃ n, yÃ²ng yÃº rÃ¨n yÃ¬ tiÃ¡o jiÃ n dÃ o zÃ¬ mÃ¹ de zhÇ lÃ¬ng tiÃ¡o yÅu. zÃ²ng hÃ© pÃ­ng gÇ” biÇo mÃ­ng, gÇi xÃ¬ tÇ’ng zÃ i kÃ¨ kÃ²ng xÃ¬ng hÃ© shÃ¬ pÇn zhÃ¬ liÃ ng fÄng miÃ n xiÇn zhÃ¹ yÅu xiÃ n yÇ’u de shÃ¬ pÇn shÄ“ng chÃ©ng mÃ³ xÃ­ng.",
        "vocab": "[{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'ç“¶é¢ˆ', 'pinyin': 'pÃ­ng jÇng', 'trans': 'bottleneck'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'æ ¸å¿ƒ', 'pinyin': 'hÃ© xÄ«n', 'trans': 'core'},\n{'word': 'æ€æƒ³', 'pinyin': 'sÄ« xiÇng', 'trans': 'idea'},\n{'word': 'è§£é‡Š', 'pinyin': 'jiÄ› shÃ¬', 'trans': 'interpretation'},\n{'word': 'æ­¥éª¤', 'pinyin': 'bÃ¹ zhÃ²u', 'trans': 'step'},\n{'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesis'},\n{'word': 'åˆ†ç¦»', 'pinyin': 'fÄ“n lÃ­', 'trans': 'separation'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ shuÃ i', 'trans': 'multimodal'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è½¬æ¢', 'pinyin': 'zhuÇn huÃ n', 'trans': 'convert'},\n{'word': 'å¯†é›†', 'pinyin': 'mÃ¬ jÃ­', 'trans': 'dense'},\n{'word': 'ç»“æ„åŒ–', 'pinyin': 'jiÃ© gÃ²u huÃ ', 'trans': 'structured'},\n{'word': 'å­—å¹•', 'pinyin': 'zÃ¬ mÃ¹', 'trans': 'subtitle'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guidance'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å®ä¾‹', 'pinyin': 'shÃ­ lÃ¬', 'trans': 'instance'},\n{'word': 'è°ƒä¼˜', 'pinyin': 'tiÃ¡o yÅu', 'trans': 'tuning'},\n{'word': 'ç»¼åˆ', 'pinyin': 'zÃ²ng hÃ©', 'trans': 'comprehensive'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'å¯æ§æ€§', 'pinyin': 'kÄ› kÃ²ng xÃ¬ng', 'trans': 'controllability'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]",
        "trans": "This article introduces a new framework called Any2Caption, aimed at addressing the bottleneck issue of accurately understanding user intent in the current video generation community. The core idea of the framework is to separate various condition interpretation steps from video synthesis steps. By leveraging modern multimodal large language models (MLLMs), Any2Caption can convert text, images, videos, and specific prompts (such as regions, motion, and camera poses) into dense, structured captions, providing better guidance for video generation. The article also introduces a large-scale dataset, Any2CapIns, containing 337K instances and 407K conditions, for instruction tuning from any condition to captions. Comprehensive evaluations indicate that the system significantly outperforms existing video generation models in terms of controllability and video quality.",
        "update_ts": "2025-04-02 09:11"
    }
}