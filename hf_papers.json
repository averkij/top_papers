{
    "date": {
        "ru": "6 августа",
        "en": "August 6",
        "zh": "8月6日"
    },
    "time_utc": "2025-08-06 20:14",
    "weekday": 2,
    "issue_id": 5215,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.02193",
            "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
            "url": "https://huggingface.co/papers/2508.02193",
            "abstract": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.",
            "score": 57,
            "issue_id": 5199,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "bec183ec45598da2",
            "authors": [
                "Yuxuan Song",
                "Zheng Zhang",
                "Cheng Luo",
                "Pengyang Gao",
                "Fan Xia",
                "Hao Luo",
                "Zheng Li",
                "Yuehang Yang",
                "Hongli Yu",
                "Xingwei Qu",
                "Yuwei Fu",
                "Jing Su",
                "Ge Zhang",
                "Wenhao Huang",
                "Mingxuan Wang",
                "Lin Yan",
                "Xiaoying Jia",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Yonghui Wu",
                "Hao Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02193.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в скорости генерации текста без потери качества",
                    "desc": "Seed Diffusion Preview - это новая языковая модель, основанная на дискретной диффузии. Она обеспечивает очень быструю генерацию текста благодаря параллельному неупорядоченному декодированию. Модель достигает скорости вывода 2146 токенов в секунду на GPU H20, значительно превосходя аналоги Mercury и Gemini Diffusion. При этом Seed Diffusion Preview сохраняет конкурентоспособное качество на стандартных бенчмарках для оценки кода."
                },
                "en": {
                    "title": "Speed Meets Quality: The Future of Code Generation",
                    "desc": "Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion."
                },
                "zh": {
                    "title": "种子扩散预览：速度与质量的新标杆",
                    "desc": "Seed Diffusion Preview是一种基于离散状态扩散的语言模型，具有极快的推理速度。通过非顺序的并行生成，离散扩散模型显著提高了推理效率，减少了逐个解码的延迟。该模型在H20 GPU上实现了每秒2,146个token的推理速度，同时在标准代码评估基准上保持了竞争力的性能。与当前的Mercury和Gemini Diffusion相比，Seed Diffusion Preview在速度和质量上都设立了新的标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03320",
            "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
            "url": "https://huggingface.co/papers/2508.03320",
            "abstract": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
            "score": 40,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "71dc78f7c773cefd",
            "authors": [
                "Peiyu Wang",
                "Yi Peng",
                "Yimeng Gan",
                "Liang Hu",
                "Tianyidan Xie",
                "Xiaokun Wang",
                "Yichen Wei",
                "Chuanxin Tang",
                "Bo Zhu",
                "Changshi Li",
                "Hongyang Wei",
                "Eric Li",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Multimodality Team, Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, создания и редактирования изображений",
                    "desc": "Skywork UniPic - это авторегрессионная модель с 1,5 миллиардами параметров, объединяющая понимание изображений, генерацию изображений по тексту и редактирование изображений в единой архитектуре. Модель достигает высоких результатов на различных бенчмарках, включая GenEval, DPG-Bench и GEditBench-EN. Skywork UniPic использует раздельную стратегию кодирования и прогрессивное обучение с увеличением разрешения. Модель демонстрирует, что высококачественная мультимодальная интеграция возможна без чрезмерных вычислительных затрат."
                },
                "en": {
                    "title": "Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic",
                    "desc": "Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources."
                },
                "zh": {
                    "title": "Skywork UniPic：统一多模态AI的高效解决方案",
                    "desc": "Skywork UniPic是一个拥有15亿参数的自回归模型，能够统一图像理解、文本到图像生成和图像编辑。该模型通过一个单一架构消除了对特定任务适配器或模块连接器的需求，展示了紧凑的多模态系统在普通硬件上也能达到最先进的性能。Skywork UniPic在多个基准测试中表现优异，尤其是在图像生成和编辑方面，显示出其高效的训练策略和数据集设计。该模型为高保真多模态AI的实际应用提供了新的范式，且代码和权重已公开。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03694",
            "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
            "url": "https://huggingface.co/papers/2508.03694",
            "abstract": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.",
            "score": 31,
            "issue_id": 5198,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "8c05bd06521b3fb7",
            "authors": [
                "Jianxiong Gao",
                "Zhaoxi Chen",
                "Xian Liu",
                "Jianfeng Feng",
                "Chenyang Si",
                "Yanwei Fu",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "NVIDIA",
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03694.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LongVie: прорыв в генерации сверхдлинных видео с сохранением качества",
                    "desc": "LongVie - это новая автореградная модель для генерации сверхдлинных видео. Она решает проблемы временной согласованности и визуальной деградации с помощью унифицированной инициализации шума и глобальной нормализации управляющих сигналов. LongVie использует мультимодальное управление, объединяя плотные и разреженные сигналы. Модель также применяет стратегию обучения с учетом деградации для сохранения визуального качества."
                },
                "en": {
                    "title": "LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality",
                    "desc": "LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing."
                },
                "zh": {
                    "title": "超长视频生成的新突破：LongVie",
                    "desc": "LongVie是一个端到端的自回归框架，旨在解决超长视频生成中的时间一致性和视觉退化问题。它通过统一的噪声初始化、全局控制信号归一化、多模态控制和退化感知训练来实现这些目标。LongVie的核心设计确保了时间一致性，并通过多模态控制框架来减轻视觉退化。实验结果表明，LongVie在长时间可控性、一致性和质量方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03686",
            "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
            "url": "https://huggingface.co/papers/2508.03686",
            "abstract": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.",
            "score": 21,
            "issue_id": 5201,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "dddc5da46c921b94",
            "authors": [
                "Shudong Liu",
                "Hongwei Liu",
                "Junnan Liu",
                "Linchen Xiao",
                "Songyang Gao",
                "Chengqi Lyu",
                "Yuzhe Gu",
                "Wenwei Zhang",
                "Derek F. Wong",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "NLP2CT Lab",
                "Shanghai AI Laboratory",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03686.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "CompassVerifier: Надежная проверка ответов LLM во многих областях",
                    "desc": "CompassVerifier - это легковесная модель для проверки выходных данных больших языковых моделей (LLM) в различных областях. Она поддерживается VerifierBench - комплексным набором данных для оценки. CompassVerifier демонстрирует компетентность в различных областях, включая математику, знания и разнообразные задачи на рассуждение. Модель способна обрабатывать различные типы ответов, включая многозадачные проблемы, формулы и последовательные ответы, эффективно идентифицируя аномальные и недействительные ответы."
                },
                "en": {
                    "title": "Revolutionizing LLM Output Verification with CompassVerifier",
                    "desc": "CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses."
                },
                "zh": {
                    "title": "CompassVerifier：多领域答案验证的轻量级解决方案",
                    "desc": "CompassVerifier 是一种轻量级且稳健的模型，用于验证大型语言模型（LLM）在不同领域的输出。它通过 VerifierBench 这一全面的基准数据集来支持验证过程。该模型能够处理多种类型的答案，包括多子问题、公式和序列答案，并有效识别异常或无效的响应。我们希望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03012",
            "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
            "url": "https://huggingface.co/papers/2508.03012",
            "abstract": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.",
            "score": 8,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "4ab74a355fed1d76",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Qunhong Zeng",
                "Pengfei Gao",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03012.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ToolTrain: Эффективная локализация проблем в коде с помощью обученных языковых моделей",
                    "desc": "ToolTrain - это двухэтапная система обучения, объединяющая контролируемую тонкую настройку и обучение с подкреплением для улучшения работы больших языковых моделей в задаче локализации проблем в программном коде. Система интегрирует инструменты поиска по репозиторию, что позволяет преодолеть семантический разрыв между описанием проблемы на естественном языке и проблемным кодом. Экспериментальные результаты показывают, что модели, обученные с помощью ToolTrain, достигают наилучших показателей в этой задаче. Улучшенная производительность в локализации проблем также приводит к лучшим результатам в полном цикле разрешения проблем в программном обеспечении."
                },
                "en": {
                    "title": "ToolTrain: Enhancing LLMs for Superior Issue Localization",
                    "desc": "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."
                },
                "zh": {
                    "title": "ToolTrain：提升问题定位的智能工具训练框架",
                    "desc": "ToolTrain是一种两阶段的训练框架，结合了监督微调和强化学习，旨在提升大型语言模型（LLMs）在问题定位方面的能力。问题定位是识别需要修改的代码位置以解决软件问题的过程，但由于自然语言描述与故障代码之间的语义差距，这一任务非常具有挑战性。ToolTrain通过整合代码库检索工具，帮助LLMs在多步骤推理和导航过程中有效利用这些工具，从而实现了最先进的性能。实验结果表明，ToolTrain训练的模型在功能级定位上超越了Claude-3.7，证明了针对问题定位的训练策略在自动化软件开发中是有效的。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02091",
            "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
            "url": "https://huggingface.co/papers/2508.02091",
            "abstract": "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
            "score": 7,
            "issue_id": 5201,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "10eb53caada711ad",
            "authors": [
                "Xiaoya Li",
                "Xiaofei Sun",
                "Albert Wang",
                "Chris Shum",
                "Jiwei Li"
            ],
            "affiliations": [
                "DeepReinforce Team",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02091.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rag",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "CRINN: Революция в поиске ближайших соседей с помощью ИИ",
                    "desc": "CRINN - это новый подход к оптимизации алгоритмов поиска приближенных ближайших соседей (ANNS), использующий обучение с подкреплением. Он автоматически генерирует все более быстрые реализации ANNS, сохраняя при этом точность. CRINN превзошел современные методы на нескольких эталонных тестах, включая GIST-960-Euclidean и MNIST-784-Euclidean. Успех CRINN показывает, что языковые модели, дополненные обучением с подкреплением, могут эффективно автоматизировать сложные алгоритмические оптимизации."
                },
                "en": {
                    "title": "CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning",
                    "desc": "CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations."
                },
                "zh": {
                    "title": "CRINN：用强化学习加速近似最近邻搜索",
                    "desc": "CRINN是一种基于强化学习的方法，旨在优化近似最近邻搜索算法的速度，同时保持准确性。该方法将近似最近邻搜索的优化视为一个强化学习问题，以执行速度作为奖励信号。通过这种方式，CRINN能够自动生成逐渐更快的近似最近邻搜索实现，并满足准确性约束。实验结果表明，CRINN在多个基准数据集上表现优异，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00367",
            "title": "Representation Shift: Unifying Token Compression with FlashAttention",
            "url": "https://huggingface.co/papers/2508.00367",
            "abstract": "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.",
            "score": 7,
            "issue_id": 5209,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "5a4ad3025ab24bd1",
            "authors": [
                "Joonmyung Choi",
                "Sanghyeok Lee",
                "Byungoh Ko",
                "Eunseo Kim",
                "Jihyung Kil",
                "Hyunwoo J. Kim"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00367.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#data",
                    "#optimization",
                    "#video"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обработки видео и текста без потери качества",
                    "desc": "Статья представляет метрику Representation Shift, которая позволяет эффективно сжимать токены в трансформерах без переобучения модели. Эта метрика совместима с FlashAttention, что значительно ускоряет обработку видео и текста. Representation Shift измеряет степень изменения представления каждого токена, что позволяет определить наиболее важные из них. Метод применим не только к трансформерам, но и к CNN и моделям пространства состояний."
                },
                "en": {
                    "title": "Boosting Video Retrieval Efficiency with Representation Shift",
                    "desc": "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."
                },
                "zh": {
                    "title": "Representation Shift：加速视频检索与问答的创新方法",
                    "desc": "Representation Shift是一种无训练、模型无关的度量方法，它将令牌压缩与FlashAttention结合，显著加快视频-文本检索和视频问答的速度。随着任务复杂性的增加，模型和令牌的规模也在扩大，导致自注意力的计算成本呈平方增长。我们的方法通过测量每个令牌表示的变化程度，来实现有效的令牌压缩，而无需构建注意力图或重新训练。实验结果表明，Representation Shift在视频-文本检索和视频问答中分别实现了高达5.5%和4.4%的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03050",
            "title": "Multi-human Interactive Talking Dataset",
            "url": "https://huggingface.co/papers/2508.03050",
            "abstract": "MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
            "score": 6,
            "issue_id": 5200,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "01ba126a166568d6",
            "authors": [
                "Zeyu Zhu",
                "Weijia Wu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03050.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Новый датасет и модель для генерации видео с разговорами нескольких людей",
                    "desc": "Исследователи представили MIT - крупномасштабный набор данных для генерации видео с разговорами нескольких людей. Этот датасет включает детальные аннотации и используется для демонстрации CovOG - базовой модели, объединяющей кодировщик поз нескольких людей и интерактивный аудиодрайвер. MIT содержит 12 часов видео высокого разрешения с 2-4 говорящими и детальными аннотациями поз тела и речевых взаимодействий. CovOG использует агрегацию индивидуальных эмбеддингов поз и модуляцию динамики головы на основе аудиопризнаков каждого говорящего."
                },
                "en": {
                    "title": "MIT: Pioneering Multi-Human Talking Video Generation",
                    "desc": "The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area."
                },
                "zh": {
                    "title": "MIT：多人人对话视频生成的新基准",
                    "desc": "MIT是一个大规模的数据集，专门用于多人的对话视频生成，包含细致的注释信息。现有的研究主要集中在单人独白或孤立的面部动画上，限制了其在真实多人的互动中的应用。我们开发了一个自动化流程，收集和注释多人的对话视频，数据集包含12小时的高分辨率视频，展示了自然的对话动态。为了展示MIT的潜力，我们提出了CovOG模型，结合了多人的姿态编码器和互动音频驱动器，展示了生成真实多人的对话视频的可行性和挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01119",
            "title": "The Promise of RL for Autoregressive Image Editing",
            "url": "https://huggingface.co/papers/2508.01119",
            "abstract": "Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.",
            "score": 6,
            "issue_id": 5215,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "b7e0974935b60296",
            "authors": [
                "Saba Ahmadi",
                "Rabiul Awal",
                "Ankur Sikarwar",
                "Amirhossein Kazemnejad",
                "Ge Ya Luo",
                "Juan A. Rodriguez",
                "Sai Rajeswar",
                "Siva Reddy",
                "Christopher Pal",
                "Benno Krojer",
                "Aishwarya Agrawal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique Montréal",
                "ServiceNow",
                "Université de Montréal",
                "École de Technologie Supérieure (ETS)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01119.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#training",
                    "#games"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Обучение с подкреплением повышает качество редактирования изображений",
                    "desc": "В статье исследуются стратегии улучшения редактирования изображений с использованием авторегрессионной мультимодальной модели. Авторы сравнивают три подхода: обучение с учителем, обучение с подкреплением и рассуждения по цепочке мыслей. Наиболее эффективным оказалось обучение с подкреплением в сочетании с большой мультимодальной языковой моделью-верификатором. В результате была разработана модель EARL, показывающая конкурентоспособные результаты на различных задачах редактирования изображений."
                },
                "en": {
                    "title": "Reinforcement Learning Meets Multimodal Image Editing",
                    "desc": "This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing."
                },
                "zh": {
                    "title": "强化学习与多模态模型结合，提升图像编辑性能",
                    "desc": "本论文探讨了如何通过结合强化学习和大型多模态语言模型验证器来提升图像编辑性能。我们提出了三种策略：监督微调、强化学习和链式思维推理，并在一个自回归多模态框架中进行研究。实验结果表明，强化学习与大型多模态语言模型验证器的结合是最有效的策略。最终，我们发布了EARL模型，它在多种图像编辑任务中表现出色，且训练数据需求较少。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03613",
            "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
            "url": "https://huggingface.co/papers/2508.03613",
            "abstract": "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.",
            "score": 5,
            "issue_id": 5204,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "50044cd9b7eb1802",
            "authors": [
                "Yong Lin",
                "Shange Tang",
                "Bohan Lyu",
                "Ziran Yang",
                "Jui-Hui Chung",
                "Haoyu Zhao",
                "Lai Jiang",
                "Yihan Geng",
                "Jiawei Ge",
                "Jingruo Sun",
                "Jiayun Wu",
                "Jiri Gesi",
                "Ximing Lu",
                "David Acuna",
                "Kaiyu Yang",
                "Hongzhou Lin",
                "Yejin Choi",
                "Danqi Chen",
                "Sanjeev Arora",
                "Chi Jin"
            ],
            "affiliations": [
                "Amazon",
                "Meta FAIR",
                "NVIDIA",
                "Peking University",
                "Princeton Language and Intelligence, Princeton University",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03613.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#small_models",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Маленькая модель - большие доказательства: Goedel-Prover-V2 переворачивает мир автоматического доказательства теорем",
                    "desc": "Goedel-Prover-V2 - это серия открытых языковых моделей для автоматического доказательства теорем. Модель использует синтез структурированных данных, самокоррекцию на основе верификатора и усреднение моделей для достижения наилучших результатов. Несмотря на небольшой размер, Goedel-Prover-V2-8B превосходит гораздо более крупные модели на бенчмарках MiniF2F и PutnamBench. На момент выпуска Goedel-Prover-V2 демонстрирует лучшую производительность среди открытых систем автоматического доказательства теорем."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with Goedel-Prover-V2",
                    "desc": "Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint."
                },
                "zh": {
                    "title": "Goedel-Prover-V2：自动定理证明的新标杆",
                    "desc": "Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域达到了最先进的性能。该模型通过三项创新技术实现了这一目标：首先，使用支架数据合成生成逐渐增加难度的合成任务，以帮助模型掌握复杂的定理；其次，采用验证器引导的自我修正，使模型能够根据Lean编译器的反馈迭代修正其证明；最后，通过模型平均技术合并模型检查点，以减少训练后期模型输出多样性的下降。Goedel-Prover-V2-32B模型在MiniF2F上达到了88.1%的通过率，显著超越了之前的最先进模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01780",
            "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
            "url": "https://huggingface.co/papers/2508.01780",
            "abstract": "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
            "score": 5,
            "issue_id": 5199,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 августа",
                "en": "August 3",
                "zh": "8月3日"
            },
            "hash": "b9c09b0ce4e2dad3",
            "authors": [
                "Guozhao Mo",
                "Wenliang Zhong",
                "Jiawei Chen",
                "Xuanang Chen",
                "Yaojie Lu",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01780.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LiveMCPBench: Новый стандарт оценки LLM-агентов в реальных MCP-средах",
                    "desc": "LiveMCPBench представляет собой комплексный бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в реальных задачах экосистемы MCP. Он включает 95 задач и использует масштабируемый конвейер оценки с адаптивной системой судейства. Бенчмарк содержит LiveMCPTool - набор из 70 MCP-серверов и 527 инструментов, а также LiveMCPEval - фреймворк для автоматизированной оценки с использованием LLM в качестве судьи. Результаты тестирования 10 ведущих моделей показали, что лучшая модель (Claude-Sonnet-4) достигла 78.95% успешности выполнения задач."
                },
                "en": {
                    "title": "Revolutionizing LLM Evaluation in Dynamic MCP Environments",
                    "desc": "LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments."
                },
                "zh": {
                    "title": "全面评估LLM代理的基准测试平台",
                    "desc": "LiveMCPBench是一个全面的基准测试平台，旨在评估大型语言模型（LLM）代理在多样化的真实世界任务中的表现。它解决了现有基准测试仅限于单一服务器设置的问题，提供了95个基于模型上下文协议（MCP）生态系统的真实任务。通过LiveMCPTool，研究人员可以使用70个MCP服务器和527个工具，支持可扩展和可重复的评估流程。此外，LiveMCPEval框架实现了自动化和自适应评估，确保在动态任务环境中与人类评审者的高一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00477",
            "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
            "url": "https://huggingface.co/papers/2508.00477",
            "abstract": "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.",
            "score": 4,
            "issue_id": 5202,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "94d96ba2b9f92b31",
            "authors": [
                "Yuzhuo Chen",
                "Zehua Ma",
                "Jianhua Wang",
                "Kai Kang",
                "Shunyu Yao",
                "Weiming Zhang"
            ],
            "affiliations": [
                "East China Normal University",
                "Onestory Team",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00477.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "LAMIC: Революция в синтезе изображений с несколькими референсами",
                    "desc": "LAMIC - это фреймворк для композиции нескольких изображений с учетом макета, который расширяет возможности диффузионных моделей с одним референсом на сценарии с несколькими референсами. Он использует два механизма внимания: Group Isolation Attention для улучшения разделения сущностей и Region-Modulated Attention для генерации с учетом макета. LAMIC достигает наилучших результатов по большинству метрик без дополнительного обучения, демонстрируя превосходные способности в сохранении идентичности, фона и контроле макета. Этот подход устанавливает новую парадигму для контролируемой композиции нескольких изображений без обучения."
                },
                "en": {
                    "title": "LAMIC: Revolutionizing Multi-Image Synthesis Without Training",
                    "desc": "LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner."
                },
                "zh": {
                    "title": "LAMIC：无训练的多图像合成新范式",
                    "desc": "LAMIC是一个布局感知的多图像合成框架，它将单参考扩散模型扩展到多参考场景，且无需训练。该框架引入了两种注意力机制：群体隔离注意力（GIA）和区域调制注意力（RMA），以增强实体分离和布局感知生成。通过引入三种评估指标，LAMIC在身份保持、背景一致性和布局控制等方面表现出色，超越了现有的多参考基线。LAMIC展示了强大的零样本泛化能力，为可控的多图像合成建立了新的无训练范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03164",
            "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
            "url": "https://huggingface.co/papers/2508.03164",
            "abstract": "ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.",
            "score": 3,
            "issue_id": 5204,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "aec1f860dbfa8231",
            "authors": [
                "Junyoung Lim",
                "Jaewoo Ahn",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03164.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ChartCap: Точные подписи к графикам без галлюцинаций",
                    "desc": "ChartCap - это масштабный набор данных, содержащий 565 тысяч реальных графиков с детальными подписями. Датасет разработан для улучшения точности генерации подписей и уменьшения галлюцинаций в мультимодальных языковых моделях. ChartCap использует четырехэтапный конвейер для создания подписей, основанных только на видимых данных графика, и применяет верификацию на основе циклической согласованности. Авторы также предлагают новую метрику - Visual Consistency Score, для оценки качества подписей без опоры на эталонные подписи."
                },
                "en": {
                    "title": "ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations",
                    "desc": "ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions."
                },
                "zh": {
                    "title": "ChartCap：提升图表说明准确性的关键数据集",
                    "desc": "ChartCap是一个大规模的数据集，包含565K个真实世界图表图像及其特定类型的详细说明。该数据集旨在提高视觉语言模型的说明准确性，并减少虚假信息的生成。通过设计四阶段的生成管道，ChartCap确保说明仅基于图表中可辨别的数据，并通过循环一致性的人类验证加速质量控制。实验结果表明，基于ChartCap微调的模型在生成准确和信息丰富的说明方面表现优于其他开源和专有模型，甚至超过人类标注的说明。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02629",
            "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
            "url": "https://huggingface.co/papers/2508.02629",
            "abstract": "HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.",
            "score": 3,
            "issue_id": 5212,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "5b4c4a212eae58d6",
            "authors": [
                "Yibin Liu",
                "Zhixuan Liang",
                "Zanxin Chen",
                "Tianxing Chen",
                "Mengkang Hu",
                "Wanxi Dong",
                "Congsheng Xu",
                "Zhaoming Han",
                "Yusen Qin",
                "Yao Mu"
            ],
            "affiliations": [
                "D-Robotics",
                "HKU MMLab",
                "NEU",
                "SJTU ScaleLab",
                "SUSTech",
                "SZU",
                "Shanghai AI Lab",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02629.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#agents",
                    "#reasoning",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самокорректирующиеся программы для роботов на основе мультимодального ИИ",
                    "desc": "HyCodePolicy - это гибридная система управления для воплощенных агентов, использующая мультимодальные языковые модели. Она объединяет синтез кода, геометрическое обоснование, перцептивный мониторинг и итеративное исправление в замкнутый цикл программирования. Система генерирует исполняемую программу на основе естественно-языковых инструкций, затем выполняет ее в симуляции, отслеживая ошибки с помощью визуально-языковой модели. HyCodePolicy способна автономно исправлять программы, значительно повышая надежность и эффективность политик для манипуляций роботов."
                },
                "en": {
                    "title": "Empowering Robots with Self-Correcting Code Synthesis",
                    "desc": "HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks."
                },
                "zh": {
                    "title": "自我修正的智能体编程策略",
                    "desc": "HyCodePolicy 是一种混合语言控制框架，旨在增强具身智能体策略的鲁棒性和效率。该系统通过将代码合成、几何基础、感知监控和迭代修复整合到一个闭环编程周期中，来实现自我修正的程序合成。它首先将自然语言指令分解为子目标，并生成基于对象中心几何原语的可执行程序。通过视觉语言模型监控执行过程，HyCodePolicy 能够检测执行失败并进行修复，从而提高机器人操作策略的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02079",
            "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
            "url": "https://huggingface.co/papers/2508.02079",
            "abstract": "AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.",
            "score": 2,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "25a555fc0f91f562",
            "authors": [
                "Amitava Das",
                "Abhilekh Borah",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "BITS Goa, India",
                "Manipal University, India",
                "Meta AI, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Сохранение безопасности при дообучении языковых моделей",
                    "desc": "AlignGuard-LoRA (AGL) - это фреймворк для сохранения выравнивания при дообучении больших языковых моделей. Он вводит методы регуляризации и диагностический бенчмарк для снижения дрейфа выравнивания. AGL включает несколько ключевых компонентов, таких как регуляризация на основе матрицы Фишера и регуляризация с учетом коллизий. Эмпирические оценки показывают, что AGL снижает дрейф выравнивания до 50% на критически важных для безопасности бенчмарках без ухудшения производительности на целевых задачах."
                },
                "en": {
                    "title": "Preserving Alignment in Fine-Tuning with AGL",
                    "desc": "AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks."
                },
                "zh": {
                    "title": "AlignGuard-LoRA：保持对齐，确保安全",
                    "desc": "AlignGuard-LoRA (AGL) 是一个框架，旨在通过引入正则化技术和诊断基准来保持大型语言模型在微调过程中的对齐性。该框架解决了低秩适应（LoRA）在更新过程中可能导致的对齐漂移问题，从而增强安全性和行为约束。AGL 采用了多种关键组件，包括基于费舍尔信息矩阵的正则化和任务特定的正则化，以稳定新知识的整合。实验证明，AGL 能够在不降低下游任务性能的情况下，将对齐漂移减少多达 50%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02630",
            "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
            "url": "https://huggingface.co/papers/2508.02630",
            "abstract": "ACES, a sandbox environment, studies AI agents' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal \"top\" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.",
            "score": 1,
            "issue_id": 5215,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "2860dd8bc6a41f92",
            "authors": [
                "Amine Allouah",
                "Omar Besbes",
                "Josué D Figueroa",
                "Yash Kanoria",
                "Akshit Kumar"
            ],
            "affiliations": [
                "Columbia University, Graduate School of Business",
                "MyCustomAI",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02630.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#ethics",
                    "#games",
                    "#alignment"
                ],
                "emoji": "🛒",
                "ru": {
                    "title": "ИИ идет за покупками: новая эра электронной коммерции",
                    "desc": "Исследование ACES изучает поведение ИИ-агентов при совершении покупок в виртуальной торговой среде. Эксперименты выявили влияние позиции товаров, спонсорских тегов, рекомендаций, цен, рейтингов и отзывов на выбор ИИ-покупателей. Результаты показывают, что разные модели машинного обучения демонстрируют различную чувствительность к этим факторам. Исследование поднимает вопросы о стратегиях продавцов, дизайне платформ и регулировании в экосистеме, где покупки осуществляются с помощью ИИ."
                },
                "en": {
                    "title": "Understanding AI Agents in E-Commerce: Insights from ACES",
                    "desc": "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."
                },
                "zh": {
                    "title": "探索AI代理在电商中的购物行为",
                    "desc": "本论文研究了人工智能代理在模拟市场中的购物行为，提出了一个名为ACES的沙盒环境。通过随机化产品位置、价格、评分、评论和赞助标签，研究了不同模型在购物时的因果关系和偏好。结果显示，AI代理对产品位置有明显的偏好，但不同模型的选择差异很大，且对价格和评分的敏感度与人类相似但幅度不同。研究还表明，卖方可以通过优化产品描述来吸引AI买家的偏好，从而在市场中获得显著的份额提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02455",
            "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs",
            "url": "https://huggingface.co/papers/2508.02455",
            "abstract": "A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.",
            "score": 1,
            "issue_id": 5215,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "70799804b6937703",
            "authors": [
                "Daniele Cipollone",
                "Egor Bogomolov",
                "Arie van Deursen",
                "Maliheh Izadi"
            ],
            "affiliations": [
                "Delft University of Technology, Delft, Netherlands",
                "JetBrains, Amsterdam, Netherlands"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02455.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Умное ранжирование автодополнений кода с помощью языковых моделей",
                    "desc": "Статья предлагает новый подход к ранжированию статических автодополнений кода в IDE с использованием языковых моделей. Метод организует варианты дополнений в префиксное дерево и выполняет однократный жадный проход декодирования для сбора оценок на уровне токенов. Это позволяет точно ранжировать варианты с учетом контекста без необходимости в лучевом поиске или адаптации модели. Подход быстрый, не зависит от архитектуры и совместим с уже развернутыми моделями автодополнения кода."
                },
                "en": {
                    "title": "Smart Code Completion with Language Models",
                    "desc": "This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly."
                },
                "zh": {
                    "title": "智能代码补全的新方法",
                    "desc": "本文提出了一种新的评分方法，利用语言模型对静态代码补全进行排名。该方法将所有有效的补全组织成前缀树，并通过单次贪婪解码来收集每个标记的分数。与传统的基于手工启发式或轻量级机器学习模型的方法相比，这种方法能够更好地捕捉上下文信息，并在不同项目和编码风格中进行泛化。最终，这种快速且与模型无关的方法为集成语言模型到现有IDE工具中提供了有效的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02063",
            "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
            "url": "https://huggingface.co/papers/2508.02063",
            "abstract": "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7",
            "score": 1,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "0b136776fbb19b26",
            "authors": [
                "Amitava Das",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon GenAI",
                "BITS Pilani Goa",
                "Meta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02063.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#rlhf",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "TraceAlign: отслеживание и устранение дрейфа выравнивания в больших языковых моделях",
                    "desc": "TraceAlign - это фреймворк для выявления и снижения дрейфа выравнивания в больших языковых моделях. Он отслеживает небезопасные завершения до их источников в обучающих данных и применяет интервенции для уменьшения дрейфа при сохранении полезности модели. Ключевым элементом является Индекс Конфликта Убеждений (BCI), количественно оценивающий семантическое несоответствие между сгенерированными фрагментами и заданными политиками. Фреймворк предлагает три дополняющих друг друга метода защиты: фильтр безопасности TraceShield, контрастивную функцию потерь и стратегию декодирования Prov-Decode."
                },
                "en": {
                    "title": "TraceAlign: Bridging the Gap in LLM Alignment",
                    "desc": "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."
                },
                "zh": {
                    "title": "TraceAlign：减轻大型语言模型对齐漂移的创新框架",
                    "desc": "TraceAlign是一个框架，用于识别和减轻大型语言模型（LLMs）中的对齐漂移。它通过追踪不安全的生成结果到其训练来源，并应用干预措施来减少漂移，同时保持模型的实用性。该框架引入了信念冲突指数（BCI），量化生成内容与对齐政策之间的语义不一致性。通过三种互补的干预措施，TraceAlign能够在保持任务性能的同时，显著降低对齐漂移的发生率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01126",
            "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation",
            "url": "https://huggingface.co/papers/2508.01126",
            "abstract": "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.",
            "score": 0,
            "issue_id": 5212,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 августа",
                "en": "August 2",
                "zh": "8月2日"
            },
            "hash": "8f30093d889bde3a",
            "authors": [
                "Chaitanya Patel",
                "Hiroki Nakamura",
                "Yuta Kyuragi",
                "Kazuki Kozuka",
                "Juan Carlos Niebles",
                "Ehsan Adeli"
            ],
            "affiliations": [
                "Panasonic Holdings Corporation",
                "Panasonic R&D Company of America",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01126.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#benchmark",
                    "#video",
                    "#diffusion",
                    "#multimodal",
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "🕶️",
                "ru": {
                    "title": "Революция в моделировании движения от первого лица",
                    "desc": "Представлена унифицированная условная модель диффузии движения UniEgoMotion для генерации и прогнозирования эгоцентрического движения с использованием изображений от первого лица. Модель достигает наилучших результатов в реконструкции эгоцентрического движения и впервые позволяет генерировать движение по одному эгоцентрическому изображению. UniEgoMotion использует новое представление движения с учетом положения головы, специально разработанное для эгоцентрических устройств. Для обучения модели создан большой набор данных EE4D-Motion на основе EgoExo4D с псевдо-разметкой трехмерного движения."
                },
                "en": {
                    "title": "Revolutionizing Egocentric Motion with UniEgoMotion",
                    "desc": "The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks."
                },
                "zh": {
                    "title": "自我中心运动生成的新突破",
                    "desc": "本文介绍了一种统一的条件运动扩散模型UniEgoMotion，用于从第一人称图像生成和预测自我中心的运动。该模型在自我中心运动重建和预测方面达到了最先进的性能，能够仅从单张图像生成运动。UniEgoMotion采用了一种新颖的头部中心运动表示，支持在没有明确3D场景的情况下进行场景感知的运动合成。通过引入EE4D-Motion数据集，本文为训练提供了丰富的伪真实3D运动注释，推动了自我中心运动建模的新标准。"
                }
            }
        }
    ],
    "link_prev": "2025-08-05.html",
    "link_next": "2025-08-07.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "05.08",
        "en": "08/05",
        "zh": "8月5日"
    },
    "short_date_next": {
        "ru": "07.08",
        "en": "08/07",
        "zh": "8月7日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 4,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}