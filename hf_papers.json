{
    "date": {
        "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 12",
        "zh": "2æœˆ12æ—¥"
    },
    "time_utc": "2025-02-12 16:12",
    "weekday": 2,
    "issue_id": 2176,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.06807",
            "title": "Competitive Programming with Large Reasoning Models",
            "url": "https://huggingface.co/papers/2502.06807",
            "abstract": "We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.",
            "score": 35,
            "issue_id": 2164,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "fd76cceb75f32321",
            "authors": [
                "OpenAI",
                ":",
                "Ahmed El-Kishky",
                "Alexander Wei",
                "Andre Saraiva",
                "Borys Minaev",
                "Daniel Selsam",
                "David Dohan",
                "Francis Song",
                "Hunter Lightman",
                "Ignasi Clavera",
                "Jakub Pachocki",
                "Jerry Tworek",
                "Lorenz Kuhn",
                "Lukasz Kaiser",
                "Mark Chen",
                "Max Schwarzer",
                "Mostafa Rohaninejad",
                "Nat McAleese",
                "o3 contributors",
                "Oleg MÃ¼rk",
                "Rhythm Garg",
                "Rui Shu",
                "Szymon Sidor",
                "Vineet Kosaraju",
                "Wenda Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.06807.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#games",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ (OpenAI o1 Ğ¸ o3) ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ o1-ioi, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ² ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ (IOI) 2024 Ğ³Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ o3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° IOI 2024 Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº."
                },
                "en": {
                    "title": "Scaling General-Purpose Learning Outshines Specialized Strategies",
                    "desc": "This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›é€šç”¨æ¨¡å‹è¶…è¶Šç‰¹å®šé¢†åŸŸç³»ç»Ÿ",
                    "desc": "æœ¬è®ºæ–‡å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚ç¼–ç å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§é€šç”¨æ¨ç†æ¨¡å‹â€”â€”OpenAIçš„o1å’Œo3çš„æ—©æœŸæ£€æŸ¥ç‚¹ï¼Œä»¥åŠä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„ç³»ç»Ÿo1-ioiï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨æ‰‹å·¥è®¾è®¡çš„æ¨ç†ç­–ç•¥ã€‚o1-ioiåœ¨2024å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è¡¨ç°è‰¯å¥½ï¼Œè·å¾—äº†ç¬¬49ç™¾åˆ†ä½çš„æˆç»©ï¼Œè€Œåœ¨æ”¾å®½ç«äº‰çº¦æŸçš„æƒ…å†µä¸‹åˆ™è·å¾—äº†é‡‘ç‰Œã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸“é—¨çš„ç®¡é“å¦‚o1-ioièƒ½å¸¦æ¥æ˜¾è‘—æå‡ï¼Œä½†æ‰©å±•çš„é€šç”¨o3æ¨¡å‹åœ¨æ²¡æœ‰ä¾èµ–æ‰‹å·¥æ¨ç†å¯å‘å¼çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†è¿™äº›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07316",
            "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
            "url": "https://huggingface.co/papers/2502.07316",
            "abstract": "Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.",
            "score": 20,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "fd4f34152d4de2c1",
            "authors": [
                "Junlong Li",
                "Daya Guo",
                "Dejian Yang",
                "Runxin Xu",
                "Yu Wu",
                "Junxian He"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "HKUST",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07316.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CodeI/O: Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ CodeI/O Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ² ĞºĞ¾Ğ´, Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CodeI/O Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ…, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with CodeI/O",
                    "desc": "This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++."
                },
                "zh": {
                    "title": "CodeI/Oï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºCodeI/Oï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å°†åŸå§‹ä»£ç è½¬æ¢ä¸ºè¾“å…¥è¾“å‡ºé¢„æµ‹æ ¼å¼ï¼ŒCodeI/Oç³»ç»Ÿåœ°æç‚¼äº†å¤šæ ·çš„æ¨ç†æ¨¡å¼ã€‚æ¨¡å‹é€šè¿‡è‡ªç„¶è¯­è¨€çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ¥é¢„æµ‹ä»£ç çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä»è€Œå¢å¼ºäº†é€»è¾‘æµè§„åˆ’ã€çŠ¶æ€ç©ºé—´æœç´¢ç­‰æ¨ç†åŸè¯­çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeI/Oåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06329",
            "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
            "url": "https://huggingface.co/papers/2502.06329",
            "abstract": "We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA",
            "score": 15,
            "issue_id": 2168,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "836d77158c8a414b",
            "authors": [
                "Kiran Kamble",
                "Melisa Russak",
                "Dmytro Mozolevskyi",
                "Muayad Ali",
                "Mateusz Russak",
                "Waseem AlShikh"
            ],
            "affiliations": [
                "Writer, Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06329.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#long_context",
                    "#hallucinations"
                ],
                "emoji": "ğŸ’¼",
                "ru": {
                    "title": "FailSafeQA: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ FailSafeQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° LLM, Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…: Ğ¾Ñ‚ĞºĞ°Ğ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¾Ñ‚ĞºĞ°Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ LLM-as-a-Judge Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-72B-Instruct, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 24 Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ½ÑƒĞ»Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ FailSafeQA ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… LLM Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing LLM Reliability in Finance with FailSafeQA",
                    "desc": "The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts."
                },
                "zh": {
                    "title": "FailSafeQAï¼šæå‡é‡‘èé¢†åŸŸLLMçš„é²æ£’æ€§ä¸ä¸Šä¸‹æ–‡æ„è¯†",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„é•¿ä¸Šä¸‹æ–‡é‡‘èåŸºå‡†ï¼ŒFailSafeQAï¼Œæ—¨åœ¨æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èé¢†åŸŸä¸­å¯¹äººæœºäº¤äº’çš„é²æ£’æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚æˆ‘ä»¬å…³æ³¨ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼šæŸ¥è¯¢å¤±è´¥å’Œä¸Šä¸‹æ–‡å¤±è´¥ã€‚åœ¨æŸ¥è¯¢å¤±è´¥åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ”¹å˜é¢†åŸŸä¸“ä¸šæ€§ã€å®Œæ•´æ€§å’Œè¯­è¨€å‡†ç¡®æ€§æ¥æ‰°åŠ¨åŸå§‹æŸ¥è¯¢ã€‚åœ¨ä¸Šä¸‹æ–‡å¤±è´¥æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸Šä¼ é™çº§ã€æ— å…³å’Œç©ºæ–‡æ¡£çš„æƒ…å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07701",
            "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
            "url": "https://huggingface.co/papers/2502.07701",
            "abstract": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
            "score": 14,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "7212b752112bcd5a",
            "authors": [
                "Hongwei Yi",
                "Shitong Shao",
                "Tian Ye",
                "Jiantong Zhao",
                "Qingyu Yin",
                "Michael Lingelbach",
                "Li Yuan",
                "Yonghong Tian",
                "Enze Xie",
                "Daquan Zhou"
            ],
            "affiliations": [
                "Hedra Inc.",
                "Nvidia",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07701.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#inference",
                    "#multimodal",
                    "#open_source",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñ‹",
                    "desc": "Magic 1-For-1 (Magic141) - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€ÑĞ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 5-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° 3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ğ° Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ - Ğ·Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Efficient Video Generation: Simplifying with Magic141",
                    "desc": "The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼Œè½»æ¾å®ç°ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹Magic 1-For-1ï¼ˆMagic141ï¼‰ï¼Œè¯¥æ¨¡å‹ä¼˜åŒ–äº†å†…å­˜æ¶ˆè€—å’Œæ¨ç†å»¶è¿Ÿã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæ›´ç®€å•çš„ä»»åŠ¡ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–ç®—æ³•ï¼Œå›¾åƒåˆ°è§†é¢‘ä»»åŠ¡çš„æ”¶æ•›é€Ÿåº¦ç¡®å®ä¼˜äºæ–‡æœ¬åˆ°è§†é¢‘ä»»åŠ¡ã€‚é€šè¿‡å¤šç§ä¼˜åŒ–æŠ€å·§ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç‰‡æ®µï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07374",
            "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
            "url": "https://huggingface.co/papers/2502.07374",
            "abstract": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
            "score": 13,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4df9e17df3250cb4",
            "authors": [
                "Dacheng Li",
                "Shiyi Cao",
                "Tyler Griggs",
                "Shu Liu",
                "Xiangxi Mo",
                "Shishir G. Patil",
                "Matei Zaharia",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "affiliations": [
                "Department of Electrical Engineering and Computer Sciences, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07374.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Long CoT) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞºĞ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-32B-Instruct, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 17 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Long CoT, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Long CoT Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ»Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning: Structure Over Content in Large Models",
                    "desc": "This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples."
                },
                "zh": {
                    "title": "é•¿é“¾æ€ç»´ï¼šæ¨ç†æ¨¡å‹çš„å…³é”®ç»“æ„",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡é•¿é“¾æ€ç»´ï¼ˆLong CoTï¼‰è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œè¿™ç§æ€ç»´æ–¹å¼åŒ…æ‹¬åæ€ã€å›æº¯å’Œè‡ªæˆ‘éªŒè¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡æ•°æ®é«˜æ•ˆçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æœ‰æ•ˆå­¦ä¹ é•¿é“¾æ€ç»´ã€‚ä»…ä½¿ç”¨17,000ä¸ªé•¿é“¾æ€ç»´è®­ç»ƒæ ·æœ¬ï¼ŒQwen2.5-32B-Instructæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿é“¾æ€ç»´çš„ç»“æ„å¯¹å­¦ä¹ è¿‡ç¨‹è‡³å…³é‡è¦ï¼Œè€Œå•ä¸ªæ¨ç†æ­¥éª¤çš„å†…å®¹å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03492",
            "title": "Teaching Language Models to Critique via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.03492",
            "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.",
            "score": 12,
            "issue_id": 2165,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "a0c98706806837c6",
            "authors": [
                "Zhihui Xie",
                "Jie chen",
                "Liyu Chen",
                "Weichao Mao",
                "Jingjing Xu",
                "Lingpeng Kong"
            ],
            "affiliations": [
                "Bytedance, Seed",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03492.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ˜Ğ˜-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CTRL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¦ĞµĞ»ÑŒ - Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Code Generation with Self-Critiquing Models",
                    "desc": "This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks."
                },
                "zh": {
                    "title": "é€šè¿‡æ‰¹è¯„è®­ç»ƒæå‡ä»£ç ç”Ÿæˆæ€§èƒ½",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„æ‰¹è¯„å’Œæ”¹è¿›èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†CTRLæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ‰¹è¯„æ¨¡å‹ï¼Œç”Ÿæˆåé¦ˆä»¥æé«˜å›ºå®šç”Ÿæˆæ¨¡å‹çš„ä¿®æ­£æ€§èƒ½ï¼Œè€Œæ— éœ€äººå·¥ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CTRLè®­ç»ƒçš„æ‰¹è¯„æ¨¡å‹æ˜¾è‘—æé«˜äº†é€šè¿‡ç‡ï¼Œå¹¶å‡å°‘äº†ç´¯ç§¯é”™è¯¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ‰¹è¯„æ¨¡å‹ä½œä¸ºç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶é€šè¿‡è¿­ä»£æ‰¹è¯„-ä¿®è®¢å®ç°æ‰©å±•ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»£ç ç”ŸæˆåŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾106.1%çš„ç›¸å¯¹æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07617",
            "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
            "url": "https://huggingface.co/papers/2502.07617",
            "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.",
            "score": 12,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "503a9dac2cae323c",
            "authors": [
                "Xiao Wang",
                "Ibrahim Alabdulmohsin",
                "Daniel Salz",
                "Zhe Li",
                "Keran Rong",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07617.jpg",
            "data": {
                "categories": [
                    "#cultural_diversity",
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ·Ğ°Ğ¿Ğ°Ğ´Ğ½Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñƒ Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚ĞµÑ€ĞµĞ³Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking Cultural Diversity with 100 Billion Examples",
                    "desc": "This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡é¢„è®­ç»ƒåŠ©åŠ›æ–‡åŒ–å¤šæ ·æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å‰æ‰€æœªæœ‰çš„è§„æ¨¡ä¸Šï¼ˆ1000äº¿ä¸ªç¤ºä¾‹ï¼‰å¯¹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒçš„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è®¸å¤šå¸¸è§çš„è¥¿æ–¹åˆ†ç±»å’Œæ£€ç´¢åŸºå‡†ä¸Šï¼Œæ¨¡å‹æ€§èƒ½åœ¨æ­¤è§„æ¨¡ä¸‹è¶‹äºé¥±å’Œã€‚ç„¶è€Œï¼Œå¯¹äºæ–‡åŒ–å¤šæ ·æ€§çš„ä»»åŠ¡ï¼Œ1000äº¿è§„æ¨¡çš„ç½‘ç»œæ•°æ®å¸¦æ¥äº†æ›´æ˜¾è‘—çš„æå‡ï¼Œå› ä¸ºå®ƒæ¶µç›–äº†é•¿å°¾æ¦‚å¿µã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ˜¾ç¤ºåœ¨ä½èµ„æºè¯­è¨€ä¸Šä¹Ÿæœ‰æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05878",
            "title": "Retrieval-augmented Large Language Models for Financial Time Series Forecasting",
            "url": "https://huggingface.co/papers/2502.05878",
            "abstract": "Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.",
            "score": 11,
            "issue_id": 2172,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "bb6c947ce857a2db",
            "authors": [
                "Mengxi Xiao",
                "Zihao Jiang",
                "Lingfei Qian",
                "Zhengyu Chen",
                "Yueru He",
                "Yijing Xu",
                "Yuecheng Jiang",
                "Dong Li",
                "Ruey-Ling Weng",
                "Min Peng",
                "Jimin Huang",
                "Sophia Ananiadou",
                "Qianqian Xie"
            ],
            "affiliations": [
                "School of Computer Science, Wuhan University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05878.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#dataset",
                    "#rag"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "FinSeer: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ°ĞºÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ StockLLM, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¾Ñ‚ LLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° FinSeer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 8% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ BIGDATA22. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Stock Prediction with RAG Framework",
                    "desc": "This paper introduces a new approach to predicting stock movements by using a retrieval-augmented generation (RAG) framework specifically designed for financial time-series data. The framework employs a large language model called StockLLM, which has been fine-tuned to better understand financial contexts. It also features a unique candidate selection method that utilizes feedback from the language model to improve the relevance of retrieved data. The results show that this method significantly enhances prediction accuracy and uncovers important patterns in complex financial datasets, outperforming traditional retrieval techniques."
                },
                "zh": {
                    "title": "é‡‘èé¢„æµ‹çš„æ–°æ¡†æ¶ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé‡‘èæ—¶é—´åºåˆ—é¢„æµ‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤§é‡æ—¶é—´åºåˆ—æ•°æ®ä¸­è¯†åˆ«å’Œæå–å…³é”®å½±å“å› ç´ ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„1Bå‚æ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆStockLLMï¼‰ä½œä¸ºåŸºç¡€ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å€™é€‰é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨LLMåé¦ˆæ¥ä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ã€‚é€šè¿‡æœ€å¤§åŒ–æŸ¥è¯¢ä¸å†å²é‡è¦åºåˆ—ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬çš„æ£€ç´¢å™¨FinSeerèƒ½å¤Ÿåœ¨å¤æ‚çš„é‡‘èæ•°æ®ä¸­å‘ç°æœ‰æ„ä¹‰çš„æ¨¡å¼ï¼ŒåŒæ—¶å‡å°‘å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥RAGæ¡†æ¶åœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¼ºè°ƒäº†å®šåˆ¶æ£€ç´¢æ¨¡å‹åœ¨é‡‘èé¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07508",
            "title": "Enhance-A-Video: Better Generated Video for Free",
            "url": "https://huggingface.co/papers/2502.07508",
            "abstract": "DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.",
            "score": 9,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "e02d3082d3b21016",
            "authors": [
                "Yang Luo",
                "Xuanlei Zhao",
                "Mengzhao Chen",
                "Kaipeng Zhang",
                "Wenqi Shao",
                "Kai Wang",
                "Zhangyang Wang",
                "Yang You"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07508.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Enhance-A-Video: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DiT (Diffusion Transformer), Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Enhance-A-Video, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DiT Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing DiT Video Generation Without Retraining",
                    "desc": "This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Enhance-A-Video çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäº DiT çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿è´¯æ€§å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¢å¼ºè·¨å¸§ç›¸å…³æ€§ï¼Œåˆ©ç”¨éå¯¹è§’æ—¶é—´æ³¨æ„åŠ›åˆ†å¸ƒæ¥å®ç°ã€‚ç”±äºå…¶è®¾è®¡ç®€å•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è½»æ¾åº”ç”¨äºå¤§å¤šæ•°åŸºäº DiT çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07527",
            "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
            "url": "https://huggingface.co/papers/2502.07527",
            "abstract": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
            "score": 8,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "a6e947f52bde9a9c",
            "authors": [
                "Yingce Xia",
                "Peiran Jin",
                "Shufang Xie",
                "Liang He",
                "Chuan Cao",
                "Renqian Luo",
                "Guoqing Liu",
                "Yue Wang",
                "Zequn Liu",
                "Yuan-Jyue Chen",
                "Zekun Guo",
                "Yeqi Bai",
                "Pan Deng",
                "Yaosen Min",
                "Ziheng Lu",
                "Hongxia Hao",
                "Han Yang",
                "Jielan Li",
                "Chang Liu",
                "Jia Zhang",
                "Jianwei Zhu",
                "Kehan Wu",
                "Wei Zhang",
                "Kaiyuan Gao",
                "Qizhi Pei",
                "Qian Wang",
                "Xixian Liu",
                "Yanting Li",
                "Houtian Zhu",
                "Yeqing Lu",
                "Mingqian Ma",
                "Zun Wang",
                "Tian Xie",
                "Krzysztof Maziarz",
                "Marwin Segler",
                "Zhao Yang",
                "Zilong Chen",
                "Yu Shi",
                "Shuxin Zheng",
                "Lijun Wu",
                "Chen Hu",
                "Peggy Dai",
                "Tie-Yan Liu",
                "Haiguang Liu",
                "Tao Qin"
            ],
            "affiliations": [
                "Microsoft Research AI for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07527.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#architecture",
                    "#science",
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "NatureLM: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ NatureLM - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. Ğ­Ñ‚Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ±ĞµĞ»ĞºĞ¸, Ğ”ĞĞš Ğ¸ Ğ ĞĞš. NatureLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¾Ñ‚ 1 Ğ´Ğ¾ 46,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "NatureLM: Unifying Science Through Language Models",
                    "desc": "This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design."
                },
                "zh": {
                    "title": "è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼šç§‘å­¦å‘ç°çš„æ–°å·¥å…·",
                    "desc": "åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€çš„èƒ½åŠ›ã€‚å—åŸºç¡€æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜ä¸ºå„ä¸ªç§‘å­¦é¢†åŸŸå¼€å‘äº†ç›¸åº”çš„åŸºç¡€æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯å­¤ç«‹è®­ç»ƒçš„ï¼Œç¼ºä¹è·¨é¢†åŸŸæ•´åˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆNatureLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåºåˆ—çš„ç§‘å­¦åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ƒè¿›ç§‘å­¦å‘ç°ã€‚NatureLMç»è¿‡å¤šé¢†åŸŸæ•°æ®çš„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿæ”¯æŒå°åˆ†å­ã€è›‹ç™½è´¨ã€RNAå’Œææ–™çš„ç”Ÿæˆä¸ä¼˜åŒ–ï¼Œå¹¶åœ¨å¤šä¸ªç§‘å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06589",
            "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
            "url": "https://huggingface.co/papers/2502.06589",
            "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
            "score": 8,
            "issue_id": 2164,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "4273eabfdc59b328",
            "authors": [
                "Yuchen Zhuang",
                "Jingfeng Yang",
                "Haoming Jiang",
                "Xin Liu",
                "Kewei Cheng",
                "Sanket Lokegaonkar",
                "Yifan Gao",
                "Qing Ping",
                "Tianyi Liu",
                "Binxuan Huang",
                "Zheng Li",
                "Zhengyang Wang",
                "Pei Chen",
                "Ruijie Wang",
                "Rongzhi Zhang",
                "Nasser Zalmout",
                "Priyanka Nigam",
                "Bing Yin",
                "Chao Zhang"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06589.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞšÑƒĞ·Ğ½Ğ¸Ñ†Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Hephaestus-Forge - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞšĞ¾Ñ€Ğ¿ÑƒÑ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 103 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 76,537 API, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Hephaestus-Forge Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Hephaestus Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ LLM Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ LLM Ğ² Ñ‚Ñ€ĞµÑ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ€ĞµĞ´."
                },
                "en": {
                    "title": "Empowering LLM Agents with Hephaestus-Forge",
                    "desc": "This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability."
                },
                "zh": {
                    "title": "Hephaestus-Forgeï¼šæå‡LLMä»£ç†èƒ½åŠ›çš„åˆ›æ–°é¢„è®­ç»ƒè¯­æ–™åº“",
                    "desc": "ç”±äºç¼ºä¹é¢å‘ä»£ç†çš„é¢„è®­ç»ƒæ•°æ®ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»ä»£ç†é€šå¸¸ä¾èµ–å¤æ‚çš„æç¤ºæˆ–å¹¿æ³›çš„å¾®è°ƒï¼Œè¿™å¾€å¾€æ— æ³•åœ¨ä¿æŒå¼ºæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å¼•å…¥æ–°åŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†Hephaestus-Forgeï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæ—¨åœ¨å¢å¼ºLLMä»£ç†åœ¨APIåŠŸèƒ½è°ƒç”¨ã€å†…åœ¨æ¨ç†å’Œè§„åˆ’ä»¥åŠé€‚åº”ç¯å¢ƒåé¦ˆæ–¹é¢çš„åŸºæœ¬èƒ½åŠ›ã€‚Hephaestus-ForgeåŒ…å«1030äº¿ä¸ªç‰¹å®šäºä»£ç†çš„æ•°æ®ï¼Œæ¶µç›–76,537ä¸ªAPIï¼ŒåŒ…æ‹¬å·¥å…·æ–‡æ¡£ä»¥ä»‹ç»APIåŠŸèƒ½çš„çŸ¥è¯†å’ŒåŠŸèƒ½è°ƒç”¨è½¨è¿¹ä»¥å¢å¼ºå†…åœ¨æ¨ç†ã€‚é€šè¿‡åœ¨Hephaestus-Forgeä¸ŠæŒç»­é¢„è®­ç»ƒï¼ŒHephaestusåœ¨ä¸‰ä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å°åˆ°ä¸­å‹çš„å¼€æºLLMï¼Œå¹¶ä¸å•†ä¸šLLMç›¸åª²ç¾ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨å¢å¼ºä»£ç†åŸºæœ¬èƒ½åŠ›å’ŒLLMå¯¹æ–°ä»»åŠ¡æˆ–ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04223",
            "title": "Ã‰clair -- Extracting Content and Layout with Integrated Reading Order for Documents",
            "url": "https://huggingface.co/papers/2502.04223",
            "abstract": "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.",
            "score": 7,
            "issue_id": 2170,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "98e15ce7f5732b9f",
            "authors": [
                "Ilia Karmanov",
                "Amala Sanjay Deshmukh",
                "Lukas Voegtle",
                "Philipp Fischer",
                "Kateryna Chumachenko",
                "Timo Roman",
                "Jarno SeppÃ¤nen",
                "Jupinder Parmar",
                "Joseph Jennings",
                "Andrew Tao",
                "Karan Sapra"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04223.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#data",
                    "#dataset",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ã‰clair: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ OCR Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ã‰clair - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ã‰clair Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚, Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑĞ½Ğ¾ÑĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ OCR Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ã‰clair Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Eclair: Revolutionizing Document Understanding with Advanced OCR",
                    "desc": "This paper presents 'Eclair', an advanced Optical Character Recognition (OCR) tool designed to extract not just text, but also the structural and semantic elements of complex documents. It recognizes formatting, tables, and reading order, which are essential for understanding multi-page documents. 'Eclair' provides bounding boxes and semantic classes for extracted text, enhancing its utility for tasks like document retrieval and question answering. The tool demonstrates state-of-the-art performance on a newly introduced benchmark, showcasing its effectiveness compared to existing methods."
                },
                "zh": {
                    "title": "Eclairï¼šå…¨é¢ç†è§£æ–‡æ¡£çš„OCRå·¥å…·",
                    "desc": "å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯å¹¿æ³›åº”ç”¨äºä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬ï¼Œä¿ƒè¿›é«˜æ•ˆçš„æ•°å­—åŒ–å’Œæ•°æ®æ£€ç´¢ã€‚ç„¶è€Œï¼Œä»…ä»…æå–æ–‡æœ¬å¯¹äºå¤„ç†å¤æ‚æ–‡æ¡£æ˜¯ä¸å¤Ÿçš„ã€‚å…¨é¢ç†è§£è¿™äº›æ–‡æ¡£éœ€è¦äº†è§£å…¶ç»“æ„ï¼ŒåŒ…æ‹¬æ ¼å¼ã€å…¬å¼ã€è¡¨æ ¼ä»¥åŠè·¨å¤šä¸ªé¡µé¢çš„å¤šä¸ªå—å’Œåˆ—çš„é˜…è¯»é¡ºåºï¼Œä»¥åŠæ£€æµ‹è„šæ³¨å’Œå›¾åƒæ ‡é¢˜ç­‰å…ƒç´ çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†'Eclair'ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ–‡æœ¬æå–å·¥å…·ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†å„ç§æ–‡æ¡£ç±»å‹ï¼Œå¹¶åœ¨æ–‡æ¡£çº§OCRå’Œè¯­ä¹‰åˆ†ç±»çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03997",
            "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
            "url": "https://huggingface.co/papers/2502.03997",
            "abstract": "Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.",
            "score": 7,
            "issue_id": 2165,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "69bfc4de9cf7106d",
            "authors": [
                "Yu Yuan",
                "Shizhao Sun",
                "Qi Liu",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03997.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "CAD-Editor: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "CAD-Editor - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'locate-then-infill', Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ… Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²ĞºĞ°Ğ¼Ğ¸. CAD-Editor Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ CAD, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing CAD Editing with Text Instructions",
                    "desc": "This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "æ–‡æœ¬é©±åŠ¨çš„CADç¼–è¾‘æ–°çºªå…ƒ",
                    "desc": "è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å„ä¸ªè¡Œä¸šä¸­è‡³å…³é‡è¦ã€‚åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘å¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤è‡ªåŠ¨ä¿®æ”¹CADæ¨¡å‹ï¼Œä½†è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å˜ä½“ç”Ÿæˆæˆ–åŸºäºæ–‡æœ¬çš„CADç”Ÿæˆï¼Œç¼ºä¹å¯¹æ–‡æœ¬æ§åˆ¶çš„æ”¯æŒæˆ–å¿½è§†äº†ç°æœ‰CADæ¨¡å‹çš„çº¦æŸã€‚æˆ‘ä»¬æå‡ºäº†CAD-Editorï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºåŸºäºæ–‡æœ¬çš„CADç¼–è¾‘çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ï¼Œå®ç°äº†é«˜æ•ˆçš„ç¼–è¾‘æŒ‡ä»¤ç”Ÿæˆå’Œæ¨¡å‹ä¿®æ”¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06428",
            "title": "CoS: Chain-of-Shot Prompting for Long Video Understanding",
            "url": "https://huggingface.co/papers/2502.06428",
            "abstract": "Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.",
            "score": 4,
            "issue_id": 2173,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "8302e2d276dc5929",
            "authors": [
                "Jian Hu",
                "Zixu Cheng",
                "Chenyang Si",
                "Wei Li",
                "Shaogang Gong"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Queen Mary University of London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06428.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Chain-of-Shot prompting (CoS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). CoS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Optimizing Video Understanding with Chain-of-Shot Prompting",
                    "desc": "This paper addresses the challenge that Multi-modal Large Language Models (MLLMs) face when processing long videos, which often contain too many visual tokens. These tokens can overwhelm the model with irrelevant information, making it difficult to understand the video's content. The authors propose a method called Chain-of-Shot prompting (CoS) that optimizes shot selection based on the specific task at hand, improving the alignment between selected shots and the semantic understanding required. CoS includes a binary video summary mechanism and a video co-reasoning module to enhance the model's focus on relevant shots, leading to better video comprehension."
                },
                "zh": {
                    "title": "ä¼˜åŒ–é•œå¤´é€‰æ‹©ï¼Œæå‡è§†é¢‘ç†è§£",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦è¿‡å¤šçš„è§†è§‰æ ‡è®°ã€‚è¿™äº›æ ‡è®°è¶…å‡ºäº†MLLMsçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¯¼è‡´å¡«å……äº†å¤§é‡ä¸ä»»åŠ¡æ— å…³çš„é•œå¤´ã€‚å¦‚ä½•é€‰æ‹©é•œå¤´æ˜¯ä¸€ä¸ªæœªè§£å†³çš„å…³é”®é—®é¢˜ï¼šç¨€ç–é‡‡æ ·å¯èƒ½ä¼šé”™è¿‡å…³é”®ç»†èŠ‚ï¼Œè€Œå…¨é¢é‡‡æ ·åˆ™ä¼šä½¿æ¨¡å‹è¢«æ— å…³å†…å®¹æ·¹æ²¡ï¼Œä»è€Œå¯¼è‡´è§†é¢‘ç†è§£é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é•œå¤´é“¾æç¤ºï¼ˆCoSï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–é•œå¤´ä¸ä»»åŠ¡çš„å¯¹é½æ¥é€‰æ‹©é€‚åˆè§†é¢‘ç†è§£çš„é•œå¤´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07490",
            "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
            "url": "https://huggingface.co/papers/2502.07490",
            "abstract": "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.",
            "score": 3,
            "issue_id": 2172,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "2d86c4124095af30",
            "authors": [
                "Xialie Zhuang",
                "Zhikai Jia",
                "Jianjin Li",
                "Zhenyu Zhang",
                "Li Shen",
                "Zheng Cao",
                "Shiwei Liu"
            ],
            "affiliations": [
                "SCITIX (SGP) TECH PTE. LTD., Singapore",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, China",
                "South China Normal University, China",
                "Sun YatSen University, China",
                "University of Oxford, UK",
                "University of Texas at Austin, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07490.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#long_context",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MEAP: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MEAP (Mask-Enhanced Autoregressive Prediction). MEAP Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (MLM) Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (NTP) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MEAP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ NTP Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Information Retrieval in LLMs with MEAP",
                    "desc": "This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a new training method for Large Language Models (LLMs) that improves their ability to retrieve important information. MEAP combines Masked Language Modeling (MLM) with Next-Token Prediction (NTP) by masking some input tokens and then predicting the next token using a decoder-only Transformer. This approach avoids the complexity of bidirectional attention and encoder-decoder structures, making it computationally efficient during training and inference. Experimental results show that MEAP significantly enhances performance in information retrieval and reasoning tasks, especially in scenarios where context is crucial, while also benefiting supervised fine-tuning."
                },
                "zh": {
                    "title": "æ©ç å¢å¼ºè‡ªå›å½’é¢„æµ‹ï¼šæå‡è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‡†ç¡®æ£€ç´¢å…³é”®ä¿¡æ¯æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ©ç å¢å¼ºè‡ªå›å½’é¢„æµ‹ï¼ˆMEAPï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œå®ƒå°†æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ä¸ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰æ— ç¼ç»“åˆï¼Œä»¥å¢å¼ºåè€…çš„ä¸Šä¸‹æ–‡æ£€ç´¢èƒ½åŠ›ã€‚MEAPé€šè¿‡éšæœºæ©ç›–è¾“å…¥æ ‡è®°çš„ä¸€å°éƒ¨åˆ†ï¼Œç„¶åä½¿ç”¨ä»…è§£ç å™¨çš„Transformerç›´æ¥è¿›è¡Œæ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMEAPåœ¨å…³é”®ä¿¡æ¯æ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºNTPï¼ŒåŒæ—¶åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07445",
            "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
            "url": "https://huggingface.co/papers/2502.07445",
            "abstract": "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.",
            "score": 3,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "3e6282dd3913750a",
            "authors": [
                "Nurit Cohen-Inger",
                "Yehonatan Elisha",
                "Bracha Shapira",
                "Lior Rokach",
                "Seffi Cohen"
            ],
            "affiliations": [
                "Ben Gurion University",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07445.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ¾Ğ±Ğ»Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ C-BOD. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¯Ğœ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ ÑƒĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ°Ñ…."
                },
                "en": {
                    "title": "Beyond Scores: Evaluating True Language Understanding in LLMs",
                    "desc": "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."
                },
                "zh": {
                    "title": "è¶…è¶Šåˆ†æ•°ï¼Œå…³æ³¨æ¨¡å‹çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†è¿™äº›é«˜åˆ†å¯èƒ½æ©ç›–äº†æ¨¡å‹å¯¹ç‰¹å®šæ•°æ®é›†è¡¨é¢ç‰¹å¾çš„è¿‡åº¦ä¾èµ–ï¼Œè€ŒéçœŸæ­£çš„è¯­è¨€ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†å˜è‰²é¾™åŸºå‡†è¿‡æ‹Ÿåˆæ£€æµ‹å™¨ï¼ˆC-BODï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å‚æ•°å˜æ¢ç³»ç»Ÿæ€§æ‰­æ›²åŸºå‡†æç¤ºçš„å…ƒè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹LLMsçš„è¿‡æ‹Ÿåˆã€‚C-BODé€šè¿‡é‡æ–°è¡¨è¿°è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒå…¶è¯­ä¹‰å†…å®¹å’Œæ ‡ç­¾ï¼Œæ­ç¤ºæ¨¡å‹æ€§èƒ½æ˜¯å¦å—åˆ°è®°å¿†æ¨¡å¼çš„é©±åŠ¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡é€‚åº¦æ‰°åŠ¨åï¼Œ26ä¸ªé¢†å…ˆçš„LLMåœ¨MMLUåŸºå‡†ä¸Šçš„å¹³å‡æ€§èƒ½ä¸‹é™äº†2.15%ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨è¯„ä¼°æ—¶éœ€è¦å…³æ³¨æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07531",
            "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.07531",
            "abstract": "Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.",
            "score": 3,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "dea5fd89dd98f3b1",
            "authors": [
                "Sixiao Zheng",
                "Zimian Peng",
                "Yanpeng Zhou",
                "Yi Zhu",
                "Hang Xu",
                "Xiangru Huang",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University, China",
                "Huawei Noahs Ark Lab, China",
                "Westlake University, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07531.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "VidCRAFT3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Spatial Triple-Attention Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VideoLightingDirection Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ÑÑŒ Ğ±ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation",
                    "desc": "This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods."
                },
                "zh": {
                    "title": "VidCRAFT3ï¼šå¤šå…ƒç´ æ§åˆ¶çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶VidCRAFT3ï¼Œèƒ½å¤ŸåŒæ—¶æ§åˆ¶ç›¸æœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨å’Œå…‰ç…§æ–¹å‘ã€‚ä¸ºäº†æ›´å¥½åœ°è§£è€¦æ¯ä¸ªè§†è§‰å…ƒç´ çš„æ§åˆ¶ï¼Œæå‡ºäº†ç©ºé—´ä¸‰é‡æ³¨æ„åŠ›å˜æ¢å™¨ï¼Œèƒ½å¤Ÿå¯¹å…‰ç…§æ–¹å‘ã€æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œå¯¹ç§°æ•´åˆã€‚ç”±äºå¤§å¤šæ•°çœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®é›†ç¼ºä¹å…‰ç…§æ³¨é‡Šï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åˆæˆè§†é¢‘æ•°æ®é›†VideoLightingDirectionï¼ˆVLDï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«å…‰ç…§æ–¹å‘æ³¨é‡Šå’Œå¤šæ ·åŒ–å¤–è§‚çš„ç‰©ä½“ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒVidCRAFT3åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘å†…å®¹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05364",
            "title": "Hypencoder: Hypernetworks for Information Retrieval",
            "url": "https://huggingface.co/papers/2502.05364",
            "abstract": "The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.",
            "score": 2,
            "issue_id": 2176,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "c3495b093acff010",
            "authors": [
                "Julian Killingback",
                "Hansi Zeng",
                "Hamed Zamani"
            ],
            "affiliations": [
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05364.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Hypencoder: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Hypencoder. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‰ÑƒÑ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ° ÑĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ ÑĞºĞ°Ğ»ÑÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hypencoder Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing Retrieval with Hypencoder: A Neural Network Approach",
                    "desc": "This paper introduces a novel approach to document retrieval by replacing traditional vector representations with a small neural network that functions as a learned relevance function. Instead of using vector inner products to score relevance, the proposed Hypencoder generates a scalar relevance score based on document representations. The use of a hypernetwork allows for efficient weight generation for the Hypencoder, leading to superior performance on various retrieval tasks compared to existing dense retrieval and reranking models. Additionally, the Hypencoder demonstrates strong generalization capabilities and can efficiently search large document collections in a short time frame."
                },
                "zh": {
                    "title": "Hypencoderï¼šè¶…è¶Šä¼ ç»Ÿæ£€ç´¢æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å¤šæ•°æ£€ç´¢æ¨¡å‹ä¾èµ–äºå‘é‡å†…ç§¯æ¥ç”ŸæˆæŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„ç›¸å…³æ€§è¯„åˆ†ï¼Œè¿™é™åˆ¶äº†ç›¸å…³æ€§è¯„åˆ†çš„è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ï¼Œä½¿ç”¨å°å‹ç¥ç»ç½‘ç»œä½œä¸ºå­¦ä¹ çš„ç›¸å…³æ€§å‡½æ•°ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå‘é‡æ¥è¡¨ç¤ºæŸ¥è¯¢ã€‚è¿™ä¸ªå°å‹ç¥ç»ç½‘ç»œæ¥æ”¶æ–‡æ¡£çš„è¡¨ç¤ºï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡çš„ç›¸å…³æ€§è¯„åˆ†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒHypencoderåœ¨é¢†åŸŸå†…æ£€ç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä¸€äº›å›°éš¾çš„æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06755",
            "title": "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models",
            "url": "https://huggingface.co/papers/2502.06755",
            "abstract": "To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.",
            "score": 2,
            "issue_id": 2175,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "cf5a833e93ca6f88",
            "authors": [
                "Samuel Stevens",
                "Wei-Lun Chao",
                "Tanya Berger-Wolf",
                "Yu Su"
            ],
            "affiliations": [
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06755.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#interpretability"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹: ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (SAE) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ÑÑ…, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ²Ğ¾ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging Interpretation and Control in Vision Models with Sparse Autoencoders",
                    "desc": "This paper introduces a new framework that uses sparse autoencoders (SAEs) to interpret and manipulate visual features in vision models. It addresses the challenge of validating the causal influence of these features through controlled experiments. By applying this framework, the authors uncover significant differences in the semantic abstractions learned by various models based on their pre-training objectives. The framework allows for reliable identification and manipulation of interpretable features without needing to retrain the models, enhancing our understanding of their behavior."
                },
                "zh": {
                    "title": "ç”¨ç¨€ç–è‡ªç¼–ç å™¨ç†è§£å’Œæ“æ§è§†è§‰æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥ç†è§£è§†è§‰æ¨¡å‹çš„ç‰¹å¾ã€‚è¯¥æ¡†æ¶ä¸ä»…å¯ä»¥å‘ç°äººç±»å¯è§£é‡Šçš„è§†è§‰ç‰¹å¾ï¼Œè¿˜èƒ½ç²¾ç¡®æ“æ§è¿™äº›ç‰¹å¾ï¼Œä»¥æµ‹è¯•æ¨¡å‹è¡Œä¸ºçš„å‡è®¾ã€‚é€šè¿‡å¯¹å…ˆè¿›è§†è§‰æ¨¡å‹çš„åº”ç”¨ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸åŒé¢„è®­ç»ƒç›®æ ‡æ¨¡å‹æ‰€å­¦ä¹ çš„è¯­ä¹‰æŠ½è±¡ä¹‹é—´çš„å…³é”®å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒSAEèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯é åœ°è¯†åˆ«å’Œæ“æ§å¯è§£é‡Šçš„è§†è§‰ç‰¹å¾ï¼Œä¸ºç†è§£å’Œæ§åˆ¶è§†è§‰æ¨¡å‹è¡Œä¸ºæä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06857",
            "title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws",
            "url": "https://huggingface.co/papers/2502.06857",
            "abstract": "Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws",
            "score": 2,
            "issue_id": 2172,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "8f1b246a774832da",
            "authors": [
                "Sean McLeish",
                "John Kirchenbauer",
                "David Yu Miller",
                "Siddharth Singh",
                "Abhinav Bhatele",
                "Micah Goldblum",
                "Ashwinee Panda",
                "Tom Goldstein"
            ],
            "affiliations": [
                "Columbia University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06857.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ’",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Gemstones, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 4000 ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞµÑ‚ÑŒ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°ĞºĞ¾Ğ½, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Scaling Laws with Gemstones Dataset",
                    "desc": "This paper explores scaling laws in machine learning by analyzing a diverse set of models with various hyper-parameters. The authors introduce the Gemstones dataset, which includes over 4000 transformer model checkpoints, allowing for extensive experimentation on scaling effects. They demonstrate that the performance of language models can be predicted based on their architecture, specifically width and depth. The findings reveal that scaling law prescriptions are sensitive to the design of experiments and the specific models used, emphasizing the importance of comprehensive datasets in understanding scaling behavior."
                },
                "zh": {
                    "title": "æ¢ç´¢ç¼©æ”¾æ³•åˆ™çš„å¤šæ ·æ€§ä¸æ•æ„Ÿæ€§",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†ç¼©æ”¾æ³•åˆ™ï¼Œä½¿ç”¨äº†å¤šç§æ¶æ„å’Œè¶…å‚æ•°é€‰æ‹©ï¼Œå¼ºè°ƒäº†è¿™äº›é€‰æ‹©å¯¹ç»“æœçš„å½±å“ã€‚æˆ‘ä»¬å‘å¸ƒäº†Gemstonesæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„å¼€æºç¼©æ”¾æ³•åˆ™æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡4000ä¸ªæ¥è‡ªå˜æ¢å™¨æ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œå‚æ•°é‡é«˜è¾¾20äº¿ã€‚é€šè¿‡è¿™äº›æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œæ›´å¤æ‚çš„ç¼©æ”¾ç ”ç©¶ï¼Œä¾‹å¦‚é¢„æµ‹è¯­è¨€å»ºæ¨¡æ€§èƒ½ä¸æ¨¡å‹å®½åº¦å’Œæ·±åº¦çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œç¼©æ”¾æ³•åˆ™çš„é€‚ç”¨æ€§å¯¹å®éªŒè®¾è®¡è¿‡ç¨‹å’Œä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹æ£€æŸ¥ç‚¹éå¸¸æ•æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05932",
            "title": "Skill Expansion and Composition in Parameter Space",
            "url": "https://huggingface.co/papers/2502.05932",
            "abstract": "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.",
            "score": 2,
            "issue_id": 2170,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "d4dd3a87e6ed9712",
            "authors": [
                "Tenglong Liu",
                "Jianxiong Li",
                "Yinan Zheng",
                "Haoyi Niu",
                "Yixing Lan",
                "Xin Xu",
                "Xianyuan Zhan"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "National University of Defense Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05932.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Parametric Skill Expansion and Composition (PSEC) Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². PSEC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Low-Rank Adaptation (LoRA) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ LoRA, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸. PSEC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Autonomous Agents with Efficient Skill Evolution",
                    "desc": "This paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enhances the ability of autonomous agents to learn new skills by building on existing knowledge. PSEC maintains a skill library that allows for efficient integration of skill primitives using Low-Rank Adaptation (LoRA) modules, which support parameter-efficient finetuning. The framework also enables the merging of these modules to create new skills by leveraging shared information, promoting flexibility in skill development. Experimental results demonstrate that PSEC significantly improves the agents' performance in adapting to new challenges while expanding their skill sets effectively."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“æŠ€èƒ½çš„é«˜æ•ˆæ‰©å±•ä¸ç»„åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå‚æ•°åŒ–æŠ€èƒ½æ‰©å±•ä¸ç»„åˆï¼ˆPSECï¼‰ï¼Œæ—¨åœ¨æé«˜è‡ªä¸»æ™ºèƒ½ä½“åœ¨é¢å¯¹æ–°æŒ‘æˆ˜æ—¶çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤ä¸€ä¸ªå¯ç®¡ç†çš„æŠ€èƒ½åº“ï¼Œé€æ­¥æ•´åˆæŠ€èƒ½åŸè¯­ï¼Œæ”¯æŒé«˜æ•ˆçš„å‚æ•°å¾®è°ƒï¼Œä»è€Œå®ç°çµæ´»çš„æŠ€èƒ½æ‰©å±•ã€‚PSECè¿˜å…è®¸åœ¨å‚æ•°ç©ºé—´ä¸­ç›´æ¥ç»„åˆæŠ€èƒ½ï¼Œé€šè¿‡åˆå¹¶ä¸åŒæŠ€èƒ½çš„LoRAæ¨¡å—ï¼Œåˆ©ç”¨å…±äº«ä¿¡æ¯æœ‰æ•ˆç¼–ç¨‹æ–°æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSECåœ¨åˆ©ç”¨å…ˆå‰çŸ¥è¯†åº”å¯¹æ–°æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½å¤Ÿæ‰©å±•å…¶æŠ€èƒ½åº“ä»¥è¿›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04465",
            "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
            "url": "https://huggingface.co/papers/2502.04465",
            "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.",
            "score": 2,
            "issue_id": 2167,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a6ebe3d69cd8bcc2",
            "authors": [
                "Luca Della Libera",
                "Francesco Paissan",
                "Cem Subakan",
                "Mirco Ravanelli"
            ],
            "affiliations": [
                "Concordia University, Montreal, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04465.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "FocalCodec: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ FocalCodec - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ´ĞµĞº Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ„Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ±ÑƒĞº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ¾ 0.16-0.65 ĞºĞ±Ğ¸Ñ‚/Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡ĞµĞ¼ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². FocalCodec Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞšĞ¾Ğ´ĞµĞº Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡ÑŒÑ Ğ¸ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FocalCodec: Efficient Speech Compression with Single Binary Codebook",
                    "desc": "This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing."
                },
                "zh": {
                    "title": "FocalCodecï¼šé«˜æ•ˆä½æ¯”ç‰¹ç‡è¯­éŸ³ç¼–è§£ç å™¨",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åœ¨æµ·é‡æ•°æ®é›†ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€‚å—åˆ°è¿™ä¸€æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜å°è¯•å°†è¿™äº›æ–¹æ³•åº”ç”¨äºè¯­éŸ³å¤„ç†ï¼Œé€šè¿‡ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨å°†è¿ç»­éŸ³é¢‘ç¦»æ•£åŒ–ä¸ºæ ‡è®°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´é«˜æ¯”ç‰¹ç‡ã€è¯­ä¹‰æˆ–å£°å­¦ä¿¡æ¯ä¸¢å¤±ä»¥åŠå¤šä»£ç æœ¬è®¾è®¡çš„å¤æ‚æ€§ç­‰é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FocalCodecï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„é«˜æ•ˆä½æ¯”ç‰¹ç‡ç¼–è§£ç å™¨ï¼Œèƒ½å¤Ÿåœ¨0.16åˆ°0.65 kbpsä¹‹é—´å‹ç¼©è¯­éŸ³ï¼ŒåŒæ—¶åœ¨è¯­éŸ³é‡åˆæˆå’Œè¯­éŸ³è½¬æ¢ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07776",
            "title": "Auditing Prompt Caching in Language Model APIs",
            "url": "https://huggingface.co/papers/2502.07776",
            "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.",
            "score": 2,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "48f7472ef1c86b27",
            "authors": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Rohith Kuditipudi",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07776.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#leakage",
                    "#inference",
                    "#ethics",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² LLM: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑÑ‚Ğ¸Ğ¼ Ñ€Ğ¸ÑĞºĞ¸ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… API-Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² LLM. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ñƒ ÑĞµĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OpenAI, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑƒÑ‚ĞµÑ‡ĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ OpenAI ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Timing Variations: A Privacy Risk in Prompt Caching for LLMs",
                    "desc": "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."
                },
                "zh": {
                    "title": "æç¤ºç¼“å­˜çš„éšç§é£é™©ä¸é€æ˜æ€§",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæç¤ºç¼“å­˜ä¼šå¯¼è‡´æ•°æ®ä¾èµ–çš„æ—¶é—´å˜åŒ–ï¼šç¼“å­˜çš„æç¤ºå¤„ç†é€Ÿåº¦æ¯”éç¼“å­˜çš„æç¤ºå¿«ã€‚è¿™äº›æ—¶é—´å·®å¼‚å¯èƒ½å¼•å‘ä¾§ä¿¡é“æ”»å‡»çš„é£é™©ï¼Œä¾‹å¦‚ï¼Œå¦‚æœç¼“å­˜è¢«å¤šä¸ªç”¨æˆ·å…±äº«ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡å¿«é€Ÿçš„APIå“åº”æ—¶é—´è¯†åˆ«å‡ºç¼“å­˜çš„æç¤ºï¼Œä»è€Œè·å–å…¶ä»–ç”¨æˆ·æç¤ºçš„ä¿¡æ¯ã€‚ç”±äºæç¤ºç¼“å­˜å¯èƒ½å¯¼è‡´éšç§æ³„éœ²ï¼Œå› æ­¤APIæä¾›å•†çš„ç¼“å­˜æ”¿ç­–é€æ˜åº¦éå¸¸é‡è¦ã€‚æˆ‘ä»¬å¼€å‘å¹¶è¿›è¡Œç»Ÿè®¡å®¡è®¡ï¼Œä»¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­LLM APIæä¾›å•†çš„æç¤ºç¼“å­˜æƒ…å†µï¼Œå‘ç°ä¸ƒä¸ªAPIæä¾›å•†ï¼ˆåŒ…æ‹¬OpenAIï¼‰ä¹‹é—´å­˜åœ¨å…¨çƒç¼“å­˜å…±äº«ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·æç¤ºçš„éšç§æ³„éœ²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07640",
            "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2502.07640",
            "abstract": "We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.",
            "score": 0,
            "issue_id": 2175,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "f0efbd784e8053e1",
            "authors": [
                "Yong Lin",
                "Shange Tang",
                "Bohan Lyu",
                "Jiayun Wu",
                "Hongzhou Lin",
                "Kaiyu Yang",
                "Jia Li",
                "Mengzhou Xia",
                "Danqi Chen",
                "Sanjeev Arora",
                "Chi Jin"
            ],
            "affiliations": [
                "Amazon",
                "Meta FAIR",
                "Numina",
                "Princeton Language and Intelligence, Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07640.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#data",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Goedel-Prover - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ - Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ· 1,64 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Goedel-Prover Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 57,6% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ miniF2F."
                },
                "en": {
                    "title": "Revolutionizing Formal Proof Generation with Goedel-Prover",
                    "desc": "Goedel-Prover is an advanced open-source large language model designed for generating formal proofs in mathematics. It addresses the challenge of limited formalized math statements by creating a dataset of 1.64 million formal statements from natural language problems. The model employs iterative training of provers, where each new prover builds on the successes of its predecessors, leading to improved proof generation capabilities. As a result, Goedel-Prover achieves state-of-the-art performance, surpassing previous models in both proof generation success rates and the volume of formal proofs produced."
                },
                "zh": {
                    "title": "Goedel-Proverï¼šæ•°å­¦è¯æ˜ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Goedel-Prover æ˜¯ä¸€ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºè‡ªåŠ¨åŒ–å½¢å¼è¯æ˜ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒè¯­å¥å½¢å¼åŒ–å™¨ï¼Œå°†è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜è½¬æ¢ä¸ºæ­£å¼è¯­è¨€ï¼ˆLean 4ï¼‰ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«164ä¸‡æ¡æ­£å¼è¯­å¥çš„æ•°æ®é›†ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥éªŒè¯è¿™äº›æ­£å¼è¯­å¥æ˜¯å¦å‡†ç¡®ä¿ç•™äº†åŸå§‹è‡ªç„¶è¯­è¨€é—®é¢˜çš„å†…å®¹ã€‚æœ€ç»ˆçš„è¯æ˜è€…åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-11.html",
    "link_next": "2025-02-13.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 6,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 6,
        "#healthcare": 2,
        "#training": 14,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 4,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 15,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1,
        "#cultural_diversity": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤æ‚ç¼–ç¨‹å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ä¸ªé€šç”¨æ¨ç†æ¨¡å‹ - OpenAI o1 å’Œ o3 çš„æ—©æœŸç‰ˆæœ¬ï¼Œä»¥åŠä¸€ä¸ªé’ˆå¯¹2024å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIOIï¼‰è®¾è®¡çš„ç‰¹å®šé¢†åŸŸç³»ç»Ÿ o1-ioiã€‚æˆ‘ä»¬åœ¨IOI 2024ä¸Šä½¿ç”¨ o1-ioi å‚èµ›ï¼Œå¹¶åœ¨æ”¾å®½çš„æ¯”èµ›çº¦æŸä¸‹è·å¾—äº†é‡‘ç‰Œã€‚ç„¶è€Œï¼Œåç»­æ¨¡å‹å¦‚ o3 åœ¨æ²¡æœ‰æ‰‹å·¥åˆ¶å®šçš„ç‰¹å®šé¢†åŸŸç­–ç•¥æˆ–æ”¾å®½çº¦æŸçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è·å¾—é‡‘ç‰Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè™½ç„¶ä¸“é—¨çš„æµæ°´çº¿å¦‚ o1-ioi å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†æ‰©å±•çš„é€šç”¨ o3 æ¨¡å‹åœ¨ä¸ä¾èµ–æ‰‹å·¥åˆ¶å®šçš„æ¨ç†å¯å‘å¼çš„æƒ…å†µä¸‹è¶…è¶Šäº†è¿™äº›ç»“æœã€‚",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "WÇ’men zhÇnshÃ¬le qiÃ¡ng huÃ  xuÃ©xÃ­ yÃ¬ngyÃ²ng yÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) kÄ›yÇ xiÇnzhÃ¹ tÃ­shÄ“ng fÃ¹zÃ¡ bÇan chÃ©ng yÇ” tuÄ«lÇ rÃ¨nwÃ¹ de biÇoxiÃ n. WÇ’men bÇjiÃ ole liÇng gÃ¨ tÅngyÃ²ng tuÄ«lÇ mÃ³xÃ­ng - OpenAI o1 hÃ© o3 de zÇoqÄ« bÇnbÄ›n, yÇjiÃ  yÄ«gÃ¨ zhÇduÃ¬ 2024 niÃ¡n guÃ³jÃ¬ xÃ¬nxÄ« xuÃ© Ã olÃ­npÇkÃ¨ (IOI) shÃ¨jÃ¬ de tÃ¨dÃ¬ng yÃ¹yÃ­ xÃ¬tÇ’ng o1-ioi. WÇ’men zÃ i IOI 2024 shÃ ng shÇyÃ²ng o1-ioi cÄnsÃ i, bÃ¬ng zÃ i fÃ ngkuÄn de bÇsÃ i yuÄ“shÃ¹ xiÃ  huÃ²dÃ©le jÄ«npÃ¡i. RÃ¡n'Ã©r, hÃ²uxÃ¹ mÃ³xÃ­ng rÃº o3 zÃ i mÃ©iyÇ’u shÇ’ugÅng zhÃ¬dÃ¬ng de tÃ¨dÃ¬ng yÃ¹yÃ­ cÃ¨lÃ¼Ã¨ huÃ² fÃ ngkuÄn yuÄ“shÃ¹ de qÃ­ngkuÃ ng xiÃ  yÄ› nÃ©ng huÃ²dÃ© jÄ«npÃ¡i. WÇ’men de fÄxiÃ n biÇomÃ­ng, suÄ«rÃ¡n zhuÄnmÃ©n de liÃºshuÇxiÃ n rÃº o1-ioi dÃ ilÃ¡i le xiÇnzhÃ¹ de gÇijÃ¬n, dÃ n kuÃ²zhÇn de tÅngyÃ²ng o3 mÃ³xÃ­ng zÃ i bÃ¹ yÄ«lÃ i shÇ’ugÅng zhÃ¬dÃ¬ng de tuÄ«lÇ qÇfÇshÃ¬ de qÃ­ngkuÃ ng xiÃ  chÄoyuÃ¨le zhÃ¨xiÄ“ jiÃ©guÇ’.",
        "vocab": "[{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'display'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ngâ€‹huÃ â€‹xuÃ©â€‹xÃ­', 'trans': 'reinforcement learning'}, {'word': 'åº”ç”¨äº', 'pinyin': 'yÃ¬ngâ€‹yÃ²ngâ€‹yÃº', 'trans': 'apply to'}, {'word': 'å¤§å‹è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ â€‹xÃ­ngâ€‹yÇ”â€‹yÃ¡nâ€‹mÃ³â€‹xÃ­ng', 'trans': 'large language model'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnâ€‹zhÃ¹', 'trans': 'significant'}, {'word': 'æå‡', 'pinyin': 'tÃ­â€‹shÄ“ng', 'trans': 'improve'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹â€‹zÃ¡', 'trans': 'complex'}, {'word': 'ç¼–ç¨‹', 'pinyin': 'biÄnâ€‹chÃ©ng', 'trans': 'programming'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«â€‹lÇ', 'trans': 'reasoning'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoâ€‹xiÃ n', 'trans': 'performance'}, {'word': 'æ¯”è¾ƒ', 'pinyin': 'bÇâ€‹jiÃ o', 'trans': 'compare'}, {'word': 'é€šç”¨', 'pinyin': 'tÅngâ€‹yÃ²ng', 'trans': 'general-purpose'}, {'word': 'é’ˆå¯¹', 'pinyin': 'zhÄ“nâ€‹duÃ¬', 'trans': 'target'}, {'word': 'ç‰¹å®šé¢†åŸŸ', 'pinyin': 'tÃ¨â€‹dÃ¬ngâ€‹lÇngâ€‹yÃ¹', 'trans': 'specific domain'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬â€‹tÇ’ng', 'trans': 'system'}, {'word': 'å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹', 'pinyin': 'guÃ³â€‹jÃ¬â€‹xÃ¬nâ€‹xÄ«â€‹xuÃ©â€‹Ã oâ€‹lÃ­nâ€‹pÇâ€‹kÃ¨', 'trans': 'International Olympiad in Informatics'}, {'word': 'è®¾è®¡', 'pinyin': 'shÃ¨â€‹jÃ¬', 'trans': 'design'}, {'word': 'å‚èµ›', 'pinyin': 'cÄnâ€‹sÃ i', 'trans': 'compete'}, {'word': 'æ”¾å®½', 'pinyin': 'fÃ ngâ€‹kuÄn', 'trans': 'relax'}, {'word': 'çº¦æŸ', 'pinyin': 'yuÄ“â€‹shÃ¹', 'trans': 'constraint'}, {'word': 'è·å¾—', 'pinyin': 'huÃ²â€‹dÃ©', 'trans': 'obtain'}, {'word': 'é‡‘ç‰Œ', 'pinyin': 'jÄ«nâ€‹pÃ¡i', 'trans': 'gold medal'}, {'word': 'åç»­', 'pinyin': 'hÃ²uâ€‹xÃ¹', 'trans': 'subsequent'}, {'word': 'æ‰‹å·¥åˆ¶å®š', 'pinyin': 'shÇ’uâ€‹gÅngâ€‹zhÃ¬â€‹dÃ¬ng', 'trans': 'manually specified'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨â€‹lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'å¯å‘å¼', 'pinyin': 'qÇâ€‹fÄâ€‹shÃ¬', 'trans': 'heuristic'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ«â€‹lÃ i', 'trans': 'rely on'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²â€‹zhÇn', 'trans': 'extend'}, {'word': 'è¶…è¶Š', 'pinyin': 'chÄoâ€‹yuÃ¨', 'trans': 'surpass'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©â€‹guÇ’', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning modelsâ€”early versions of OpenAI o1 and o3â€”and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}