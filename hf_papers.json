{
    "date": {
        "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
        "en": "February 10",
        "zh": "2Êúà10Êó•"
    },
    "time_utc": "2025-02-10 05:11",
    "weekday": 0,
    "issue_id": 2119,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.05173",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "url": "https://huggingface.co/papers/2502.05173",
            "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "score": 15,
            "issue_id": 2118,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ba284ed1a62b3c2c",
            "authors": [
                "Xilin Wei",
                "Xiaoran Liu",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Jian Tong",
                "Haodong Duan",
                "Qipeng Guo",
                "Jiaqi Wang",
                "Xipeng Qiu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05173.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "VideoRoPE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoRoPE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ Rotary Position Embedding. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—è–≤–∏–ª–∏ 4 –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ RoPE –∫ –≤–∏–¥–µ–æ. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É V-NIAH-D –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ RoPE. VideoRoPE –∏–º–µ–µ—Ç 3D-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ—Ö—Ä–∞–Ω—è—é—â—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã RoPE –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VideoRoPE",
                    "desc": "This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data."
                },
                "zh": {
                    "title": "VideoRoPEÔºöËßÜÈ¢ë‰∏≠ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•Êñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊúâÊïàÂú∞Êâ©Â±ïÂà∞ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠„ÄÇÁ†îÁ©∂ÂàÜÊûê‰∫ÜÂõõ‰∏™ÂÖ≥ÈîÆÁâπÊÄßÔºåËøô‰∫õÁâπÊÄßÂØπ‰∫éRoPEÂú®ËßÜÈ¢ë‰∏≠ÁöÑÈÄÇÂ∫îÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ‰ªªÂä°V-NIAH-DÔºåÂ±ïÁ§∫‰∫ÜÁé∞ÊúâRoPEÂèò‰ΩìÂú®Â§ÑÁêÜËßÜÈ¢ëÊó∂ÂÆπÊòìÂèóÂà∞Âπ≤Êâ∞ÁöÑÁº∫Èô∑„ÄÇÂü∫‰∫éËøô‰∫õÂàÜÊûêÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVideoRoPEÔºåÂÆÉÈÄöËøá3DÁªìÊûÑÊù•‰øùÊåÅÊó∂Á©∫ÂÖ≥Á≥ªÔºåÂπ∂Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰πãÂâçÁöÑRoPEÂèò‰Ωì„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05176",
            "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360¬∞ Unbounded Scene Inpainting",
            "url": "https://huggingface.co/papers/2502.05176",
            "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.",
            "score": 4,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "9b52f2788f53c3c0",
            "authors": [
                "Chung-Ho Wu",
                "Yang-Jung Chen",
                "Ying-Huan Chen",
                "Jie-Ying Lee",
                "Bo-Hsu Ke",
                "Chun-Wei Tuan Mu",
                "Yi-Chuan Huang",
                "Chin-Yang Lin",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: AuraFusion360 –¥–ª—è –±–µ–∑—É–ø—Ä–µ—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω",
                    "desc": "AuraFusion360 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian Splatting. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Å–æ–∫ –Ω–µ–≤–∏–¥–∏–º—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã, –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≥–ª—É–±–∏–Ω—ã –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SDEdit –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–æ—á–∫–∏ –æ–±–∑–æ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 360-USID –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω —Å –æ—Ö–≤–∞—Ç–æ–º 360 –≥—Ä–∞–¥—É—Å–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Inpainting with AuraFusion360",
                    "desc": "AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy."
                },
                "zh": {
                    "title": "AuraFusion360Ôºö‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÂú®ËôöÊãüÁé∞ÂÆûÂíåÂª∫Á≠ëÂèØËßÜÂåñÁ≠âÂ∫îÁî®‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®360Â∫¶Êó†ÁïåÂú∫ÊôØ‰∏≠Èù¢‰∏¥ËßÜÂõæ‰∏ÄËá¥ÊÄßÂíåÂá†‰ΩïÁ≤æÂ∫¶ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAuraFusion360ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂèÇËÄÉÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®È´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ‰∏≠ËøõË°åÁâ©‰ΩìÁßªÈô§ÂíåÂ≠îÂ°´ÂÖÖ„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊ∑±Â∫¶ÊÑüÁü•ÁöÑÊú™ËßÅÊé©Á†ÅÁîüÊàê„ÄÅÈÄÇÂ∫îÊÄßÂºïÂØºÊ∑±Â∫¶Êâ©Êï£ÂíåÂü∫‰∫éSDEditÁöÑÁªÜËäÇÂ¢ûÂº∫ÔºåÁ°Æ‰øùÂ§öËßÜÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåAuraFusion360Âú®ÊÑüÁü•Ë¥®ÈáèÂíåÂá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§üÂú®ÂâßÁÉàËßÜËßíÂèòÂåñ‰∏≠‰øùÊåÅÈ´òË¥®ÈáèÁöÑ‰øÆÂ§çÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04520",
            "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
            "url": "https://huggingface.co/papers/2502.04520",
            "abstract": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.",
            "score": 2,
            "issue_id": 2119,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "41ef9027d1533f06",
            "authors": [
                "Letian Peng",
                "Chenyang An",
                "Shibo Hao",
                "Chengyu Dong",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04520.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#data",
                    "#hallucinations"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω –ª–∏–Ω–µ–π–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä–æ–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –æ—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∫ –¥—Ä—É–≥–æ–º—É. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ –∫ –º–∞—Å—à—Ç–∞–±–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –æ–±–æ–±—â–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–∞–∫–∏–µ –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤–∞—Ä—è."
                },
                "en": {
                    "title": "Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition",
                    "desc": "This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge."
                },
                "zh": {
                    "title": "ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥ÊÄß‰∏éÁü•ËØÜÁªÑÂêà",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂú®Áü•ËØÜÁªÑÂêà‰∏≠ÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥Áé∞Ë±°„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊüê‰∫õÁõ∏ÂÖ≥Áü•ËØÜ‰πãÈó¥Â≠òÂú®Á∫øÊÄßÂèòÊç¢ÔºåËøôÁßçÂèòÊç¢ÂèØ‰ª•Â∞Ü‰∏Ä‰∏™ÊèêÁ§∫ÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµã‰ªé‰∏Ä‰∏™Êò†Â∞ÑÂà∞Âè¶‰∏Ä‰∏™„ÄÇÊØîÂ¶ÇÔºå‰ªé\"X ‰ΩèÂú®ÂüéÂ∏Ç\"ÂèØ‰ª•ËΩ¨Âèò‰∏∫\"X ‰ΩèÂú®ÂõΩÂÆ∂\"„ÄÇÁªìÊûúË°®ÊòéÔºåÁ∫øÊÄßÂèòÊç¢Âú®Â§ßËßÑÊ®°ÂæÆË∞É‰∏≠ÂÖ∑ÊúâÈüßÊÄßÔºåÂπ∂‰∏îÂΩì‰∏éÁé∞ÂÆû‰∏ñÁïåÂÖ≥Á≥ª‰∏ÄËá¥Êó∂ËÉΩÂ§üÊé®ÂπøÊõ¥Êñ∞ÁöÑÁü•ËØÜÔºå‰ΩÜÂΩìÂÅèÁ¶ªÊó∂Âàô‰ºöÂØºËá¥ÂπªËßâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04403",
            "title": "Agency Is Frame-Dependent",
            "url": "https://huggingface.co/papers/2502.04403",
            "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
            "score": 2,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "32ceb8df4d77794a",
            "authors": [
                "David Abel",
                "Andr√© Barreto",
                "Michael Bowling",
                "Will Dabney",
                "Shi Dong",
                "Steven Hansen",
                "Anna Harutyunyan",
                "Khimya Khetarpal",
                "Clare Lyle",
                "Razvan Pascanu",
                "Georgios Piliouras",
                "Doina Precup",
                "Jonathan Richens",
                "Mark Rowland",
                "Tom Schaul",
                "Satinder Singh"
            ],
            "affiliations": [
                "Amii, University of Alberta",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04403.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ê–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å: –≤—Å–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞. –û–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —ç—Ç–æ—Ç —Ç–µ–∑–∏—Å, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∫–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É—á–µ—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞ –≤ –∏–∑—É—á–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –æ–±—Å—É–∂–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º."
                },
                "en": {
                    "title": "Agency in Reinforcement Learning: A Frame-Dependent Perspective",
                    "desc": "This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning."
                },
                "zh": {
                    "title": "ËÉΩÂä®ÊÄßÔºö‰æùËµñ‰∫éÊ°ÜÊû∂ÁöÑÁ≥ªÁªüËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁ≥ªÁªüÁöÑËÉΩÂä®ÊÄßÔºåÁâπÂà´ÊòØÂú®Âº∫ÂåñÂ≠¶‰π†ÁöÑËÉåÊôØ‰∏ã„ÄÇËÉΩÂä®ÊÄßÊòØÊåáÁ≥ªÁªüÊúùÁùÄÁõÆÊ†áÂºïÂØºÁªìÊûúÁöÑËÉΩÂäõÔºå‰ΩÜÂà§Êñ≠‰∏Ä‰∏™Á≥ªÁªüÊòØÂê¶ÂÖ∑Â§áËÉΩÂä®ÊÄßÊòØ‰∏Ä‰∏™Â§çÊùÇÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÉΩÂä®ÊÄßÊòØ‰æùËµñ‰∫éÂèÇËÄÉÊ°ÜÊû∂ÁöÑÔºå‰ªª‰ΩïÂØπÁ≥ªÁªüËÉΩÂä®ÊÄßÁöÑÊµãÈáèÈÉΩÂøÖÈ°ªÁõ∏ÂØπ‰∫éÊüê‰∏™ÂèÇËÄÉÊ°ÜÊû∂ËøõË°å„ÄÇÈÄöËøáÂì≤Â≠¶ËÆ∫ËØÅÔºåÊàë‰ª¨ÊîØÊåÅËøô‰∏ÄËßÇÁÇπÔºåÂπ∂ËÆ®ËÆ∫‰∫ÜËøô‰∏ÄÁªìËÆ∫ÂØπÂº∫ÂåñÂ≠¶‰π†ÁöÑÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04896",
            "title": "Goku: Flow Based Video Generative Foundation Models",
            "url": "https://huggingface.co/papers/2502.04896",
            "abstract": "This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",
            "score": 1,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "ad6ef6eed233cc90",
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Yuqi Zhang",
                "Yida Zhang",
                "Fengda Zhu",
                "Hao Yang",
                "Hongxiang Hao",
                "Hui Wu",
                "Zhichao Lai",
                "Yifei Hu",
                "Ting-Che Lin",
                "Shilong Zhang",
                "Fu Li",
                "Chuan Li",
                "Xing Wang",
                "Yanghua Peng",
                "Peize Sun",
                "Ping Luo",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "Bytedance Inc",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04896.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#video",
                    "#architecture",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üêâ",
                "ru": {
                    "title": "Goku: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Goku –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Goku –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Goku: Revolutionizing Image and Video Generation with Transformers",
                    "desc": "This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models."
                },
                "zh": {
                    "title": "GokuÔºöÂõæÂÉè‰∏éËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜGokuÔºåËøôÊòØ‰∏ÄÁßçÂÖàËøõÁöÑËÅîÂêàÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÂà©Áî®‰∫Ü‰øÆÊ≠£ÊµÅTransformer‰ª•ÂÆûÁé∞Ë°å‰∏öÈ¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ËØ¶ÁªÜÈòêËø∞‰∫ÜÈ´òË¥®ÈáèËßÜËßâÁîüÊàêÁöÑÂü∫Á°ÄË¶ÅÁ¥†ÔºåÂåÖÊã¨Êï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ã„ÄÅÊ®°ÂûãÊû∂ÊûÑËÆæËÆ°„ÄÅÊµÅÁöÑÂÖ¨ÂºèÂåñ‰ª•ÂèäÈ´òÊïàÁ®≥ÂÅ•ÁöÑÂ§ßËßÑÊ®°ËÆ≠ÁªÉÂü∫Á°ÄËÆæÊñΩ„ÄÇGokuÊ®°ÂûãÂú®ÂÆöÊÄßÂíåÂÆöÈáèËØÑ‰º∞‰∏≠Ë°®Áé∞‰ºòË∂äÔºå‰∏∫‰∏ªË¶Å‰ªªÂä°ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåGokuÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ËææÂà∞‰∫Ü0.76ÁöÑGenEvalÂíå83.65ÁöÑDPG-BenchÔºåÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ë‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü84.85ÁöÑVBench„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05171",
            "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
            "url": "https://huggingface.co/papers/2502.05171",
            "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
            "score": 1,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "4386159312d9856b",
            "authors": [
                "Jonas Geiping",
                "Sean McLeish",
                "Neel Jain",
                "John Kirchenbauer",
                "Siddharth Singh",
                "Brian R. Bartoldson",
                "Bhavya Kailkhura",
                "Abhinav Bhatele",
                "Tom Goldstein"
            ],
            "affiliations": [
                "ELLIS Institute T√ºbingen, Max-Planck Institute for Intelligent Systems, T√ºbingen AI Center",
                "Lawrence Livermore National Laboratory",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ì–ª—É–±–æ–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ–º –Ω–µ—è–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞, —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—è—Å—å –¥–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–æ 3,5 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ 800 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ–∫–∞–∑–∞–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Scaling Reasoning with Latent Space Computation",
                    "desc": "This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts."
                },
                "zh": {
                    "title": "ÈöêÂºèÊé®ÁêÜÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ°ÁÆóËÉΩÂäõ",
                    "desc": "Êàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑÔºåËØ•Êû∂ÊûÑËÉΩÂ§üÈÄöËøáÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ÈöêÂºèÊé®ÁêÜÊù•Êâ©Â±ïÊµãËØïÊó∂ÁöÑËÆ°ÁÆóËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáËø≠‰ª£ÈÄíÂΩíÂùóÂ∑•‰ΩúÔºå‰ªéËÄåÂú®ÊµãËØïÊó∂ÂèØ‰ª•Â±ïÂºÄÂà∞‰ªªÊÑèÊ∑±Â∫¶„ÄÇËøô‰∏é‰∏ªÊµÅÊé®ÁêÜÊ®°Âûã‰∏çÂêåÔºåÂêéËÄÖÈÄöËøáÁîüÊàêÊõ¥Â§öÁöÑÊ†áËÆ∞Êù•Â¢ûÂä†ËÆ°ÁÆóÈáè„ÄÇÊàë‰ª¨ÁöÑÊ®°Âûã‰∏çÈúÄË¶ÅÁâπÊÆäÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåËÉΩÂ§üÂ§ÑÁêÜÂ∞èÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂπ∂‰∏îËÉΩÂ§üÊçïÊçâ‰∏çÊòìÁî®ËØ≠Ë®ÄË°®Á§∫ÁöÑÊé®ÁêÜÁ±ªÂûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04363",
            "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
            "url": "https://huggingface.co/papers/2502.04363",
            "abstract": "We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.",
            "score": 0,
            "issue_id": 2119,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 5",
                "zh": "2Êúà5Êó•"
            },
            "hash": "339b45dee733174c",
            "authors": [
                "Bosung Kim",
                "Kyuhwan Lee",
                "Isu Jeong",
                "Jungmin Cheon",
                "Yeojin Lee",
                "Seulki Lee"
            ],
            "affiliations": [
                "Ulsan National Institute of Science and Technology South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04363.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "üì±",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –ø—Ä—è–º–æ –Ω–∞ –≤–∞—à–µ–º —Å–º–∞—Ä—Ç—Ñ–æ–Ω–µ",
                    "desc": "On-device Sora –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–µ–µ –Ω–∞ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∏ –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: Linear Proportional Leap (LPL) –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, Temporal Dimension Token Merging (TDTM) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è, –∏ Concurrent Inference with Dynamic Loading (CI-DL) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ iPhone 15 Pro –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ On-device Sora —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ Open-Sora –Ω–∞ –º–æ—â–Ω—ã—Ö GPU."
                },
                "en": {
                    "title": "Empowering Video Creation on Your Smartphone!",
                    "desc": "On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services."
                },
                "zh": {
                    "title": "ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÈ´òÊïàËßÜÈ¢ëÁîüÊàêÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫ÜOn-device SoraÔºåËøôÊòØÈ¶ñ‰∏™Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÁßªÂä®ËÆæÂ§áÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÔºåËÉΩÂ§üÈ´òÊïàÂú∞Âú®Êô∫ËÉΩÊâãÊú∫‰∏äËøêË°å„ÄÇËØ•Á≥ªÁªüÈááÁî®‰∫Ü‰∏âÁßçÊñ∞ÊäÄÊúØÊù•Ëß£ÂÜ≥ÁßªÂä®ËÆæÂ§áÂú®ËÆ°ÁÆóÂíåÂÜÖÂ≠òÊñπÈù¢ÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÁ∫øÊÄßÊØî‰æãË∑≥Ë∑ÉÔºàLPLÔºâÈÄöËøáÈ´òÊïàÁöÑË∑≥Ë∑ÉÊñπÊ≥ïÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÊâ©Êï£‰∏≠ÊâÄÈúÄÁöÑÂéªÂô™Ê≠•È™§„ÄÇÂÖ∂Ê¨°ÔºåÊó∂Èó¥Áª¥Â∫¶‰ª§ÁâåÂêàÂπ∂ÔºàTDTMÔºâÈÄöËøáÊ≤øÊó∂Èó¥Áª¥Â∫¶ÂêàÂπ∂ËøûÁª≠‰ª§ÁâåÔºåÈôç‰Ωé‰∫ÜÊ≥®ÊÑèÂäõÂ±Ç‰∏≠ÂØÜÈõÜÁöÑ‰ª§ÁâåÂ§ÑÁêÜËÆ°ÁÆó„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04728",
            "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
            "url": "https://huggingface.co/papers/2502.04728",
            "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.",
            "score": 0,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 7",
                "zh": "2Êúà7Êó•"
            },
            "hash": "cd97668cd0eee0ee",
            "authors": [
                "Zhouliang Yu",
                "Yuhuan Yuan",
                "Tim Z. Xiao",
                "Fuxiang Frank Xia",
                "Jie Fu",
                "Ge Zhang",
                "Ge Lin",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Environmental Systems Research Institute, Inc.",
                "Max Planck Institute for Intelligent Systems, T√ºbingen",
                "SEED, Bytedance",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04728.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é PDDL –∏ LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (PDDL) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω–æ–π —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Best-of-N –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–±–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–æ–≤ PDDL –∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è."
                },
                "en": {
                    "title": "Enhancing LLMs for Optimal Planning with PDDL",
                    "desc": "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."
                },
                "zh": {
                    "title": "Âà©Áî®PDDLÊèêÂçáËßÑÂàíÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËß£ÂÜ≥Â§çÊùÇÁöÑËßÑÂàíÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçËßÑÂàôËøùÂèçÂíåÁ°Æ‰øùÊúÄ‰ºòÊÄßÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºïÂÖ•‰∫ÜËßÑÂàíÈ¢ÜÂüüÂÆö‰πâËØ≠Ë®ÄÔºàPDDLÔºâÔºå‰Ωú‰∏∫‰∏ÄÁßçÁ≤æÁ°ÆÁöÑÁä∂ÊÄÅÊèèËø∞Â∑•ÂÖ∑„ÄÇÈÄöËøáPDDLÔºåÂèØ‰ª•ÁîüÊàêÁ¨¶Âè∑‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Â∫îÁî®ÁªèÂÖ∏ÊêúÁ¥¢ÁÆóÊ≥ïÔºàÂ¶ÇA*ÔºâÊù•ÂØªÊâæÊúÄ‰ºòËÆ°Âàí„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÁÆóÊ≥ïÔºåÈÄöËøáBest-of-NÈááÊ†∑ÂíåÁªÜËá¥ÁöÑÊú∫Âô®Â≠¶‰π†‰ºòÂåñÔºåÊòæËëóÊèêÈ´ò‰∫ÜPDDLÈ¢ÜÂüüÁöÑÁîüÊàêË¥®ÈáèÔºåÊàêÂäüÁéáË∂ÖËøá50%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04404",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
            "url": "https://huggingface.co/papers/2502.04404",
            "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 6",
                "zh": "2Êúà6Êó•"
            },
            "hash": "a7bd201755c7ea1d",
            "authors": [
                "Xiao-Wen Yang",
                "Xuan-Yi Zhu",
                "Wen-Da Wei",
                "Ding-Chu Zhang",
                "Jie-Jing Shao",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University, China",
                "School of Artificial Intelligence, Nanjing University, China",
                "School of Intelligence Science and Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04404.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#inference",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –≤–æ–∑–≤—Ä–∞—Ç: –ø—É—Ç—å –∫ –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–º –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ (self-backtracking) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –∏ –≥–¥–µ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–≤—Ä–∞—â–∞—è –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –±—ã—Å—Ç—Ä–æ–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Backtracking for Enhanced Reasoning",
                    "desc": "This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods."
                },
                "zh": {
                    "title": "Ëá™ÊàëÂõûÊ∫ØÊú∫Âà∂ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Â∞ÜÊÖ¢ÊÄùËÄÉÊú∫Âà∂Êï¥ÂêàÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Ôºå‰∏∫ÂÆûÁé∞‰∫åÁ∫ßAGIÊé®ÁêÜÂô®Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæÑ„ÄÇÂΩìÂâçÁöÑÊåëÊàòÂåÖÊã¨‰ΩéÊïàÁöÑËøáÂ∫¶ÊÄùËÄÉÂíåÂØπËæÖÂä©Â•ñÂä±Ê®°ÂûãÁöÑËøáÂ∫¶‰æùËµñ„ÄÇÊàë‰ª¨ÊåáÂá∫ÔºåËøô‰∫õÈôêÂà∂Ê∫ê‰∫éLLMsÊó†Ê≥ïÂÜÖÂåñÊêúÁ¥¢ËøáÁ®ãÔºåËÄåÊêúÁ¥¢ËøáÁ®ãÊòØÊúâÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÊàëÂõûÊ∫ØÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠Ëá™‰∏ªÂÜ≥ÂÆö‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰ΩïÂõûÊ∫ØÔºå‰ªéËÄåÊòæËëóÊèêÂçáÊé®ÁêÜËÉΩÂäõÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04350",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "url": "https://huggingface.co/papers/2502.04350",
            "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 4",
                "zh": "2Êúà4Êó•"
            },
            "hash": "ad7829c09c28de41",
            "authors": [
                "Yongchao Chen",
                "Yilun Hao",
                "Yueying Liu",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard University, Boston, MA, USA",
                "MIT-IBM Watson AI Lab, Boston, MA, USA",
                "Massachusetts Institute of Technology, Boston, MA, USA",
                "University of Illinois Urbana-Champaign, Urbana, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04350.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "üß≠",
                "ru": {
                    "title": "CodeSteer: –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ LLM –≤ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö",
                    "desc": "CodeSteer - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–¥–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SymBench –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –û–Ω–∏ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Llama-3-8B —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –º–æ–¥–µ–ª—å CodeSteerLLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "CodeSteer: Guiding LLMs to Master Code and Reasoning!",
                    "desc": "This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks."
                },
                "zh": {
                    "title": "CodeSteerÔºöÂºïÂØºLLMÂÆûÁé∞Á¨¶Âè∑ËÆ°ÁÆóÁöÑÁ™ÅÁ†¥",
                    "desc": "Áé∞ÊúâÁöÑÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂºïÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰πãÈó¥ÂàáÊç¢ÔºåÂØºËá¥Á¨¶Âè∑ËÆ°ÁÆóËÉΩÂäõÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CodeSteerÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÊåáÂØºLLMÁöÑ‰ª£Á†ÅÂíåÊñáÊú¨ÁîüÊàê„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜSymBenchÔºåÂåÖÂê´37‰∏™ÂÖ∑ÊúâÂèØË∞ÉÂ§çÊùÇÂ∫¶ÁöÑÁ¨¶Âè∑‰ªªÂä°ÔºåÂπ∂ÂêàÊàê‰∫ÜÂåÖÂê´1.2‰∏áÂ§öËΩÆÊåáÂØº/ÁîüÊàêËΩ®ËøπÂíå5500ÂØπÊåáÂØºÊØîËæÉÁöÑÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂØπLlama-3-8BÊ®°ÂûãËøõË°åÂ§öËΩÆÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÔºåÊàë‰ª¨ÂæóÂà∞ÁöÑCodeSteerLLMÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂºïÂØºÊõ¥Â§ßÊ®°ÂûãÁöÑ‰ª£Á†Å/ÊñáÊú¨ÁîüÊàê„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-02-07.html",
    "link_next": "2025-02-11.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2Êúà7Êó•"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2Êúà11Êó•"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 3,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁ≥ªÁªüÂú∞Êò†Â∞ÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®Âú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËøûÁª≠Â±Ç‰∏≠ÂèëÁé∞ÁöÑÁâπÂæÅ„ÄÇÊàë‰ª¨‰ΩøÁî®Êó†Êï∞ÊçÆ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÊäÄÊúØÔºåËøΩË∏™ÁâπÂÆöÁâπÂæÅÂú®ÊØè‰∏™Èò∂ÊÆµÁöÑÊåÅÁª≠„ÄÅËΩ¨ÂèòÊàñÈ¶ñÊ¨°Âá∫Áé∞„ÄÇËØ•ÊñπÊ≥ï‰∫ßÁîüÁâπÂæÅÊºîÂèòÁöÑÁªÜÁ≤íÂ∫¶ÊµÅÂõæÔºåÊèê‰æõÁªÜÁ≤íÂ∫¶ÁöÑÂèØËß£ÈáäÊÄßÂíåÊú∫Âà∂Ê¥ûÂØü„ÄÇÂÖ≥ÈîÆÊòØÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜËøô‰∫õË∑®Â±ÇÁâπÂæÅÂõæÂ¶Ç‰ΩïÈÄöËøáÊîæÂ§ßÊàñÊäëÂà∂ÈÄâÂÆöÁâπÂæÅÔºåÁõ¥Êé•ÂºïÂØºÊ®°ÂûãË°å‰∏∫ÔºåÂÆûÁé∞ÊñáÊú¨ÁîüÊàêÁöÑÂÆöÂêë‰∏ªÈ¢òÊéßÂà∂„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÊàë‰ª¨ÁöÑÂèëÁé∞Á™ÅÊòæ‰∫ÜÂõ†Êûú„ÄÅË∑®Â±ÇÂèØËß£ÈáäÊÄßÊ°ÜÊû∂ÁöÑÂÆûÁî®ÊÄßÔºå‰∏ç‰ªÖÊæÑÊ∏Ö‰∫ÜÁâπÂæÅÈÄöËøáÂâçÂêë‰º†ÈÄíÁöÑÂèëÂ±ïÔºåËøòÊèê‰æõ‰∫ÜÈÄèÊòéÊìç‰ΩúÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ",
        "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "pinyin": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁ≥ªÁªüÂú∞Êò†Â∞ÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®Âú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËøûÁª≠Â±Ç‰∏≠ÂèëÁé∞ÁöÑÁâπÂæÅ„ÄÇÊàë‰ª¨‰ΩøÁî®Êó†Êï∞ÊçÆ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÊäÄÊúØÔºåËøΩË∏™ÁâπÂÆöÁâπÂæÅÂú®ÊØè‰∏™Èò∂ÊÆµÁöÑÊåÅÁª≠„ÄÅËΩ¨ÂèòÊàñÈ¶ñÊ¨°Âá∫Áé∞„ÄÇËØ•ÊñπÊ≥ï‰∫ßÁîüÁâπÂæÅÊºîÂèòÁöÑÁªÜÁ≤íÂ∫¶ÊµÅÂõæÔºåÊèê‰æõÁªÜÁ≤íÂ∫¶ÁöÑÂèØËß£ÈáäÊÄßÂíåÊú∫Âà∂Ê¥ûÂØü„ÄÇÂÖ≥ÈîÆÊòØÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜËøô‰∫õË∑®Â±ÇÁâπÂæÅÂõæÂ¶Ç‰ΩïÈÄöËøáÊîæÂ§ßÊàñÊäëÂà∂ÈÄâÂÆöÁâπÂæÅÔºåÁõ¥Êé•ÂºïÂØºÊ®°ÂûãË°å‰∏∫ÔºåÂÆûÁé∞ÊñáÊú¨ÁîüÊàêÁöÑÂÆöÂêë‰∏ªÈ¢òÊéßÂà∂„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÊàë‰ª¨ÁöÑÂèëÁé∞Á™ÅÊòæ‰∫ÜÂõ†Êûú„ÄÅË∑®Â±ÇÂèØËß£ÈáäÊÄßÊ°ÜÊû∂ÁöÑÂÆûÁî®ÊÄßÔºå‰∏ç‰ªÖÊæÑÊ∏Ö‰∫ÜÁâπÂæÅÈÄöËøáÂâçÂêë‰º†ÈÄíÁöÑÂèëÂ±ïÔºåËøòÊèê‰æõ‰∫ÜÈÄèÊòéÊìç‰ΩúÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ\n\nW«ímen ji√®sh√†o le yƒ´ zh«íng xƒ´n fƒÅngf«é, x√¨t«íng de y√¨ngsh√® xƒ´sh≈´ z√¨biƒÅnm«éq√¨ z√†i d√† y«îy√°n m√≥x√≠ng de li√°nx√π c√©ng zh≈çng fƒÅxi√†n de t√®zhƒìng. W«ímen sh«êy√≤ng w√∫ sh√πj√π y√∫xi√†n xi√†ngs√¨d√π j√¨sh√π, zhuƒ´z≈çng t√®d√¨ng t√®zhƒìng z√†i mƒõi ge jiƒìdu√†n de ch√≠x√π, zhu«énbi√†n hu√≤ sh«íuc√¨ ch≈´xi√†n. G«éi fƒÅngf«é ch«énshƒìng t√®zhƒìng y«énbi√†n de x√¨l√¨d√π li√∫ t√∫, t√≠g≈çng x√¨l√¨d√π de kƒõ jiƒõsh√¨x√¨ng h√© jƒ´zh√¨ d√≤ngch√†. Gu«énji√†n sh√¨, w«ímen zh«énsh√¨ le zh√®xiƒì ku√†c√©ng t√®zhƒìng t√∫ r√∫h√© t≈çnggu√≤ f√†ngd√† hu√≤ y√¨zh√¨ xu«énd√¨ng t√®zhƒìng, zh√≠jiƒì y«ênd«éo m√≥x√≠ng x√≠ngw√©i, sh√≠xi√†n w√©nbƒõn shƒìngch√©ng de d√¨ngxi√†ng zh«ît√≠ k√≤ngzh√¨. Z«íng de l√°i shu≈ç, w«ímen de fƒÅxi√†n t≈´x√¨ le yƒ´ngu«í, ku√†c√©ng kƒõ jiƒõsh√¨x√¨ng kuƒÅngji√† de sh√≠y√≤ngx√¨ng, b√πj«ên ch√©ngqƒ´ng le t√®zhƒìng t≈çnggu√≤ qi√°nxi√†ng chu√°nd√¨ de fƒÅzh«én, h√°i t√≠g≈çng le t√≤um√≠ng cƒÅozu√≤ d√† y«îy√°n m√≥x√≠ng de xƒ´n fƒÅngf«é.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"Á≥ªÁªüÂú∞\", \"pinyin\": \"x√¨ t«íng de\", \"trans\": \"systematically\"},\n    {\"word\": \"Êò†Â∞Ñ\", \"pinyin\": \"y√¨ng sh√®\", \"trans\": \"map\"},\n    {\"word\": \"Á®ÄÁñè\", \"pinyin\": \"xƒ´ sh≈´\", \"trans\": \"sparse\"},\n    {\"word\": \"Ëá™ÁºñÁ†ÅÂô®\", \"pinyin\": \"z√¨ biƒÅn m«é q√¨\", \"trans\": \"autoencoder\"},\n    {\"word\": \"Â§ßËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"d√† y«î y√°n m√≥ x√≠ng\", \"trans\": \"large language model\"},\n    {\"word\": \"ËøûÁª≠Â±Ç\", \"pinyin\": \"li√°n x√π c√©ng\", \"trans\": \"continuous layer\"},\n    {\"word\": \"ÂèëÁé∞\", \"pinyin\": \"fƒÅ xi√†n\", \"trans\": \"discover\"},\n    {\"word\": \"ÁâπÂæÅ\", \"pinyin\": \"t√® zhƒìng\", \"trans\": \"feature\"},\n    {\"word\": \"Êó†Êï∞ÊçÆ\", \"pinyin\": \"w√∫ sh√π j√π\", \"trans\": \"data-free\"},\n    {\"word\": \"‰ΩôÂº¶Áõ∏‰ººÂ∫¶\", \"pinyin\": \"y√∫ xi√†n xiƒÅng s√¨ d√π\", \"trans\": \"cosine similarity\"},\n    {\"word\": \"ÊäÄÊúØ\", \"pinyin\": \"j√¨ sh√π\", \"trans\": \"technique\"},\n    {\"word\": \"ËøΩË∏™\", \"pinyin\": \"zhuƒ´ z≈çng\", \"trans\": \"track\"},\n    {\"word\": \"ÊåÅÁª≠\", \"pinyin\": \"ch√≠ x√π\", \"trans\": \"persist\"},\n    {\"word\": \"ËΩ¨Âèò\", \"pinyin\": \"zhu«én bi√†n\", \"trans\": \"transform\"},\n    {\"word\": \"È¶ñÊ¨°\", \"pinyin\": \"sh«íu c√¨\", \"trans\": \"first time\"},\n    {\"word\": \"Âá∫Áé∞\", \"pinyin\": \"ch≈´ xi√†n\", \"trans\": \"appear\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"‰∫ßÁîü\", \"pinyin\": \"ch«én shƒìng\", \"trans\": \"generate\"},\n    {\"word\": \"ÊºîÂèò\", \"pinyin\": \"y«én bi√†n\", \"trans\": \"evolution\"},\n    {\"word\": \"ÁªÜÁ≤íÂ∫¶\", \"pinyin\": \"x√¨ l√¨ d√π\", \"trans\": \"fine-grained\"},\n    {\"word\": \"ÊµÅÂõæ\", \"pinyin\": \"li√∫ t√∫\", \"trans\": \"flow chart\"},\n    {\"word\": \"Êèê‰æõ\", \"pinyin\": \"t√≠ g≈çng\", \"trans\": \"provide\"},\n    {\"word\": \"ÂèØËß£ÈáäÊÄß\", \"pinyin\": \"kƒõ jiƒõ sh√¨ x√¨ng\", \"trans\": \"interpretability\"},\n    {\"word\": \"Êú∫Âà∂\", \"pinyin\": \"jƒ´ zh√¨\", \"trans\": \"mechanism\"},\n    {\"word\": \"Ê¥ûÂØü\", \"pinyin\": \"d√≤ng chƒÅ\", \"trans\": \"insight\"},\n    {\"word\": \"ÂÖ≥ÈîÆ\", \"pinyin\": \"gu«én ji√†n\", \"trans\": \"key\"},\n    {\"word\": \"Â±ïÁ§∫\", \"pinyin\": \"zh«én sh√¨\", \"trans\": \"demonstrate\"},\n    {\"word\": \"Ë∑®Â±Ç\", \"pinyin\": \"ku√† c√©ng\", \"trans\": \"cross-layer\"},\n    {\"word\": \"ÁâπÂæÅÂõæ\", \"pinyin\": \"t√® zhƒìng t√∫\", \"trans\": \"feature map\"},\n    {\"word\": \"ÊîæÂ§ß\", \"pinyin\": \"f√†ng d√†\", \"trans\": \"amplify\"},\n    {\"word\": \"ÊäëÂà∂\", \"pinyin\": \"y√¨ zh√¨\", \"trans\": \"suppress\"},\n    {\"word\": \"ÈÄâÂÆö\", \"pinyin\": \"xu«én d√¨ng\", \"trans\": \"select\"},\n    {\"word\": \"ÂºïÂØº\", \"pinyin\": \"y«ên d«éo\", \"trans\": \"guide\"},\n    {\"word\": \"Ê®°ÂûãË°å‰∏∫\", \"pinyin\": \"m√≥ x√≠ng x√≠ng w√©i\", \"trans\": \"model behavior\"},\n    {\"word\": \"ÂÆûÁé∞\", \"pinyin\": \"sh√≠ xi√†n\", \"trans\": \"achieve\"},\n    {\"word\": \"ÊñáÊú¨ÁîüÊàê\", \"pinyin\": \"w√©n bƒõn shƒìng ch√©ng\", \"trans\": \"text generation\"},\n    {\"word\": \"ÂÆöÂêë\", \"pinyin\": \"d√¨ng xi√†ng\", \"trans\": \"directed\"},\n    {\"word\": \"‰∏ªÈ¢òÊéßÂà∂\", \"pinyin\": \"zh«î t√≠ k√≤ng zh√¨\", \"trans\": \"theme control\"},\n    {\"word\": \"ÊÄªÁöÑÊù•ËØ¥\", \"pinyin\": \"z«íng de l√°i shu≈ç\", \"trans\": \"in summary\"},\n    {\"word\": \"ÂèëÁé∞\", \"pinyin\": \"fƒÅ xi√†n\", \"trans\": \"discover\"},\n    {\"word\": \"Á™ÅÊòæ\", \"pinyin\": \"t≈´ xi«én\", \"trans\": \"highlight\"},\n    {\"word\": \"Âõ†Êûú\", \"pinyin\": \"yƒ´n gu«í\", \"trans\": \"causal\"},\n    {\"word\": \"Ê°ÜÊû∂\", \"pinyin\": \"ku√†ng ji√†\", \"trans\": \"framework\"},\n    {\"word\": \"ÂÆûÁî®ÊÄß\", \"pinyin\": \"sh√≠ y√≤ng x√¨ng\", \"trans\": \"practicality\"},\n    {\"word\": \"ÊæÑÊ∏Ö\", \"pinyin\": \"ch√©ng qƒ´ng\", \"trans\": \"clarify\"},\n    {\"word\": \"ÂâçÂêë‰º†ÈÄí\", \"pinyin\": \"qi√°n xi√†ng chu√°n d√¨\", \"trans\": \"forward propagation\"},\n    {\"word\": \"ÂèëÂ±ï\", \"pinyin\": \"fƒÅ zh«én\", \"trans\": \"develop\"},\n    {\"word\": \"Êèê‰æõ\", \"pinyin\": \"t√≠ g≈çng\", \"trans\": \"provide\"},\n    {\"word\": \"ÈÄèÊòé\", \"pinyin\": \"t√≤u m√≠ng\", \"trans\": \"transparent\"},\n    {\"word\": \"Êìç‰Ωú\", \"pinyin\": \"cƒÅo zu√≤\", \"trans\": \"operate\"},\n    {\"word\": \"Êñ∞ÊñπÊ≥ï\", \"pinyin\": \"xƒ´n fƒÅng f«é\", \"trans\": \"new method\"}\n]",
        "trans": "We introduce a new method that systematically maps the features discovered by sparse autoencoders in the continuous layers of large language models. Utilizing data-free cosine similarity techniques, we track the persistence, transformation, or initial appearance of specific features at each stage. This method generates fine-grained flow maps of feature evolution, offering detailed interpretability and mechanistic insights. Crucially, we demonstrate how these cross-layer feature maps directly guide model behavior by amplifying or suppressing selected features, achieving directed thematic control over text generation. Overall, our findings highlight the practicality of a causal, cross-layer interpretability framework, not only clarifying the development of features through forward propagation but also providing new methods for transparently operating large language models.",
        "update_ts": "2025-02-09 12:37"
    }
}