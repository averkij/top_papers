{
    "date": {
        "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 26",
        "zh": "3æœˆ26æ—¥"
    },
    "time_utc": "2025-03-26 02:20",
    "weekday": 2,
    "issue_id": 2896,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.19385",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
            "url": "https://huggingface.co/papers/2503.19385",
            "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "e0ead8fbe973f326",
            "authors": [
                "Jaihoon Kim",
                "Taehoon Yoon",
                "Jisung Hwang",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19385.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ (SDE), Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SDE, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Flow Models with Efficient Inference-Time Scaling",
                    "desc": "This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos."
                },
                "zh": {
                    "title": "æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾åœ¨å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ç¬¦åˆç”¨æˆ·åå¥½ã€‚å°½ç®¡æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç”±äºå…¶ç¡®å®šæ€§ç”Ÿæˆè¿‡ç¨‹ï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…³é”®æ€æƒ³ï¼Œä»¥å®ç°æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’Œè‡ªé€‚åº”è®¡ç®—èµ„æºåˆ†é…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19325",
            "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
            "url": "https://huggingface.co/papers/2503.19325",
            "abstract": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "543c7dbfad83ed73",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "FAR: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Frame AutoRegressive (FAR) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ FlexRoPE - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² 16 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. FAR Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Frame AutoRegressive Modeling",
                    "desc": "This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency."
                },
                "zh": {
                    "title": "é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è‡ªå›å½’å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºFrame AutoRegressive (FAR)ï¼Œæ—¨åœ¨è§£å†³é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚FARé€šè¿‡å»ºæ¨¡è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´å› æœå…³ç³»ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚ä¸ºäº†åº”å¯¹è§†è§‰å†—ä½™å’Œè®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FlexRoPEæŠ€æœ¯ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´è¿œç¨‹ä¸Šä¸‹æ–‡çš„æ—¶é—´è¡°å‡ï¼Œå¹¶å¼•å…¥äº†é•¿çŸ­æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFARåœ¨çŸ­è§†é¢‘å’Œé•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆä¸ºè§†é¢‘è‡ªå›å½’å»ºæ¨¡çš„æœ‰æ•ˆåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19910",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2503.19910",
            "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.",
            "score": 2,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "63f36082e6c27f3e",
            "authors": [
                "Chuong Huynh",
                "Jinyu Yang",
                "Ashish Tawari",
                "Mubarak Shah",
                "Son Tran",
                "Raffay Hamid",
                "Trishul Chilimbi",
                "Abhinav Shrivastava"
            ],
            "affiliations": [
                "Amazon",
                "Center for Research in Computer Vision, University of Central Florida",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "CoLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoLLM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (CIR). CoLLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MTCIR Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CIR."
                },
                "en": {
                    "title": "Revolutionizing Composed Image Retrieval with CoLLM",
                    "desc": "This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks."
                },
                "zh": {
                    "title": "CoLLMï¼šå¤åˆå›¾åƒæ£€ç´¢çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoLLMçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å›¾åƒ-æ–‡æœ¬å¯¹ä¸­åŠ¨æ€ç”Ÿæˆä¸‰å…ƒç»„ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ï¼Œä»è€Œå®ç°äº†ç›‘ç£å­¦ä¹ ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„è”åˆåµŒå…¥ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å«340ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†MTCIRï¼Œå¹¶æ”¹è¿›äº†ç°æœ‰çš„CIRåŸºå‡†ï¼Œä»¥æé«˜è¯„ä¼°çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19041",
            "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
            "url": "https://huggingface.co/papers/2503.19041",
            "abstract": "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.",
            "score": 2,
            "issue_id": 2896,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "8495b5a09ddf5611",
            "authors": [
                "Kangwei Liu",
                "Mengru Wang",
                "Yujie Luo",
                "Lin Yuan",
                "Mengshu Sun",
                "Ningyu Zhang",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19041.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#low_resource"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LookAhead Tuning. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LookAhead Tuning ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "LookAhead Tuning: Safeguarding LLMs During Fine-Tuning",
                    "desc": "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."
                },
                "zh": {
                    "title": "LookAhead Tuningï¼šå®‰å…¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLookAhead Tuningçš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å®‰å…¨æ€§ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„è§ˆéƒ¨åˆ†ç­”æ¡ˆå‰ç¼€ï¼Œé‡‡ç”¨ä¸¤ç§ç®€å•ä¸”ä½èµ„æºçš„æ•°æ®é©±åŠ¨æ–¹æ³•æ¥ä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡æœ€å°åŒ–åˆå§‹æ ‡è®°åˆ†å¸ƒçš„æ‰°åŠ¨ï¼Œä¿æŒæ¨¡å‹å›ºæœ‰çš„å®‰å…¨æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLookAhead Tuningèƒ½å¤Ÿæœ‰æ•ˆç»´æŠ¤æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17361",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
            "url": "https://huggingface.co/papers/2503.17361",
            "abstract": "Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.",
            "score": 0,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "b5389a3e5ab241c3",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Management and Technology Program, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17361.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Gumbel-Softmax Flow and Score Matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚ Gumbel-Softmax Ñ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑÑ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Straight-Through Guided Flows Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Sequence Generation with Gumbel-Softmax Flows",
                    "desc": "This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆé«˜ç»´åºåˆ—çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºGumbel-Softmaxæµå’Œè¯„åˆ†åŒ¹é…ï¼Œæ—¨åœ¨è§£å†³DNAåºåˆ—è®¾è®¡ä¸­çš„é«˜ç»´ç®€å•å½¢é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ—¶é—´ä¾èµ–çš„Gumbel-Softmaxæ’å€¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ç®€å•å½¢ä¸Šå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ç§åä¸ºSTGFlowçš„åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ¡ä»¶DNAå¯åŠ¨å­è®¾è®¡ã€åºåˆ—ç”Ÿæˆçš„è›‹ç™½è´¨å’Œé¶å‘ç»“åˆè‚½çš„è®¾è®¡ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16965",
            "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
            "url": "https://huggingface.co/papers/2503.16965",
            "abstract": "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.",
            "score": 0,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "ca2e599ff0665dfe",
            "authors": [
                "Zhe Hu",
                "Jing Li",
                "Yu Yin"
            ],
            "affiliations": [
                "Department of Computer and Data Sciences, Case Western Reserve University",
                "Department of Computing, The Hong Kong Polytechnic University",
                "Research Centre for Data Science & Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16965.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ VLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ VLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ñ… LLM-Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing VLMs through Text-Only Training and Self-Improvement",
                    "desc": "This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable."
                },
                "zh": {
                    "title": "æå‡VLMäººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚äººç±»ä¸­å¿ƒå†³ç­–ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æè¿°çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸäº›ä»»åŠ¡ä¸Šæ„å¤–åœ°è¶…è¶Šäº†å¤„ç†å›¾åƒçš„VLMsï¼Œè¿™è¡¨æ˜è§†è§‰å¯¹é½å¯èƒ½ä¼šé™åˆ¶VLMçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»…åŸºäºæ–‡æœ¬çš„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆæ–‡æœ¬æ•°æ®æ¥å¢å¼ºVLMçš„è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›ï¼ŒVLMså¯ä»¥æ˜¾è‘—æå‡å…¶äººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›ï¼Œå¼€è¾Ÿäº†ä¼˜åŒ–VLMçš„æ–°é€”å¾„ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-25.html",
    "link_next": "2025-03-27.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "25.03",
        "en": "03/25",
        "zh": "3æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.03",
        "en": "03/27",
        "zh": "3æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "ç°ä»£æ¸¸æˆå¼€å‘é¢ä¸´åˆ›æ„å’Œæˆæœ¬æŒ‘æˆ˜ã€‚æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆ›é€ é€¼çœŸä¸”äº’åŠ¨çš„è™šæ‹Ÿç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ä½œä¸ºç”Ÿæˆæ¸¸æˆå¼•æ“ï¼ˆGGEï¼‰çš„åŸºç¡€ã€‚GGEåˆ©ç”¨IGVçš„ä¼˜åŠ¿ï¼Œå¦‚æ— é™åˆ¶çš„é«˜è´¨é‡å†…å®¹ç”Ÿæˆå’Œç‰©ç†æ„è¯†å»ºæ¨¡ã€‚æˆ‘ä»¬å±•ç¤ºäº†GGEçš„æ ¸å¿ƒæ¨¡å—å’Œåˆ†å±‚æˆç†Ÿåº¦è·¯çº¿å›¾ï¼ˆL0-L4ï¼‰ã€‚è¿™é¡¹å·¥ä½œä¸ºAIæ—¶ä»£çš„æ¸¸æˆå¼€å‘å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
        "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
        "pinyin": "ç°ä»£æ¸¸æˆå¼€å‘é¢ä¸´åˆ›æ„å’Œæˆæœ¬æŒ‘æˆ˜ã€‚\nXiÃ ndÃ i yÃ³uxÃ¬ kÄifÄ miÃ nlÃ­n chuÃ ngyÃ¬ hÃ© chÃ©ngbÄ›n tiÇozhÃ n.\n\næ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆ›é€ é€¼çœŸä¸”äº’åŠ¨çš„è™šæ‹Ÿç¯å¢ƒã€‚\nXÄ«n de shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng kÄ›yÇ chuÃ ngzÃ o bÄ«zhÃ¨n qiÄ› hÃ¹dÃ²ng de xÅ«nÇ huÃ¡njÃ¬ng.\n\næˆ‘ä»¬æå‡ºäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ä½œä¸ºç”Ÿæˆæ¸¸æˆå¼•æ“ï¼ˆGGEï¼‰çš„åŸºç¡€ã€‚\nWÇ’men tÃ­chÅ« hÃ¹dÃ²ng shÄ“ngchÃ©ng shÃ¬pÃ­n (IGV) zuÃ²wÃ©i shÄ“ngchÃ©ng yÃ³uxÃ¬ yÇnqÃ­ng (GGE) de jÄ«chÇ”.\n\nGGEåˆ©ç”¨IGVçš„ä¼˜åŠ¿ï¼Œå¦‚æ— é™åˆ¶çš„é«˜è´¨é‡å†…å®¹ç”Ÿæˆå’Œç‰©ç†æ„è¯†å»ºæ¨¡ã€‚\nGGE lÃ¬yÃ²ng IGV de yÅushÃ¬, rÃº wÃºxiÃ nzhÃ¬ de gÄo zhÃ¬liÃ ng nÃ¨irÃ³ng shÄ“ngchÃ©ng hÃ© wÃ¹lÇ yÃ¬shÃ­ jiÃ nmÃ³.\n\næˆ‘ä»¬å±•ç¤ºäº†GGEçš„æ ¸å¿ƒæ¨¡å—å’Œåˆ†å±‚æˆç†Ÿåº¦è·¯çº¿å›¾ï¼ˆL0-L4ï¼‰ã€‚\nWÇ’men zhÇnshÃ¬le GGE de hÃ©xÄ«n mÃ³kuÃ i hÃ© fÄ“ncÃ©ng chÃ©ngshÃºdÃ¹ lÃ¹xiÃ ntÃº (L0-L4).\n\nè¿™é¡¹å·¥ä½œä¸ºAIæ—¶ä»£çš„æ¸¸æˆå¼€å‘å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚\nZhÃ¨ xiÃ ng gÅngzuÃ² wÃ¨i AI shÃ­dÃ i de yÃ³uxÃ¬ kÄifÄ kÄipÃ¬le xÄ«n tÃºjÃ¬ng.",
        "vocab": "[\n    {\"word\": \"ç°ä»£\", \"pinyin\": \"xiÃ ndÃ i\", \"trans\": \"modern\"},\n    {\"word\": \"é¢ä¸´\", \"pinyin\": \"miÃ nlÃ­n\", \"trans\": \"face\"},\n    {\"word\": \"åˆ›æ„\", \"pinyin\": \"chuÃ ngyÃ¬\", \"trans\": \"creativity\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ngbÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇozhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"é€¼çœŸ\", \"pinyin\": \"bÄ«zhÄ“n\", \"trans\": \"realistic\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹dÃ²ng\", \"trans\": \"interactive\"},\n    {\"word\": \"è™šæ‹Ÿ\", \"pinyin\": \"xÅ«nÇ\", \"trans\": \"virtual\"},\n    {\"word\": \"ç¯å¢ƒ\", \"pinyin\": \"huÃ¡njÃ¬ng\", \"trans\": \"environment\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"å¼•æ“\", \"pinyin\": \"yÇnqÃ­ng\", \"trans\": \"engine\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ«chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅushÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"æ— é™åˆ¶\", \"pinyin\": \"wÃºxiÃ nzhÃ¬\", \"trans\": \"unlimited\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨irÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"ç‰©ç†\", \"pinyin\": \"wÃ¹lÇ\", \"trans\": \"physical\"},\n    {\"word\": \"æ„è¯†\", \"pinyin\": \"yÃ¬shÃ­\", \"trans\": \"awareness\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ nmÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇnshÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ©xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³kuÃ i\", \"trans\": \"module\"},\n    {\"word\": \"åˆ†å±‚\", \"pinyin\": \"fÄ“ncÃ©ng\", \"trans\": \"layered\"},\n    {\"word\": \"æˆç†Ÿåº¦\", \"pinyin\": \"chÃ©ngshÃºdÃ¹\", \"trans\": \"maturity\"},\n    {\"word\": \"è·¯çº¿å›¾\", \"pinyin\": \"lÃ¹xiÃ ntÃº\", \"trans\": \"roadmap\"},\n    {\"word\": \"å¼€è¾Ÿ\", \"pinyin\": \"kÄipÃ¬\", \"trans\": \"open up\"},\n    {\"word\": \"é€”å¾„\", \"pinyin\": \"tÃºjÃ¬ng\", \"trans\": \"path\"},\n    {\"word\": \"æ—¶ä»£\", \"pinyin\": \"shÃ­dÃ i\", \"trans\": \"era\"}\n]",
        "trans": "Modern game development faces challenges in creativity and cost. New video generation models can create realistic and interactive virtual environments. We propose Interactive Generated Video (IGV) as the foundation for a Generative Game Engine (GGE). GGE leverages the advantages of IGV, such as unlimited high-quality content generation and physics-aware modeling. We demonstrate the core modules of GGE and a layered maturity roadmap (L0-L4). This work paves new ways for game development in the AI era.",
        "update_ts": "2025-03-25 09:11"
    }
}