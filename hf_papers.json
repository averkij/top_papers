{
    "date": {
        "ru": "11 апреля",
        "en": "April 11",
        "zh": "4月11日"
    },
    "time_utc": "2025-04-11 04:13",
    "weekday": 4,
    "issue_id": 3184,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.07956",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2504.07956",
            "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
            "score": 18,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "88860725e51f3629",
            "authors": [
                "Yukun Qi",
                "Yiming Zhao",
                "Yu Zeng",
                "Xikun Bao",
                "Wenxuan Huang",
                "Lin Chen",
                "Zehui Chen",
                "Jie Zhao",
                "Zhongang Qi",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Huawei Noahs Ark Lab",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07956.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VCR-Bench: новый стандарт оценки рассуждений ИИ по видео",
                    "desc": "В статье представлен новый бенчмарк VCR-Bench для оценки способностей больших визуально-языковых моделей к рассуждениям на основе цепочки мыслей при анализе видео. Бенчмарк включает 859 видео и 1034 пары вопрос-ответ с пошаговыми обоснованиями, помеченными для оценки восприятия и рассуждений. Эксперименты показали существенные ограничения современных моделей, особенно в обработке пространственно-временной информации. VCR-Bench призван стать стандартизированным инструментом для выявления недостатков в сложных задачах видеоанализа."
                },
                "en": {
                    "title": "VCR-Bench: Evaluating Video Reasoning in LVLMs",
                    "desc": "This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area."
                },
                "zh": {
                    "title": "VCR-Bench：视频推理的新标准",
                    "desc": "链式思维（CoT）推理的进步显著提升了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的能力。然而，目前缺乏一个严格的视频CoT推理评估框架，现有的视频基准无法充分评估推理过程。为此，我们提出了VCR-Bench，这是一个新颖的基准，旨在全面评估LVLMs在视频链式思维推理方面的能力。通过859个视频和1034对高质量问答对，VCR-Bench为每个问答对提供了逐步的CoT推理依据，揭示了当前LVLMs在复杂视频推理中的关键瓶颈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07128",
            "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.07128",
            "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
            "score": 14,
            "issue_id": 3184,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "6317a88ae7643fe2",
            "authors": [
                "Sara Vera Marjanović",
                "Arkil Patel",
                "Vaibhav Adlakha",
                "Milad Aghajohari",
                "Parishad BehnamGhader",
                "Mehar Bhatia",
                "Aditi Khandelwal",
                "Austin Kraft",
                "Benno Krojer",
                "Xing Han Lù",
                "Nicholas Meade",
                "Dongchan Shin",
                "Amirhossein Kazemnejad",
                "Gaurav Kamath",
                "Marius Mosbach",
                "Karolina Stańczak",
                "Siva Reddy"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07128.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#ethics",
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DeepSeek-R1: мышление машин через призму рассуждений",
                    "desc": "Статья описывает новую модель машинного обучения DeepSeek-R1, которая использует многоступенчатые цепочки рассуждений для решения сложных задач. Исследователи анализируют влияние длины рассуждений, управление длинными контекстами, культурные и этические аспекты, а также сравнивают модель с когнитивными процессами человека. Результаты показывают, что у DeepSeek-R1 есть оптимальное время рассуждения, при превышении которого производительность модели может ухудшаться. Также отмечается тенденция модели застревать на ранее исследованных формулировках проблем и уязвимости в плане безопасности по сравнению с моделями без рассуждений."
                },
                "en": {
                    "title": "DeepSeek-R1: Revolutionizing Reasoning in Language Models",
                    "desc": "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."
                },
                "zh": {
                    "title": "深度推理，思维的未来",
                    "desc": "DeepSeek-R1是一种大型推理模型，标志着大语言模型（LLM）在处理复杂问题时的根本转变。它通过创建详细的多步骤推理链来“思考”问题，而不是直接给出答案。这种推理过程对用户是公开的，为研究模型的推理行为提供了无限可能，并开启了思维学（Thoughtology）领域。我们的分析显示，DeepSeek-R1在推理时存在一个“甜蜜点”，过长的推理时间可能会影响模型的表现，同时它在处理已探索的问题时容易陷入反复思考，影响进一步的探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07943",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "url": "https://huggingface.co/papers/2504.07943",
            "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
            "score": 13,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "4cc1401ee10a171e",
            "authors": [
                "Yunhan Yang",
                "Yuan-Chen Guo",
                "Yukun Huang",
                "Zi-Xin Zou",
                "Zhipeng Yu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Xihui Liu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07943.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Революция в 3D-сегментации: видеть невидимое",
                    "desc": "Статья представляет новый подход к амодальной сегментации трехмерных частей объектов. Авторы предлагают двухэтапный метод: сначала используется существующая сегментация 3D-частей для получения начальных неполных сегментов, затем применяется новая модель HoloPart на основе диффузии для завершения этих сегментов в полные 3D-части. HoloPart использует специализированную архитектуру с локальным и глобальным вниманием для захвата геометрии частей и обеспечения общей согласованности формы. Результаты показывают, что предложенный метод значительно превосходит современные методы завершения форм и открывает новые возможности для приложений в редактировании геометрии, анимации и назначении материалов."
                },
                "en": {
                    "title": "Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation",
                    "desc": "This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "突破3D分割：HoloPart模型的创新之路",
                    "desc": "本文提出了一种新的3D部分无模态分割任务，旨在将3D形状分解为完整且具有语义意义的部分，即使在被遮挡的情况下也能实现。现有的3D部分分割方法仅能识别可见的表面，限制了其应用。我们提出了一种实用的两阶段方法，首先利用现有的3D部分分割获取初步的不完整部分，然后引入HoloPart模型，通过扩散方法完成这些部分。HoloPart采用了专门的架构，结合局部注意力和全局形状一致性，显著提升了3D部分无模态分割的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07960",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "url": "https://huggingface.co/papers/2504.07960",
            "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
            "score": 11,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a8b5331ac40d6a3d",
            "authors": [
                "Zhong-Yu Li",
                "Ruoyi Du",
                "Juncheng Yan",
                "Le Zhuo",
                "Zhen Li",
                "Peng Gao",
                "Zhanyu Ma",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07960.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#transfer_learning",
                    "#diffusion",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "VisualCloze: универсальная генерация изображений с визуальным контекстным обучением",
                    "desc": "VisualCloze - это универсальная система генерации изображений, использующая визуальное обучение в контексте для идентификации задач. Она поддерживает широкий спектр задач, включая генерализацию на новые задачи и обратную генерацию. Система использует Graph200K - графовую структуру данных для создания взаимосвязанных задач и улучшения переноса знаний. VisualCloze объединяет генерацию изображений с их дополнением, используя сильные генеративные приоры предобученных моделей."
                },
                "en": {
                    "title": "VisualCloze: Bridging Tasks with Visual Learning in Image Generation",
                    "desc": "This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance."
                },
                "zh": {
                    "title": "VisualCloze：通用图像生成的新框架",
                    "desc": "本论文介绍了一种名为VisualCloze的通用图像生成框架，旨在解决当前图像生成任务中存在的效率和通用性问题。与传统依赖语言指令的方法不同，VisualCloze通过视觉示例进行任务识别，从而减少了任务模糊性和提高了泛化能力。为了增强任务之间的可转移知识，我们引入了Graph200K数据集，该数据集通过图结构建立了多种相关任务。最后，我们发现我们的图像生成方法与图像填充具有一致的目标，从而能够利用预训练填充模型的强生成先验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07957",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "url": "https://huggingface.co/papers/2504.07957",
            "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.",
            "score": 8,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "dbfc6cfeb60e05d7",
            "authors": [
                "Shengyuan Ding",
                "Shenxi Wu",
                "Xiangyu Zhao",
                "Yuhang Zang",
                "Haodong Duan",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07957.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение способности мультимодальных ИИ-моделей следовать инструкциям",
                    "desc": "Статья представляет MM-IFEngine - пайплайн для создания высококачественных пар изображение-инструкция для обучения мультимодальных языковых моделей. На основе этого пайплайна созданы наборы данных MM-IFInstruct-23k и MM-IFDPO-23k для обучения с учителем и прямой оптимизации предпочтений соответственно. Также предложен бенчмарк MM-IFEval для оценки способности моделей следовать сложным мультимодальным инструкциям. Эксперименты показали значительное улучшение результатов на различных бенчмарках после дообучения моделей на созданных наборах данных."
                },
                "en": {
                    "title": "Enhancing Instruction Following in MLLMs with MM-IFEngine",
                    "desc": "This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks."
                },
                "zh": {
                    "title": "提升多模态指令跟随能力的创新方法",
                    "desc": "这篇论文介绍了一种新的多模态指令跟随能力评估方法，称为MM-IFEngine。该方法生成高质量的图像-指令对，创建了一个大规模的训练数据集MM-IFInstruct-23k，适用于监督微调和直接偏好优化。论文还提出了MM-IFEval，一个具有挑战性的多模态基准，包含输出响应和输入图像的约束。通过实验，微调后的多模态大语言模型在多个基准测试中表现出显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07830",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2504.07830",
            "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
            "score": 8,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "85dbaddf009300e0",
            "authors": [
                "Genglin Liu",
                "Salman Rahman",
                "Elisa Kreiss",
                "Marzyeh Ghassemi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#graphs",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Цифровое общество под микроскопом ИИ",
                    "desc": "MOSAIC - это новая система моделирования социальных сетей с открытым исходным кодом, использующая языковые модели для имитации поведения пользователей. Она сочетает агентов на основе больших языковых моделей с направленным социальным графом для анализа поведения, связанного с обманом. Система позволяет проводить многоагентные симуляции, моделирующие распространение контента и динамику взаимодействия пользователей. Исследователи оценили три стратегии модерации контента и обнаружили, что они снижают распространение недостоверной информации и повышают вовлеченность пользователей."
                },
                "en": {
                    "title": "MOSAIC: Simulating Social Networks to Combat Misinformation",
                    "desc": "The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction."
                },
                "zh": {
                    "title": "MOSAIC：社交网络行为模拟与内容审核新探索",
                    "desc": "我们提出了一种新颖的开源社交网络模拟框架MOSAIC，利用生成语言代理预测用户行为，如点赞、分享和标记内容。该模拟结合了大型语言模型（LLM）代理和有向社交图，分析新出现的欺骗行为，帮助理解用户如何判断在线社交内容的真实性。通过构建多样化的细粒度用户画像，我们的系统支持大规模的多代理模拟，模拟内容传播和用户参与的动态。我们评估了三种不同的内容审核策略，发现它们不仅能减缓虚假信息的传播，还能提高用户参与度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07964",
            "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
            "url": "https://huggingface.co/papers/2504.07964",
            "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.",
            "score": 6,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "1f6d28b26eec9879",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07964.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация путей экспертов для повышения эффективности языковых моделей",
                    "desc": "Исследование показывает, что модели языка на основе смеси экспертов (MoE LLM) имеют значительный потенциал для улучшения точности на 10-20%. Авторы разработали новый метод оптимизации C3PO, который переоценивает веса экспертов во время вывода для каждого тестового примера. Метод использует суррогатную целевую функцию, основанную на 'успешных соседях' из эталонного набора. Применение C3PO к современным MoE LLM показало улучшение точности на 7-15% и превзошло базовые методы обучения во время тестирования."
                },
                "en": {
                    "title": "Optimize Expert Pathways for Better Performance!",
                    "desc": "This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs."
                },
                "zh": {
                    "title": "优化混合专家模型的关键路径",
                    "desc": "混合专家（MoE）的大型语言模型（LLMs）在专家路径选择上存在显著的优化不足。我们的研究发现，简单的专家选择方法在预训练阶段会导致10-20%的准确率差距。为了解决这个问题，我们提出了一种新的测试时优化方法，通过对每个测试样本的不同层次的专家进行重新加权或“重新混合”。这种方法称为“关键层、核心专家、协作路径优化（C3PO）”，在多个基准测试中显示出显著的准确率提升，并且在计算效率上优于传统的学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07934",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "url": "https://huggingface.co/papers/2504.07934",
            "abstract": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.",
            "score": 5,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "419fda5d0a4bcadf",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Hongjin Lu",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07934.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Меньше данных, больше мышления: революция в визуальном рассуждении ИИ",
                    "desc": "В этой статье представлен эффективный метод улучшения визуального рассуждения с использованием значительно меньшего количества обучающих примеров, основанный на самосовершенствовании без передачи знаний. Ключевой идеей является важность сложности обучающих данных при тонкой настройке с подкреплением (RFT). Авторы предлагают новый способ использования поиска по дереву Монте-Карло (MCTS) для количественной оценки сложности примеров и эффективной фильтрации данных. Разработанная модель ThinkLite-VL, обученная на отфильтрованном наборе из 11 тысяч примеров, превосходит существующие модели визуально-языкового рассуждения уровня 7B и достигает лучших результатов на ряде бенчмарков."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection",
                    "desc": "This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "用少量样本提升视觉推理能力",
                    "desc": "本文提出了一种有效的方法，通过自我改进来增强视觉推理，且所需的训练样本显著减少，不依赖知识蒸馏。我们发现，在强化微调（RFT）过程中，训练数据的难度至关重要，适当具有挑战性的样本可以显著提升推理能力。我们提出了一种新颖的方式，利用蒙特卡洛树搜索（MCTS）来量化样本的难度，从而实现有效的数据筛选。最终，我们的模型ThinkLite-VL在八个基准测试中表现出色，使用仅11k个训练样本，平均性能提升了7%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04974",
            "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2504.04974",
            "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.",
            "score": 1,
            "issue_id": 3184,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "f382ad07e678a558",
            "authors": [
                "Ming Li",
                "Ruiyi Zhang",
                "Jian Chen",
                "Jiuxiang Gu",
                "Yufan Zhou",
                "Franck Dernoncourt",
                "Wanrong Zhu",
                "Tianyi Zhou",
                "Tong Sun"
            ],
            "affiliations": [
                "Adobe Research",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04974.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Улучшение понимания текста на изображениях документов для мультимодальных ИИ",
                    "desc": "Статья представляет новую задачу TRIG для оценки и улучшения способностей мультимодальных языковых моделей (MLLM) в работе с текстом на изображениях документов. Авторы создали набор данных из 800 вручную размеченных пар вопрос-ответ и 90 тысяч синтетических примеров для обучения моделей. Оценка существующих MLLM на этом бенчмарке выявила значительные ограничения в их способности к пространственному рассуждению и привязке к тексту на изображениях. Предложены два метода для улучшения этих способностей: обучение на инструкциях и эффективное встраивание plug-and-play."
                },
                "en": {
                    "title": "Enhancing MLLMs for Text-Rich Image Grounding",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."
                },
                "zh": {
                    "title": "提升文档图像的文本定位能力",
                    "desc": "尽管多模态大型语言模型（MLLMs）已经取得了一定进展，但在视觉文本定位方面仍然存在显著的局限性，尤其是在文本丰富的文档图像中。文档图像如扫描表单和信息图表由于其复杂的布局和文本内容，带来了重大挑战。为了解决这一问题，我们提出了TRIG任务，并设计了一个新的指令数据集，以评估和提升MLLMs在文档问答中的文本丰富图像定位能力。通过对MLLMs进行微调，我们的方法在空间推理和定位能力上显示出显著的改进。"
                }
            }
        }
    ],
    "link_prev": "2025-04-10.html",
    "link_next": "2025-04-14.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4月10日"
    },
    "short_date_next": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。",
        "title": "DDT: Decoupled Diffusion Transformer",
        "pinyin": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。\n\nkuò sàn biàn yā qì zài shēng chéng zhì liàng shàng biǎo xiàn chū sè, dàn xū yào gèng cháng de xùn liàn dié dǎi hé duō cì tuī lǐ bù zhòu. měi gè qù zào bù zhòu zhōng, kuò sàn biàn yā qì biān mǎ shēng yīn yùn tǐ yǐ tí qǔ dī pín yǔ yì chéng fēn, rán hòu jiě mǎ gāo pín chéng fēn. zhè zhǒng fāng àn dǎo zhì yòu huà kùn jìng: biān mǎ dī pín yǔ yì xū yào jiǎn shǎo gāo pín chéng fēn, zào chéng yǔ yì biān mǎ hé gāo pín jiě mǎ zhī jiān de máo dùn. wǒ men tí chū yī zhǒng xīn de jiě kǒu kuò sàn biàn yā qì (DDT), jù yǒu zhuān mén de tiáo jiàn biān mǎ qì hé sù dù jiě mǎ qì. shí yàn shì zhù, gèng dà de biān mǎ qì suí zhě mó xíng guī mó zēng jiā tí gāo xíng néng. zài ImageNet 256x256 shàng, DDT-XL/2 dá dào 1.31 FID (xùn liàn shōu liǎn sù dù jìn hū kuài 4 bèi); zài ImageNet 512x512 shàng, dá dào 1.28 FID. cǐ wài, jiě kǒu jià gòu tí gāo tuī lǐ sù dù, yǔn xǔ xiāng lín qù zào bù zhòu jiān gòng xiǎng zì wǒ tiáo jiàn. wǒ men tí chū yī zhǒng xīn de tǒng jì dòng tài guī huà fāng fǎ lái zuì shǎo huà xíng néng xià jiàng.",
        "vocab": "[{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'},\n{'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iteration'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '去噪', 'pinyin': 'qù zào', 'trans': 'denoising'},\n{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encode'},\n{'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'},\n{'word': '成分', 'pinyin': 'chéng fèn', 'trans': 'component'},\n{'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '困境', 'pinyin': 'kùn jìng', 'trans': 'dilemma'},\n{'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'},\n{'word': '矛盾', 'pinyin': 'máo dùn', 'trans': 'contradiction'},\n{'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'},\n{'word': '条件', 'pinyin': 'tiāo jiàn', 'trans': 'condition'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '收敛', 'pinyin': 'shōu liǎn', 'trans': 'convergence'},\n{'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'},\n{'word': '共享', 'pinyin': 'gòng xiǎng', 'trans': 'share'},\n{'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '规划', 'pinyin': 'guī huà', 'trans': 'planning'},\n{'word': '最小化', 'pinyin': 'zuì xiǎo huà', 'trans': 'minimize'},\n{'word': '下降', 'pinyin': 'xià jiàng', 'trans': 'decrease'}]",
        "trans": "The diffusion transformer performs excellently in terms of generation quality but requires longer training iterations and multiple inference steps. In each denoising step, the diffusion transformer encodes noisy input to extract low-frequency semantic components and then decodes high-frequency components. This scheme leads to an optimization dilemma: encoding low-frequency semantics requires reducing high-frequency components, creating a conflict between semantic encoding and high-frequency decoding. We propose a new decoupled diffusion transformer (DDT) with dedicated conditional encoders and velocity decoders. Experiments show that larger encoders improve performance as the model scale increases. On ImageNet 256x256, DDT-XL/2 achieves 1.31 FID (with training convergence speed nearly 4 times faster); on ImageNet 512x512, it achieves 1.28 FID. Additionally, the decoupled architecture increases inference speed, allowing adjacent denoising steps to share self-conditioning. We propose a new statistical dynamic programming method to minimize performance degradation.",
        "update_ts": "2025-04-10 09:12"
    }
}