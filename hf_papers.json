{
    "date": {
        "ru": "13 марта",
        "en": "March 13",
        "zh": "3月13日"
    },
    "time_utc": "2025-03-13 06:15",
    "weekday": 3,
    "issue_id": 2680,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.09573",
            "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
            "url": "https://huggingface.co/papers/2503.09573",
            "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
            "score": 9,
            "issue_id": 2678,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "32f097e93cbf5f3a",
            "authors": [
                "Marianne Arriola",
                "Aaron Gokaslan",
                "Justin T Chiu",
                "Zhihan Yang",
                "Zhixuan Qi",
                "Jiaqi Han",
                "Subham Sekhar Sahoo",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Cohere, NY, USA",
                "Cornell Tech, NY, USA",
                "Stanford University, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09573.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Блочные диффузионные модели: лучшее из двух миров в языковом моделировании",
                    "desc": "Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования."
                },
                "en": {
                    "title": "Block Diffusion: The Future of Flexible Language Generation",
                    "desc": "This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths."
                },
                "zh": {
                    "title": "块扩散模型：灵活生成与高效推理的结合",
                    "desc": "扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09427",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "url": "https://huggingface.co/papers/2503.09427",
            "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "score": 1,
            "issue_id": 2677,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "491beb48064068d2",
            "authors": [
                "Yaorui Shi",
                "Jiaqi Yang",
                "Sihang Li",
                "Junfeng Fang",
                "Xiang Wang",
                "Zhiyuan Liu",
                "Yang Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09427.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#transfer_learning",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Единая модель для анализа клеток и текста",
                    "desc": "scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день."
                },
                "en": {
                    "title": "Bridging Cells and Text: The Power of scMMGPT",
                    "desc": "This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models."
                },
                "zh": {
                    "title": "单细胞多模态生成预训练变换器的创新应用",
                    "desc": "预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09419",
            "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
            "url": "https://huggingface.co/papers/2503.09419",
            "abstract": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM",
            "score": 0,
            "issue_id": 2680,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "1c497a991b18da6a",
            "authors": [
                "Yifan Zhou",
                "Zeqi Xiao",
                "Shuai Yang",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09419.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей",
                    "desc": "Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений."
                },
                "en": {
                    "title": "Achieving Consistency in Latent Diffusion Models with Shift-Equivariance",
                    "desc": "Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs."
                },
                "zh": {
                    "title": "提升潜在扩散模型的一致性",
                    "desc": "潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06955",
            "title": "Motion Anything: Any to Motion Generation",
            "url": "https://huggingface.co/papers/2503.06955",
            "abstract": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything",
            "score": 0,
            "issue_id": 2680,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "9199d2d99b75d862",
            "authors": [
                "Zeyu Zhang",
                "Yiran Wang",
                "Wei Mao",
                "Danning Li",
                "Rui Zhao",
                "Biao Wu",
                "Zirui Song",
                "Bohan Zhuang",
                "Ian Reid",
                "Richard Hartley"
            ],
            "affiliations": [
                "ANU",
                "Google",
                "JD.com",
                "MBZUAI",
                "McGill",
                "Tencent",
                "USYD",
                "UTS",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06955.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#synthetic",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Универсальная генерация движений с мультимодальным контролем",
                    "desc": "Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D."
                },
                "en": {
                    "title": "Revolutionizing Motion Generation with Multimodal Control",
                    "desc": "This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods."
                },
                "zh": {
                    "title": "多模态运动生成的新突破",
                    "desc": "本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。"
                }
            }
        }
    ],
    "link_prev": "2025-03-12.html",
    "link_next": "2025-03-14.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3月12日"
    },
    "short_date_next": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3月14日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了YuE，一种基于LLaMA2架构的开放基础模型。它能生成长达五分钟的音乐，保持歌词对齐、连贯的音乐结构和引人入胜的歌唱旋律。YuE通过多任务、多阶段的预训练方法实现这一点。它还能进行风格转换和双向生成。实验显示，YuE在音乐性和声乐灵活性上匹敌或超越一些专有系统。此外，YuE在音乐理解任务中也表现出色。",
        "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
        "pinyin": "这篇文章介绍了YuE，一种基于LLaMA2架构的开放基础模型。它能生成长达五分钟的音乐，保持歌词对齐、连贯的音乐结构和引人入胜的歌唱旋律。YuE通过多任务、多阶段的预训练方法实现这一点。它还能进行风格转换和双向生成。实验显示，YuE在音乐性和声乐灵活性上匹敌或超越一些专有系统。此外，YuE在音乐理解任务中也表现出色。\n\nZhè piān wénzhāng jièshào le YuE, yī zhǒng jīyú LLaMA2 jiàgòu de kāifàng jīchǔ móxíng. Tā néng shēngchéng cháng dá wǔ fēnzhōng de yīnyuè, bǎochí gēcí duìqí, liánhé de yīnyuè jiégòu hé yǐnrénrùshèng de gēchàng xuánlǜ. YuE tōngguò duō rènwù, duō jiēduàn de yùxùnliàn fāngfǎ shíxiàn zhè yīdiǎn. Tā hái néng jìnxíng fēnggé zhuǎnhuàn hé shuāngxiàng shēngchéng. Shíyàn xiǎnshì, YuE zài yīnyuèxìng hé shēngyuè línghuóxìng shàng pǐdí huò chāoyuè yīxiē zhuānyǒu xìtǒng. Cǐwài,YuE zài yīnyuè lǐjiě rènwù zhōng yě biǎoxiàn chūsè.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"开放\", \"pinyin\": \"kāi fàng\", \"trans\": \"open\"},\n    {\"word\": \"基础\", \"pinyin\": \"jī chǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"保持\", \"pinyin\": \"bǎo chí\", \"trans\": \"maintain\"},\n    {\"word\": \"对齐\", \"pinyin\": \"duì qí\", \"trans\": \"alignment\"},\n    {\"word\": \"连贯\", \"pinyin\": \"lián guàn\", \"trans\": \"coherent\"},\n    {\"word\": \"结构\", \"pinyin\": \"jié gòu\", \"trans\": \"structure\"},\n    {\"word\": \"引人入胜\", \"pinyin\": \"yǐn rén rù shèng\", \"trans\": \"fascinating\"},\n    {\"word\": \"旋律\", \"pinyin\": \"xuán lǜ\", \"trans\": \"melody\"},\n    {\"word\": \"多任务\", \"pinyin\": \"duō rèn wù\", \"trans\": \"multi-task\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiē duàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"风格\", \"pinyin\": \"fēng gé\", \"trans\": \"style\"},\n    {\"word\": \"转换\", \"pinyin\": \"zhuǎn huàn\", \"trans\": \"conversion\"},\n    {\"word\": \"双向\", \"pinyin\": \"shuāng xiàng\", \"trans\": \"bidirectional\"},\n    {\"word\": \"匹敌\", \"pinyin\": \"pǐ dí\", \"trans\": \"match\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"专有\", \"pinyin\": \"zhuān yǒu\", \"trans\": \"proprietary\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"此外\", \"pinyin\": \"cǐ wài\", \"trans\": \"moreover\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"}\n]",
        "trans": "This article introduces YuE, an open-source foundational model based on the LLaMA2 architecture. It can generate music up to five minutes long, maintaining lyric alignment, coherent musical structure, and captivating vocal melodies. YuE achieves this through a multi-task, multi-stage pre-training method. It is also capable of style transfer and bidirectional generation. Experiments show that YuE matches or surpasses some proprietary systems in terms of musicality and vocal flexibility. Additionally, YuE performs excellently in music understanding tasks.",
        "update_ts": "2025-03-12 09:11"
    }
}