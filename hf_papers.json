{
    "date": {
        "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 11",
        "zh": "3æœˆ11æ—¥"
    },
    "time_utc": "2025-03-11 02:17",
    "weekday": 1,
    "issue_id": 2630,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.06680",
            "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
            "url": "https://huggingface.co/papers/2503.06680",
            "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",
            "score": 4,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 9",
                "zh": "3æœˆ9æ—¥"
            },
            "hash": "4394ce17a18696a3",
            "authors": [
                "Wei Li",
                "Xin Zhang",
                "Zhongxin Guo",
                "Shaoguang Mao",
                "Wen Luo",
                "Guangyue Peng",
                "Yangyu Huang",
                "Houfeng Wang",
                "Scarlett Li"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06680.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "FEA-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "FEA-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑƒĞ»-Ñ€ĞµĞºĞ²ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ· 83 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ FEA-Bench, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ²."
                },
                "en": {
                    "title": "FEA-Bench: Evaluating Code Generation in Repositories",
                    "desc": "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."
                },
                "zh": {
                    "title": "è¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ–°åŸºå‡†ï¼šFEA-Bench",
                    "desc": "åœ¨ä»£ç ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå®ç°æ–°ç‰¹æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„åº”ç”¨ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ç¼ºä¹ä¸“é—¨è¯„ä¼°è¿™ä¸€èƒ½åŠ›çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FEA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç åº“ä¸­è¿›è¡Œå¢é‡å¼€å‘èƒ½åŠ›çš„åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨FEA-Benchä¸­çš„è¡¨ç°æ˜¾è‘—è¾ƒå·®ï¼Œçªæ˜¾äº†åœ¨ä»£ç åº“çº§åˆ«å¢é‡å¼€å‘ä¸­é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07027",
            "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2503.07027",
            "abstract": "Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.",
            "score": 3,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "1b1589f9873720c7",
            "authors": [
                "Yuxuan Zhang",
                "Yirui Yuan",
                "Yiren Song",
                "Haofan Wang",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Liblib AI",
                "National University of Singapore",
                "ShanghaiTech University",
                "Tiamat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07027.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "EasyControl: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyControl - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ KV-ĞºÑÑˆĞµĞ¼. EasyControl Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers",
                    "desc": "This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning."
                },
                "zh": {
                    "title": "é«˜æ•ˆçµæ´»çš„æ¡ä»¶ç”Ÿæˆæ¡†æ¶EasyControl",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEasyControlçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šé¦–å…ˆï¼Œè½»é‡çº§çš„æ¡ä»¶æ³¨å…¥LoRAæ¨¡å—å¯ä»¥ç‹¬ç«‹å¤„ç†æ¡ä»¶ä¿¡å·ï¼Œé¿å…ä¿®æ”¹åŸºç¡€æ¨¡å‹æƒé‡ï¼Œä»è€Œå®ç°ä¸å®šåˆ¶æ¨¡å‹çš„å…¼å®¹æ€§ã€‚å…¶æ¬¡ï¼Œä½ç½®æ„ŸçŸ¥è®­ç»ƒèŒƒå¼æ ‡å‡†åŒ–è¾“å…¥æ¡ä»¶ï¼Œä½¿å¾—ç”Ÿæˆä»»æ„é•¿å®½æ¯”å’Œçµæ´»åˆ†è¾¨ç‡çš„å›¾åƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œç»“åˆKVç¼“å­˜æŠ€æœ¯çš„å› æœæ³¨æ„æœºåˆ¶æ˜¾è‘—é™ä½äº†å›¾åƒåˆæˆçš„å»¶è¿Ÿï¼Œæå‡äº†æ•´ä½“æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07365",
            "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.07365",
            "abstract": "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "765d475b38d9f289",
            "authors": [
                "Fanqing Meng",
                "Lingxiao Du",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Quanfeng Lu",
                "Daocheng Fu",
                "Botian Shi",
                "Wenhai Wang",
                "Junjun He",
                "Kaipeng Zhang",
                "Ping Luo",
                "Yu Qiao",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07365.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MM-Eureka â€“ ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MM-Eureka!",
                    "desc": "MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning."
                },
                "zh": {
                    "title": "MM-Eurekaï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†MM-Eurekaï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒæˆåŠŸåœ°å°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚è™½ç„¶åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬é¢†åŸŸæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨å¤šæ¨¡æ€ç©ºé—´ä¸­é‡ç°äº†æ–‡æœ¬åŸºç¡€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢åŠ ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ— ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤è°ƒä¼˜å’Œé¢„è®­ç»ƒæ¨¡å‹éƒ½èƒ½é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä¸”æ•°æ®æ•ˆç‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06520",
            "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
            "url": "https://huggingface.co/papers/2503.06520",
            "abstract": "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 9",
                "zh": "3æœˆ9æ—¥"
            },
            "hash": "b21eb23a448d282e",
            "authors": [
                "Yuqi Liu",
                "Bohao Peng",
                "Zhisheng Zhong",
                "Zihao Yue",
                "Fanbin Lu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "RUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06520.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Seg-Zero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Seg-Zero-7B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 18% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ReasonSeg."
                },
                "en": {
                    "title": "Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization",
                    "desc": "The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models."
                },
                "zh": {
                    "title": "Seg-Zeroï¼šçªç ´æ€§æ¨ç†ä¸åˆ†å‰²çš„ç»“åˆ",
                    "desc": "ä¼ ç»Ÿçš„åˆ†å‰²æ¨ç†æ–¹æ³•ä¾èµ–äºå¸¦æœ‰ç±»åˆ«æ ‡ç­¾çš„ç›‘ç£å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg-Zeroï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è®¤çŸ¥å¼ºåŒ–æ¨å¯¼å‡ºæ˜ç¡®çš„æ¨ç†é“¾ã€‚Seg-Zeroå¼•å…¥äº†ä¸€ä¸ªè§£è€¦æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œæ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼Œå¹¶äº§ç”Ÿä½ç½®æç¤ºï¼Œéšåç”±åˆ†å‰²æ¨¡å‹ç”Ÿæˆç²¾ç¡®çš„åƒç´ çº§æ©ç ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒSeg-Zeroåœ¨æ²¡æœ‰æ˜ç¡®æ¨ç†æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºçªå‡ºçš„æµ‹è¯•æ—¶æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03499",
            "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
            "url": "https://huggingface.co/papers/2503.03499",
            "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "80ab6abd822f2976",
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Yuchen Zeng",
                "Minjae Lee",
                "Hyung Il Koo",
                "Nam Ik Cho"
            ],
            "affiliations": [
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03499.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…, ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ SSM. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State-offset Tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Fine-Tuning with State-Based Methods for SSMs",
                    "desc": "This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks."
                },
                "zh": {
                    "title": "åŸºäºçŠ¶æ€çš„å¾®è°ƒï¼šè¶…è¶Šæç¤ºçš„æ–¹æ³•",
                    "desc": "çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºå˜æ¢å™¨çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå‡è½»å…¶äºŒæ¬¡è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨SSMsä¸Šçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºåŸºäºçŠ¶æ€çš„æ–¹æ³•ï¼Œä½œä¸ºä¼˜äºåŸºäºæç¤ºçš„æ–¹æ³•çš„æ–°é€‰æ‹©ï¼Œè¿™äº›æ–¹æ³•ç›´æ¥è°ƒæ•´ä¸çŠ¶æ€ç›¸å…³çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºçŠ¶æ€çš„PEFTæ–¹æ³•ï¼šçŠ¶æ€åç§»å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç›´æ¥å½±å“å½“å‰çŠ¶æ€ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„é€‚åº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03511",
            "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
            "url": "https://huggingface.co/papers/2503.03511",
            "abstract": "Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.",
            "score": 0,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "ba660b9e0f676df1",
            "authors": [
                "Qingyu Fan",
                "Yinghao Cai",
                "Chao Li",
                "Wenzhe He",
                "Xudong Zheng",
                "Tao Lu",
                "Bin Liang",
                "Shuo Wang"
            ],
            "affiliations": [
                "Qiyuan Lab",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03511.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#agents",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "NeuGrasp: Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "NeuGrasp - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. NeuGrasp Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeuGrasp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "NeuGrasp: Mastering Grasping with Transparent and Specular Objects",
                    "desc": "This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments."
                },
                "zh": {
                    "title": "NeuGraspï¼šé€æ˜ç‰©ä½“æŠ“å–çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNeuGraspçš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é€æ˜å’Œé•œé¢ç‰©ä½“æŠ“å–ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èƒŒæ™¯å…ˆéªŒè¿›è¡Œææ–™æ— å…³çš„æŠ“å–æ£€æµ‹ï¼Œç»“åˆäº†å˜æ¢å™¨å’Œå…¨å±€å…ˆéªŒä½“ç§¯ï¼Œä»¥èšåˆå¤šè§†è§’ç‰¹å¾å¹¶è¿›è¡Œç©ºé—´ç¼–ç ã€‚NeuGraspé€šè¿‡æ®‹å·®ç‰¹å¾å¢å¼ºèšç„¦äºå‰æ™¯ç‰©ä½“ï¼Œå¹¶åˆ©ç”¨å ç”¨å…ˆéªŒä½“ç§¯æ¥æ”¹å–„ç©ºé—´æ„ŸçŸ¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é€æ˜å’Œé•œé¢è¡¨é¢çš„ç‰©ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuGraspåœ¨æŠ“å–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„é‡å»ºè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02199",
            "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
            "url": "https://huggingface.co/papers/2503.02199",
            "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.",
            "score": 0,
            "issue_id": 2630,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "a354f8de058f0f84",
            "authors": [
                "Ailin Deng",
                "Tri Cao",
                "Zhirui Chen",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02199.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ VLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ 'ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚', Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ°ÑĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ñƒ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞµĞ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Balancing Vision and Text: Overcoming Bias in Vision-Language Models",
                    "desc": "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."
                },
                "zh": {
                    "title": "å¹³è¡¡è®­ç»ƒï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹æ¨¡æ€ä¸ä¸€è‡´æ—¶çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æ¢è®¨äº†VLMsåœ¨è§†è§‰æ•°æ®å’Œä¸åŒæ–‡æœ¬è¾“å…¥ä¸‹çš„æ¨¡æ€åå¥½ï¼Œå‘ç°äº†â€œå¯¹æ–‡æœ¬çš„ç›²ç›®ä¿¡ä»»â€ç°è±¡ï¼šå½“å‡ºç°ä¸ä¸€è‡´æ—¶ï¼ŒVLMsè¿‡åº¦ä¾èµ–æ–‡æœ¬æ•°æ®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬åˆ†æäº†å½±å“è¿™ç§æ–‡æœ¬åè§çš„å› ç´ ï¼ŒåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€è¯­è¨€æ¨¡å‹å¤§å°ã€æ–‡æœ¬ç›¸å…³æ€§ã€æ ‡è®°é¡ºåºä»¥åŠè§†è§‰å’Œæ–‡æœ¬ç¡®å®šæ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¸¦æœ‰æ–‡æœ¬å¢å¼ºçš„ç›‘ç£å¾®è°ƒï¼Œå¹¶è¯æ˜å…¶åœ¨å‡å°‘æ–‡æœ¬åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-10.html",
    "link_next": "2025-03-12.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æœ€è¿‘äººç±»åå¥½å¯¹é½çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å…³é”®æ–¹æ³•æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹æŒ‡å¯¼åå¥½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒè§†è§‰åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè”åˆå­¦ä¹ è¯„ä¼°å¤šä»»åŠ¡å¯èƒ½äº§ç”ŸååŒæ•ˆåº”ï¼Œæ”¹å–„å›¾åƒç†è§£å’Œç”Ÿæˆè¯„ä¼°ï¼Œå¹¶é€šè¿‡æ›´å¥½çš„å¸§åˆ†ææå‡è§†é¢‘è¯„ä¼°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UnifiedRewardï¼Œç¬¬ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ã€‚",
        "title": "Unified Reward Model for Multimodal Understanding and Generation",
        "pinyin": "æœ€è¿‘äººç±»åå¥½å¯¹é½çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å…³é”®æ–¹æ³•æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹æŒ‡å¯¼åå¥½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒè§†è§‰åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè”åˆå­¦ä¹ è¯„ä¼°å¤šä»»åŠ¡å¯èƒ½äº§ç”ŸååŒæ•ˆåº”ï¼Œæ”¹å–„å›¾åƒç†è§£å’Œç”Ÿæˆè¯„ä¼°ï¼Œå¹¶é€šè¿‡æ›´å¥½çš„å¸§åˆ†ææå‡è§†é¢‘è¯„ä¼°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UnifiedRewardï¼Œç¬¬ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ã€‚\n\nZuÃ¬jÃ¬n rÃ©nlÃ¨i piÄnhÃ o duÃ¬qÃ­ de jÃ¬nzhÇn xiÇnzhÃ¹ tÃ­shÄ“ng le duÅ mÃ³shÃ¹ shÄ“ngchÃ©ng hÃ© lÇjiÄ›. GuÇnjiÃ n fÄngfÇ shÃ¬ xÃ¹nliÃ n jiÇnglÃ¬ mÃ³xÃ­ng zhÇdÇo piÄnhÃ o yÅuhuÃ . RÃ¡n'Ã©r, xiÃ nyÇ’u mÃ³xÃ­ng tÅngchÃ¡ng shÃ¬ rÃ¨nwÃ¹ tÃ¨dÃ¬ng de, xiÃ nzhÃ¬ le tÄmen zÃ i bÃ¹tÃ³ng shÃ¬juÄ“ yÃ¬ngyÃ²ng zhÅng de shÃ¬yÃ¬ngxÃ¬ng. WÇ’men rÃ¨nwÃ©i, liÃ¡nhÃ© xuÃ©xÃ­ pÃ­ngjiÃ  duÅ rÃ¨nwÃ¹ kÄ›nÃ©ng chÇnshÄ“ng xiÃ©tÃ³ng xiÃ oyÃ¬ng, gÇishÃ n tÃºxiÃ ng lÇjiÄ› hÃ© shÄ“ngchÃ©ng pÃ­ngjiÃ , bÃ¬ng tÅngguÃ² gÃ¨ng hÇo de zhÄ“n fÄ“nxi tÃ­shÄ“ng shÃ¬pÃ­n pÃ­ngjiÃ . YÄ«ncÇ, bÄ›nwÃ©n tÃ­chÅ« le UnifiedReward, dÃ¬-yÄ«gÃ¨ yÃ²ngyÃº duÅ mÃ³shÃ¹ lÇjiÄ› hÃ© shÄ“ngchÃ©ng pÃ­ngjiÃ  de tÇ’ngyÄ« jiÇnglÃ¬ mÃ³xÃ­ng.",
        "vocab": "[\n    {\"word\": \"åå¥½\", \"pinyin\": \"piÄn hÃ o\", \"trans\": \"preference\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improve\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"å¥–åŠ±\", \"pinyin\": \"jiÇng lÃ¬\", \"trans\": \"reward\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"æŒ‡å¯¼\", \"pinyin\": \"zhÇ dÇo\", \"trans\": \"guide\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨ dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"é™åˆ¶\", \"pinyin\": \"xiÃ n zhÃ¬\", \"trans\": \"limit\"},\n    {\"word\": \"é€‚åº”æ€§\", \"pinyin\": \"shÃ¬ yÃ¬ng xÃ¬ng\", \"trans\": \"adaptability\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"è”åˆ\", \"pinyin\": \"liÃ¡n hÃ©\", \"trans\": \"joint\"},\n    {\"word\": \"å­¦ä¹ \", \"pinyin\": \"xuÃ© xÃ­\", \"trans\": \"learning\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"ååŒ\", \"pinyin\": \"xiÃ© tÃ³ng\", \"trans\": \"synergy\"},\n    {\"word\": \"æ”¹å–„\", \"pinyin\": \"gÇi shÃ n\", \"trans\": \"improve\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"å¸§\", \"pinyin\": \"zhÄ“n\", \"trans\": \"frame\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"analysis\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unified\"},\n    {\"word\": \"ç¬¬ä¸€ä¸ª\", \"pinyin\": \"dÃ¬ yÄ« gÃ¨\", \"trans\": \"first\"}\n]",
        "trans": "Recent advancements in human preference alignment have significantly enhanced multimodal generation and understanding. The key method involves training reward models to guide preference optimization. However, existing models are typically task-specific, limiting their adaptability across different visual applications. We believe that jointly learning to evaluate multiple tasks may produce synergistic effects, improving image understanding and generation evaluation, and enhancing video evaluation through better frame analysis. Therefore, this paper introduces UnifiedReward, the first unified reward model for multimodal understanding and generation evaluation.",
        "update_ts": "2025-03-10 09:10"
    }
}