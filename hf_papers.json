{
    "date": {
        "ru": "26 марта",
        "en": "March 26",
        "zh": "3月26日"
    },
    "time_utc": "2025-03-26 13:21",
    "weekday": 2,
    "issue_id": 2907,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.19325",
            "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
            "url": "https://huggingface.co/papers/2503.19325",
            "abstract": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.",
            "score": 52,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "543c7dbfad83ed73",
            "authors": [
                "Yuchao Gu",
                "Weijia Mao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19325.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "FAR: эффективное моделирование длинных видеопоследовательностей",
                    "desc": "Статья представляет Frame AutoRegressive (FAR) - новый подход к авторегрессионному моделированию видео с длинным контекстом. Авторы вводят FlexRoPE - технику, позволяющую экстраполировать модель на контексты в 16 раз длиннее обучающих. Предлагается комбинировать моделирование краткосрочного и долгосрочного контекста для эффективной обработки длинных видеопоследовательностей. FAR демонстрирует наилучшие результаты в генерации как коротких, так и длинных видео."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Frame AutoRegressive Modeling",
                    "desc": "This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency."
                },
                "zh": {
                    "title": "长时间上下文视频生成的新突破",
                    "desc": "本文介绍了一种新的视频自回归建模方法，称为Frame AutoRegressive (FAR)，旨在解决长时间上下文视频生成中的挑战。FAR通过建模连续帧之间的时间因果关系，超越了传统的语言模型，取得了更好的收敛效果。为了应对视觉冗余和计算成本问题，本文提出了FlexRoPE技术，能够灵活调整远程上下文的时间衰减，并引入了长短期上下文建模方法，以确保时间一致性。实验结果表明，FAR在短视频和长视频生成任务中均达到了最先进的性能，成为视频自回归建模的有效基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18931",
            "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
            "url": "https://huggingface.co/papers/2503.18931",
            "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.",
            "score": 21,
            "issue_id": 2897,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "59128d6a0bd3862c",
            "authors": [
                "Yitong Chen",
                "Lingchen Meng",
                "Wujian Peng",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Shanghai Innovation Institute",
                "Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18931.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Универсальное мультимодальное дообучение для улучшения визуальных моделей",
                    "desc": "Статья представляет CoMP - новый метод мультимодального дообучения предобученных моделей компьютерного зрения (VFM). CoMP использует непрерывное ротационное позиционное кодирование для обработки изображений разного размера и функцию выравнивания для согласования визуальных и текстовых представлений. Трехэтапное обучение значительно улучшает результаты как в мультимодальном понимании, так и в задачах классификации и сегментации. Модель CoMP-SigLIP достигает высоких показателей на различных бенчмарках, сохраняя при этом хорошую точность на ImageNet-1K и ADE20K."
                },
                "en": {
                    "title": "Enhancing Visual Models with Multimodal Pre-Training",
                    "desc": "This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline."
                },
                "zh": {
                    "title": "提升视觉模型的多模态预训练方法",
                    "desc": "本文介绍了一种新的多模态预训练方法CoMP，用于提升视觉基础模型（VFM）的表现。通过持续的多模态预训练，模型能够处理不同大小的视觉输入，并生成与语言表示更一致的视觉表示。CoMP采用了持续旋转位置嵌入和视觉与文本特征之间的对齐损失，以实现多模态表示的对齐。经过三阶段训练，我们的模型在多模态理解和其他下游任务上都取得了显著的提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19385",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
            "url": "https://huggingface.co/papers/2503.19385",
            "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
            "score": 20,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "e0ead8fbe973f326",
            "authors": [
                "Jaihoon Kim",
                "Taehoon Yoon",
                "Jisung Hwang",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19385.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#video"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективное масштабирование потоковых моделей при выводе",
                    "desc": "Статья предлагает метод масштабирования во время вывода для предобученных потоковых моделей. Авторы вводят три ключевые идеи: генерация на основе стохастических дифференциальных уравнений (SDE), преобразование интерполянтов и адаптивное распределение вычислительных ресурсов. Эксперименты показывают, что генерация на основе SDE, особенно с сохранением дисперсии, улучшает производительность методов выборки частиц для масштабирования во время вывода в потоковых моделях. Предложенный подход превосходит предыдущие методы масштабирования во время вывода."
                },
                "en": {
                    "title": "Enhancing Flow Models with Efficient Inference-Time Scaling",
                    "desc": "This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos."
                },
                "zh": {
                    "title": "流模型的高效推理时间缩放新方法",
                    "desc": "本文提出了一种针对预训练流模型的推理时间缩放方法。近年来，推理时间缩放在大语言模型和扩散模型中受到广泛关注，通过利用额外的计算来提高样本质量或更好地符合用户偏好。尽管流模型作为扩散模型的替代方案越来越受欢迎，但由于其确定性生成过程，现有的扩散模型推理时间缩放方法无法直接应用于流模型。我们提出了三种关键思想，以实现流模型的高效推理时间缩放：基于SDE的生成、插值转换和自适应计算资源分配。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19622",
            "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
            "url": "https://huggingface.co/papers/2503.19622",
            "abstract": "The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.",
            "score": 19,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "c727aefeccdca380",
            "authors": [
                "Hongcheng Gao",
                "Jiashu Qu",
                "Jingyi Tang",
                "Baolong Bi",
                "Yue Liu",
                "Hongyu Chen",
                "Li Liang",
                "Li Su",
                "Qingming Huang"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS",
                "National University of Singapore",
                "University of Chinese Academy of Sciences",
                "University of Cincinnati"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19622.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#hallucinations",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Борьба с галлюцинациями в видеоанализе: новый бенчмарк и мыслящая модель",
                    "desc": "Статья представляет комплексный подход к изучению проблемы галлюцинаций в крупных мультимодальных моделях (LMM) при обработке видео. Авторы создали бенчмарк HAVEN для оценки галлюцинаций LMM в задачах понимания видео, охватывающий различные аспекты и форматы вопросов. Исследование включает анализ семи факторов, влияющих на галлюцинации, и эксперименты с 16 различными LMM. Предложена модель 'video-thinking' для снижения галлюцинаций с использованием методов SRFT и TDPO, показавшая значительное улучшение точности и снижение предвзятости."
                },
                "en": {
                    "title": "Mitigating Hallucinations in Video Understanding with HAVEN",
                    "desc": "This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods."
                },
                "zh": {
                    "title": "解决视频模态中的幻觉问题",
                    "desc": "本文研究了大型多模态模型（LMMs）在视频理解任务中的幻觉问题，这种问题使得模型的输出看似正确但实际上不准确。我们提出了一个名为HAVEN的基准，用于评估LMMs在视频模态下的幻觉，涵盖了幻觉的原因、方面和问题格式等三个维度，共包含6000个问题。通过对16个LMMs进行实验，我们定量分析了影响幻觉的7个因素，如视频时长、模型规模和推理能力。最后，我们提出了一种视频思维模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来减轻幻觉现象，实验结果显示该方法在准确性上提高了7.65%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14905",
            "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
            "url": "https://huggingface.co/papers/2503.14905",
            "abstract": "With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.",
            "score": 14,
            "issue_id": 2900,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "e8053458773c179b",
            "authors": [
                "Siwei Wen",
                "Junyan Ye",
                "Peilin Feng",
                "Hengrui Kang",
                "Zichen Wen",
                "Yize Chen",
                "Jiang Wu",
                "Wenjun Wu",
                "Conghui He",
                "Weijia Li"
            ],
            "affiliations": [
                "Beihang University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14905.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "FakeVLM: Интерпретируемое обнаружение синтетических изображений с помощью мультимодальной модели",
                    "desc": "FakeVLM - это специализированная большая мультимодальная модель для обнаружения синтетических изображений и DeepFake. Она не только различает реальные и поддельные изображения, но и предоставляет понятные объяснения артефактов на естественном языке. Авторы также представили набор данных FakeClue с более чем 100 000 изображений, аннотированных подробными подсказками об артефактах. FakeVLM демонстрирует производительность на уровне экспертных моделей, устанавливая новый стандарт в обнаружении синтетических изображений."
                },
                "en": {
                    "title": "FakeVLM: Unmasking Synthetic Images with Clarity",
                    "desc": "This paper introduces FakeVLM, a large multimodal model designed to detect synthetic images and DeepFakes while providing human-readable explanations for its decisions. Unlike existing methods, FakeVLM enhances interpretability by explaining image artifacts in natural language, making it easier for users to understand the detection process. The model is evaluated on a new dataset called FakeClue, which contains over 100,000 annotated images, allowing for fine-grained analysis of image authenticity. Results show that FakeVLM performs comparably to expert models without needing extra classifiers, establishing a new standard in synthetic image detection."
                },
                "zh": {
                    "title": "FakeVLM：合成图像检测的新标杆",
                    "desc": "随着人工智能生成内容（AIGC）技术的快速发展，合成图像在日常生活中变得越来越普遍，这给真实性评估和检测带来了新的挑战。现有的方法虽然在评估图像真实性和定位伪造方面有效，但往往缺乏人类可解释性，无法完全应对合成数据日益复杂的情况。为了解决这些问题，我们提出了FakeVLM，这是一种专门针对合成图像和深度伪造检测任务的大型多模态模型。FakeVLM不仅在区分真实与伪造图像方面表现出色，还能提供清晰的自然语言解释，增强了可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19903",
            "title": "Scaling Vision Pre-Training to 4K Resolution",
            "url": "https://huggingface.co/papers/2503.19903",
            "abstract": "High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.",
            "score": 11,
            "issue_id": 2898,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "7b81adffd1f39557",
            "authors": [
                "Baifeng Shi",
                "Boyi Li",
                "Han Cai",
                "Yao Lu",
                "Sifei Liu",
                "Marco Pavone",
                "Jan Kautz",
                "Song Han",
                "Trevor Darrell",
                "Pavlo Molchanov",
                "Hongxu Yin"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19903.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "PS3: Эффективное предобучение для восприятия изображений сверхвысокого разрешения",
                    "desc": "PS3 - это новый метод предобучения моделей компьютерного зрения, позволяющий работать с изображениями сверхвысокого разрешения (4K) при почти постоянных вычислительных затратах. Вместо контрастивного обучения на глобальном представлении изображения, PS3 обрабатывает выборочно локальные области и сопоставляет их с детальными текстовыми описаниями. Применение PS3 в мультимодальных языковых моделях (MLLM) значительно улучшает восприятие деталей изображений высокого разрешения по сравнению с базовыми моделями. PS3 также позволяет масштабировать разрешение и вычислительные ресурсы без дополнительных затрат, что приводит к повышению производительности."
                },
                "en": {
                    "title": "Scaling Vision Pre-Training to 4K with PS3",
                    "desc": "This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models."
                },
                "zh": {
                    "title": "高分辨率视觉感知的新突破",
                    "desc": "本论文介绍了一种名为PS3的视觉预训练方法，能够以接近恒定的成本将CLIP风格的视觉预训练扩展到4K分辨率。PS3通过选择性处理局部区域并与局部详细描述进行对比，来实现高分辨率表示学习，从而大幅降低计算开销。预训练后的PS3能够在低分辨率下编码全局图像，并根据文本提示的显著性或相关性选择性处理局部高分辨率区域。最终，基于PS3的多模态大语言模型VILA-HD在多个基准测试中显著提升了高分辨率视觉感知能力，超越了现有的最先进模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19855",
            "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
            "url": "https://huggingface.co/papers/2503.19855",
            "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.",
            "score": 7,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "c9d16e0d2423104a",
            "authors": [
                "Xiaoyu Tian",
                "Sitong Zhao",
                "Haotian Wang",
                "Shuaiting Chen",
                "Yunjie Ji",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.19855.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Итеративное улучшение ответов ИИ: простой путь к повышению точности",
                    "desc": "Статья представляет новый подход к масштабированию языковых моделей во время тестирования, называемый 'Многораундовое мышление'. Этот метод итеративно улучшает рассуждения модели, используя предыдущие ответы в качестве подсказок для последующих раундов. Эксперименты с различными моделями, включая QwQ-32B и DeepSeek-R1, показали стабильное улучшение производительности на нескольких бенчмарках. Результаты подтверждают, что 'Многораундовое мышление' - это широко применимый подход для повышения эффективности языковых моделей."
                },
                "en": {
                    "title": "Enhancing Model Performance with Multi-round Thinking",
                    "desc": "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."
                },
                "zh": {
                    "title": "多轮思考：提升模型推理的有效方法",
                    "desc": "本文介绍了一种名为多轮思考的测试时间扩展方法，旨在提高大型语言模型的推理能力。该方法通过将之前的答案作为后续轮次的提示，迭代地优化模型的推理过程。实验结果表明，使用多轮思考后，多个模型在不同基准测试上的表现均有显著提升。比如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19910",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2503.19910",
            "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.",
            "score": 6,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "63f36082e6c27f3e",
            "authors": [
                "Chuong Huynh",
                "Jinyu Yang",
                "Ashish Tawari",
                "Mubarak Shah",
                "Son Tran",
                "Raffay Hamid",
                "Trishul Chilimbi",
                "Abhinav Shrivastava"
            ],
            "affiliations": [
                "Amazon",
                "Center for Research in Computer Vision, University of Central Florida",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "CoLLM: Революция в композиционном поиске изображений",
                    "desc": "Статья представляет CoLLM - новый фреймворк для решения задачи композиционного поиска изображений (CIR). CoLLM генерирует обучающие триплеты на лету из пар изображение-подпись, что позволяет обучаться без ручной разметки. Авторы используют большие языковые модели для создания совместных эмбеддингов изображений и текстов модификации, улучшая мультимодальное слияние. Также представлен новый крупномасштабный датасет MTCIR и уточнены существующие бенчмарки для более надежной оценки моделей CIR."
                },
                "en": {
                    "title": "Revolutionizing Composed Image Retrieval with CoLLM",
                    "desc": "This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks."
                },
                "zh": {
                    "title": "CoLLM：复合图像检索的新突破",
                    "desc": "本文介绍了一种名为CoLLM的框架，用于解决复合图像检索（CIR）中的数据稀缺问题。该框架通过从图像-文本对中动态生成三元组，避免了手动标注的需求，从而实现了监督学习。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进了多模态的深度融合。此外，我们还推出了一个包含340万样本的大规模数据集MTCIR，并改进了现有的CIR基准，以提高评估的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13964",
            "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
            "url": "https://huggingface.co/papers/2503.13964",
            "abstract": "Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.",
            "score": 5,
            "issue_id": 2900,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "ac3677766b7897a0",
            "authors": [
                "Siwei Han",
                "Peng Xia",
                "Ruiyi Zhang",
                "Tong Sun",
                "Yun Li",
                "Hongtu Zhu",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "Adobe Research",
                "UNC-Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13964.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#rag"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мультиагентный подход к пониманию документов: объединяя текст и изображения",
                    "desc": "Статья представляет MDocAgent - новую мультимодальную мультиагентную систему для понимания документов. В отличие от существующих методов, использующих большие языковые модели или RAG, MDocAgent эффективно интегрирует текстовую и визуальную информацию. Система использует пять специализированных агентов для многомодального поиска контекста и синтеза информации. Эксперименты показали улучшение точности ответов на вопросы в среднем на 12.1% по сравнению с современными методами."
                },
                "en": {
                    "title": "MDocAgent: Uniting Text and Images for Smarter Document Understanding",
                    "desc": "This paper introduces MDocAgent, a new framework designed for Document Question Answering (DocQA) that effectively combines text and image data. Unlike existing methods that often focus on a single type of information, MDocAgent utilizes a multi-agent system with five specialized agents to enhance multi-modal reasoning. These agents work together to retrieve and synthesize information from both textual and visual sources, improving the accuracy of answers to questions about documents. Preliminary tests show that MDocAgent outperforms current leading methods by an average of 12.1%, demonstrating its potential for better handling complex real-world documents."
                },
                "zh": {
                    "title": "多模态文档理解的新突破",
                    "desc": "本文介绍了一种新的文档问答系统MDocAgent，它结合了文本和图像信息，旨在提高多模态文档理解的能力。现有的方法往往只关注单一模态，导致在复杂的多模态推理中表现不佳。MDocAgent采用了五个专门的代理，分别负责不同的任务，通过协作检索多模态上下文，从而更全面地理解文档内容。实验结果表明，MDocAgent在多个基准测试中表现优异，平均提高了12.1%的准确率，展示了其在处理现实世界文档中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18446",
            "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
            "url": "https://huggingface.co/papers/2503.18446",
            "abstract": "In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.",
            "score": 4,
            "issue_id": 2897,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "d3e203fb399d6eee",
            "authors": [
                "Jinho Jeong",
                "Sangmin Han",
                "Jinwoo Kim",
                "Seon Joo Kim"
            ],
            "affiliations": [
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18446.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#3d",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "LSRNA: Суперразрешение в латентном пространстве для генерации детализированных изображений",
                    "desc": "LSRNA - это новый фреймворк для генерации изображений высокого разрешения (более 1K) с использованием диффузионных моделей. Он решает проблемы существующих методов, которые часто приводят к искажениям структуры или повторению контента при масштабировании. LSRNA сочетает суперразрешение в латентном пространстве (LSR) для выравнивания многообразия и добавление шума по регионам (RNA) для улучшения высокочастотных деталей. Эксперименты показывают, что LSRNA превосходит современные методы на основе референсов по различным разрешениям и метрикам."
                },
                "en": {
                    "title": "Enhancing High-Resolution Image Generation with LSRNA",
                    "desc": "This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality."
                },
                "zh": {
                    "title": "LSRNA：超分辨率生成的创新框架",
                    "desc": "本文提出了一种新颖的框架LSRNA，用于生成高分辨率（超过1K）的图像，利用扩散模型直接在潜在空间中进行超分辨率处理。现有的扩散模型在超出训练分辨率时常常出现结构失真或内容重复的问题。参考基础的方法通过将低分辨率参考图像上采样来指导高分辨率生成，但在潜在空间中上采样会导致流形偏差，从而降低输出质量。LSRNA结合了潜在空间超分辨率（LSR）和区域噪声添加（RNA），有效提升了高频细节，实验结果表明其在各个分辨率和指标上均优于现有的参考基础方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19470",
            "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2503.19470",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.",
            "score": 3,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "3e50fa3f4f4a6c0c",
            "authors": [
                "Mingyang Chen",
                "Tianpeng Li",
                "Haoze Sun",
                "Yijie Zhou",
                "Chenzheng Zhu",
                "Fan Yang",
                "Zenan Zhou",
                "Weipeng Chen",
                "Haofen Wang",
                "Jeff Z. Pan",
                "Wen Zhang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "The University of Edinburgh",
                "Tongji University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19470.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#rag",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Усиление рассуждений ИИ через интеграцию поиска",
                    "desc": "Авторы предлагают новый фреймворк ReSearch, который обучает большие языковые модели (LLM) рассуждать с использованием поиска через обучение с подкреплением. Модель учится интегрировать операции поиска в цепочку рассуждений, где текстовое мышление направляет, когда и как выполнять поиск. Эксперименты показывают, что, несмотря на обучение только на одном наборе данных, модели демонстрируют сильную обобщаемость на различных бенчмарках. Анализ выявляет, что ReSearch естественным образом вызывает продвинутые способности рассуждения, такие как рефлексия и самокоррекция."
                },
                "en": {
                    "title": "Empowering LLMs: Reasoning Meets Search with ReSearch",
                    "desc": "This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering."
                },
                "zh": {
                    "title": "推理与搜索的完美结合",
                    "desc": "大型语言模型（LLMs）在推理方面表现出色，但将推理与外部搜索过程结合仍然具有挑战性，尤其是对于复杂的多跳问题。我们提出了ReSearch，一个新颖的框架，通过强化学习训练LLMs进行搜索推理，而不使用任何监督数据。该方法将搜索操作视为推理链的核心部分，搜索的时机和方式由基于文本的思维指导，搜索结果进一步影响推理过程。我们的实验表明，尽管只在一个数据集上训练，ReSearch模型在多个基准测试中展现出强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19041",
            "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
            "url": "https://huggingface.co/papers/2503.19041",
            "abstract": "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "8495b5a09ddf5611",
            "authors": [
                "Kangwei Liu",
                "Mengru Wang",
                "Yujie Luo",
                "Lin Yuan",
                "Mengshu Sun",
                "Ningyu Zhang",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19041.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#low_resource"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Безопасная адаптация языковых моделей с сохранением производительности",
                    "desc": "Статья представляет новый метод тонкой настройки больших языковых моделей под названием LookAhead Tuning. Этот подход позволяет адаптировать модели к конкретным доменам, сохраняя при этом их изначальную безопасность. Метод основан на модификации обучающих данных путем предварительного просмотра частичных префиксов ответов. Эксперименты показывают, что LookAhead Tuning эффективно поддерживает безопасность модели без ущерба для производительности на целевых задачах."
                },
                "en": {
                    "title": "LookAhead Tuning: Safeguarding LLMs During Fine-Tuning",
                    "desc": "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."
                },
                "zh": {
                    "title": "LookAhead Tuning：安全微调大型语言模型的新方法",
                    "desc": "本论文介绍了一种名为LookAhead Tuning的技术，旨在解决在微调大型语言模型（LLMs）时安全性下降的问题。该方法通过预览部分答案前缀，采用两种简单且低资源的数据驱动方法来修改训练数据。其目标是通过最小化初始标记分布的扰动，保持模型固有的安全机制。实验结果表明，LookAhead Tuning能够有效维护模型的安全性，同时在下游任务中保持强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18783",
            "title": "Frequency Dynamic Convolution for Dense Image Prediction",
            "url": "https://huggingface.co/papers/2503.18783",
            "abstract": "",
            "score": 2,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "0be08263b46c092e",
            "authors": [
                "Linwei Chen",
                "Lin Gu",
                "Liang Li",
                "Chenggang Yan",
                "Ying Fu"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "RIKEN",
                "The University of Tokyo",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18783.jpg",
            "data": {
                "categories": [],
                "emoji": "🤖",
                "ru": {
                    "title": "Эффективное обучение больших языковых моделей",
                    "desc": "В статье рассматривается новый подход к обучению больших языковых моделей (LLM), который позволяет значительно сократить время и ресурсы, необходимые для их тренировки. Авторы предлагают использовать метод оптимизации, который адаптируется к особенностям данных, что повышает эффективность обучения. Эксперименты показывают, что предложенный метод позволяет достичь более высоких результатов на стандартных тестах. Это открывает новые возможности для применения LLM в различных областях, таких как обработка естественного языка и генерация текста."
                },
                "en": {
                    "title": "Hybrid Models: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "提升预测准确性的创新算法",
                    "desc": "这篇论文探讨了一种新的机器学习算法，旨在提高模型的预测准确性。作者提出了一种改进的特征选择方法，可以有效减少数据维度，同时保留重要信息。实验结果表明，该算法在多个数据集上表现优于传统方法。通过优化模型的训练过程，研究者希望推动机器学习在实际应用中的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19777",
            "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
            "url": "https://huggingface.co/papers/2503.19777",
            "abstract": "We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS",
            "score": 1,
            "issue_id": 2905,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "adee7b684f0c25f1",
            "authors": [
                "Vladan Stojnić",
                "Yannis Kalantidis",
                "Jiří Matas",
                "Giorgos Tolias"
            ],
            "affiliations": [
                "NAVER LABS Europe",
                "VRG, FEE, Czech Technical University in Prague"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19777.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#inference",
                    "#cv",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Улучшение сегментации изображений без обучения с помощью распространения меток",
                    "desc": "В статье предлагается метод семантической сегментации изображений с открытым словарем без обучения, использующий модели зрения и языка (VLM). Метод улучшает начальные попиксельные предсказания VLM с помощью распространения меток, оптимизируя предсказания с учетом отношений между патчами. Для захвата внутримодальных сходств используется отдельная модель зрения (VM). Применение распространения меток на уровне пикселей позволяет преодолеть ограничения разрешения, присущие кодировщикам на основе патчей."
                },
                "en": {
                    "title": "Revolutionizing Semantic Segmentation with Training-Free Label Propagation",
                    "desc": "This paper introduces a novel method for open-vocabulary semantic segmentation that does not require training. The method leverages Vision-and-Language Models (VLMs) and enhances their predictions by using label propagation to optimize relationships between image patches. To improve accuracy, especially near class boundaries, the authors apply label propagation at the pixel level, addressing the limitations of patch-based encoders. The proposed method, LPOSS+, outperforms existing training-free approaches and effectively captures contextual interactions across the entire image."
                },
                "zh": {
                    "title": "无训练的开放词汇语义分割新方法",
                    "desc": "我们提出了一种无训练的开放词汇语义分割方法，利用视觉与语言模型（VLMs）。该方法通过标签传播增强了VLMs的初始每个补丁预测，优化了补丁之间的关系。我们使用视觉模型（VM）来更好地捕捉这些关系，并在像素级别应用标签传播，以解决基于补丁编码器的分辨率限制，从而显著提高了类边界附近的分割精度。我们的LPOSS+方法在整个图像上进行推理，避免了基于窗口的处理，能够捕捉全图的上下文交互，并在无训练方法中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19123",
            "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling",
            "url": "https://huggingface.co/papers/2503.19123",
            "abstract": "Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.",
            "score": 1,
            "issue_id": 2907,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "b5efb0f50380f253",
            "authors": [
                "Haebin Shin",
                "Lei Ji",
                "Xiao Liu",
                "Yeyun Gong"
            ],
            "affiliations": [
                "KAIST AI",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19123.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models",
                    "#training"
                ],
                "emoji": "🔠",
                "ru": {
                    "title": "Преодоление языкового барьера в обучении языковых моделей",
                    "desc": "Статья представляет новый подход к обучению языковых моделей под названием VocAgnoLM. Этот метод решает проблему несоответствия словарей между учительской и ученической моделями с помощью лексического выравнивания токенов и использования функции потерь учительской модели. VocAgnoLM показывает значительное улучшение производительности по сравнению с обычным дообучением, особенно когда словари моделей сильно различаются. Авторы демонстрируют, что их подход позволяет эффективно использовать более сильные учительские модели для обучения меньших ученических моделей."
                },
                "en": {
                    "title": "Bridging Vocabulary Gaps for Better Language Learning",
                    "desc": "This paper introduces a new method called Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM) to improve the training of smaller language models using larger teacher models. It addresses the problem of vocabulary mismatches that can lead to different token sequences and output distributions between the teacher and student models. VocAgnoLM employs two main techniques: Token-level Lexical Alignment to synchronize token sequences and Teacher Guided Loss to utilize the teacher's loss for better student training. The results show that this approach significantly enhances performance, achieving a 46% improvement in language modeling tasks, especially when using stronger teacher models."
                },
                "zh": {
                    "title": "打破词汇壁垒，提升语言建模效率",
                    "desc": "本文提出了一种新的语言建模方法，称为Vocabulary-agnostic Teacher Guided Language Modeling（VocAgnoLM），旨在解决教师模型与学生模型之间的词汇不匹配问题。该方法通过两种关键技术实现：一是令牌级词汇对齐，二是教师引导损失，帮助学生模型更有效地学习。实验表明，VocAgnoLM在使用不同词汇的教师模型时，能够显著提高学生模型的性能，尤其是在词汇重叠较少的情况下。该方法为语言建模中的词汇不匹配问题提供了有效的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17361",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
            "url": "https://huggingface.co/papers/2503.17361",
            "abstract": "Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.",
            "score": 1,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "b5389a3e5ab241c3",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Management and Technology Program, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "Université de Montréal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17361.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Новый подход к генерации биологических последовательностей на симплексе",
                    "desc": "Статья представляет новый метод генеративного моделирования на симплексе, называемый Gumbel-Softmax Flow and Score Matching. Авторы вводят новый интерполянт Gumbel-Softmax с зависящей от времени температурой и используют его для создания параметризованного поля скоростей. Метод позволяет получать высококачественные и разнообразные результаты, эффективно масштабируясь на симплексы высокой размерности. Также предложен метод Straight-Through Guided Flows для управления генерацией без дополнительного обучения."
                },
                "en": {
                    "title": "Revolutionizing Sequence Generation with Gumbel-Softmax Flows",
                    "desc": "This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications."
                },
                "zh": {
                    "title": "高效生成高维序列的创新框架",
                    "desc": "本文提出了一种新的生成框架，称为Gumbel-Softmax流和评分匹配，旨在解决DNA序列设计中的高维简单形问题。通过引入时间依赖的Gumbel-Softmax插值，我们能够在简单形上实现高质量和多样化的生成。该框架还包括一种名为STGFlow的分类器引导方法，能够在推理时有效地引导生成过程。我们的研究在条件DNA启动子设计、序列生成的蛋白质和靶向结合肽的设计中展示了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17237",
            "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
            "url": "https://huggingface.co/papers/2503.17237",
            "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a \"Strong Baseline\" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
            "score": 1,
            "issue_id": 2900,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "03b184a7ba5377e7",
            "authors": [
                "Yu-Hsi Chen"
            ],
            "affiliations": [
                "The University of Melbourne, Parkville, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17237.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🚁",
                "ru": {
                    "title": "Эффективное отслеживание БПЛА на тепловизионном видео без дополнительной обработки",
                    "desc": "Статья представляет новый подход к отслеживанию множественных БПЛА на инфракрасном видео с использованием YOLOv12 и BoT-SORT. Авторы разработали фреймворк, который превосходит стандартную связку YOLOv5 и DeepSORT, не прибегая к улучшению контраста или слиянию временной информации. Метод показал конкурентоспособные результаты на метриках 4-го Anti-UAV Challenge. Исследователи предоставляют подробности реализации, экспериментальный анализ и обсуждение возможных улучшений."
                },
                "en": {
                    "title": "Revolutionizing UAV Tracking with YOLOv12 and BoT-SORT",
                    "desc": "This paper addresses the challenge of detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video, which is difficult due to low visibility and small sizes of the targets. The authors propose a new tracking framework that utilizes YOLOv12 and BoT-SORT, moving away from the traditional YOLOv5 and DeepSORT methods. Their approach includes specific training and inference strategies that enhance performance without relying on contrast enhancement or temporal information. The results show that their method performs competitively in the 4th Anti-UAV Challenge, establishing it as a strong baseline for future multi-UAV tracking research."
                },
                "zh": {
                    "title": "热红外视频中的多无人机跟踪新方法",
                    "desc": "本论文针对热红外视频中多无人机（UAV）的检测与跟踪问题，提出了一种简单有效的方法。我们采用了YOLOv12和BoT-SORT构建跟踪框架，并通过定制的训练和推理策略进行增强。我们的评估基于第四届反无人机挑战赛的指标，结果显示出竞争力的性能。值得注意的是，我们在不使用对比度增强或时间信息融合的情况下，依然取得了良好的结果，标志着我们的方法是多无人机跟踪任务的“强基线”。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16965",
            "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
            "url": "https://huggingface.co/papers/2503.16965",
            "abstract": "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.",
            "score": 1,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "ca2e599ff0665dfe",
            "authors": [
                "Zhe Hu",
                "Jing Li",
                "Yu Yin"
            ],
            "affiliations": [
                "Department of Computer and Data Sciences, Case Western Reserve University",
                "Department of Computing, The Hong Kong Polytechnic University",
                "Research Centre for Data Science & Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16965.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самосовершенствование VLM через текстовое обучение",
                    "desc": "Это исследование сосредоточено на оценке визуальных языковых моделей (VLM) в задачах принятия решений, ориентированных на человека. Авторы обнаружили, что языковые модели, работающие только с текстом, превосходят VLM аналогичного масштаба, обрабатывающие изображения. Для решения этой проблемы предложен новый подход к обучению, использующий синтезированные текстовые данные. Исследование также демонстрирует, что VLM могут значительно улучшить свою производительность через самосовершенствование, используя данные, сгенерированные их LLM-аналогами."
                },
                "en": {
                    "title": "Enhancing VLMs through Text-Only Training and Self-Improvement",
                    "desc": "This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable."
                },
                "zh": {
                    "title": "提升VLM人类中心决策能力的新方法",
                    "desc": "本研究探讨了视觉语言模型（VLMs）在复杂人类中心决策中的表现。我们发现，仅使用文本描述的语言模型（LLMs）在某些任务上意外地超越了处理图像的VLMs，这表明视觉对齐可能会限制VLM的能力。为了解决这个问题，我们提出了一种新的仅基于文本的训练方法，利用合成文本数据来增强VLM的语言能力。我们的研究结果表明，通过自我改进，VLMs可以显著提升其人类中心决策能力，开辟了优化VLM的新途径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19207",
            "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images",
            "url": "https://huggingface.co/papers/2503.19207",
            "abstract": "We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.",
            "score": 0,
            "issue_id": 2907,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "051b6e95d11816f7",
            "authors": [
                "Rong Wang",
                "Fabian Prada",
                "Ziyan Wang",
                "Zhongshi Jiang",
                "Chengxiang Yin",
                "Junxuan Li",
                "Shunsuke Saito",
                "Igor Santesteban",
                "Javier Romero",
                "Rohan Joshi",
                "Hongdong Li",
                "Jason Saragih",
                "Yaser Sheikh"
            ],
            "affiliations": [
                "Australian National University",
                "Meta Reality Labs Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19207.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🧍",
                "ru": {
                    "title": "Мгновенное создание реалистичных 3D-аватаров по нескольким фотографиям",
                    "desc": "В статье представлен новый метод создания персонализированных 3D-аватаров людей с реалистичной анимацией на основе всего нескольких изображений. Авторы обучили универсальную модель на тысяче примеров одетых людей, что позволяет мгновенно генерировать аватары без дополнительной оптимизации. Метод совместно оценивает форму аватара, веса скиннинга и зависящие от позы деформации, что улучшает геометрическую точность. Предложенный процесс 3D-канонизации и агрегации признаков помогает восстанавливать мелкие детали и сохранять индивидуальность человека."
                },
                "en": {
                    "title": "Instant 3D Avatars from Few Images!",
                    "desc": "This paper introduces a new technique for creating personalized 3D human avatars with realistic animations using just a few images. Unlike previous methods that require extensive optimization for each individual, this approach leverages a universal model learned from a large dataset of clothed humans, allowing for quick and efficient avatar generation. The method improves the accuracy of the avatar's shape and movement by jointly inferring skinning weights and pose-dependent deformations, which minimizes visual artifacts. Additionally, a 3D canonicalization process is implemented to align poses and enhance geometric details, resulting in high-quality reconstructions that can be generated from casual photos."
                },
                "zh": {
                    "title": "个性化3D头像重建的新方法",
                    "desc": "我们提出了一种新方法，可以仅通过几张图片重建个性化的3D人类头像，并实现逼真的动画。现有方法通常需要在推理过程中对每个对象进行数小时的优化，这限制了它们的实际应用。我们的技术通过学习来自一千多名穿衣人类的通用先验，实现了即时前馈生成和零样本泛化。我们设计了一种3D标准化过程，以解决姿势变化和形状之间的模糊性，从而提高几何精度并减少变形伪影。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11849",
            "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
            "url": "https://huggingface.co/papers/2503.11849",
            "abstract": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.",
            "score": 0,
            "issue_id": 2904,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "73a149c0c20956cf",
            "authors": [
                "Yi Wang",
                "Zhitong Xiong",
                "Chenying Liu",
                "Adam J. Stewart",
                "Thomas Dujardin",
                "Nikolaos Ioannis Bountos",
                "Angelos Zavras",
                "Franziska Gerken",
                "Ioannis Papoutsis",
                "Laura Leal-Taixé",
                "Xiao Xiang Zhu"
            ],
            "affiliations": [
                "Harokopio University of Athens",
                "Munich Center for Machine Learning",
                "NVIDIA",
                "National Technical University of Athens & National Observatory of Athens",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11849.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Универсальная модель машинного обучения для комплексного анализа спутниковых данных",
                    "desc": "Статья представляет новый подход к созданию фундаментальных моделей для наблюдения Земли. Авторы разработали Copernicus-FM - унифицированную модель, способную обрабатывать различные сенсорные модальности с использованием расширенных динамических гиперсетей. Для обучения модели был создан массивный набор данных Copernicus-Pretrain, включающий 18,7 миллионов изображений со спутников миссии Copernicus Sentinel. Также представлен бенчмарк Copernicus-Bench для оценки модели на 15 иерархических задачах."
                },
                "en": {
                    "title": "Unlocking Earth Observation with Advanced Foundation Models",
                    "desc": "This paper presents advancements in Earth observation (EO) foundation models that utilize large satellite datasets to enhance learning from space imagery. The authors introduce Copernicus-Pretrain, a comprehensive dataset with 18.7 million aligned images from various Copernicus Sentinel missions, which includes data from both the Earth's surface and atmosphere. They also propose Copernicus-FM, a versatile foundation model that can handle different types of sensor data and incorporates metadata for improved analysis. Finally, the paper outlines Copernicus-Bench, a benchmark for evaluating the model's performance across 15 diverse tasks, thereby enhancing the scalability and adaptability of EO applications."
                },
                "zh": {
                    "title": "地球观测基础模型的未来：多模态与可扩展性",
                    "desc": "本论文介绍了地球观测基础模型的进展，利用大规模卫星数据学习通用表示，促进了多种重要应用的发展。我们提出了三个关键组件：Copernicus-Pretrain，一个包含1870万对齐图像的大规模预训练数据集，涵盖地球表面到大气层的所有主要Copernicus Sentinel任务；Copernicus-FM，一个统一的基础模型，能够处理任何光谱或非光谱传感器的模态，并使用扩展的动态超网络和灵活的元数据编码；以及Copernicus-Bench，一个系统的评估基准，包含15个层次的下游任务，从预处理到每个Sentinel任务的专业应用。我们的工作显著提高了地球观测基础模型的可扩展性、多功能性和多模态适应性，同时为连接地球观测、天气和气候研究创造了新的机会。"
                }
            }
        }
    ],
    "link_prev": "2025-03-25.html",
    "link_next": "2025-03-27.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "25.03",
        "en": "03/25",
        "zh": "3月25日"
    },
    "short_date_next": {
        "ru": "27.03",
        "en": "03/27",
        "zh": "3月27日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了长上下文自回归模型在语言生成中的进步，但在视频生成中仍面临挑战。作者提出了Frame AutoRegressive (FAR)，一种用于视频自回归建模的强大基准。FAR通过建模连续帧之间的时间因果依赖性，实现了比Token AR和视频扩散变压器更好的收敛。然而，长上下文视觉建模面临视觉冗余和计算成本高的问题。为解决这些问题，作者提出了FlexRoPE和长短期上下文建模技术，使得在长视频序列上的训练更加高效，并在短长视频生成中取得了最佳性能。",
        "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
        "pinyin": "Zhè piān wénzhāng tǎolùnle cháng shàngxiàwén zìhuíguī móxíng zài yǔyán shēngchéng zhōng de jìnbù, dàn zài shìpǐn shēngchéng zhōng réng miànliào tiǎozhàn. Zuòzhě tíchūle Frame AutoRegressive (FAR), yīzhǒng yòngyú shìpǐn zìhuíguī jiànmó de qiángdà jīzhǔn. FAR tōngguò jiànmó liánxù zhēn jiān de shíjiān yīnguǒ yīlàixìng, shíxiànle bǐ Token AR hé shìpǐn kuòsàn biànshūqì gèng hǎo de shōulǐan. Rán'ér, cháng shàngxiàwén shìjùe jiànmó miànliào shìjùe rǒngyú hé jìsuàn chéngběn gāo de wèntí. Wèi jiějué zhèxiē wèntí, zuòzhě tíchūle FlexRoPE hé chángduǎnqī shàngxiàwén jiànmó jìshù, shǐdé zài cháng shìpǐn xùliè shàng de xùnliàn gèngjiā gāoxiào, bìng zài duǎn cháng shìpǐn shēngchéng zhōng qudéle zuìjiā xìngnéng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"长上下文\", \"pinyin\": \"cháng shàng xià wén\", \"trans\": \"long context\"},\n    {\"word\": \"自回归\", \"pinyin\": \"zì huí guī\", \"trans\": \"autoregressive\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"进步\", \"pinyin\": \"jìn bù\", \"trans\": \"progress\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"Frame\", \"pinyin\": \"Frame\", \"trans\": \"Frame\"},\n    {\"word\": \"AutoRegressive\", \"pinyin\": \"AutoRegressive\", \"trans\": \"AutoRegressive\"},\n    {\"word\": \"FAR\", \"pinyin\": \"FAR\", \"trans\": \"FAR\"},\n    {\"word\": \"用于\", \"pinyin\": \"yòng yú\", \"trans\": \"used for\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"自回归建模\", \"pinyin\": \"zì huí guī jiàn mó\", \"trans\": \"autoregressive modeling\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"建模\", \"pinyin\": \"jiàn mó\", \"trans\": \"modeling\"},\n    {\"word\": \"连续\", \"pinyin\": \"lián xù\", \"trans\": \"continuous\"},\n    {\"word\": \"帧\", \"pinyin\": \"zhèn\", \"trans\": \"frame\"},\n    {\"word\": \"之间\", \"pinyin\": \"zhī jiān\", \"trans\": \"between\"},\n    {\"word\": \"时间\", \"pinyin\": \"shí jiān\", \"trans\": \"time\"},\n    {\"word\": \"因果\", \"pinyin\": \"yīn guǒ\", \"trans\": \"causal\"},\n    {\"word\": \"依赖性\", \"pinyin\": \"yī lài xìng\", \"trans\": \"dependency\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"收敛\", \"pinyin\": \"shōu liǎn\", \"trans\": \"convergence\"},\n    {\"word\": \"Token\", \"pinyin\": \"Token\", \"trans\": \"Token\"},\n    {\"word\": \"AR\", \"pinyin\": \"AR\", \"trans\": \"AR\"},\n    {\"word\": \"视频扩散变压器\", \"pinyin\": \"shì pín kuò sàn biàn yā qì\", \"trans\": \"video diffusion transformer\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"冗余\", \"pinyin\": \"róng yú\", \"trans\": \"redundancy\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"computation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"FlexRoPE\", \"pinyin\": \"FlexRoPE\", \"trans\": \"FlexRoPE\"},\n    {\"word\": \"长短期\", \"pinyin\": \"cháng duǎn qī\", \"trans\": \"long short-term\"},\n    {\"word\": \"上下文建模技术\", \"pinyin\": \"shàng xià wén jiàn mó jì shù\", \"trans\": \"context modeling techniques\"},\n    {\"word\": \"使得\", \"pinyin\": \"shǐ dé\", \"trans\": \"make\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"序列\", \"pinyin\": \"xù liè\", \"trans\": \"sequence\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔ dé\", \"trans\": \"achieve\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"optimal\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"}\n]",
        "trans": "This article discusses the advancements of long-context autoregressive models in language generation but highlights the challenges they still face in video generation. The authors introduce Frame AutoRegressive (FAR), a powerful benchmark for autoregressive modeling in video. FAR achieves better convergence than Token AR and Video Diffusion Transformers by modeling the temporal causal dependencies between consecutive frames. However, long-context visual modeling faces issues of visual redundancy and high computational costs. To address these problems, the authors propose FlexRoPE and long-short term context modeling techniques, making training on long video sequences more efficient and achieving state-of-the-art performance in both short and long video generation.",
        "update_ts": "2025-03-26 09:12"
    }
}