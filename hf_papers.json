{
    "date": {
        "ru": "4 апреля",
        "en": "April 4",
        "zh": "4月4日"
    },
    "time_utc": "2025-04-04 05:11",
    "weekday": 4,
    "issue_id": 3066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02782",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.02782",
            "abstract": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.",
            "score": 16,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "5346697bd326eed4",
            "authors": [
                "Zhiyuan Yan",
                "Junyan Ye",
                "Weijia Li",
                "Zilong Huang",
                "Shenghai Yuan",
                "Xiangyang He",
                "Kaiqing Lin",
                "Jun He",
                "Conghui He",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "Shanghai AI Laboratory",
                "Shenzhen University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#diffusion",
                    "#interpretability",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GPT-4o: Новый рубеж в генерации и редактировании изображений с помощью ИИ",
                    "desc": "Статья представляет первый оценочный бенчмарк (GPT-ImgEval) для модели GPT-4o от OpenAI, анализирующий ее способности в генерации и редактировании изображений. Исследование оценивает качество генерации, мастерство редактирования и семантический синтез на основе мировых знаний, демонстрируя превосходство GPT-4o над существующими методами. Авторы также предполагают, что архитектура GPT-4o включает авторегрессионную модель в сочетании с диффузионной головкой для декодирования изображений. Кроме того, статья анализирует ограничения GPT-4o, сравнивает ее с Gemini 2.0 Flash и обсуждает вопросы безопасности, связанные с выходными данными модели."
                },
                "en": {
                    "title": "Unleashing the Power of GPT-4o in Image Generation and Editing",
                    "desc": "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."
                },
                "zh": {
                    "title": "GPT-4o：图像生成与编辑的新突破",
                    "desc": "本论文介绍了OpenAI的GPT-4o模型在图像生成和编辑方面的最新突破。我们提出了一个名为GPT-ImgEval的评估基准，定量和定性地分析了GPT-4o在生成质量、编辑能力和知识推理等三个关键维度的表现。研究表明，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，我们还探讨了GPT-4o的架构，并识别了其在图像生成中常见的合成伪影和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 9,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прозрачное обучение с подкреплением для визуально-языковых моделей",
                    "desc": "Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "建立可重复的强化学习框架",
                    "desc": "强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 5,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Контролируемая генерация видео из отдельных элементов",
                    "desc": "SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2：可控视频生成的新突破",
                    "desc": "本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02826",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "url": "https://huggingface.co/papers/2504.02826",
            "abstract": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.",
            "score": 4,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "dbb1c07cd5a01838",
            "authors": [
                "Xiangyu Zhao",
                "Peiyuan Zhang",
                "Kexian Tang",
                "Hao Li",
                "Zicheng Zhang",
                "Guangtao Zhai",
                "Junchi Yan",
                "Hua Yang",
                "Xue Yang",
                "Haodong Duan"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02826.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RISEBench: Новый рубеж в оценке визуального редактирования с рассуждениями",
                    "desc": "RISEBench - это новый эталонный тест для оценки визуального редактирования с учетом рассуждений в мультимодальных моделях. Он фокусируется на четырех типах рассуждений: временном, причинно-следственном, пространственном и логическом. Тест оценивает понимание инструкций, сохранение внешнего вида и визуальную правдоподобность с помощью как человеческих оценщиков, так и LLM-судей. Эксперименты показали, что даже современные модели, такие как GPT-4, испытывают трудности с задачами логического рассуждения."
                },
                "en": {
                    "title": "RISEBench: Advancing Reasoning in Visual Editing",
                    "desc": "This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area."
                },
                "zh": {
                    "title": "推理驱动的视觉编辑新基准",
                    "desc": "大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑中仍面临挑战，尤其是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一问题，我们引入了RISEBench，这是第一个用于评估推理驱动视觉编辑（RISE）的基准。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们的实验表明，尽管GPT-4o-Native在性能上显著优于其他模型，但在逻辑推理任务上仍然存在困难，显示出这一领域仍需深入探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02542",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "url": "https://huggingface.co/papers/2504.02542",
            "abstract": "Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.",
            "score": 4,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "fa93ea3aeacd0dbc",
            "authors": [
                "Fa-Ting Hong",
                "Zunnan Xu",
                "Zixiang Zhou",
                "Jun Zhou",
                "Xiu Li",
                "Qin Lin",
                "Qinglin Lu",
                "Dan Xu"
            ],
            "affiliations": [
                "HKUST",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02542.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Гибкий синтез говорящей головы с множественным контролем",
                    "desc": "ACTalker - это новая модель для синтеза видео с говорящей головой, использующая диффузионный подход. Она поддерживает как мультимодальное, так и одномодальное управление генерацией видео. Модель использует параллельную структуру mamba с несколькими ветвями для обработки различных управляющих сигналов. ACTalker применяет механизм маскирования для предотвращения конфликтов между разными модальностями управления."
                },
                "en": {
                    "title": "ACTalker: Multi-Signal Control for Natural Talking Head Synthesis",
                    "desc": "This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs."
                },
                "zh": {
                    "title": "多信号控制的对话头像生成新方法",
                    "desc": "本文介绍了一种名为ACTalker的端到端视频扩散框架，旨在生成虚拟头像的对话视频。该方法支持多信号和单信号控制，克服了现有方法的局限性。通过设计并行的mamba结构，允许不同的驱动信号控制面部的特定区域，并使用门控机制实现灵活控制。实验结果表明，ACTalker能够生成自然的面部视频，并且能够无冲突地整合多种驱动信号。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 3,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов",
                    "desc": "Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "利用大型语言模型优化时间序列预测的模型选择",
                    "desc": "本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02507",
            "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
            "url": "https://huggingface.co/papers/2504.02507",
            "abstract": "Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.",
            "score": 2,
            "issue_id": 3065,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "290019150fe5b4c9",
            "authors": [
                "Abhay Kumar",
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Fabian Güra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02507.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ZClip: адаптивное ограничение градиентов для стабильного обучения языковых моделей",
                    "desc": "В статье представлен новый алгоритм адаптивного ограничения градиентов под названием ZClip для обучения больших языковых моделей (LLM). ZClip динамически корректирует порог ограничения на основе статистических свойств норм градиентов во времени. Алгоритм использует обнаружение аномалий на основе z-оценки для выявления и смягчения больших всплесков градиентов. ZClip помогает предотвратить вредные всплески потерь, не мешая сходимости в остальных случаях."
                },
                "en": {
                    "title": "ZClip: Smart Gradient Clipping for Stable LLM Training",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model."
                },
                "zh": {
                    "title": "自适应梯度裁剪，提升训练稳定性",
                    "desc": "在训练大型语言模型时，常常会遇到梯度不稳定和损失峰值等问题，这可能导致灾难性的发散。传统的梯度裁剪技术无法有效解决这些问题，因为它们依赖于固定的阈值或启发式方法，导致学习效率低下。我们提出了一种名为ZClip的自适应梯度裁剪算法，它根据梯度范数的统计特性动态调整裁剪阈值。ZClip通过基于z-score的异常检测来识别和减轻大梯度峰值，从而防止损失峰值，同时不干扰收敛过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02012",
            "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
            "url": "https://huggingface.co/papers/2504.02012",
            "abstract": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.",
            "score": 1,
            "issue_id": 3065,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "cbdd586ccd2b682d",
            "authors": [
                "Soro Bedionita",
                "Bruno Andreis",
                "Song Chong",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, South Korea",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02012.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "IGPG: Универсальный генератор параметров нейросетей для различных задач и архитектур",
                    "desc": "IGPG - это авторегрессивная система для генерации параметров нейронных сетей, основанная на VQ-VAE и авторегрессивной модели. Она создает параметры на основе описания задачи, датасета и архитектуры сети, обеспечивая согласованность между слоями. IGPG работает на уровне токенов, эффективно охватывая сложные распределения параметров из широкого спектра предобученных моделей. Эксперименты показывают, что IGPG превосходит современные методы по масштабируемости и эффективности для крупных архитектур."
                },
                "en": {
                    "title": "IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability",
                    "desc": "This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning."
                },
                "zh": {
                    "title": "IGPG：灵活的神经网络参数生成工具",
                    "desc": "本文提出了一种名为IGPG（指令引导参数生成）的自回归框架，旨在生成神经网络参数，以适应不同的任务描述和架构规范。IGPG通过结合VQ-VAE和自回归模型，能够在多种任务和架构中统一参数合成，确保层间一致性。该方法在生成神经网络权重时，采用了基于token的生成方式，有效捕捉来自多种预训练模型的复杂参数分布。实验结果表明，IGPG在多个视觉数据集上表现出色，尤其在大规模架构的可扩展性和效率方面，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00891",
            "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
            "url": "https://huggingface.co/papers/2504.00891",
            "abstract": "Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.",
            "score": 1,
            "issue_id": 3066,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "b22a54f43f9d7a89",
            "authors": [
                "Jian Zhao",
                "Runze Liu",
                "Kaiyan Zhang",
                "Zhimu Zhou",
                "Junqi Gao",
                "Dong Li",
                "Jiafei Lyu",
                "Zhouyi Qian",
                "Biqing Qi",
                "Xiu Li",
                "Bowen Zhou"
            ],
            "affiliations": [
                "BUPT",
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00891.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GenPRM: Новая парадигма контроля процесса рассуждений в больших языковых моделях",
                    "desc": "Статья представляет GenPRM - генеративную модель вознаграждения процесса, которая использует рассуждения по цепочке мыслей и верификацию кода для оценки каждого шага рассуждений. Авторы предлагают методы относительной оценки прогресса и синтеза обоснований для получения качественных обучающих данных. Эксперименты показывают, что GenPRM значительно превосходит предыдущие модели PRM на нескольких задачах математических рассуждений, используя всего 23 тысячи обучающих примеров. Модель также демонстрирует способность выступать в роли критика для улучшения политики в больших языковых моделях."
                },
                "en": {
                    "title": "GenPRM: Elevating LLMs with Generative Process Reward Models",
                    "desc": "This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs."
                },
                "zh": {
                    "title": "生成性过程奖励模型：提升LLMs的新范式",
                    "desc": "最近，大型语言模型（LLMs）的进展表明，使用过程奖励模型（PRMs）作为验证器可以提升LLMs的性能。然而，当前的PRMs面临三个主要挑战：有限的过程监督和泛化能力、依赖于标量值预测而未利用LLMs的生成能力，以及无法扩展PRMs的测试时间计算。本文提出了GenPRM，这是一种生成性过程奖励模型，通过代码验证进行明确的思维链推理，然后对每个推理步骤进行判断。实验结果表明，GenPRM在多个数学推理任务上显著优于之前的PRMs，展示了其作为政策模型精炼的批评模型的强大能力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "pinyin": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de túxiàng shēngchéng móxíng, jiàozuò MergeVQ. Tā jiēhé le xiàngliàng liànggéhuà hé token hébìng jìshù, zhǐyú jiějué túxiàng shēngchéng zhìliàng hé biǎoshì xuéxí xiàoyì zhījiān de pínghéng wèntí. MergeVQ zài yùxùnliàn jiēduàn tōngguò token hébìng mókuài tíqǔ yǔyì xìnxī, bìng zài jiěmǎ jiēduàn huīfù xìjiè. Shíyàn jiéguǒ xiǎnshì, MergeVQ zài túxiàng shēngchéng hé biǎoshì xuéxí rènwù zhōng dōu biǎoxiàn chūsè, qiě xiàoyì gāo. Dàimǎ hé móxíng jiāng zài wǎngshàng gōngkāi.",
        "vocab": "[\n    {\"word\": \"向量量化\", \"pinyin\": \"xiàngliàng liàngzhì\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"tōukèn\", \"trans\": \"token\"},\n    {\"word\": \"合并\", \"pinyin\": \"hébìng\", \"trans\": \"merge\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"平衡\", \"pinyin\": \"pínghéng\", \"trans\": \"balance\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"解码\", \"pinyin\": \"jiěmǎ\", \"trans\": \"decode\"},\n    {\"word\": \"恢复\", \"pinyin\": \"huīfù\", \"trans\": \"recover\"},\n    {\"word\": \"细节\", \"pinyin\": \"xìjié\", \"trans\": \"detail\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōngkāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.",
        "update_ts": "2025-04-03 09:11"
    }
}