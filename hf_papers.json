{
    "date": {
        "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 24",
        "zh": "12æœˆ24æ—¥"
    },
    "time_utc": "2024-12-24 08:13",
    "weekday": 1,
    "issue_id": 1286,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17256",
            "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
            "url": "https://huggingface.co/papers/2412.17256",
            "abstract": "In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.",
            "score": 21,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "1a1ee4818597feae",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Lulu Zhao",
                "Yijun Wang",
                "Zifei Shan",
                "Junxian He"
            ],
            "affiliations": [
                "BAAI",
                "Tencent",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17256.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ (Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² (ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ). ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº B-STaR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ B-STaR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ°, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°."
                },
                "en": {
                    "title": "Balancing Exploration and Exploitation for Self-Improvement in ML Models",
                    "desc": "This paper addresses the challenge of improving machine learning models in the absence of large human-annotated datasets, focusing on self-improvement through iterative training on their own outputs. It identifies two key factors that influence this process: the model's ability to explore diverse responses and the effectiveness of external rewards in selecting high-quality outputs. The authors introduce B-STaR, a framework that dynamically adjusts training configurations to maintain a balance between exploration and exploitation. Experiments show that B-STaR enhances exploratory capabilities and improves overall model performance in reasoning tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ”¹è¿›ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„å…³é”®",
                    "desc": "åœ¨ç¼ºä¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œè‡ªæˆ‘æ”¹è¿›æˆä¸ºæå‡æ¨¡å‹æ€§èƒ½çš„ä¸»è¦æ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªå…³é”®å› ç´ ï¼šæ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–å“åº”çš„èƒ½åŠ›ï¼ˆæ¢ç´¢ï¼‰å’Œå¤–éƒ¨å¥–åŠ±åœ¨åŒºåˆ†é«˜è´¨é‡å€™é€‰é¡¹ä¸ä½è´¨é‡å€™é€‰é¡¹ä¸­çš„æœ‰æ•ˆæ€§ï¼ˆåˆ©ç”¨ï¼‰ã€‚é€šè¿‡å¯¹æ•°å­¦æ¨ç†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›åœ¨è¿­ä»£è¿‡ç¨‹ä¸­è¿…é€Ÿä¸‹é™ï¼Œè€Œå¤–éƒ¨å¥–åŠ±çš„æœ‰æ•ˆæ€§ä¹Ÿéšä¹‹å‡å¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†B-STaRæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¿­ä»£ä¸­è‡ªæˆ‘è°ƒæ•´é…ç½®ï¼Œä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä»è€Œä¼˜åŒ–è‡ªæˆ‘æ”¹è¿›çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14922",
            "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
            "url": "https://huggingface.co/papers/2412.14922",
            "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
            "score": 17,
            "issue_id": 1281,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "9cc4a703e686ea87",
            "authors": [
                "Junyu Luo",
                "Xiao Luo",
                "Kaize Ding",
                "Jingyang Yuan",
                "Zhiping Xiao",
                "Ming Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Peking University",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14922.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§¼",
                "ru": {
                    "title": "Ğ§Ğ¸ÑÑ‚Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞºĞ°: RobustFT Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RobustFT Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RobustFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RobustFT Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Language Models with Robust Fine-Tuning",
                    "desc": "This paper presents a new framework called RobustFT for supervised fine-tuning (SFT) of large language models (LLMs) that addresses the issue of noisy data in training. The framework includes a multi-expert system for detecting noise in the data, which helps improve the quality of the training process. It also features a context-enhanced strategy for relabeling data, ensuring that only the most relevant and reliable information is used for fine-tuning. Experiments show that RobustFT significantly enhances model performance on downstream tasks, even in the presence of noise."
                },
                "zh": {
                    "title": "æ„å»ºæŠ—å™ªå£°çš„å¾®è°ƒæ¡†æ¶ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­æ”¶é›†çš„æ•°æ®ä¸å¯é¿å…åœ°åŒ…å«å™ªå£°ï¼Œè¿™å¯¹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°é€ æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§æŠ—å™ªå£°çš„SFTæ¡†æ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„SFTæ¡†æ¶ï¼ˆRobustFTï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ä¸Šæ‰§è¡Œå™ªå£°æ£€æµ‹å’Œé‡æ–°æ ‡æ³¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17451",
            "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2412.17451",
            "abstract": "Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.",
            "score": 16,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "2866001b23a585e1",
            "authors": [
                "Wei Liu",
                "Junlong Li",
                "Xiwen Zhang",
                "Fan Zhou",
                "Yu Cheng",
                "Junxian He"
            ],
            "affiliations": [
                "Helixon Research",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17451.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MSTaR Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with Self-Evolving Training",
                    "desc": "This paper focuses on improving reasoning abilities in Large Multimodal Models (LMMs) through a method called self-evolving training, which allows models to learn from their own outputs. The authors identify three critical factors that influence the effectiveness of this training: the Training Method, Reward Model, and Prompt Variation. They provide a detailed analysis of how different configurations of these factors can optimize multimodal reasoning performance. The study culminates in the development of a framework named MSTaR, which demonstrates significant improvements in reasoning tasks across various model sizes and benchmarks without requiring additional human annotations."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–è®­ç»ƒï¼šæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘è¿›åŒ–è®­ç»ƒåœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¯†åˆ«äº†å½±å“è®­ç»ƒæ•ˆæœçš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šè®­ç»ƒæ–¹æ³•ã€å¥–åŠ±æ¨¡å‹å’Œæç¤ºå˜ä½“ï¼Œå¹¶ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›å› ç´ çš„ä¸åŒé…ç½®ã€‚ç ”ç©¶ç»“æœæä¾›äº†ä¸€å¥—æœ€ä½³å®è·µï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€æ¨ç†çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†MSTaRæ¡†æ¶ï¼Œå±•ç¤ºäº†è‡ªæˆ‘è¿›åŒ–è®­ç»ƒåœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šçš„æ™®éæœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†é¢„å…ˆè¿›åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16926",
            "title": "Revisiting In-Context Learning with Long Context Language Models",
            "url": "https://huggingface.co/papers/2412.16926",
            "abstract": "In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.",
            "score": 10,
            "issue_id": 1281,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "764c013157322e0d",
            "authors": [
                "Jinheon Baek",
                "Sun Jae Lee",
                "Prakhar Gupta",
                "Geunseob",
                "Oh",
                "Siddharth Dalmia",
                "Prateek Kolhar"
            ],
            "affiliations": [
                "Google DeepMind",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16926.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² - Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (LCLM) Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 18 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğµ Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ² ÑĞ±Ğ¾Ñ€Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°. ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ICL Ğ½Ğ° 5% Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Maximizing ICL Performance with Long Contexts: Simplicity Over Sophistication",
                    "desc": "In-Context Learning (ICL) allows language models to make predictions based on examples in their input. With the introduction of Long Context Language Models (LCLMs), the number of examples that can be included has increased, prompting a reevaluation of example selection methods. The study finds that complex selection techniques do not significantly outperform simple random sampling in many-shot scenarios. Instead, the focus shifts to ensuring enough examples fill the context window, and using data augmentation can enhance ICL performance by 5%."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä¸‹çš„ç¤ºä¾‹é€‰æ‹©æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰ä¸­ï¼Œç¤ºä¾‹é€‰æ‹©å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å¤æ‚çš„ç¤ºä¾‹é€‰æ‹©æŠ€æœ¯å¹¶æœªæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†ç®€å•çš„éšæœºé€‰æ‹©æ–¹æ³•åœ¨è®¸å¤šæƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚éšç€LCLMsçš„å‡ºç°ï¼ŒICLçš„æŒ‘æˆ˜å·²ä»é€‰æ‹©æœ€æœ‰æ•ˆçš„ç¤ºä¾‹è½¬å˜ä¸ºæ”¶é›†è¶³å¤Ÿçš„ç¤ºä¾‹ä»¥å¡«å……ä¸Šä¸‹æ–‡çª—å£ã€‚é€šè¿‡ç®€å•çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æŸäº›æ•°æ®é›†ä¸Šæé«˜äº†ICLæ€§èƒ½ï¼Œæå‡å¹…åº¦è¾¾åˆ°5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15118",
            "title": "Outcome-Refining Process Supervision for Code Generation",
            "url": "https://huggingface.co/papers/2412.15118",
            "abstract": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS",
            "score": 8,
            "issue_id": 1284,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "88c43fc4e946d78c",
            "authors": [
                "Zhuohao Yu",
                "Weizheng Gu",
                "Yidong Wang",
                "Zhengran Zeng",
                "Jindong Wang",
                "Wei Ye",
                "Shikun Zhang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15118.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#plp",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Outcome-Refining Process Supervision', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Code Generation with Outcome-Refining Supervision",
                    "desc": "This paper introduces Outcome-Refining Process Supervision (ORPS), a new method to improve code generation in large language models, especially for complex programming tasks. Instead of relying on traditional reward models, ORPS supervises the reasoning process by refining outcomes using concrete execution signals. This approach allows models to explore multiple solution paths simultaneously, enhancing their ability to solve challenging problems. The results show that ORPS significantly boosts the accuracy and efficiency of various models on competitive programming tasks, demonstrating the importance of structured reasoning and reliable verification."
                },
                "zh": {
                    "title": "ç»“æœç²¾ç‚¼ï¼šæå‡ç¼–ç¨‹ä»»åŠ¡çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ç®—æ³•æ¨ç†çš„å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç›‘ç£å­¦ä¹ èŒƒå¼â€”â€”ç»“æœç²¾ç‚¼è¿‡ç¨‹ç›‘ç£ï¼Œæ—¨åœ¨å°†ç»“æœç²¾ç‚¼æœ¬èº«ä½œä¸ºéœ€è¦ç›‘ç£çš„è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…·ä½“çš„æ‰§è¡Œä¿¡å·æ¥æŒ‡å¯¼æ¨ç†æ­¥éª¤çš„ç›‘ç£ï¼ŒåŒæ—¶é‡‡ç”¨æ ‘çŠ¶ç»“æ„æ¢ç´¢æ¥åŒæ—¶ç»´æŠ¤å¤šä¸ªè§£å†³æ–¹æ¡ˆè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—å³ä½¿æ˜¯è¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½åœ¨ç«äº‰æ€§ç¼–ç¨‹ä»»åŠ¡ä¸­å®ç°é«˜æˆåŠŸç‡å’Œæ€§èƒ½æŒ‡æ ‡ï¼Œæ˜¾è‘—æé«˜äº†æ­£ç¡®æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17747",
            "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
            "url": "https://huggingface.co/papers/2412.17747",
            "abstract": "Techniques enabling large language models (LLMs) to \"think more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",
            "score": 8,
            "issue_id": 1283,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "b3ee8264ebbee41e",
            "authors": [
                "Luyang Liu",
                "Jonas Pfeiffer",
                "Jiaxing Wu",
                "Jun Xie",
                "Arthur Szlam"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17747.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ĞºÑÑˆĞµĞ¼ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ² Ğ½ĞµĞ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "Enhancing LLMs with Offline Cache Augmentation for Better Reasoning",
                    "desc": "This paper presents a method to enhance large language models (LLMs) by using an offline coprocessor that improves the model's key-value (kv) cache. The coprocessor adds latent embeddings to the cache, which helps the model generate better responses by refining its reasoning process. By training the coprocessor with language modeling loss while keeping the main decoder unchanged, the system can learn to optimize its computations without increasing latency. Experimental results show that this cache augmentation leads to lower perplexity and better performance on various reasoning tasks, even without specific training for those tasks."
                },
                "zh": {
                    "title": "å¢å¼ºç¼“å­˜ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªç¦»çº¿åå¤„ç†å™¨ï¼Œè¯¥åå¤„ç†å™¨åœ¨æ¨¡å‹çš„é”®å€¼ç¼“å­˜ä¸Šæ“ä½œï¼Œä»è€Œæé«˜åç»­è§£ç çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ¨¡å‹ä»¥ç«¯åˆ°ç«¯å¯å¾®åˆ†çš„æ–¹å¼å­¦ä¹ å¦‚ä½•å°†é¢å¤–çš„è®¡ç®—æç‚¼åˆ°å…¶ç¼“å­˜ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºç¼“å­˜åï¼Œè§£ç å™¨åœ¨å¤šä¸ªåç»­æ ‡è®°ä¸Šè¡¨ç°å‡ºæ›´ä½çš„å›°æƒ‘åº¦ï¼Œä¸”åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17153",
            "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
            "url": "https://huggingface.co/papers/2412.17153",
            "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3times speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.",
            "score": 6,
            "issue_id": 1283,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "b1968a6263a19386",
            "authors": [
                "Enshu Liu",
                "Xuefei Ning",
                "Yu Wang",
                "Zinan Lin"
            ],
            "affiliations": [
                "Department of EE, Tsinghua University",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17153.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Distilled Decoding (DD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… (AR) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ flow matching Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ AR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° ÑˆĞ°Ğ³Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ AR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DD Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 6-217 Ñ€Ğ°Ğ· Ñ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¼ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID."
                },
                "en": {
                    "title": "Revolutionizing Autoregressive Generation: Fast and Efficient with Distilled Decoding!",
                    "desc": "This paper introduces Distilled Decoding (DD), a novel approach to enhance the efficiency of autoregressive (AR) models in generating text and images. Traditional AR models generate outputs token-by-token, which can be slow, but DD aims to enable generation in just one or two steps by creating a deterministic mapping from a Gaussian distribution to the AR model's output distribution. By training a network to distill this mapping, DD allows for rapid generation without requiring the original training data, making it more practical for real-world applications. The results show significant speed-ups in generation times while maintaining acceptable quality, challenging the belief that AR models are inherently slow."
                },
                "zh": {
                    "title": "è’¸é¦è§£ç ï¼šåŠ é€Ÿè‡ªå›å½’æ¨¡å‹ç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºé€ä¸ªç”Ÿæˆçš„è¿‡ç¨‹ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè’¸é¦è§£ç ï¼ˆDDï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„ARæ¨¡å‹é€‚åº”ä¸ºä»…éœ€ä¸€æ­¥æˆ–ä¸¤æ­¥ç”Ÿæˆè¾“å‡ºã€‚DDé€šè¿‡æµåŒ¹é…åˆ›å»ºä»é«˜æ–¯åˆ†å¸ƒåˆ°ARæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„ç¡®å®šæ€§æ˜ å°„ï¼Œä»è€Œå®ç°å¿«é€Ÿç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDåœ¨å¤šä¸ªå›¾åƒARæ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—çš„ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17805",
            "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
            "url": "https://huggingface.co/papers/2412.17805",
            "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~https://yzxing87.github.io/vae/{https://yzxing87.github.io/vae/}.",
            "score": 5,
            "issue_id": 1285,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "58490715e8055e65",
            "authors": [
                "Yazhou Xing",
                "Yang Fei",
                "Yingqing He",
                "Jingye Chen",
                "Jiaxin Xie",
                "Xiaowei Chi",
                "Qifeng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.17805.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞµÑ‘ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Video Generation with Temporal-Aware Compression and Text Guidance",
                    "desc": "This paper introduces a new video Variational Autoencoder (VAE) that improves video generation by addressing the limitations of existing models. It focuses on enhancing both spatial and temporal compression to avoid issues like motion blur and detail loss. The model incorporates text guidance from text-to-video datasets, which boosts the quality of video reconstruction. Additionally, it is trained on both images and videos, allowing it to effectively encode and decode both types of data, leading to superior performance compared to previous methods."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç¼–ç è´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç¼–ç çš„è´¨é‡å’Œæ•ˆç‡ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ—¶åºæ„ŸçŸ¥çš„ç©ºé—´å‹ç¼©æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿ3D VAEåœ¨è¿åŠ¨æ¨¡ç³Šå’Œç»†èŠ‚å¤±çœŸçš„é—®é¢˜ã€‚åŒæ—¶ï¼Œç»“åˆè½»é‡çº§çš„è¿åŠ¨å‹ç¼©æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ—¶åºå‹ç¼©æ•ˆæœã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘æ•°æ®é›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæå‡äº†é‡å»ºè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†èŠ‚ä¿ç•™å’Œæ—¶åºç¨³å®šæ€§æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16686",
            "title": "NILE: Internal Consistency Alignment in Large Language Models",
            "url": "https://huggingface.co/papers/2412.16686",
            "abstract": "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.",
            "score": 4,
            "issue_id": 1282,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "0d1e640729e131e5",
            "authors": [
                "Minda Hu",
                "Qiyuan Zhang",
                "Yufei Wang",
                "Bowei He",
                "Hongru Wang",
                "Jingyan Zhou",
                "Liangyou Li",
                "Yasheng Wang",
                "Chen Ma",
                "Irwin King"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16686.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#data",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº NILE Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (IFT) ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. NILE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IFT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Aligning IFT Datasets for Enhanced LLM Performance",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human intentions through Instruction Fine-Tuning (IFT). It highlights the problem of existing IFT datasets containing inconsistent knowledge that conflicts with the LLMs' pre-trained knowledge, which can hinder their performance. To solve this, the authors introduce the NILE framework, which optimizes IFT datasets by aligning them with the internal knowledge of the LLMs. The framework includes a method called Internal Consistency Filtering (ICF) to ensure high consistency, leading to significant performance improvements in LLM evaluations."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ•°æ®é›†ï¼Œæå‡LLMæ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºNILEçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰æ•°æ®é›†ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»æ„å›¾çš„ä¸€è‡´æ€§ã€‚ç°æœ‰çš„IFTæ•°æ®é›†å¸¸å¸¸åŒ…å«ä¸LLMsé¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ä¸ä¸€è‡´çš„ä¿¡æ¯ï¼Œè¿™ä¼šå½±å“IFTçš„æ•ˆæœã€‚NILEé€šè¿‡æå–ç›®æ ‡é¢„è®­ç»ƒLLMçš„å†…éƒ¨çŸ¥è¯†æ¥ä¿®æ­£IFTæ•°æ®é›†ä¸­çš„ç­”æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å†…éƒ¨ä¸€è‡´æ€§è¿‡æ»¤ï¼ˆICFï¼‰æ–¹æ³•ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ ·æœ¬ä¸LLMçš„å†…éƒ¨çŸ¥è¯†é«˜åº¦ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNILEå¯¹IFTæ•°æ®é›†çš„ä¼˜åŒ–æ˜¾è‘—æå‡äº†LLMåœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜äº†æ•°æ®é›†ä¸é¢„è®­ç»ƒå†…éƒ¨çŸ¥è¯†ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17498",
            "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
            "url": "https://huggingface.co/papers/2412.17498",
            "abstract": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at https://github.com/krystalan/DRT-o1",
            "score": 3,
            "issue_id": 1286,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "10ee2563b13fe4ff",
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Yunlong Liang",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17498.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#reasoning",
                    "#translation",
                    "#multilingual",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DRT-o1 - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ°, ÑĞ¾Ğ²ĞµÑ‚Ğ½Ğ¸ĞºĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°, Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DRT-o1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ BLEU Ğ¸ CometScore. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Literary Translation with Long Chain-of-Thought Reasoning",
                    "desc": "This paper presents DRT-o1, a novel approach to enhance neural machine translation (MT) by leveraging long chain-of-thought (CoT) reasoning. The method specifically addresses the challenges of translating literature that contains similes and metaphors, which often require deeper semantic understanding due to cultural nuances. DRT-o1 employs a multi-agent framework where a translator iteratively refines translations with guidance from an advisor and feedback from an evaluator. Experimental results indicate that DRT-o1 significantly improves translation quality, as evidenced by higher BLEU and CometScore metrics compared to existing models."
                },
                "zh": {
                    "title": "é•¿é“¾æ€ç»´åŠ©åŠ›æ–‡å­¦ç¿»è¯‘çš„çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹DRT-o1ï¼Œæ—¨åœ¨å°†é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çš„æˆåŠŸåº”ç”¨äºæ–‡å­¦ç¿»è¯‘ã€‚ç”±äºæ–‡åŒ–å·®å¼‚ï¼Œç¿»è¯‘åŒ…å«æ¯”å–»å’Œéšå–»çš„æ–‡æœ¬éå¸¸å›°éš¾ï¼Œä¼ ç»Ÿçš„é€å­—ç¿»è¯‘å¾€å¾€æ— æ³•æœ‰æ•ˆä¼ è¾¾åŸæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç¿»è¯‘å’Œè¯„ä¼°æ¥ä¼˜åŒ–ç¿»è¯‘ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRT-o1åœ¨æ–‡å­¦ç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16429",
            "title": "LearnLM: Improving Gemini for Learning",
            "url": "https://huggingface.co/papers/2412.16429",
            "abstract": "Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of pedagogical instruction following, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\\% over GPT-4o, 11\\% over Claude 3.5, and 13\\% over the Gemini 1.5 Pro model LearnLM was based on.",
            "score": 2,
            "issue_id": 1286,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "6cae08cebf8bc6ae",
            "authors": [
                "LearnLM Team",
                "Abhinit Modi",
                "Aditya Srikanth Veerubhotla",
                "Aliya Rysbek",
                "Andrea Huber",
                "Brett Wiltshire",
                "Brian Veprek",
                "Daniel Gillick",
                "Daniel Kasenberg",
                "Derek Ahmed",
                "Irina Jurenka",
                "James Cohan",
                "Jennifer She",
                "Julia Wilkowski",
                "Kaiz Alarakyia",
                "Kevin McKee",
                "Lisa Wang",
                "Markus Kunesch",
                "Mike Schaekermann",
                "Miruna PÃ®slar",
                "Nikhil Joshi",
                "Parsa Mahmoudieh",
                "Paul Jhun",
                "Sara Wiltberger",
                "Shakir Mohamed",
                "Shashank Agarwal",
                "Shubham Milind Phal",
                "Sun Jae Lee",
                "Theofilos Strinopoulos",
                "Wei-Jen Ko",
                "Amy Wang",
                "Ankit Anand",
                "Avishkar Bhoopchand",
                "Dan Wild",
                "Divya Pandya",
                "Filip Bar",
                "Garth Graham",
                "Holger Winnemoeller",
                "Mahvish Nagda",
                "Prateek Kolhar",
                "Renee Schneider",
                "Shaojian Zhu",
                "Stephanie Chan",
                "Steve Yadlowsky",
                "Viknesh Sounderajah",
                "Yannis Assael"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16429.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#alignment",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ˜Ğ˜-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ: Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµĞ´Ğ°Ğ³Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµĞ´Ğ°Ğ³Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LearnLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Empowering AI with Human-Like Teaching Skills",
                    "desc": "This paper discusses how current generative AI systems primarily provide information instead of facilitating learning like a human tutor. The authors propose a new approach called pedagogical instruction following, which allows for the inclusion of specific teaching behaviors in the training and evaluation of AI models. This method gives educators the flexibility to define desired pedagogical attributes without being tied to a single definition of pedagogy. The results show that the LearnLM model, trained with this approach, significantly outperforms other models in various learning scenarios, indicating its effectiveness in educational applications."
                },
                "zh": {
                    "title": "æ•™å­¦æŒ‡ä»¤è·Ÿéšï¼šæå‡ç”Ÿæˆå¼AIçš„å­¦ä¹ èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨æ•™è‚²ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œå³æ•™å­¦æŒ‡ä»¤è·Ÿéšã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ•™å¸ˆæˆ–å¼€å‘è€…å¯ä»¥æŒ‡å®šæœŸæœ›çš„æ•™å­¦è¡Œä¸ºï¼Œè€Œä¸éœ€è¦å›ºå®šçš„æ•™å­¦å®šä¹‰ã€‚è¿™ç§çµæ´»æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„å­¦ä¹ åœºæ™¯ï¼Œå¹¶æå‡å…¶å­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ•™å­¦æŒ‡ä»¤è·Ÿéšè®­ç»ƒçš„LearnLMæ¨¡å‹åœ¨å¤šç§å­¦ä¹ åœºæ™¯ä¸­å¾—åˆ°äº†ä¸“å®¶è¯„å®¡çš„é«˜åº¦è®¤å¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16720",
            "title": "OpenAI o1 System Card",
            "url": "https://huggingface.co/papers/2412.16720",
            "abstract": "The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",
            "score": 1,
            "issue_id": 1286,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "5eed348dd7fb2826",
            "authors": [
                "OpenAI",
                ":",
                "Aaron Jaech",
                "Adam Kalai",
                "Adam Lerer",
                "Adam Richardson",
                "Ahmed El-Kishky",
                "Aiden Low",
                "Alec Helyar",
                "Aleksander Madry",
                "Alex Beutel",
                "Alex Carney",
                "Alex Iftimie",
                "Alex Karpenko",
                "Alex Tachard Passos",
                "Alexander Neitz",
                "Alexander Prokofiev",
                "Alexander Wei",
                "Allison Tam",
                "Ally Bennett",
                "Ananya Kumar",
                "Andre Saraiva",
                "Andrea Vallone",
                "Andrew Duberstein",
                "Andrew Kondrich",
                "Andrey Mishchenko",
                "Andy Applebaum",
                "Angela Jiang",
                "Ashvin Nair",
                "Barret Zoph",
                "Behrooz Ghorbani",
                "Ben Rossen",
                "Benjamin Sokolowsky",
                "Boaz Barak",
                "Bob McGrew",
                "Borys Minaiev",
                "Botao Hao",
                "Bowen Baker",
                "Brandon Houghton",
                "Brandon McKinzie",
                "Brydon Eastman",
                "Camillo Lugaresi",
                "Cary Bassin",
                "Cary Hudson",
                "Chak Ming Li",
                "Charles de Bourcy",
                "Chelsea Voss",
                "Chen Shen",
                "Chong Zhang",
                "Chris Koch",
                "Chris Orsinger",
                "Christopher Hesse",
                "Claudia Fischer",
                "Clive Chan",
                "Dan Roberts",
                "Daniel Kappler",
                "Daniel Levy",
                "Daniel Selsam",
                "David Dohan",
                "David Farhi",
                "David Mely",
                "David Robinson",
                "Dimitris Tsipras",
                "Doug Li",
                "Dragos Oprica",
                "Eben Freeman",
                "Eddie Zhang",
                "Edmund Wong",
                "Elizabeth Proehl",
                "Enoch Cheung",
                "Eric Mitchell",
                "Eric Wallace",
                "Erik Ritter",
                "Evan Mays",
                "Fan Wang",
                "Felipe Petroski Such",
                "Filippo Raso",
                "Florencia Leoni",
                "Foivos Tsimpourlas",
                "Francis Song",
                "Fred von Lohmann",
                "Freddie Sulit",
                "Geoff Salmon",
                "Giambattista Parascandolo",
                "Gildas Chabot",
                "Grace Zhao",
                "Greg Brockman",
                "Guillaume Leclerc",
                "Hadi Salman",
                "Haiming Bao",
                "Hao Sheng",
                "Hart Andrin",
                "Hessam Bagherinezhad",
                "Hongyu Ren",
                "Hunter Lightman",
                "Hyung Won Chung",
                "Ian Kivlichan",
                "Ian O'Connell",
                "Ian Osband",
                "Ignasi Clavera Gilaberte",
                "Ilge Akkaya",
                "Ilya Kostrikov",
                "Ilya Sutskever",
                "Irina Kofman",
                "Jakub Pachocki",
                "James Lennon",
                "Jason Wei",
                "Jean Harb",
                "Jerry Twore",
                "Jiacheng Feng",
                "Jiahui Yu",
                "Jiayi Weng",
                "Jie Tang",
                "Jieqi Yu",
                "Joaquin QuiÃ±onero Candela",
                "Joe Palermo",
                "Joel Parish",
                "Johannes Heidecke",
                "John Hallman",
                "John Rizzo",
                "Jonathan Gordon",
                "Jonathan Uesato",
                "Jonathan Uesato",
                "Jonathan Ward",
                "Joost Huizinga",
                "Julie Wang",
                "Kai Chen",
                "Kai Xiao",
                "Karan Singhal",
                "Karina Nguyen",
                "Karl Cobbe",
                "Katy Shi",
                "Kayla Wood",
                "Kendra Rimbach",
                "Keren Gu-Lemberg",
                "Keren GuLemberg",
                "Kevin Liu",
                "Kevin Lu",
                "Kevin Stone",
                "Kevin Yu",
                "Lama Ahmad",
                "Lauren Yang",
                "Leo Liu",
                "Leon Maksin",
                "Leyton Ho",
                "Liam Fedus",
                "Lilian Weng",
                "Linden Li",
                "Lindsay McCallum",
                "Lindsey Held",
                "Lorenz Kuhn",
                "Lukas Kondraciuk",
                "Lukasz Kaiser",
                "Luke Metz",
                "Madelaine Boyd",
                "Maja Trebacz",
                "Manas Joglekar",
                "Mark Chen",
                "Marko Tintor",
                "Mason Meyer",
                "Matt Jones",
                "Matt Kaufer",
                "Max Schwarzer",
                "Meghan Shah",
                "Mehmet Yatbaz",
                "Melody Guan",
                "Mengyuan Xu",
                "Mengyuan Yan",
                "Mia Glaese",
                "Mianna Chen",
                "Mianna Chen",
                "Michael Lampe",
                "Michael Malek",
                "Michele Wang",
                "Michelle Fradin",
                "Mike McClay",
                "Mikhail Pavlov",
                "Miles Wang",
                "Mingxuan Wang",
                "Mira Murati",
                "Mo Bavarian",
                "Mostafa Rohaninejad",
                "Nat McAleese",
                "Neil Chowdhury",
                "Neil Chowdhury",
                "Nick Ryder",
                "Nikolas Tezak",
                "Noam Brown",
                "Ofir Nachum",
                "Oleg Boiko",
                "Oleg Murk",
                "Olivia Watkins",
                "Patrick Chao",
                "Paul Ashbourne",
                "Pavel Izmailov",
                "Peter Zhokhov",
                "Rachel Dias",
                "Rahul Arora",
                "Randall Lin",
                "Rapha Gontijo Lopes",
                "Raz Gaon",
                "Reah Miyara",
                "Reimar Leike",
                "Renny Hwang",
                "Rhythm Garg",
                "Robin Brown",
                "Roshan James",
                "Rui Shu",
                "Ryan Cheu",
                "Ryan Greene",
                "Saachi Jain",
                "Sam Altman",
                "Sam Toizer",
                "Sam Toyer",
                "Samuel Miserendino",
                "Sandhini Agarwal",
                "Santiago Hernandez",
                "Sasha Baker",
                "Scott McKinney",
                "Scottie Yan",
                "Shengjia Zhao",
                "Shengli Hu",
                "Shibani Santurkar",
                "Shraman Ray Chaudhuri",
                "Shuyuan Zhang",
                "Siyuan Fu",
                "Spencer Papay",
                "Steph Lin",
                "Suchir Balaji",
                "Suvansh Sanjeev",
                "Szymon Sidor",
                "Tal Broda",
                "Aidan Clark",
                "Tao Wang",
                "Taylor Gordon",
                "Ted Sanders",
                "Tejal Patwardhan",
                "Thibault Sottiaux",
                "Thomas Degry",
                "Thomas Dimson",
                "Tianhao Zheng",
                "Timur Garipov",
                "Tom Stasi",
                "Trapit Bansal",
                "Trevor Creech",
                "Troy Peterson",
                "Tyna Eloundou",
                "Valerie Qi",
                "Vineet Kosaraju",
                "Vinnie Monaco",
                "Vitchyr Pong",
                "Vlad Fomenko",
                "Weiyi Zheng",
                "Wenda Zhou",
                "Wes McCabe",
                "Wojciech Zaremba",
                "Yann Dubois",
                "Yinghai Lu",
                "Yining Chen",
                "Young Cha",
                "Yu Bai",
                "Yuchen He",
                "Yuchen Zhang",
                "Yunyun Wang",
                "Zheng Shao",
                "Zhuohan Li"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16720.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#alignment",
                    "#benchmark",
                    "#healthcare",
                    "#rl",
                    "#training",
                    "#ethics"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞµÑ€Ğ¸Ğ¸ o1 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞ»Ğ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ñ€Ğ¸ÑĞºĞ°Ğ¼, ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ·Ğ°ĞºĞ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²ĞµÑ‚Ğ¾Ğ², Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing AI Safety through Chain of Thought Reasoning",
                    "desc": "The o1 model series utilizes large-scale reinforcement learning to enhance reasoning through a chain of thought approach. This method improves the models' ability to align with safety policies when faced with potentially harmful prompts, thereby increasing their robustness. The models demonstrate superior performance on benchmarks related to generating unsafe content and responding to biased queries. The findings highlight the importance of developing strong alignment techniques and rigorous risk management strategies to ensure the safe deployment of advanced AI systems."
                },
                "zh": {
                    "title": "æ€ç»´é“¾æ¨ç†ï¼šæå‡æ¨¡å‹å®‰å…¨æ€§çš„å…³é”®",
                    "desc": "o1æ¨¡å‹ç³»åˆ—é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä½¿ç”¨æ€ç»´é“¾è¿›è¡Œæ¨ç†ã€‚è¿™ç§å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ä¸ºæé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¨³å¥æ€§æä¾›äº†æ–°çš„é€”å¾„ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨å“åº”æ½œåœ¨ä¸å®‰å…¨çš„æç¤ºæ—¶ï¼Œè€ƒè™‘å®‰å…¨æ”¿ç­–çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°æ·±æ€ç†Ÿè™‘çš„å¯¹é½ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†æ„å»ºç¨³å¥çš„å¯¹é½æ–¹æ³•å’Œè¿›è¡Œå…¨é¢çš„é£é™©ç®¡ç†çš„é‡è¦æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-23.html",
    "link_next": "2024-12-25.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#translation": 1
    },
    "zh": {
        "text": "è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚",
        "title": "Parallelized Autoregressive Visual Generation",
        "pinyin": "ZÃ¬ huÃ­guÄ« mÃ³xÃ­ng zÃ i shÃ¬juÃ© shÄ“ngchÃ©ng zhÅng biÇoxiÃ n chÅ« qiÃ¡ngdÃ  de xÃ¬ngnÃ©ng, dÃ n yÃ³uyÃº qÃ­ shÃ¹nxÃ¹ zhÃº biÄojÃ¬ yÃ¹cÃ¨ de guÃ²chÃ©ng, tuÄ«lÇ sÃ¹dÃ¹ jiÃ o mÃ n. BÄ›nwÃ©n tÃ­chÅ« le yÄ«zhÇ’ng jiÇndÄn yÇ’uxiÃ o de bÃ¬ngxÃ­ng zÃ¬huÃ­guÄ« shÃ¬juÃ© shÄ“ngchÃ©ng fÄngfÇ, tÃ­gÄo shÄ“ngchÃ©ng xiÃ olÇœ, tÃ³ngshÃ­ bÇoliÃº zÃ¬huÃ­guÄ« jiÃ nmÃ³ de yÅushÃ¬. WÇ’men de guÇnjiÃ n dÃ²ngjiÃ n shÃ¬ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng yÄ«lÃ i yÃº shÃ¬juÃ© biÄojÃ¬ de yÄ«lÃ i guÄnxÃ¬: ruÃ² yÄ«lÃ i de biÄojÃ¬ kÄ›yÇ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng, Ã©r qiÃ¡ng yÄ«lÃ i de xiÄnglÃ­n biÄojÃ¬ nÃ¡n yÇ yÄ«qÇ shÄ“ngchÃ©ng, yÄ«nwÃ¨i tÄmen de dÃºlÃ¬ cÇiyÇng kÄ›nÃ©ng dÇozhÃ¬ bÃ¹ yÄ«zhÃ¬. JÄ«yÃº cÇ guÄnchÃ¡, wÇ’men kÄifÄ le yÄ«zhÇ’ng bÃ¬ngxÃ­ng shÄ“ngchÃ©ng cÃ¨lÃ¼Ã¨, bÃ¬ngxÃ­ng shÄ“ngchÃ©ng ruÃ² yÄ«lÃ i de yuÇn jÃ¹lÃ­ biÄojÃ¬, tÃ³ngshÃ­ bÇochÃ­ qiÃ¡ng yÄ«lÃ i de jÃºbÃ¹ biÄojÃ¬ de shÃ¹nxÃ¹ shÄ“ngchÃ©ng. WÇ’men de fÄngfÇ kÄ›yÇ wÃºfÃ¨ng jÃ­chÃ©ng dÃ o biÄozhÇ”n zÃ¬huÃ­guÄ« mÃ³xÃ­ng zhÅng, wÃºxÅ« xiÅ«gÇi jiÃ gÃ²u huÃ² biÄojÃ¬qÃ¬. ZÃ i ImageNet hÃ© UCF-101 shÃ ng de shÃ­yÃ n biÇomÃ­ng, wÇ’men de fÄngfÇ zÃ i tÃºxiÃ ng hÃ© shÃ¬pÇn shÄ“ngchÃ©ng rÃ¨nwÃ¹ zhÅng shÃ­xiÃ n le zuÃ¬gÄo 9.5 bÃ¨i de jiÄsÃ¹, tÃ³ngshÃ­ zhÃ¬liÃ ng sÇ”nshÄ« zuÃ¬shÇo. WÇ’men xÄ«wÃ ng zhÃ¨ jiÃ n gÅngzuÃ² nÃ©ng jÄ«fÄ wÃ¨ilÃ¡i zÃ i gÄoxiÃ o shÃ¬juÃ© shÄ“ngchÃ©ng hÃ© tÇ’ngyÄ« zÃ¬huÃ­guÄ« jiÃ nmÃ³ fÄngmiÃ n de yÃ¡njiÅ«. XiÃ ngmÃ¹ yÃ¨miÃ n: https://epiphqny.github.io/PAR-project.",
        "vocab": "[{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ng dÃ ', 'trans': 'powerful'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'é¡ºåº', 'pinyin': 'shÃ¹n xÃ¹', 'trans': 'sequential'},\n{'word': 'é€', 'pinyin': 'zhÃº', 'trans': 'gradual'},\n{'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'},\n{'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'å¹¶è¡Œ', 'pinyin': 'bÃ¬ng xÃ­ng', 'trans': 'parallel'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'},\n{'word': 'ä¿ç•™', 'pinyin': 'bÇo liÃº', 'trans': 'retain'},\n{'word': 'ä¼˜åŠ¿', 'pinyin': 'yÅu shÃ¬', 'trans': 'advantage'},\n{'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'},\n{'word': 'æ´è§', 'pinyin': 'dÃ²ng jiÃ n', 'trans': 'insight'},\n{'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'dependency'},\n{'word': 'å…³ç³»', 'pinyin': 'guÄn xÃ¬', 'trans': 'relationship'},\n{'word': 'å¼±', 'pinyin': 'ruÃ²', 'trans': 'weak'},\n{'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'},\n{'word': 'ç›¸é‚»', 'pinyin': 'xiÄng lÃ­n', 'trans': 'adjacent'},\n{'word': 'éš¾ä»¥', 'pinyin': 'nÃ¡n yÇ', 'trans': 'difficult'},\n{'word': 'ç‹¬ç«‹', 'pinyin': 'dÃº lÃ¬', 'trans': 'independent'},\n{'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'},\n{'word': 'ä¸ä¸€è‡´', 'pinyin': 'bÃ¹ yÄ« zhÃ¬', 'trans': 'inconsistency'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'è¿œè·ç¦»', 'pinyin': 'yuÇn jÃ¹ lÃ­', 'trans': 'long-distance'},\n{'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'},\n{'word': 'æ— ç¼', 'pinyin': 'wÃº fÃ¨ng', 'trans': 'seamless'},\n{'word': 'é›†æˆ', 'pinyin': 'jÃ­ chÃ©ng', 'trans': 'integrate'},\n{'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'},\n{'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'},\n{'word': 'ä¿®æ”¹', 'pinyin': 'xiÅ« gÇi', 'trans': 'modify'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'},\n{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'acceleration'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},\n{'word': 'æŸå¤±', 'pinyin': 'sÇ”n shÄ«', 'trans': 'loss'},\n{'word': 'æ¿€å‘', 'pinyin': 'jÄ« fÄ', 'trans': 'inspire'},\n{'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'},\n{'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},\n{'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'},\n{'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]",
        "trans": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
        "update_ts": "2024-12-23 09:11"
    }
}