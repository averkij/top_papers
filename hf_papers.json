{
    "date": {
        "ru": "30 апреля",
        "en": "April 30",
        "zh": "4月30日"
    },
    "time_utc": "2025-04-30 04:13",
    "weekday": 2,
    "issue_id": 3504,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20734",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "url": "https://huggingface.co/papers/2504.20734",
            "abstract": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.",
            "score": 23,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "53427aff6a4d7ed5",
            "authors": [
                "Woongyeong Yeo",
                "Kangsan Kim",
                "Soyeong Jeong",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20734.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "UniversalRAG: Универсальное извлечение знаний из разнородных источников",
                    "desc": "UniversalRAG - это новая система извлечения и интеграции знаний из разнородных источников с различными модальностями и уровнями детализации. В отличие от существующих подходов RAG, ограниченных одномодальными корпусами, UniversalRAG использует механизм маршрутизации с учетом модальности для динамического выбора наиболее подходящего корпуса. Система также организует каждую модальность на несколько уровней детализации, позволяя точно настраивать извлечение информации в соответствии со сложностью и объемом запроса. Эксперименты на 8 тестовых наборах данных показали превосходство UniversalRAG над одномодальными и унифицированными базовыми моделями."
                },
                "en": {
                    "title": "UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval",
                    "desc": "The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations."
                },
                "zh": {
                    "title": "多模态知识整合的全新框架",
                    "desc": "本文介绍了一种新的检索增强生成框架，称为UniversalRAG，旨在从多种异构知识源中检索和整合信息，以提高模型的事实准确性。现有的检索增强生成方法通常仅限于文本数据，而UniversalRAG能够处理多种模态的信息，如图像和视频。该框架采用了一种模态感知的路由机制，能够动态识别最合适的模态特定语料库，从而进行针对性的检索。此外，UniversalRAG还将每种模态组织为多个粒度级别，以便根据查询的复杂性和范围进行精细化检索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20595",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "url": "https://huggingface.co/papers/2504.20595",
            "abstract": "We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.",
            "score": 15,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "244cff2e64afeaa0",
            "authors": [
                "Rulin Shao",
                "Rui Qiao",
                "Varsha Kishore",
                "Niklas Muennighoff",
                "Xi Victoria Lin",
                "Daniela Rus",
                "Bryan Kian Hsiang Low",
                "Sewon Min",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Luke Zettlemoyer"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "FAIR at Meta",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Singapore-MIT Alliance for Research and Technology",
                "Stanford University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20595.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#rag",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ReasonIR-8B: Прорыв в информационном поиске для задач рассуждения",
                    "desc": "ReasonIR-8B - это первая модель для поиска информации, специально обученная для задач рассуждения. Авторы разработали конвейер генерации синтетических данных, создающий сложные и релевантные запросы для каждого документа. Модель, обученная на смеси синтетических и существующих публичных данных, достигает нового уровня производительности на бенчмарке BRIGHT. При применении к задачам RAG, ReasonIR-8B значительно улучшает результаты на MMLU и GPQA по сравнению с baseline без доступа к внешней информации."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with ReasonIR-8B",
                    "desc": "ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications."
                },
                "zh": {
                    "title": "推理任务的专属检索器：ReasonIR-8B",
                    "desc": "我们提出了ReasonIR-8B，这是第一个专门为一般推理任务训练的检索器。现有的检索器在推理任务上的表现有限，部分原因是现有的训练数据集主要集中在与文档直接相关的短小事实查询上。我们开发了一种合成数据生成管道，为每个文档创建具有挑战性和相关性的查询，以及一个看似相关但实际上无用的困难负样本。通过在合成数据和现有公共数据的混合上进行训练，ReasonIR-8B在BRIGHT基准上达到了29.9的nDCG@10（不使用重排序器）和36.9的nDCG@10（使用重排序器），并在RAG任务中显著提高了MMLU和GPQA的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20571",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "url": "https://huggingface.co/papers/2504.20571",
            "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "score": 12,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "5392cdfe5ab1de59",
            "authors": [
                "Yiping Wang",
                "Qing Yang",
                "Zhiyuan Zeng",
                "Liliang Ren",
                "Lucas Liu",
                "Baolin Peng",
                "Hao Cheng",
                "Xuehai He",
                "Kuan Wang",
                "Jianfeng Gao",
                "Weizhu Chen",
                "Shuohang Wang",
                "Simon Shaolei Du",
                "Yelong Shen"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Microsoft",
                "University of California, Santa Cruz",
                "University of Southern California",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#math",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Один пример - огромный скачок в математических способностях ИИ",
                    "desc": "Исследование показывает эффективность обучения с подкреплением с верифицируемым вознаграждением (RLVR) на одном примере для улучшения математических рассуждений больших языковых моделей. Применение этого метода к базовой модели Qwen2.5-Math-1.5B значительно повысило ее производительность на различных математических тестах. Авторы наблюдали интересные явления, такие как межобластная генерализация и улучшение тестовых показателей после насыщения точности обучения. Исследование также подчеркивает важность поощрения исследования в процессе обучения и отличие этого метода от феномена 'грокинга'."
                },
                "en": {
                    "title": "Boosting Math Skills in LLMs with 1-Shot RLVR",
                    "desc": "This paper presents a method called 1-shot Reinforcement Learning with Verifiable Reward (RLVR) that significantly enhances the mathematical reasoning abilities of large language models (LLMs). By using just one training example, the authors demonstrate a remarkable increase in performance on the MATH500 benchmark, achieving a score of 73.6% compared to 36.0% before. The study also reveals that this approach leads to improvements across various models and algorithms, highlighting the importance of exploration in training. Additionally, the authors introduce the concept of post-saturation generalization, where performance continues to improve even after training accuracy levels off, suggesting new avenues for research in RLVR."
                },
                "zh": {
                    "title": "一例强化学习，提升数学推理能力！",
                    "desc": "本文展示了使用可验证奖励的强化学习（1-shot RLVR）在提升大型语言模型（LLMs）数学推理能力方面的有效性。通过将RLVR应用于基础模型Qwen2.5-Math-1.5B，我们发现一个单一示例可以将模型在MATH500上的表现从36.0%提升至73.6%。此外，本文还观察到在1-shot RLVR过程中出现了一些有趣现象，如跨领域泛化和自我反思频率增加。我们验证了1-shot RLVR的有效性主要源于策略梯度损失，并强调了促进探索在训练中的关键作用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20157",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "url": "https://huggingface.co/papers/2504.20157",
            "abstract": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.",
            "score": 7,
            "issue_id": 3504,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "18f16590c380c078",
            "authors": [
                "Zae Myung Kim",
                "Chanwoo Park",
                "Vipul Raheja",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Grammarly",
                "MIT",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20157.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rl",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное обучение с подкреплением для больших языковых моделей",
                    "desc": "Статья представляет новый метод под названием Meta Policy Optimization (MPO) для обучения с подкреплением больших языковых моделей. MPO использует мета-модель вознаграждения, которая динамически корректирует промпт модели вознаграждения в процессе обучения. Это позволяет решить проблемы эксплуатации сигнала вознаграждения и зависимости от ручной разработки промптов. Метод показывает результаты на уровне или лучше моделей с тщательно разработанными вручную промптами вознаграждения."
                },
                "en": {
                    "title": "Dynamic Reward Adjustment for Robust LLM Alignment",
                    "desc": "This paper presents Meta Policy Optimization (MPO), a new framework designed to improve reward-based alignment methods for large language models (LLMs). MPO tackles two main issues: the risk of reward hacking and the need for complex prompt engineering. By using a meta-reward model, MPO dynamically adjusts the reward prompts during training, ensuring that the reward signal remains effective and less exploitable. The results show that MPO achieves comparable or superior performance to traditional methods while simplifying the alignment process across various tasks."
                },
                "zh": {
                    "title": "元策略优化：提升大型语言模型的对齐能力",
                    "desc": "本文介绍了一种名为元策略优化（MPO）的新框架，旨在解决大型语言模型（LLMs）在基于奖励的对齐方法中面临的两个主要问题：奖励黑客和脆弱的提示工程。MPO通过引入一个动态调整奖励模型提示的元奖励模型，来提高对齐的稳定性和适应性。该方法能够在训练过程中监控环境变化，持续优化奖励信号，从而减少手动设计奖励提示的需求。实验结果表明，MPO在多种任务上表现优异，且不需要专门的奖励设计，具有更强的适应性和鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16046",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "url": "https://huggingface.co/papers/2504.16046",
            "abstract": "The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of \"copyright takedown\" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.",
            "score": 6,
            "issue_id": 3504,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "3a2630d279485a85",
            "authors": [
                "Jingyu Zhang",
                "Jiacan Yu",
                "Marc Marone",
                "Benjamin Van Durme",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16046.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#ethics",
                    "#leakage"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "BloomScrub: защита языковых моделей от нарушения авторских прав",
                    "desc": "Статья представляет метод BloomScrub для снижения риска нарушения авторских прав крупными языковыми моделями. Этот подход использует обнаружение цитат и техники переписывания текста во время вывода модели, чтобы трансформировать потенциально нарушающие авторские права сегменты. BloomScrub применяет эффективные структуры данных (фильтры Блума) для масштабируемой проверки авторских прав. Метод показывает высокую эффективность в снижении риска нарушений при сохранении полезности модели."
                },
                "en": {
                    "title": "BloomScrub: Smart Copyright Protection for Language Models",
                    "desc": "This paper addresses the issue of copyright infringement by large language models (LLMs) that may unintentionally generate content similar to copyrighted material. It introduces BloomScrub, a novel method that detects and rewrites long quotes from copyrighted sources during the model's inference phase. By using Bloom filters for efficient quote detection, BloomScrub can effectively screen large datasets while maintaining the model's utility. The approach not only reduces the risk of copyright infringement but also allows for flexible enforcement levels through adaptive abstention when necessary."
                },
                "zh": {
                    "title": "BloomScrub：有效的版权保护方法",
                    "desc": "本文讨论了大型语言模型在预训练过程中接触到版权材料所带来的版权侵权风险。为了解决这一问题，研究者们开发了\"版权撤销\"方法，旨在防止模型生成与版权内容相似的文本。我们提出了一种名为BloomScrub的方法，通过在推理时检测引用并进行重写，来有效地减少版权侵权风险。实验结果表明，BloomScrub在降低侵权风险的同时，保持了模型的实用性，并能够适应不同的执行严格性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20998",
            "title": "YoChameleon: Personalized Vision and Language Generation",
            "url": "https://huggingface.co/papers/2504.20998",
            "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting.",
            "score": 4,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "21fed074912b3e2f",
            "authors": [
                "Thao Nguyen",
                "Krishna Kumar Singh",
                "Jing Shi",
                "Trung Bui",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🦎",
                "ru": {
                    "title": "Персонализация мультимодальных ИИ-моделей на основе нескольких примеров",
                    "desc": "Статья представляет Yo'Chameleon - первую попытку персонализации больших мультимодальных моделей. Система использует метод soft-prompt tuning для встраивания информации о конкретном объекте на основе 3-5 изображений. Yo'Chameleon обучается отвечать на вопросы об объекте и воссоздавать его детали на новых изображениях. В процессе обучения применяются механизм самопромптинга и подход 'soft-positive' для улучшения качества генерации изображений."
                },
                "en": {
                    "title": "Personalizing Multimodal Models with Yo'Chameleon",
                    "desc": "This paper presents Yo'Chameleon, a novel approach to personalize large multimodal models like GPT-4 for specific user concepts. By using 3-5 images of a subject, Yo'Chameleon applies soft-prompt tuning to incorporate personalized information, enabling the model to answer questions and generate contextually relevant images. The training involves a self-prompting optimization mechanism that ensures balanced performance across different modalities, as well as a 'soft-positive' image generation technique to improve image quality in few-shot scenarios. This work addresses the gap in adapting personalization methods for image generation within multimodal frameworks."
                },
                "zh": {
                    "title": "个性化多模态模型的创新探索",
                    "desc": "大型多模态模型（如GPT-4、Gemini、Chameleon）已经发展成为强大的工具，拥有数百万用户。然而，这些模型仍然是通用的，缺乏对特定用户概念的个性化知识。本文介绍了Yo'Chameleon，这是首次研究大型多模态模型个性化的方法。Yo'Chameleon通过软提示调优，将特定主题的信息嵌入模型，以回答关于该主题的问题并在新环境中重建图像的像素级细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20995",
            "title": "TesserAct: Learning 4D Embodied World Models",
            "url": "https://huggingface.co/papers/2504.20995",
            "abstract": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.",
            "score": 4,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "6339aba982c02561",
            "authors": [
                "Haoyu Zhen",
                "Qiao Sun",
                "Hongxin Zhang",
                "Junyan Li",
                "Siyuan Zhou",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20995.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "4D-модели мира: новый уровень предсказания динамики воплощенных агентов",
                    "desc": "В статье представлен эффективный подход к обучению новых 4D-моделей воплощенного мира, которые предсказывают динамическую эволюцию 3D-сцен во времени в ответ на действия воплощенного агента. Авторы предлагают обучать 4D-модель мира на видео RGB-DN (RGB, глубина и нормали), что превосходит традиционные 2D-модели. Метод включает дополнение существующих наборов данных видео роботизированных манипуляций информацией о глубине и нормалях, а также тонкую настройку модели генерации видео. Предложенный алгоритм позволяет напрямую преобразовывать сгенерированные RGB-DN видео в высококачественную 4D-сцену мира."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Prediction for Embodied Agents",
                    "desc": "This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments."
                },
                "zh": {
                    "title": "学习四维世界模型，提升具身智能的预测能力",
                    "desc": "本文提出了一种有效的方法，用于学习新颖的四维具身世界模型，该模型能够预测三维场景在具身智能体动作下的动态演变，确保时空一致性。我们通过训练RGB-DN（RGB、深度和法线）视频来学习四维世界模型，这种方法超越了传统的二维模型，能够将详细的形状、配置和时间变化纳入预测中。具体而言，我们首先利用现成模型扩展现有的机器人操作视频数据集，加入深度和法线信息。接着，我们在这个标注数据集上微调视频生成模型，联合预测每一帧的RGB-DN，最终将生成的RGB、深度和法线视频直接转换为高质量的四维场景。"
                }
            }
        }
    ],
    "link_prev": "2025-04-29.html",
    "link_next": "2025-05-01.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4月29日"
    },
    "short_date_next": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5月1日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了当前文本到图像生成模型的局限性，特别是在生成非拉丁字母的精确和灵活的排版元素方面。为了解决这些问题，作者提出了RepText，这是一种能够准确渲染多语言视觉文本的模型，而无需真正理解这些文本。RepText 采用了ControlNet的设置，并额外整合了语言无关的字形和渲染文本的位置，使用户能够根据需要自定义文本内容、字体和位置。通过使用文本感知损失和扩散损失，以及在推理阶段采用噪声字形潜在初始化和区域掩码，RepText 显著提高了渲染的准确性和稳定性。实验结果表明，RepText 在开源方法中表现出色，并且与封闭源的多语言模型相媲美。文章最后还讨论了RepText的局限性。",
        "title": "RepText: Rendering Visual Text via Replicating",
        "pinyin": "这篇文章讨论了当前文本到图像生成模型的局限性，特别是在生成非拉丁字母的精确和灵活的排版元素方面。为了解决这些问题，作者提出了RepText，这是一种能够准确渲染多语言视觉文本的模型，而无需真正理解这些文本。RepText 采用了ControlNet的设置，并额外整合了语言无关的字形和渲染文本的位置，使用户能够根据需要自定义文本内容、字体和位置。通过使用文本感知损失和扩散损失，以及在推理阶段采用噪声字形潜在初始化和区域掩码，RepText 显著提高了渲染的准确性和稳定性。实验结果表明，RepText 在开源方法中表现出色，并且与封闭源的多语言模型相媲美。文章最后还讨论了RepText的局限性。\n\nzhè piān wén zhāng tǎo lùn le dāng qián wén běn dào tú xiàng shēng chéng mó xíng de jú xiàn xìng, tè bié shì zài shēng chéng fēi lā dīng zì mǔ de jīng què hé líng huó de pái bǎn yuán sǔ fāng miàn. wèi le jiě jué zhè xiē wèn tí, zuò zhě tí chū le RepText, zhè shì yī zhǒng néng gòu zhǔn què xuàn rán duō yǔ yán shì jù wén běn de mó xíng, ér wú xū zhēn zhèng lǐ jiě zhè xiē wén běn. RepText cuō yòng le ControlNet de shè zhì, bìng é wài zhěng hé le yǔ yán wú guān de zì xíng hé xuàn rán wén běn de wèi zhì, shǐ yòng hù néng gòu yīn yòng zhèn dīng wén běn nèi róng, zì tǐ hé wèi zhì. tōng guò shǐ yòng wén běn gǎn jué sǔn shī hé kuò sàn sǔn shī, yǐ jiā shǐ yòng tuī lǐ jiē duàn cuò yòng zào shēng zì xíng qián zài chū shǐ huà hé qū yù mó zhào, RepText xiǎn zhù tí gāo le xuàn rán de zhǔn què xìng hé wěn dìng xìng. shí yàn jié guǒ biǎo míng, RepText zài kāi yuán fāng fǎ zhōng biǎo xiàn chū sè, bìng qiě yǔ fēng bì yuán de duō yǔ yán mó xíng xiāng jì mǐ. wén zhāng zuì hòu hái tǎo lùn le RepText de jú xiàn xìng.\n\nzhè piān wén zhāng tǎo lùn le dāng qián wén běn dào tú xiàng shēng chéng mó xíng de jú xiàn xìng, tè bié shì zài shēng chéng fēi lā dīng zì mǔ de jīng què hé líng huó de pái bǎn yuán sǔ fāng miàn. wèi le jiě jué zhè xiē wèn tí, zuò zhě tí chū le RepText, zhè shì yī zhǒng néng gòu zhǔn què xuàn rán duō yǔ yán shì jù wén běn de mó xíng, ér wú xū zhēn zhèng lǐ jiě zhè xiē wén běn. RepText cuō yòng le ControlNet de shè zhì, bìng é wài zhěng hé le yǔ yán wú guān de zì xíng hé xuàn rán wén běn de wèi zhì, shǐ yòng hù néng gòu yīn yòng zhèn dīng wén běn nèi róng, zì tǐ hé wèi zhì. tōng guò shǐ yòng wén běn gǎn jué sǔn shī hé kuò sàn sǔn shī, yǐ jiā shǐ yòng tuī lǐ jiē duàn cuò yòng zào shēng zì xíng qián zài chū shǐ huà hé qū yù mó zhào, RepText xiǎn zhù tí gāo le xuàn rán de zhǔn què xìng hé wěn dìng xìng. shí yàn jié guǒ biǎo míng, RepText zài kāi yuán fāng fǎ zhōng biǎo xiàn chū sè, bìng qiě yǔ fēng bì yuán de duō yǔ yán mó xíng xiāng jì mǐ. wén zhāng zuì hòu hái tǎo lùn le RepText de jú xiàn xìng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"当前\", \"pinyin\": \"dāng qián\", \"trans\": \"current\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"局限性\", \"pinyin\": \"jú xiàn xìng\", \"trans\": \"limitations\"},\n    {\"word\": \"特别\", \"pinyin\": \"tè bié\", \"trans\": \"especially\"},\n    {\"word\": \"非拉丁字母\", \"pinyin\": \"fēi lā dīng zì mǔ\", \"trans\": \"non-Latin characters\"},\n    {\"word\": \"精确\", \"pinyin\": \"jīng què\", \"trans\": \"precise\"},\n    {\"word\": \"灵活\", \"pinyin\": \"líng huó\", \"trans\": \"flexible\"},\n    {\"word\": \"排版\", \"pinyin\": \"pái bǎn\", \"trans\": \"typesetting\"},\n    {\"word\": \"元素\", \"pinyin\": \"yuán sù\", \"trans\": \"elements\"},\n    {\"word\": \"方面\", \"pinyin\": \"fāng miàn\", \"trans\": \"aspect\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"RepText\", \"pinyin\": \"RepText\", \"trans\": \"RepText\"},\n    {\"word\": \"准确\", \"pinyin\": \"zhǔn què\", \"trans\": \"accurate\"},\n    {\"word\": \"渲染\", \"pinyin\": \"xuàn rán\", \"trans\": \"render\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duō yǔ yán\", \"trans\": \"multilingual\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"without needing\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"采用\", \"pinyin\": \"cǎi yòng\", \"trans\": \"adopt\"},\n    {\"word\": \"ControlNet\", \"pinyin\": \"ControlNet\", \"trans\": \"ControlNet\"},\n    {\"word\": \"设置\", \"pinyin\": \"shè zhì\", \"trans\": \"setting\"},\n    {\"word\": \"额外\", \"pinyin\": \"é wài\", \"trans\": \"additional\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"语言无关\", \"pinyin\": \"yǔ yán wú guān\", \"trans\": \"language-agnostic\"},\n    {\"word\": \"字形\", \"pinyin\": \"zì xíng\", \"trans\": \"glyph\"},\n    {\"word\": \"位置\", \"pinyin\": \"wèi zhì\", \"trans\": \"position\"},\n    {\"word\": \"使用户\", \"pinyin\": \"shǐ yòng hù\", \"trans\": \"enable users\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēn jù\", \"trans\": \"according to\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"自定义\", \"pinyin\": \"zì dìng yì\", \"trans\": \"customize\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"字体\", \"pinyin\": \"zì tǐ\", \"trans\": \"font\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"文本感知\", \"pinyin\": \"wén běn gǎn zhī\", \"trans\": \"text-aware\"},\n    {\"word\": \"损失\", \"pinyin\": \"sǔn shī\", \"trans\": \"loss\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"阶段\", \"pinyin\": \"jiē duàn\", \"trans\": \"stage\"},\n    {\"word\": \"噪声\", \"pinyin\": \"zào shēng\", \"trans\": \"noise\"},\n    {\"word\": \"潜在\", \"pinyin\": \"qián zài\", \"trans\": \"latent\"},\n    {\"word\": \"初始化\", \"pinyin\": \"chū shǐ huà\", \"trans\": \"initialization\"},\n    {\"word\": \"区域\", \"pinyin\": \"qū yù\", \"trans\": \"region\"},\n    {\"word\": \"掩码\", \"pinyin\": \"yǎn mǎ\", \"trans\": \"mask\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"稳定性\", \"pinyin\": \"wěn dìng xìng\", \"trans\": \"stability\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēng bì yuán\", \"trans\": \"closed-source\"},\n    {\"word\": \"相媲美\", \"pinyin\": \"xiāng pì měi\", \"trans\": \"compare favorably\"},\n    {\"word\": \"文章\", \"pinyin\": \"wén zhāng\", \"trans\": \"article\"},\n    {\"word\": \"最后\", \"pinyin\": \"zuì hòu\", \"trans\": \"finally\"},\n    {\"word\": \"还\", \"pinyin\": \"hái\", \"trans\": \"still\"}\n]",
        "trans": "This article discusses the limitations of current text-to-image generation models, particularly in generating precise and flexible typographic elements for non-Latin scripts. To address these issues, the authors propose RepText, a model capable of accurately rendering multilingual visual text without truly understanding the text. RepText adopts the setup of ControlNet and additionally integrates language-agnostic glyphs and the positioning of rendered text, allowing users to customize text content, fonts, and positions as needed. By utilizing text-aware loss and diffusion loss, along with noisy glyph latent initialization and regional masking during the inference stage, RepText significantly improves the accuracy and stability of rendering. Experimental results demonstrate that RepText performs excellently among open-source methods and is comparable to closed-source multilingual models. The article concludes with a discussion of RepText's limitations.",
        "update_ts": "2025-04-29 09:12"
    }
}