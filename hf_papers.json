{
    "date": {
        "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 12",
        "zh": "2æœˆ12æ—¥"
    },
    "time_utc": "2025-02-12 04:12",
    "weekday": 2,
    "issue_id": 2164,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.06807",
            "title": "Competitive Programming with Large Reasoning Models",
            "url": "https://huggingface.co/papers/2502.06807",
            "abstract": "We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.",
            "score": 4,
            "issue_id": 2164,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "fd76cceb75f32321",
            "authors": [
                "OpenAI",
                ":",
                "Ahmed El-Kishky",
                "Alexander Wei",
                "Andre Saraiva",
                "Borys Minaev",
                "Daniel Selsam",
                "David Dohan",
                "Francis Song",
                "Hunter Lightman",
                "Ignasi Clavera",
                "Jakub Pachocki",
                "Jerry Tworek",
                "Lorenz Kuhn",
                "Lukasz Kaiser",
                "Mark Chen",
                "Max Schwarzer",
                "Mostafa Rohaninejad",
                "Nat McAleese",
                "o3 contributors",
                "Oleg MÃ¼rk",
                "Rhythm Garg",
                "Rui Shu",
                "Szymon Sidor",
                "Vineet Kosaraju",
                "Wenda Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.06807.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#games",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ (OpenAI o1 Ğ¸ o3) ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ o1-ioi, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ² ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ (IOI) 2024 Ğ³Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ o3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° IOI 2024 Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº."
                },
                "en": {
                    "title": "Scaling General-Purpose Learning Outshines Specialized Strategies",
                    "desc": "This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›é€šç”¨æ¨¡å‹è¶…è¶Šç‰¹å®šé¢†åŸŸç³»ç»Ÿ",
                    "desc": "æœ¬è®ºæ–‡å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚ç¼–ç å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§é€šç”¨æ¨ç†æ¨¡å‹â€”â€”OpenAIçš„o1å’Œo3çš„æ—©æœŸæ£€æŸ¥ç‚¹ï¼Œä»¥åŠä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„ç³»ç»Ÿo1-ioiï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨æ‰‹å·¥è®¾è®¡çš„æ¨ç†ç­–ç•¥ã€‚o1-ioiåœ¨2024å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è¡¨ç°è‰¯å¥½ï¼Œè·å¾—äº†ç¬¬49ç™¾åˆ†ä½çš„æˆç»©ï¼Œè€Œåœ¨æ”¾å®½ç«äº‰çº¦æŸçš„æƒ…å†µä¸‹åˆ™è·å¾—äº†é‡‘ç‰Œã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸“é—¨çš„ç®¡é“å¦‚o1-ioièƒ½å¸¦æ¥æ˜¾è‘—æå‡ï¼Œä½†æ‰©å±•çš„é€šç”¨o3æ¨¡å‹åœ¨æ²¡æœ‰ä¾èµ–æ‰‹å·¥æ¨ç†å¯å‘å¼çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†è¿™äº›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06589",
            "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
            "url": "https://huggingface.co/papers/2502.06589",
            "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
            "score": 2,
            "issue_id": 2164,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "4273eabfdc59b328",
            "authors": [
                "Yuchen Zhuang",
                "Jingfeng Yang",
                "Haoming Jiang",
                "Xin Liu",
                "Kewei Cheng",
                "Sanket Lokegaonkar",
                "Yifan Gao",
                "Qing Ping",
                "Tianyi Liu",
                "Binxuan Huang",
                "Zheng Li",
                "Zhengyang Wang",
                "Pei Chen",
                "Ruijie Wang",
                "Rongzhi Zhang",
                "Nasser Zalmout",
                "Priyanka Nigam",
                "Bing Yin",
                "Chao Zhang"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06589.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞšÑƒĞ·Ğ½Ğ¸Ñ†Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Hephaestus-Forge - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞšĞ¾Ñ€Ğ¿ÑƒÑ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 103 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 76,537 API, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Hephaestus-Forge Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Hephaestus Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ LLM Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ LLM Ğ² Ñ‚Ñ€ĞµÑ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ€ĞµĞ´."
                },
                "en": {
                    "title": "Empowering LLM Agents with Hephaestus-Forge",
                    "desc": "This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability."
                },
                "zh": {
                    "title": "Hephaestus-Forgeï¼šæå‡LLMä»£ç†èƒ½åŠ›çš„åˆ›æ–°é¢„è®­ç»ƒè¯­æ–™åº“",
                    "desc": "ç”±äºç¼ºä¹é¢å‘ä»£ç†çš„é¢„è®­ç»ƒæ•°æ®ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»ä»£ç†é€šå¸¸ä¾èµ–å¤æ‚çš„æç¤ºæˆ–å¹¿æ³›çš„å¾®è°ƒï¼Œè¿™å¾€å¾€æ— æ³•åœ¨ä¿æŒå¼ºæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å¼•å…¥æ–°åŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†Hephaestus-Forgeï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæ—¨åœ¨å¢å¼ºLLMä»£ç†åœ¨APIåŠŸèƒ½è°ƒç”¨ã€å†…åœ¨æ¨ç†å’Œè§„åˆ’ä»¥åŠé€‚åº”ç¯å¢ƒåé¦ˆæ–¹é¢çš„åŸºæœ¬èƒ½åŠ›ã€‚Hephaestus-ForgeåŒ…å«1030äº¿ä¸ªç‰¹å®šäºä»£ç†çš„æ•°æ®ï¼Œæ¶µç›–76,537ä¸ªAPIï¼ŒåŒ…æ‹¬å·¥å…·æ–‡æ¡£ä»¥ä»‹ç»APIåŠŸèƒ½çš„çŸ¥è¯†å’ŒåŠŸèƒ½è°ƒç”¨è½¨è¿¹ä»¥å¢å¼ºå†…åœ¨æ¨ç†ã€‚é€šè¿‡åœ¨Hephaestus-Forgeä¸ŠæŒç»­é¢„è®­ç»ƒï¼ŒHephaestusåœ¨ä¸‰ä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å°åˆ°ä¸­å‹çš„å¼€æºLLMï¼Œå¹¶ä¸å•†ä¸šLLMç›¸åª²ç¾ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨å¢å¼ºä»£ç†åŸºæœ¬èƒ½åŠ›å’ŒLLMå¯¹æ–°ä»»åŠ¡æˆ–ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07316",
            "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
            "url": "https://huggingface.co/papers/2502.07316",
            "abstract": "Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.",
            "score": 2,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "fd4f34152d4de2c1",
            "authors": [
                "Junlong Li",
                "Daya Guo",
                "Dejian Yang",
                "Runxin Xu",
                "Yu Wu",
                "Junxian He"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "HKUST",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07316.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CodeI/O: Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ CodeI/O Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ² ĞºĞ¾Ğ´, Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CodeI/O Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ…, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with CodeI/O",
                    "desc": "This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++."
                },
                "zh": {
                    "title": "CodeI/Oï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºCodeI/Oï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å°†åŸå§‹ä»£ç è½¬æ¢ä¸ºè¾“å…¥è¾“å‡ºé¢„æµ‹æ ¼å¼ï¼ŒCodeI/Oç³»ç»Ÿåœ°æç‚¼äº†å¤šæ ·çš„æ¨ç†æ¨¡å¼ã€‚æ¨¡å‹é€šè¿‡è‡ªç„¶è¯­è¨€çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ¥é¢„æµ‹ä»£ç çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä»è€Œå¢å¼ºäº†é€»è¾‘æµè§„åˆ’ã€çŠ¶æ€ç©ºé—´æœç´¢ç­‰æ¨ç†åŸè¯­çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeI/Oåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07617",
            "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
            "url": "https://huggingface.co/papers/2502.07617",
            "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.",
            "score": 1,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "503a9dac2cae323c",
            "authors": [
                "Xiao Wang",
                "Ibrahim Alabdulmohsin",
                "Daniel Salz",
                "Zhe Li",
                "Keran Rong",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07617.jpg",
            "data": {
                "categories": [
                    "#cultural_diversity",
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ·Ğ°Ğ¿Ğ°Ğ´Ğ½Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñƒ Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚ĞµÑ€ĞµĞ³Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking Cultural Diversity with 100 Billion Examples",
                    "desc": "This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡é¢„è®­ç»ƒåŠ©åŠ›æ–‡åŒ–å¤šæ ·æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å‰æ‰€æœªæœ‰çš„è§„æ¨¡ä¸Šï¼ˆ1000äº¿ä¸ªç¤ºä¾‹ï¼‰å¯¹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒçš„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è®¸å¤šå¸¸è§çš„è¥¿æ–¹åˆ†ç±»å’Œæ£€ç´¢åŸºå‡†ä¸Šï¼Œæ¨¡å‹æ€§èƒ½åœ¨æ­¤è§„æ¨¡ä¸‹è¶‹äºé¥±å’Œã€‚ç„¶è€Œï¼Œå¯¹äºæ–‡åŒ–å¤šæ ·æ€§çš„ä»»åŠ¡ï¼Œ1000äº¿è§„æ¨¡çš„ç½‘ç»œæ•°æ®å¸¦æ¥äº†æ›´æ˜¾è‘—çš„æå‡ï¼Œå› ä¸ºå®ƒæ¶µç›–äº†é•¿å°¾æ¦‚å¿µã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ˜¾ç¤ºåœ¨ä½èµ„æºè¯­è¨€ä¸Šä¹Ÿæœ‰æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07374",
            "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
            "url": "https://huggingface.co/papers/2502.07374",
            "abstract": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
            "score": 1,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4df9e17df3250cb4",
            "authors": [
                "Dacheng Li",
                "Shiyi Cao",
                "Tyler Griggs",
                "Shu Liu",
                "Xiangxi Mo",
                "Shishir G. Patil",
                "Matei Zaharia",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "affiliations": [
                "Department of Electrical Engineering and Computer Sciences, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07374.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Long CoT) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞºĞ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-32B-Instruct, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 17 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Long CoT, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Long CoT Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ»Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning: Structure Over Content in Large Models",
                    "desc": "This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples."
                },
                "zh": {
                    "title": "é•¿é“¾æ€ç»´ï¼šæ¨ç†æ¨¡å‹çš„å…³é”®ç»“æ„",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡é•¿é“¾æ€ç»´ï¼ˆLong CoTï¼‰è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œè¿™ç§æ€ç»´æ–¹å¼åŒ…æ‹¬åæ€ã€å›æº¯å’Œè‡ªæˆ‘éªŒè¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡æ•°æ®é«˜æ•ˆçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æœ‰æ•ˆå­¦ä¹ é•¿é“¾æ€ç»´ã€‚ä»…ä½¿ç”¨17,000ä¸ªé•¿é“¾æ€ç»´è®­ç»ƒæ ·æœ¬ï¼ŒQwen2.5-32B-Instructæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿é“¾æ€ç»´çš„ç»“æ„å¯¹å­¦ä¹ è¿‡ç¨‹è‡³å…³é‡è¦ï¼Œè€Œå•ä¸ªæ¨ç†æ­¥éª¤çš„å†…å®¹å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07776",
            "title": "Auditing Prompt Caching in Language Model APIs",
            "url": "https://huggingface.co/papers/2502.07776",
            "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.",
            "score": 0,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "48f7472ef1c86b27",
            "authors": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Rohith Kuditipudi",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07776.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#leakage",
                    "#inference",
                    "#ethics",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² LLM: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑÑ‚Ğ¸Ğ¼ Ñ€Ğ¸ÑĞºĞ¸ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… API-Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² LLM. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ñƒ ÑĞµĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OpenAI, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑƒÑ‚ĞµÑ‡ĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ OpenAI ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Timing Variations: A Privacy Risk in Prompt Caching for LLMs",
                    "desc": "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."
                },
                "zh": {
                    "title": "æç¤ºç¼“å­˜çš„éšç§é£é™©ä¸é€æ˜æ€§",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæç¤ºç¼“å­˜ä¼šå¯¼è‡´æ•°æ®ä¾èµ–çš„æ—¶é—´å˜åŒ–ï¼šç¼“å­˜çš„æç¤ºå¤„ç†é€Ÿåº¦æ¯”éç¼“å­˜çš„æç¤ºå¿«ã€‚è¿™äº›æ—¶é—´å·®å¼‚å¯èƒ½å¼•å‘ä¾§ä¿¡é“æ”»å‡»çš„é£é™©ï¼Œä¾‹å¦‚ï¼Œå¦‚æœç¼“å­˜è¢«å¤šä¸ªç”¨æˆ·å…±äº«ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡å¿«é€Ÿçš„APIå“åº”æ—¶é—´è¯†åˆ«å‡ºç¼“å­˜çš„æç¤ºï¼Œä»è€Œè·å–å…¶ä»–ç”¨æˆ·æç¤ºçš„ä¿¡æ¯ã€‚ç”±äºæç¤ºç¼“å­˜å¯èƒ½å¯¼è‡´éšç§æ³„éœ²ï¼Œå› æ­¤APIæä¾›å•†çš„ç¼“å­˜æ”¿ç­–é€æ˜åº¦éå¸¸é‡è¦ã€‚æˆ‘ä»¬å¼€å‘å¹¶è¿›è¡Œç»Ÿè®¡å®¡è®¡ï¼Œä»¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­LLM APIæä¾›å•†çš„æç¤ºç¼“å­˜æƒ…å†µï¼Œå‘ç°ä¸ƒä¸ªAPIæä¾›å•†ï¼ˆåŒ…æ‹¬OpenAIï¼‰ä¹‹é—´å­˜åœ¨å…¨çƒç¼“å­˜å…±äº«ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·æç¤ºçš„éšç§æ³„éœ²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07527",
            "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
            "url": "https://huggingface.co/papers/2502.07527",
            "abstract": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
            "score": 0,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "a6e947f52bde9a9c",
            "authors": [
                "Yingce Xia",
                "Peiran Jin",
                "Shufang Xie",
                "Liang He",
                "Chuan Cao",
                "Renqian Luo",
                "Guoqing Liu",
                "Yue Wang",
                "Zequn Liu",
                "Yuan-Jyue Chen",
                "Zekun Guo",
                "Yeqi Bai",
                "Pan Deng",
                "Yaosen Min",
                "Ziheng Lu",
                "Hongxia Hao",
                "Han Yang",
                "Jielan Li",
                "Chang Liu",
                "Jia Zhang",
                "Jianwei Zhu",
                "Kehan Wu",
                "Wei Zhang",
                "Kaiyuan Gao",
                "Qizhi Pei",
                "Qian Wang",
                "Xixian Liu",
                "Yanting Li",
                "Houtian Zhu",
                "Yeqing Lu",
                "Mingqian Ma",
                "Zun Wang",
                "Tian Xie",
                "Krzysztof Maziarz",
                "Marwin Segler",
                "Zhao Yang",
                "Zilong Chen",
                "Yu Shi",
                "Shuxin Zheng",
                "Lijun Wu",
                "Chen Hu",
                "Peggy Dai",
                "Tie-Yan Liu",
                "Haiguang Liu",
                "Tao Qin"
            ],
            "affiliations": [
                "Microsoft Research AI for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07527.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#architecture",
                    "#science",
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "NatureLM: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ NatureLM - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. Ğ­Ñ‚Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ±ĞµĞ»ĞºĞ¸, Ğ”ĞĞš Ğ¸ Ğ ĞĞš. NatureLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¾Ñ‚ 1 Ğ´Ğ¾ 46,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "NatureLM: Unifying Science Through Language Models",
                    "desc": "This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design."
                },
                "zh": {
                    "title": "è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼šç§‘å­¦å‘ç°çš„æ–°å·¥å…·",
                    "desc": "åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€çš„èƒ½åŠ›ã€‚å—åŸºç¡€æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜ä¸ºå„ä¸ªç§‘å­¦é¢†åŸŸå¼€å‘äº†ç›¸åº”çš„åŸºç¡€æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯å­¤ç«‹è®­ç»ƒçš„ï¼Œç¼ºä¹è·¨é¢†åŸŸæ•´åˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆNatureLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåºåˆ—çš„ç§‘å­¦åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ƒè¿›ç§‘å­¦å‘ç°ã€‚NatureLMç»è¿‡å¤šé¢†åŸŸæ•°æ®çš„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿæ”¯æŒå°åˆ†å­ã€è›‹ç™½è´¨ã€RNAå’Œææ–™çš„ç”Ÿæˆä¸ä¼˜åŒ–ï¼Œå¹¶åœ¨å¤šä¸ªç§‘å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-11.html",
    "link_next": "2025-02-13.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 2,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1,
        "#cultural_diversity": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šè¯­è¨€æ–‡æœ¬å»æ¯’çš„ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç¼ºä¹å¹³è¡Œçš„å¤šè¯­è¨€æ•°æ®é›†ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç”Ÿæˆå¤šè¯­è¨€å¹³è¡Œå»æ¯’æ•°æ®çš„æµæ°´çº¿æ–¹æ³•ã€‚ä»–ä»¬è¿˜ä»‹ç»äº†SynthDetoxMï¼Œä¸€ä¸ªåŒ…å«16,000ä¸ªé«˜è´¨é‡å»æ¯’å¥å¯¹çš„å¤šè¯­è¨€å¹³è¡Œæ–‡æœ¬å»æ¯’æ•°æ®é›†ï¼Œæ¶µç›–å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’Œä¿„è¯­ã€‚æ•°æ®æ¥æºäºä¸åŒçš„æ¯’æ€§è¯„ä¼°æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ä¹ä¸ªç°ä»£å¼€æºLLMè¿›è¡Œå°‘æ ·æœ¬é‡å†™ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºä½¿ç”¨äººå·¥æ ‡æ³¨çš„MultiParaDetoxæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚ä½œè€…å…¬å¼€äº†ä»–ä»¬çš„æ•°æ®é›†å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¤šè¯­è¨€æ–‡æœ¬å»æ¯’ç ”ç©¶ã€‚",
        "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šè¯­è¨€æ–‡æœ¬å»æ¯’çš„ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç¼ºä¹å¹³è¡Œçš„å¤šè¯­è¨€æ•°æ®é›†ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç”Ÿæˆå¤šè¯­è¨€å¹³è¡Œå»æ¯’æ•°æ®çš„æµæ°´çº¿æ–¹æ³•ã€‚ä»–ä»¬è¿˜ä»‹ç»äº†SynthDetoxMï¼Œä¸€ä¸ªåŒ…å«16,000ä¸ªé«˜è´¨é‡å»æ¯’å¥å¯¹çš„å¤šè¯­è¨€å¹³è¡Œæ–‡æœ¬å»æ¯’æ•°æ®é›†ï¼Œæ¶µç›–å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’Œä¿„è¯­ã€‚æ•°æ®æ¥æºäºä¸åŒçš„æ¯’æ€§è¯„ä¼°æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ä¹ä¸ªç°ä»£å¼€æºLLMè¿›è¡Œå°‘æ ·æœ¬é‡å†™ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºä½¿ç”¨äººå·¥æ ‡æ³¨çš„MultiParaDetoxæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚ä½œè€…å…¬å¼€äº†ä»–ä»¬çš„æ•°æ®é›†å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¤šè¯­è¨€æ–‡æœ¬å»æ¯’ç ”ç©¶ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le duÅyÇ”yÃ¡n wÃ©nbÄ›n qÃ¹dÃº de xiÃ n yÇ’u fÄngfÇ cÃºnzÃ i de wÃ¨ntÃ­, zhÇ”yÃ o shÃ¬ quÄ“fÃ¡ pÃ­ngxÃ­ng de duÅyÇ”yÃ¡n shÃ¹jÃ¹jÃ­. ZuÃ²zhÄ› tÃ­chÅ« le yÄ« zhÇ’ng shÄ“ngchÃ©ng duÅyÇ”yÃ¡n pÃ­ngxÃ­ng qÃ¹dÃº shÃ¹jÃ¹ de liÃºshuÇxiÃ n fÄngfÇ. TÄmen hÃ¡i jiÃ¨shÃ o le SynthDetoxM, yÄ«gÃ¨ bÄohÃ¡n 16,000 gÃ¨ gÄo zhÃ¬liÃ ng qÃ¹dÃº jÃ¹ duÃ¬ de duÅyÇ”yÃ¡n pÃ­ngxÃ­ng wÃ©nbÄ›n qÃ¹dÃº shÃ¹jÃ¹jÃ­, hÃ¡n gÇi dÃ©yÇ”, fÇyÇ”, xÄ«bÄnyÃ¡yÇ” hÃ© Ã©yÇ”. ShÃ¹jÃ¹ lÃ¡iyuÃ¡n yÃº bÃ¹tÃ³ng de dÃºxÃ¬ng pÃ­ngjiÄ shÃ¹jÃ¹jÃ­, bÃ¬ng shÇyÃ²ng jiÇ” gÃ¨ xiÃ ndÃ i kÄiyuÃ¡n LLM jÃ¬nxÃ­ng shÇo yÃ ngbÄ›n chÃ³ngxiÄ›. ShÃ­yÃ n biÇomÃ­ng, zÃ i shÃ¹jÃ¹ yÇ’uxiÃ n de qÃ­ngkuÃ ng xiÃ , shÇyÃ²ng hÃ©chÃ©ng shÃ¹jÃ¹jÃ­ xÃ¹nliÃ n de mÃ³xÃ­ng biÇoxiÃ n yÅu yÃº shÇyÃ²ng rÃ©ngÅng biÄozhÃ¹ de MultiParaDetox shÃ¹jÃ¹jÃ­ xÃ¹nliÃ n de mÃ³xÃ­ng. ZuÃ²zhÄ› gÅngkÄi le tÄmen de shÃ¹jÃ¹jÃ­ hÃ© dÃ imÇ, yÇ cÃ¹jÃ¬n duÅyÇ”yÃ¡n wÃ©nbÄ›n qÃ¹dÃº yÃ¡njiÅ«.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'å¤šè¯­è¨€', 'pinyin': 'duÅ yÇ” yÃ¡n', 'trans': 'multilingual'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'å»æ¯’', 'pinyin': 'qÃ¹ dÃº', 'trans': 'detoxification'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},\n{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'},\n{'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'main'},\n{'word': 'ç¼ºä¹', 'pinyin': 'quÄ“ fÃ¡', 'trans': 'lack'},\n{'word': 'å¹³è¡Œ', 'pinyin': 'pÃ­ng xÃ­ng', 'trans': 'parallel'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'æµæ°´çº¿', 'pinyin': 'liÃº shuÇ xiÃ n', 'trans': 'pipeline'},\n{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'},\n{'word': 'SynthDetoxM', 'pinyin': '', 'trans': 'SynthDetoxM'},\n{'word': 'åŒ…å«', 'pinyin': 'bÄo hÃ¡n', 'trans': 'contain'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high-quality'},\n{'word': 'å¥å¯¹', 'pinyin': 'jÃ¹ duÃ¬', 'trans': 'sentence pairs'},\n{'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'},\n{'word': 'å¾·è¯­', 'pinyin': 'dÃ© yÇ”', 'trans': 'German'},\n{'word': 'æ³•è¯­', 'pinyin': 'fÇ yÇ”', 'trans': 'French'},\n{'word': 'è¥¿ç­ç‰™è¯­', 'pinyin': 'xÄ« bÄn yÃ¡ yÇ”', 'trans': 'Spanish'},\n{'word': 'ä¿„è¯­', 'pinyin': 'Ã© yÇ”', 'trans': 'Russian'},\n{'word': 'æ¥æº', 'pinyin': 'lÃ¡i yuÃ¡n', 'trans': 'source'},\n{'word': 'æ¯’æ€§', 'pinyin': 'dÃº xÃ¬ng', 'trans': 'toxicity'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'},\n{'word': 'ä¹ä¸ª', 'pinyin': 'jiÇ” gÃ¨', 'trans': 'nine'},\n{'word': 'ç°ä»£', 'pinyin': 'xiÃ n dÃ i', 'trans': 'modern'},\n{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open-source'},\n{'word': 'LLM', 'pinyin': '', 'trans': 'LLM'},\n{'word': 'è¿›è¡Œ', 'pinyin': 'jÃ¬n xÃ­ng', 'trans': 'conduct'},\n{'word': 'å°‘æ ·æœ¬', 'pinyin': 'shÇo yÃ ng bÄ›n', 'trans': 'few-shot'},\n{'word': 'é‡å†™', 'pinyin': 'chÃ³ng xiÄ›', 'trans': 'rewrite'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'},\n{'word': 'æœ‰é™', 'pinyin': 'yÇ’u xiÃ n', 'trans': 'limited'},\n{'word': 'æƒ…å†µ', 'pinyin': 'qÃ­ng kuÃ ng', 'trans': 'situation'},\n{'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthetic'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'},\n{'word': 'äººå·¥', 'pinyin': 'rÃ©n gÅng', 'trans': 'manual'},\n{'word': 'æ ‡æ³¨', 'pinyin': 'biÄo zhÃ¹', 'trans': 'annotation'},\n{'word': 'MultiParaDetox', 'pinyin': '', 'trans': 'MultiParaDetox'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},\n{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'},\n{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}]",
        "trans": "This article discusses the problems with existing methods for detoxifying multilingual text, primarily the lack of parallel multilingual datasets. The authors propose a pipeline method for generating parallel multilingual detoxification data. They also introduce SynthDetoxM, a multilingual parallel text detoxification dataset containing 16,000 high-quality detoxified sentence pairs, covering German, French, Spanish, and Russian. The data is sourced from various toxicity evaluation datasets and is rewritten using nine modern open-source LLMs with few-shot learning. Experiments show that models trained on the synthetic dataset perform better than models trained on the human-annotated MultiParaDetox dataset when data is limited. The authors have made their dataset and code publicly available to promote research on multilingual text detoxification.",
        "update_ts": "2025-02-11 09:10"
    }
}