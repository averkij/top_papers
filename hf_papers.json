{
    "date": {
        "ru": "15 августа",
        "en": "August 15",
        "zh": "8月15日"
    },
    "time_utc": "2025-08-17 06:36",
    "weekday": 4,
    "issue_id": 5390,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.10433",
            "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2508.10433",
            "abstract": "We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.",
            "score": 127,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "fabdc1bdb3fd820b",
            "authors": [
                "Runqi Qiao",
                "Qiuna Tan",
                "Peiqing Yang",
                "Yanzi Wang",
                "Xiaowan Wang",
                "Enhui Wan",
                "Sitong Zhou",
                "Guanting Dong",
                "Yuchen Zeng",
                "Yida Xu",
                "Jie Wang",
                "Chong Sun",
                "Chen Li",
                "Honggang Zhang"
            ],
            "affiliations": [
                "BUPT",
                "Tsinghua University",
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10433.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#data",
                    "#rl"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Усиление математических способностей ИИ через структурированные знания и обучение с подкреплением",
                    "desc": "We-Math 2.0 - это унифицированная система для улучшения математических рассуждений мультимодальных больших языковых моделей (MLLM). Она включает структурированную систему математических знаний, моделирование пространства данных с ориентацией на модель и обучение с подкреплением. Система состоит из иерархической базы знаний MathBook, наборов данных MathBook-Standard и Pro, двухэтапного фреймворка обучения с подкреплением MathBook-RL. We-Math 2.0 демонстрирует конкурентоспособные результаты на существующих бенчмарках и новом комплексном тесте MathBookEval."
                },
                "en": {
                    "title": "Empowering MLLMs with Enhanced Mathematical Reasoning",
                    "desc": "We-Math 2.0 is a system designed to improve the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). It incorporates a structured knowledge system, model-centric data space modeling, and a reinforcement learning approach to enhance performance. The system includes a hierarchical knowledge framework, a comprehensive dataset for training, and a two-stage reinforcement learning strategy to align models with reasoning tasks. Experimental results indicate that We-Math 2.0 achieves competitive performance on various benchmarks, demonstrating its effectiveness in advancing mathematical reasoning capabilities."
                },
                "zh": {
                    "title": "We-Math 2.0：提升数学推理的智能系统",
                    "desc": "We-Math 2.0 是一个增强多模态大型语言模型（MLLMs）数学推理能力的系统。它通过构建结构化的数学知识体系、以模型为中心的数据空间建模和基于强化学习的训练方法来实现这一目标。该系统包括五级层次的知识点和基本原则，确保了广泛的概念覆盖和灵活性。实验结果表明，We-Math 2.0 在多个基准测试中表现出色，显示出其在数学推理方面的良好泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10711",
            "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
            "url": "https://huggingface.co/papers/2508.10711",
            "abstract": "NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
            "score": 115,
            "issue_id": 5364,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "47af2f1f4d3a49d6",
            "authors": [
                "NextStep Team",
                "Chunrui Han",
                "Guopeng Li",
                "Jingwei Wu",
                "Quan Sun",
                "Yan Cai",
                "Yuang Peng",
                "Zheng Ge",
                "Deyu Zhou",
                "Haomiao Tang",
                "Hongyu Zhou",
                "Kenkun Liu",
                "Ailin Huang",
                "Bin Wang",
                "Changxin Miao",
                "Deshan Sun",
                "En Yu",
                "Fukun Yin",
                "Gang Yu",
                "Hao Nie",
                "Haoran Lv",
                "Hanpeng Hu",
                "Jia Wang",
                "Jian Zhou",
                "Jianjian Sun",
                "Kaijun Tan",
                "Kang An",
                "Kangheng Lin",
                "Liang Zhao",
                "Mei Chen",
                "Peng Xing",
                "Rui Wang",
                "Shiyu Liu",
                "Shutao Xia",
                "Tianhao You",
                "Wei Ji",
                "Xianfang Zeng",
                "Xin Han",
                "Xuelin Zhang",
                "Yana Wei",
                "Yanming Xu",
                "Yimin Jiang",
                "Yingming Wang",
                "Yu Zhou",
                "Yucheng Han",
                "Ziyang Meng",
                "Binxing Jiao",
                "Daxin Jiang",
                "Xiangyu Zhang",
                "Yibo Zhu"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10711.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#cv",
                    "#architecture",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в генерации изображений: NextStep-1 объединяет текст и визуализацию",
                    "desc": "NextStep-1 - это новая модель для генерации изображений по тексту, состоящая из авторегрессионной части (14 млрд параметров) и головы для сопоставления потоков (157 млн параметров). Модель обрабатывает дискретные текстовые токены и непрерывные токены изображений, достигая высоких результатов в генерации и редактировании изображений. В отличие от предыдущих подходов, NextStep-1 не использует тяжелые диффузионные модели или векторное квантование. Авторы планируют открыть исходный код и модели для исследовательского сообщества."
                },
                "en": {
                    "title": "NextStep-1: Revolutionizing Text-to-Image Generation with Autoregressive Power",
                    "desc": "NextStep-1 is a 14 billion parameter autoregressive model designed for text-to-image generation and image editing. It uniquely combines discrete text tokens with continuous image tokens, using a 157 million parameter flow matching head to enhance performance. This model outperforms existing methods by avoiding the computational heaviness of diffusion models and the quantization loss associated with vector quantization. The results demonstrate its ability to generate high-fidelity images and effectively edit them, showcasing the model's versatility and potential for future research."
                },
                "zh": {
                    "title": "NextStep-1：文本到图像生成的新突破",
                    "desc": "NextStep-1是一种14B的自回归模型，配备157M的流匹配头，专注于文本到图像生成和图像编辑。与传统的自回归模型不同，它通过处理离散的文本标记和连续的图像标记，避免了量化损失。该模型在文本到图像生成任务中表现出色，能够生成高保真的图像。此外，NextStep-1在图像编辑方面也展现了强大的能力，证明了其方法的强大和多样性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10881",
            "title": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing",
            "url": "https://huggingface.co/papers/2508.10881",
            "abstract": "ToonComposer is a generative model that unifies inbetweening and colorization in cartoon production, using sparse sketches and a cartoon adaptation method to improve visual quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.",
            "score": 41,
            "issue_id": 5369,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "a47c9a9f44da1314",
            "authors": [
                "Lingen Li",
                "Guangzhi Wang",
                "Zhaoyang Zhang",
                "Yaowei Li",
                "Xiaoyu Li",
                "Qi Dou",
                "Jinwei Gu",
                "Tianfan Xue",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10881.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#games",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в анимации: единая модель для промежуточных кадров и раскрашивания",
                    "desc": "ToonComposer - это генеративная модель, объединяющая процессы создания промежуточных кадров и раскрашивания в производстве мультфильмов. Модель использует разреженные эскизы и метод адаптации к мультипликационному стилю для улучшения визуального качества и эффективности. ToonComposer применяет механизм внедрения разреженных эскизов для точного контроля с помощью ключевых кадров. Модель превосходит существующие методы по визуальному качеству, согласованности движений и эффективности производства."
                },
                "en": {
                    "title": "Streamlining Cartoon Production with ToonComposer",
                    "desc": "ToonComposer is a generative model designed to streamline cartoon production by integrating inbetweening and colorization into one process. It utilizes sparse sketches, allowing artists to provide minimal input while maintaining high visual quality and motion accuracy. The model adapts a modern video foundation to the cartoon domain, ensuring that it can handle large motions without losing temporal consistency. By reducing the manual effort required in traditional animation workflows, ToonComposer enhances both efficiency and flexibility for artists."
                },
                "zh": {
                    "title": "ToonComposer：卡通制作的智能新方式",
                    "desc": "ToonComposer是一种生成模型，旨在将卡通制作中的插帧和上色过程统一为一个后关键帧阶段。它通过稀疏草图注入机制，利用关键帧草图提供精确控制，从而减少人工工作量。该模型还采用了卡通适应方法，能够将现代视频基础模型调整到卡通领域，同时保持时间先验不变。通过使用少量草图和上色参考帧，ToonComposer在视觉质量和生产效率上优于现有方法，极大地提升了艺术家的创作灵活性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09848",
            "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
            "url": "https://huggingface.co/papers/2508.09848",
            "abstract": "A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.",
            "score": 40,
            "issue_id": 5363,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "2ffa20c1920780a2",
            "authors": [
                "Mo Yu",
                "Tsz Ting Chung",
                "Chulun Zhou",
                "Tong Li",
                "Rui Lu",
                "Jiangnan Li",
                "Liyan Xu",
                "Haoshu Lu",
                "Ning Zhang",
                "Jing Li",
                "Jie Zhou"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "NJIT",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09848.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "PRELUDE: новый рубеж в оценке глубины понимания текста искусственным интеллектом",
                    "desc": "Представлен новый бенчмарк PRELUDE для оценки понимания длинного контекста моделями машинного обучения. Задача заключается в определении согласованности приквелов персонажей с оригинальными книгами, что требует глобального понимания и глубокого рассуждения. Результаты показывают значительное отставание современных языковых моделей от людей более чем на 15%. Исследование выявило, что модели часто дают правильные ответы с ошибочными рассуждениями, что указывает на существенные возможности для улучшения в области понимания длинного контекста."
                },
                "en": {
                    "title": "PRELUDE: Bridging the Gap in Long-Context Understanding",
                    "desc": "The paper introduces PRELUDE, a benchmark designed to evaluate how well models understand long contexts by checking if prequel stories align with original narratives. This task requires models to demonstrate global comprehension and deep reasoning, as prequels are not directly part of the original story. The study shows that current state-of-the-art models struggle significantly, with a performance gap of over 15% compared to human reasoning. Additionally, while models may provide correct answers, they often do so with flawed reasoning, highlighting the need for advancements in long-context understanding."
                },
                "zh": {
                    "title": "长文本理解的新挑战：PRELUDE基准",
                    "desc": "PRELUDE是一个评估长文本理解的新基准，主要通过判断角色的前传故事与原著的叙述是否一致来进行评估。这个任务对模型的全球理解和深度推理能力提出了更高的要求，因为前传故事并不是原故事的一部分，评估其合理性通常需要整合间接相关的信息。实验结果显示，当前的先进模型在这一任务上表现不如人类，推理准确率相差超过30%。这些发现强调了在长文本理解和推理方面仍有很大的改进空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10893",
            "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
            "url": "https://huggingface.co/papers/2508.10893",
            "abstract": "STream3R reformulates 3D reconstruction as a decoder-only Transformer problem, using causal attention to efficiently process image sequences and outperform existing methods in both static and dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.",
            "score": 24,
            "issue_id": 5371,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "060a2bebdecb841f",
            "authors": [
                "Yushi Lan",
                "Yihang Luo",
                "Fangzhou Hong",
                "Shangchen Zhou",
                "Honghua Chen",
                "Zhaoyang Lyu",
                "Shuai Yang",
                "Bo Dai",
                "Chen Change Loy",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong",
                "WICT, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10893.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#3d",
                    "#agi"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Революция в 3D-реконструкции: трансформеры для потокового анализа",
                    "desc": "STream3R - это новый подход к 3D-реконструкции, использующий архитектуру декодер-трансформер для предсказания облака точек. Метод применяет каузальное внимание для эффективной обработки последовательностей изображений, что позволяет ему превзойти существующие методы как для статичных, так и для динамических сцен. STream3R обучается на масштабных 3D-датасетах, что обеспечивает хорошую обобщаемость на разнообразные сценарии. Благодаря совместимости с инфраструктурой обучения языковых моделей, метод позволяет эффективно выполнять предобучение и тонкую настройку для различных задач 3D-восприятия."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Transformers",
                    "desc": "STream3R is a new method for 3D reconstruction that treats the problem as a decoder-only Transformer task. It uses causal attention to efficiently handle sequences of images, which allows it to outperform traditional methods that often rely on complex global optimization or simple memory techniques. By leveraging geometric knowledge from large 3D datasets, STream3R adapts well to both static and dynamic scenes, where other methods struggle. This approach not only enhances performance but also fits well with modern training systems, making it suitable for real-time 3D applications."
                },
                "zh": {
                    "title": "STream3R：高效的3D重建新方法",
                    "desc": "STream3R是一种新颖的3D重建方法，将点图预测重新定义为仅使用解码器的Transformer问题。与现有的多视角重建方法相比，STream3R采用因果注意力机制，能够高效处理图像序列，避免了昂贵的全局优化和简单的内存机制。该方法通过从大规模3D数据集中学习几何先验，能够在静态和动态场景中表现出色，克服了传统方法的局限性。实验结果表明，STream3R在各种基准测试中均优于之前的工作，展示了因果Transformer模型在实时3D感知中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10898",
            "title": "Puppeteer: Rig and Animate Your 3D Models",
            "url": "https://huggingface.co/papers/2508.10898",
            "abstract": "Puppeteer is a framework that automates rigging and animation of 3D models using an auto-regressive transformer, attention-based architecture, and differentiable optimization, outperforming existing methods in accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.",
            "score": 22,
            "issue_id": 5376,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "fc24813c49c2e145",
            "authors": [
                "Chaoyue Song",
                "Xiu Li",
                "Fan Yang",
                "Zhongcong Xu",
                "Jiacheng Wei",
                "Fayao Liu",
                "Jiashi Feng",
                "Guosheng Lin",
                "Jianfeng Zhang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute for Infocomm Research, A*STAR",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10898.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Автоматизация риггинга и анимации 3D-моделей с помощью ИИ",
                    "desc": "Puppeteer - это фреймворк для автоматизации риггинга и анимации 3D-моделей, использующий авторегрессионный трансформер и архитектуру на основе механизма внимания. Система сначала предсказывает скелетную структуру с помощью трансформера, а затем определяет веса скиннинга, используя топологически-осведомленное внимание между суставами. Заключительный этап включает дифференцируемую оптимизацию для создания стабильных анимаций высокого качества. Puppeteer превосходит существующие методы по точности и эффективности, успешно обрабатывая разнообразный 3D-контент."
                },
                "en": {
                    "title": "Automating 3D Animation with Puppeteer: Efficiency Meets Precision",
                    "desc": "Puppeteer is a novel framework designed to automate the rigging and animation of 3D models using advanced machine learning techniques. It employs an auto-regressive transformer for predicting skeletal structures, enhancing the representation of joints through a unique tokenization strategy. The framework also utilizes an attention-based architecture to accurately infer skinning weights by considering the relationships between joints. By integrating differentiable optimization, Puppeteer achieves high-quality, stable animations efficiently, surpassing current methods in both accuracy and performance."
                },
                "zh": {
                    "title": "Puppeteer：自动化3D模型绑定与动画的革命性框架",
                    "desc": "Puppeteer是一个框架，旨在自动化3D模型的绑定和动画，使用自回归变换器和基于注意力的架构，具有更高的准确性和效率。该系统通过自回归变换器预测合理的骨骼结构，并采用基于拓扑的注意力机制来推断皮肤权重，从而有效编码关节之间的关系。最后，Puppeteer结合可微优化的动画管道，生成稳定且高保真的动画，计算效率优于现有方法。通过广泛的评估，Puppeteer在骨骼预测准确性和皮肤质量方面显著超越了最先进的技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10833",
            "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
            "url": "https://huggingface.co/papers/2508.10833",
            "abstract": "UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.",
            "score": 21,
            "issue_id": 5364,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "23e6ea5081c3c925",
            "authors": [
                "Zhangxuan Gu",
                "Zhengwen Zeng",
                "Zhenyu Xu",
                "Xingran Zhou",
                "Shuheng Shen",
                "Yunfei Liu",
                "Beitong Zhou",
                "Changhua Meng",
                "Tianyu Xia",
                "Weizhi Chen",
                "Yue Wen",
                "Jingya Dou",
                "Fei Tang",
                "Jinzhen Lin",
                "Yulin Liu",
                "Zhenlin Guo",
                "Yichen Gong",
                "Heng Jia",
                "Changlong Gao",
                "Yuan Guo",
                "Yong Deng",
                "Zhenyu Guo",
                "Liang Chen",
                "Weiqiang Wang"
            ],
            "affiliations": [
                "Ant Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10833.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#rl",
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#agents"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "UI-Venus: ИИ-агент нового поколения для работы с пользовательскими интерфейсами",
                    "desc": "UI-Venus - это мультимодальная модель на основе большой языковой модели для работы с пользовательскими интерфейсами. Она достигает наилучших результатов в задачах навигации и привязки к элементам интерфейса, используя только скриншоты в качестве входных данных. Модель обучается с помощью подкрепляющего обучения на основе Qwen2.5-VL. Авторы представляют новые методы самоэволюции для улучшения производительности в сложных задачах навигации."
                },
                "en": {
                    "title": "UI-Venus: Redefining UI Navigation with Multimodal Intelligence",
                    "desc": "UI-Venus is a multimodal large language model-based UI agent that excels in UI grounding and navigation tasks. It utilizes reinforcement fine-tuning and innovative self-evolving frameworks to achieve state-of-the-art performance with only a few hundred thousand high-quality training samples. The model's variants, 7B and 72B, have surpassed previous benchmarks, demonstrating impressive success rates in both grounding and navigation tasks. Key contributions include the introduction of specialized reward functions and data cleaning strategies, along with a novel framework that enhances navigation performance through improved planning and reasoning."
                },
                "zh": {
                    "title": "UI-Venus：用户界面任务的新标杆",
                    "desc": "UI-Venus是一种基于多模态大语言模型的用户界面代理，能够仅通过截图输入来完成任务。它在用户界面定位和导航任务中表现出色，采用了强化微调和自我演化框架。UI-Venus的7B和72B版本在标准基准测试中超越了之前的最佳模型，显示出其强大的能力。该模型还引入了精心设计的奖励函数和高效的数据清理策略，以提升导航性能和规划能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10875",
            "title": "A Survey on Diffusion Language Models",
            "url": "https://huggingface.co/papers/2508.10875",
            "abstract": "Diffusion Language Models offer parallel token generation, reducing inference latency and capturing bidirectional context, and are compared to autoregressive models in various NLP tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
            "score": 19,
            "issue_id": 5370,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "752e7ed618064491",
            "authors": [
                "Tianyi Li",
                "Mingda Chen",
                "Bowei Guo",
                "Zhiqiang Shen"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "VILA Lab, Mohamed bin Zayed University of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10875.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#inference",
                    "#data",
                    "#survey",
                    "#diffusion"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Диффузионные языковые модели: революция в параллельной генерации текста",
                    "desc": "Диффузионные языковые модели (DLM) предлагают параллельную генерацию токенов, что снижает задержку при выводе и позволяет учитывать двунаправленный контекст. В статье представлен всесторонний обзор текущего ландшафта DLM, включая их эволюцию, основные принципы и современные модели. Авторы анализируют стратегии предобучения, методы оптимизации вывода и мультимодальные расширения DLM. Также обсуждаются ограничения и проблемы DLM, такие как эффективность и обработка длинных последовательностей."
                },
                "en": {
                    "title": "Revolutionizing Text Generation with Diffusion Language Models",
                    "desc": "Diffusion Language Models (DLMs) are a new type of model that generate text tokens simultaneously, which helps to speed up the process and allows for better understanding of context in both directions. They use a method called iterative denoising to improve the quality of generated text while maintaining performance similar to traditional autoregressive models. This paper reviews the development of DLMs, comparing them with other models and discussing their strengths, weaknesses, and applications in natural language processing. It also explores future research opportunities to enhance DLM efficiency and capabilities, particularly in handling longer sequences and multimodal tasks."
                },
                "zh": {
                    "title": "扩散语言模型：并行生成的未来",
                    "desc": "扩散语言模型（DLMs）是一种新兴的强大替代方案，能够并行生成标记，从而减少推理延迟并捕捉双向上下文。与自回归模型相比，DLMs通过迭代去噪过程实现了更高的生成速度，并在多个自然语言处理任务中表现出与自回归模型相当的性能。本文提供了DLMs的全面概述，涵盖了其演变、基础原理及最新模型，并分析了推理策略和优化方法。我们还讨论了DLMs的局限性和挑战，并提出了未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10751",
            "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
            "url": "https://huggingface.co/papers/2508.10751",
            "abstract": "Using Pass@k as a reward in reinforcement learning with verifiable rewards improves exploration and reveals that exploration and exploitation can mutually enhance each other.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., Pass@k Training), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.",
            "score": 13,
            "issue_id": 5367,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "643dd15b9c2eddf6",
            "authors": [
                "Zhipeng Chen",
                "Xiaobo Qin",
                "Youbin Wu",
                "Yue Ling",
                "Qinghao Ye",
                "Wayne Xin Zhao",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10751.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Pass@k: ключ к балансу исследования и эксплуатации в обучении с подкреплением",
                    "desc": "Статья исследует использование метрики Pass@k в качестве награды в обучении с подкреплением с верифицируемыми наградами (RLVR). Авторы обнаружили, что Pass@k улучшает исследовательские способности модели политики. Анализ показал, что исследование и эксплуатация могут взаимно усиливать друг друга, а не конфликтовать. Предложенный подход включает прямое проектирование функции преимущества, что открывает перспективное направление для будущих исследований в RLVR."
                },
                "en": {
                    "title": "Unlocking Exploration: Pass@k Enhances Reinforcement Learning",
                    "desc": "This paper explores the use of Pass@k as a reward in reinforcement learning with verifiable rewards (RLVR) to improve exploration strategies. Traditional methods often rely on Pass@1, which can lead to conservative policies that get stuck in local optima. By employing Pass@k, the authors demonstrate that exploration and exploitation can work together to enhance overall performance. The study also provides an analytical framework for understanding the benefits of this approach and suggests new directions for designing advantage functions in RLVR."
                },
                "zh": {
                    "title": "探索与利用的相互增强：Pass@k的力量",
                    "desc": "本文探讨了在强化学习中使用可验证奖励的策略，特别是使用Pass@k作为奖励来改善探索能力。传统上，使用Pass@1作为奖励会导致策略偏向保守行为，难以平衡探索与利用。通过将Pass@k作为奖励进行训练，研究发现探索与利用可以相互增强，而不是相互对立。本文还提出了一种分析解决方案，展示了Pass@k训练的优势，并为未来的研究方向提供了启示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10576",
            "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
            "url": "https://huggingface.co/papers/2508.10576",
            "abstract": "HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: brightpinkhttps://digital-avatar.github.io/ai/HumanSense/",
            "score": 11,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "94390c2120c8d69a",
            "authors": [
                "Zheng Qin",
                "Ruobing Zheng",
                "Yabing Wang",
                "Tianqi Li",
                "Yi Yuan",
                "Jingdong Chen",
                "Le Wang"
            ],
            "affiliations": [
                "Ant Group",
                "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10576.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оценка человекоориентированного восприятия в мультимодальных ИИ-моделях",
                    "desc": "HumanSense - это комплексный бенчмарк для оценки восприятия и взаимодействия, ориентированного на человека, в мультимодальных больших языковых моделях (MLLM). Он фокусируется на глубоком понимании сложных мультимодальных контекстов и формировании рациональной обратной связи. Исследование показывает, что ведущие MLLM все еще имеют значительный потенциал для улучшения, особенно в задачах продвинутого взаимодействия. Авторы применяют многоэтапное, модально-прогрессивное обучение с подкреплением для улучшения способностей к рассуждению омни-модальной модели, что приводит к существенному повышению результатов оценки."
                },
                "en": {
                    "title": "Enhancing Human-Centered Interaction in MLLMs with HumanSense",
                    "desc": "HumanSense is a new benchmark aimed at assessing how well Multimodal Large Language Models (MLLMs) understand and interact in human-centered scenarios. It focuses on evaluating the models' ability to comprehend complex human intentions and provide empathetic responses based on multimodal inputs like text, audio, and visual data. The research shows that while current MLLMs have made progress, they still need improvement in advanced interaction tasks, especially when it comes to reasoning and contextual understanding. By using a multi-stage reinforcement learning approach, the study enhances the reasoning capabilities of these models, leading to better performance in understanding and responding to human needs and emotions."
                },
                "zh": {
                    "title": "提升人机交互的多模态基准",
                    "desc": "HumanSense是一个基准，用于评估多模态大型语言模型（MLLMs）在以人为中心的感知和交互能力。该基准特别关注对复杂人类意图的理解和提供富有同理心的、上下文相关的反馈。研究表明，尽管领先的MLLMs在这些任务上仍有很大的改进空间，但通过结合视觉、音频和文本信息，可以显著提升其表现。我们采用多阶段的强化学习方法，增强模型的推理能力，从而在评估结果上取得了显著的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10637",
            "title": "Processing and acquisition traces in visual encoders: What does CLIP\n  know about your camera?",
            "url": "https://huggingface.co/papers/2508.10637",
            "abstract": "Visual encoders encode subtle image acquisition parameters that can significantly impact semantic predictions based on their correlation with semantic labels.  \t\t\t\t\tAI-generated summary \t\t\t\t Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions.   We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces",
            "score": 4,
            "issue_id": 5370,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "22c2fd212668ba4a",
            "authors": [
                "Ryan Ramos",
                "Vladan Stojnić",
                "Giorgos Kordopatis-Zilos",
                "Yuta Nakashima",
                "Giorgos Tolias",
                "Noa Garcia"
            ],
            "affiliations": [
                "The University of Osaka",
                "VRG, FEE, Czech Technical University in Prague"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10637.jpg",
            "data": {
                "categories": [
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Невидимые следы в визуальных данных: скрытое влияние на ИИ",
                    "desc": "Исследование показало, что визуальные энкодеры кодируют тонкие параметры получения изображений, которые могут значительно влиять на семантические предсказания. Эти параметры, хотя и незаметны для человеческого глаза, систематически кодируются в обученных визуальных представлениях и легко восстанавливаются. Их присутствие может оказывать существенное влияние на семантические предсказания, в зависимости от корреляции с семантическими метками. Это открытие имеет важные последствия для понимания работы и надежности моделей компьютерного зрения."
                },
                "en": {
                    "title": "Unlocking the Hidden Impact of Image Acquisition on Semantic Predictions",
                    "desc": "This paper investigates how visual encoders capture subtle image acquisition parameters that can influence semantic predictions. Unlike previous studies that focused on severe image corruptions, this research highlights the impact of minor, often unnoticed changes in image acquisition. The authors demonstrate that these parameters are embedded in the visual representations learned by the model and can significantly affect prediction accuracy. The relationship between these parameters and semantic labels can either enhance or degrade performance, depending on their correlation."
                },
                "zh": {
                    "title": "微妙参数对语义预测的深远影响",
                    "desc": "本论文探讨了视觉编码器如何编码图像获取参数，这些参数对语义预测有显著影响。以往的研究主要关注视觉编码器在图像变换和损坏下的鲁棒性，而我们则分析了那些微妙的、甚至人眼难以察觉的图像获取过程参数。研究发现，这些参数在学习的视觉表示中被系统性地编码，并且可以轻易恢复。更重要的是，这些参数的存在对语义预测的影响深远，取决于它们与语义标签之间的相关性或反相关性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10860",
            "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
            "url": "https://huggingface.co/papers/2508.10860",
            "abstract": "A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.",
            "score": 3,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "a06076355558e87a",
            "authors": [
                "Zhaokun Jiang",
                "Ziyin Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10860.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#science",
                    "#dataset",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#data"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Прозрачная оценка качества устного перевода с помощью ИИ",
                    "desc": "Статья представляет многомерную модель для автоматической оценки качества устного перевода. Она объединяет инженерию признаков, аугментацию данных и интерпретируемое машинное обучение. Модель использует прозрачные признаки и анализ SHAP для объяснения предсказаний. Результаты показывают высокую предсказательную способность на новом наборе данных последовательного перевода с английского на китайский."
                },
                "en": {
                    "title": "Enhancing Automated Interpreting Quality with Explainable AI",
                    "desc": "This paper introduces a multi-dimensional modeling framework aimed at improving the quality assessment of automated interpreting. It combines feature engineering, data augmentation, and explainable machine learning to enhance transparency and provide detailed feedback. The framework addresses issues like data scarcity and the need for clearer model predictions by using relevant features and Shapley Value analysis. The results show that this approach not only predicts interpreting quality effectively but also supports learners with valuable insights for self-improvement."
                },
                "zh": {
                    "title": "提升自动化口译质量评估的透明性与可靠性",
                    "desc": "这篇论文提出了一种多维建模框架，旨在提高自动化口译质量评估的效果。该框架结合了特征工程、数据增强和可解释的机器学习，强调透明性和详细的诊断反馈。研究表明，使用BLEURT和CometKiwi评分作为忠实度的预测特征，以及与流利度相关的停顿特征，能够有效提升模型的预测性能。通过这种方法，论文为学习者提供了更可靠的反馈，支持自我调节学习的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10482",
            "title": "When Explainability Meets Privacy: An Investigation at the Intersection\n  of Post-hoc Explainability and Differential Privacy in the Context of Natural\n  Language Processing",
            "url": "https://huggingface.co/papers/2508.10482",
            "abstract": "The study investigates the relationship between privacy and explainability in NLP, using Differential Privacy and Post-hoc Explainability methods, and provides recommendations for balancing both.  \t\t\t\t\tAI-generated summary \t\t\t\t In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.",
            "score": 0,
            "issue_id": 5371,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "2d10a0b967f0f4d6",
            "authors": [
                "Mahdi Dhaini",
                "Stephen Meisenbacher",
                "Ege Erdogan",
                "Florian Matthes",
                "Gjergji Kasneci"
            ],
            "affiliations": [
                "Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10482.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#data",
                    "#ethics",
                    "#interpretability"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Баланс конфиденциальности и объяснимости в NLP: возможности и вызовы",
                    "desc": "Исследование посвящено изучению взаимосвязи между конфиденциальностью и объяснимостью в области обработки естественного языка (NLP). Авторы используют методы дифференциальной приватности и пост-хок объяснимости для анализа этой взаимосвязи. Результаты показывают, что отношения между конфиденциальностью и объяснимостью зависят от различных факторов, включая тип задачи и выбор методов приватизации текста и объяснения. На основе полученных данных, исследователи предлагают практические рекомендации для будущих работ в этой области."
                },
                "en": {
                    "title": "Balancing Privacy and Explainability in NLP",
                    "desc": "This paper explores how privacy and explainability can coexist in Natural Language Processing (NLP). It focuses on Differential Privacy (DP) and Post-hoc Explainability methods to analyze their relationship. The study reveals that achieving both privacy and explainability is possible, but it depends on various factors like the specific task and the chosen methods. The authors provide practical recommendations for balancing these two important aspects in future NLP research."
                },
                "zh": {
                    "title": "隐私与可解释性的平衡之道",
                    "desc": "本研究探讨了自然语言处理（NLP）中隐私与可解释性之间的关系，采用了差分隐私和事后可解释性的方法。尽管近年来对可解释性和隐私保护的研究兴趣显著增加，但两者交集的研究仍然不足。我们的实证研究揭示了隐私与可解释性之间复杂的关系，受多种因素影响，包括下游任务的性质和文本隐私化及可解释性方法的选择。我们强调隐私与可解释性可以共存，并为未来在这一重要交集领域的研究提供了一系列实用建议。"
                }
            }
        }
    ],
    "link_prev": "2025-08-14.html",
    "link_next": "2025-08-18.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "14.08",
        "en": "08/14",
        "zh": "8月14日"
    },
    "short_date_next": {
        "ru": "18.08",
        "en": "08/18",
        "zh": "8月18日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 5,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}