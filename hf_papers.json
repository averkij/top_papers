{
    "date": {
        "ru": "5 декабря",
        "en": "December 5",
        "zh": "12月5日"
    },
    "time_utc": "2024-12-05 08:14",
    "weekday": 3,
    "issue_id": 962,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.02687",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "url": "https://huggingface.co/papers/2412.02687",
            "abstract": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.",
            "score": 23,
            "issue_id": 961,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "d766bad745d5f322",
            "authors": [
                "Viet Nguyen",
                "Anh Nguyen",
                "Trung Dao",
                "Khoi Nguyen",
                "Cuong Pham",
                "Toan Tran",
                "Anh Tran"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech.",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02687.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Повышение стабильности и гибкости одношаговых диффузионных моделей",
                    "desc": "Статья представляет SNOOPI - новый фреймворк для улучшения одношаговых диффузионных моделей генерации изображений. Авторы предлагают метод PG-SB для повышения стабильности обучения путем использования случайного масштаба бесклассификаторного руководства. Также вводится метод NASA для интеграции негативных промптов через кросс-внимание. Эксперименты показывают значительное улучшение базовых моделей по различным метрикам, достигая нового рекорда HPSv2 в 31.08 для одношаговых диффузионных моделей."
                },
                "en": {
                    "title": "SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance",
                    "desc": "This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08."
                },
                "zh": {
                    "title": "SNOOPI：提升一步扩散模型的稳定性与生成质量",
                    "desc": "本论文提出了一种新框架SNOOPI，旨在解决现有一步扩散模型的局限性。我们通过Proper Guidance-SwiftBrush (PG-SB)方法增强了训练的稳定性，采用随机尺度的无分类器引导策略。我们还提出了一种无训练的方法Negative-Away Steer Attention (NASA)，通过交叉注意力将负提示集成到一步扩散模型中，以抑制生成图像中的不必要元素。实验结果表明，我们的方法在多个指标上显著提高了基线模型的性能，创造了一步扩散模型的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03552",
            "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
            "url": "https://huggingface.co/papers/2412.03552",
            "abstract": "360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.",
            "score": 15,
            "issue_id": 958,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "90dc986cabb575af",
            "authors": [
                "Jing Tan",
                "Shuai Yang",
                "Tong Wu",
                "Jingwen He",
                "Yuwei Guo",
                "Ziwei Liu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03552.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Погружение в 360°: от обычного видео к панорамному опыту",
                    "desc": "Статья представляет Imagine360 - первую систему для генерации 360-градусных видео из обычных перспективных видео. Система использует двухветвевую архитектуру с модулями шумоподавления для перспективного и панорамного видео, а также антиподальную маску для захвата дальних зависимостей движения. Предложены решения для адаптации к изменениям угла обзора во входных видео. Эксперименты показывают превосходное качество графики и согласованность движения по сравнению с существующими методами."
                },
                "en": {
                    "title": "Transforming Perspective Videos into Immersive 360° Experiences",
                    "desc": "The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos."
                },
                "zh": {
                    "title": "Imagine360：个性化沉浸式360度视频创作的未来",
                    "desc": "360度视频提供了一种超沉浸式体验，让观众可以从全方位探索动态场景。为实现更友好和个性化的360度视频内容创作，我们提出了Imagine360，这是首个将标准视角视频转换为360度视频的框架。Imagine360通过有限的360度视频数据学习细致的球面视觉和运动模式，采用双分支设计来提供局部和全局约束。实验表明，Imagine360在图形质量和运动一致性方面优于现有的360度视频生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03515",
            "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2412.03515",
            "abstract": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.",
            "score": 14,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6e733cf9c0a1b851",
            "authors": [
                "Shengyuan Zhang",
                "An Zhao",
                "Ling Yang",
                "Zejian Li",
                "Chenye Meng",
                "Haoran Xu",
                "Tianrun Chen",
                "AnYang Wei",
                "Perry Pengyun GU",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Быстрое и качественное завершение 3D LiDAR-сцен для автономных транспортных средств",
                    "desc": "Статья представляет новый метод дистилляции для моделей завершения 3D LiDAR-сцен под названием ScoreLiDAR. Этот метод позволяет значительно ускорить процесс семплирования при сохранении высокого качества завершения сцены. Авторы также вводят новую Структурную Потерю, которая помогает дистиллированной модели лучше улавливать геометрическую структуру 3D LiDAR-сцены. Эксперименты показывают, что ScoreLiDAR ускоряет время завершения более чем в 5 раз и превосходит современные модели завершения 3D LiDAR-сцен."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with ScoreLiDAR",
                    "desc": "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."
                },
                "zh": {
                    "title": "高效3D LiDAR场景补全的新方法",
                    "desc": "扩散模型因其强大的训练稳定性和高质量的场景补全而被应用于3D LiDAR场景补全。然而，慢速采样速度限制了基于扩散的场景补全模型的实际应用，因为自动驾驶车辆需要高效感知周围环境。本文提出了一种新颖的蒸馏方法，称为ScoreLiDAR，旨在实现高效且高质量的场景补全。通过引入结构损失，ScoreLiDAR能够更好地捕捉3D LiDAR场景的几何结构，同时显著加快了每帧的补全时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03069",
            "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2412.03069",
            "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.",
            "score": 9,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "820e62e1bd498d55",
            "authors": [
                "Liao Qu",
                "Huichao Zhang",
                "Yiheng Liu",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Daniel K. Du",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "TokenFlow: единый токенизатор для понимания и генерации изображений",
                    "desc": "TokenFlow - это новый универсальный токенизатор изображений, объединяющий задачи мультимодального понимания и генерации. Он использует инновационную архитектуру с двойным кодбуком, которая разделяет обучение семантических и пиксельных признаков, сохраняя их выравнивание через общий механизм отображения. TokenFlow превосходит существующие модели в задачах понимания, достигая улучшения на 7.2% по сравнению с LLaVA-1.5 13B. Модель также показывает высокие результаты в реконструкции изображений и авторегрессивной генерации, достигая показателей на уровне современных моделей."
                },
                "en": {
                    "title": "TokenFlow: Bridging Understanding and Generation in Image Processing",
                    "desc": "TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics."
                },
                "zh": {
                    "title": "TokenFlow：多模态理解与生成的桥梁",
                    "desc": "本文介绍了一种新颖的图像标记器TokenFlow，它弥合了多模态理解与生成之间的差距。研究表明，理解和生成任务需要不同粒度的视觉信息，传统的单一重建目标向量量化编码器无法有效处理这一问题。TokenFlow通过创新的双代码本架构，解耦了语义和像素级特征学习，同时通过共享映射机制保持它们的对齐。实验结果表明，TokenFlow在多项任务中表现优越，首次证明离散视觉输入在理解性能上超越了LLaVA-1.5 13B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03517",
            "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
            "url": "https://huggingface.co/papers/2412.03517",
            "abstract": "Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.",
            "score": 6,
            "issue_id": 960,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "9d51bf0b60be344b",
            "authors": [
                "Lingen Li",
                "Zhaoyang Zhang",
                "Yaowei Li",
                "Jiale Xu",
                "Xiaoyu Li",
                "Wenbo Hu",
                "Weihao Cheng",
                "Jinwei Gu",
                "Tianfan Xue",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтез новых ракурсов без явного выравнивания видов",
                    "desc": "NVComposer - это новый подход к синтезу новых ракурсов, который устраняет необходимость во внешнем выравнивании видов. Он использует двухпоточную модель диффузии для одновременной генерации целевых ракурсов и позиций камер. Метод включает модуль выравнивания признаков с учетом геометрии, который извлекает геометрические закономерности из плотных стерео моделей во время обучения. Эксперименты показывают, что NVComposer достигает наилучших результатов в задачах генеративного многоракурсного синтеза новых видов."
                },
                "en": {
                    "title": "NVComposer: Generating Novel Views Without External Alignment",
                    "desc": "This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis."
                },
                "zh": {
                    "title": "NVComposer：无须外部对齐的生成新视图合成",
                    "desc": "最近生成模型的进展显著提升了多视图数据的新的视图合成（NVS）能力。然而，现有方法依赖于外部的多视图对齐过程，如显式的姿态估计或预重建，这限制了它们的灵活性和可访问性，尤其是在视图之间重叠不足或遮挡时对齐不稳定的情况下。本文提出了NVComposer，这是一种新颖的方法，消除了对显式外部对齐的需求。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系，从而在生成多视图NVS任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01106",
            "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
            "url": "https://huggingface.co/papers/2412.01106",
            "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.",
            "score": 6,
            "issue_id": 957,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13d96f9bb346e344",
            "authors": [
                "Jun Xiang",
                "Yudong Guo",
                "Leipeng Hu",
                "Boyang Guo",
                "Yancheng Yuan",
                "Juyong Zhang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01106.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Реалистичный говорящий аватар из одного фото",
                    "desc": "Статья описывает новый метод создания реалистичных аватаров, способных говорить и двигаться, на основе всего одного изображения. Авторы используют диффузионные модели для генерации псевдо-видео кадров, которые служат обучающими данными. Они предлагают гибридное представление аватара, сочетающее 3D гауссовы сплаты и полигональную сетку. Метод позволяет точно контролировать жесты и мимику аватара, преодолевая ограничения существующих подходов."
                },
                "en": {
                    "title": "From One Image to a Lifelike Talking Avatar!",
                    "desc": "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."
                },
                "zh": {
                    "title": "从单张图像生成全身会说话的虚拟头像",
                    "desc": "本文提出了一种从单张图像构建全身会说话的虚拟头像的方法。我们解决了复杂动态建模和对新手势与表情的泛化这两个关键问题。通过使用姿态引导的图像到视频扩散模型，我们生成了不完美的视频帧作为伪标签，以实现无缝泛化。实验结果表明，我们的方法能够从单张图像创建出逼真、可精确动画和富有表现力的全身虚拟头像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03558",
            "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
            "url": "https://huggingface.co/papers/2412.03558",
            "abstract": "This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.",
            "score": 4,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "5e1a4c1e1017e7af",
            "authors": [
                "Zehuan Huang",
                "Yuan-Chen Guo",
                "Xingqiao An",
                "Yunhan Yang",
                "Yangguang Li",
                "Zi-Xin Zou",
                "Ding Liang",
                "Xihui Liu",
                "Yan-Pei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "The University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03558.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#diffusion",
                    "#training",
                    "#3d"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "MIDI: Революционный подход к генерации 3D-сцен из одного изображения",
                    "desc": "Статья представляет MIDI - новый подход к генерации трехмерных сцен из одного изображения. MIDI использует предобученные модели генерации 3D-объектов и расширяет их до многоэкземплярных диффузионных моделей, позволяя одновременно генерировать несколько 3D-объектов с точными пространственными отношениями. Ключевой особенностью является новый механизм многоэкземплярного внимания, который эффективно учитывает взаимодействия между объектами и пространственную согласованность непосредственно в процессе генерации. MIDI демонстрирует передовые результаты в генерации сцен из изображений, что подтверждается оценками на синтетических данных, реальных сценах и стилизованных изображениях."
                },
                "en": {
                    "title": "MIDI: Revolutionizing 3D Scene Generation from Single Images",
                    "desc": "This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes."
                },
                "zh": {
                    "title": "MIDI：从单图像生成3D场景的新方法",
                    "desc": "本文介绍了一种名为MIDI的新方法，用于从单张图像生成组合3D场景。与现有依赖重建或检索技术的方法不同，MIDI扩展了预训练的图像到3D对象生成模型，采用多实例扩散模型，实现了多个3D实例的同时生成。MIDI的核心是一个新颖的多实例注意机制，能够有效捕捉对象间的交互和空间一致性，简化了生成过程。该方法在图像到场景生成方面表现出色，经过合成数据、真实场景数据和文本到图像扩散模型生成的风格化场景图像的评估验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03187",
            "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
            "url": "https://huggingface.co/papers/2412.03187",
            "abstract": "While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.",
            "score": 2,
            "issue_id": 961,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6da11fbf4e1ea7d9",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Tianyuan Shi",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "WRPO: Эффективное слияние языковых моделей без прямого объединения параметров",
                    "desc": "В этой статье предлагается новый метод слияния разнородных языковых моделей с открытым исходным кодом - Weighted-Reward Preference Optimization (WRPO). WRPO использует оптимизацию предпочтений между исходными и целевой моделями для эффективного переноса их возможностей, устраняя необходимость в выравнивании словарей и слиянии матриц распределения. Метод вводит стратегию прогрессивной адаптации для решения проблемы различий в распределениях между моделями. Эксперименты показывают, что WRPO превосходит существующие методы слияния знаний и базовые подходы к дообучению на нескольких бенчмарках."
                },
                "en": {
                    "title": "Effortless Fusion of LLMs with WRPO!",
                    "desc": "This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "加权奖励偏好优化：高效融合多种大语言模型",
                    "desc": "本论文提出了一种隐式融合方法，称为加权奖励偏好优化（WRPO），旨在有效整合不同架构和规模的开源大语言模型（LLMs）。WRPO通过优化源模型与目标模型之间的偏好，避免了词汇对齐和矩阵融合的复杂性。该方法引入了渐进适应策略，逐步调整对目标模型和源模型的依赖，从而解决了分布偏差问题。实验结果表明，WRPO在多个基准测试中表现优于现有的知识融合方法和微调基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03085",
            "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
            "url": "https://huggingface.co/papers/2412.03085",
            "abstract": "Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/",
            "score": 2,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "a065164e5fdadf2c",
            "authors": [
                "Shuai Tan",
                "Biao Gong",
                "Yutong Feng",
                "Kecheng Zheng",
                "Dandan Zheng",
                "Shuwei Shi",
                "Yujun Shen",
                "Jingdong Chen",
                "Ming Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03085.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Mimir: Улучшение генерации видео с помощью больших языковых моделей",
                    "desc": "Эта статья представляет Mimir - новый подход к генерации видео на основе текста. Авторы предлагают использовать большие языковые модели (LLM) для улучшения понимания текста и воображения в процессе генерации. Ключевой элемент Mimir - специальный 'token fuser', который объединяет выходы текстовых энкодеров и LLM. Результаты показывают, что Mimir эффективен в создании качественных видео с хорошим пониманием текста, особенно для коротких описаний и динамичных сцен."
                },
                "en": {
                    "title": "Mimir: Bridging Text Understanding and Video Generation",
                    "desc": "This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements."
                },
                "zh": {
                    "title": "Mimir：提升文本到视频生成的智能",
                    "desc": "本文提出了一种名为Mimir的端到端训练框架，用于文本到视频生成（T2V）。该框架通过精心设计的令牌融合器，解决了文本编码器与大型语言模型（LLMs）之间的特征分布差距。Mimir能够充分利用学习到的视频先验，同时增强LLMs在文本理解方面的能力。实验结果表明，Mimir在生成高质量视频时，尤其在处理短文本和动态变化时，表现出色。"
                }
            }
        }
    ],
    "link_prev": "2024-12-04.html",
    "link_next": "2024-12-06.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "short_date_next": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "当前的视频生成模型擅长生成短片，但在创建多镜头、电影般的视频时仍然面临挑战。现有模型通常以单镜头为目标进行训练，难以维持连贯的故事情节和视觉一致性。我们提出了VideoGen-of-Thought (VGoT)，一种专为多镜头视频生成设计的协作且无需训练的架构。VGoT通过结构化、模块化的过程生成视频，包括脚本生成、关键帧生成、镜头级视频生成和平滑机制，确保视频的连贯性和一致性。实验证明，VGoT在生成高质量、连贯的多镜头视频方面超越了现有方法。",
        "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
        "pinyin": "当前的视频生成模型擅长生成短片，但在创建多镜头、电影般的视频时仍然面临挑战。现有模型通常以单镜头为目标进行训练，难以维持连贯的故事情节和视觉一致性。我们提出了VideoGen-of-Thought (VGoT)，一种专为多镜头视频生成设计的协作且无需训练的架构。VGoT通过结构化、模块化的过程生成视频，包括脚本生成、关键帧生成、镜头级视频生成和平滑机制，确保视频的连贯性和一致性。实验证明，VGoT在生成高质量、连贯的多镜头视频方面超越了现有方法。\n\ndāng qián de shì pín shēng chéng mó xíng shàn cháng shēng chéng duǎn piàn, dàn zài chuàng jiàn duō jìng tóu, diàn yǐng bān de shì pín shí réng rán miàn línn tiǎo zhàn. xiàn yǒu mó xíng tōng cháng yǐ dān jìng tóu wéi mù biāo jìn xíng xùn liàn, nán yǐ wéi chí lián de gù shi qíng jié hé shì jué yī zhì xìng. wǒ men tí chū le VideoGen-of-Thought (VGoT), yī zhǒng zhuān wéi duō jìng tóu shì pín shēng chéng shè jì de xié zuò qiě wú xū xùn liàn de jià gòu. VGoT tōng guò jié gòu huà, mó kuài huà de guò chéng shēng chéng shì pín, bāo kuò jiǎo běn shēng chéng, guǎn jiàn zhēn shēng chéng, jìng tóu jí shì pín shēng chéng hé píng huá jī zhì, què bǎo shì pín de lián hé xìng hé yī zhì xìng. shí yàn zhèng míng,VGoT zài shēng chéng gāo zhì liàng, lián hé de duō jìng tóu shì pín fāng miàn chāo yuè le xiàn yǒu fāng fǎ.",
        "vocab": "[{'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '擅长', 'pinyin': 'shàn cháng', 'trans': 'be good at'}, {'word': '短片', 'pinyin': 'duǎn piàn', 'trans': 'short film'}, {'word': '但在', 'pinyin': 'dàn zài', 'trans': 'but in'}, {'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'}, {'word': '多镜头', 'pinyin': 'duō jìng tóu', 'trans': 'multi-shot'}, {'word': '电影般', 'pinyin': 'diàn yǐng bān', 'trans': 'cinematic'}, {'word': '面临', 'pinyin': 'miàn lín', 'trans': 'face'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'}, {'word': '通常', 'pinyin': 'tōng cháng', 'trans': 'usually'}, {'word': '以...为目标', 'pinyin': 'yǐ ... wéi mù biāo', 'trans': 'aim at'}, {'word': '进行', 'pinyin': 'jìn xíng', 'trans': 'carry out'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'hardly'}, {'word': '维持', 'pinyin': 'wéi chí', 'trans': 'maintain'}, {'word': '连贯', 'pinyin': 'lián guàn', 'trans': 'coherent'}, {'word': '故事情节', 'pinyin': 'gù shi qíng jié', 'trans': 'storyline'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '我们', 'pinyin': 'wǒ men', 'trans': 'we'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': 'VideoGen-of-Thought', 'pinyin': 'VideoGen-of-Thought', 'trans': 'VideoGen-of-Thought'}, {'word': 'VGoT', 'pinyin': 'VGoT', 'trans': 'VGoT'}, {'word': '专为', 'pinyin': 'zhuān wéi', 'trans': 'specifically for'}, {'word': '协作', 'pinyin': 'xié zuò', 'trans': 'collaborative'}, {'word': '且', 'pinyin': 'qiě', 'trans': 'and'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'without needing'}, {'word': '训练的', 'pinyin': 'xùn liàn de', 'trans': 'training'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '结构化', 'pinyin': 'jié gòu huà', 'trans': 'structured'}, {'word': '模块化', 'pinyin': 'mó kuài huà', 'trans': 'modularized'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '包括', 'pinyin': 'bāo kuò', 'trans': 'include'}, {'word': '脚本', 'pinyin': 'jiǎo běn', 'trans': 'script'}, {'word': '关键帧', 'pinyin': 'guǎn jiàn zhēn', 'trans': 'keyframe'}, {'word': '镜头级', 'pinyin': 'jìng tóu jí', 'trans': 'shot-level'}, {'word': '平滑', 'pinyin': 'píng huá', 'trans': 'smooth'}, {'word': '机制', 'pinyin': 'jī zhì', 'trans': 'mechanism'}, {'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '超越', 'pinyin': 'chāo yuè', 'trans': 'surpass'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]",
        "trans": "The current video generation models excel at producing short clips but still face challenges in creating multi-shot, cinematic videos. Existing models are typically trained with a single-shot target, making it difficult to maintain coherent storylines and visual consistency. We propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture specifically designed for multi-shot video generation. VGoT generates videos through a structured, modular process that includes script generation, keyframe generation, shot-level video generation, and smoothing mechanisms, ensuring the coherence and consistency of the video. Experiments demonstrate that VGoT surpasses existing methods in generating high-quality, coherent multi-shot videos.",
        "update_ts": "2024-12-04 09:11"
    }
}