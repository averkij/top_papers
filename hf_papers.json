{
    "date": {
        "ru": "25 —Ñ–µ–≤—Ä–∞–ª—è",
        "en": "February 25",
        "zh": "2Êúà25Êó•"
    },
    "time_utc": "2025-02-25 07:10",
    "weekday": 1,
    "issue_id": 2390,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.17157",
            "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
            "url": "https://huggingface.co/papers/2502.17157",
            "abstract": "Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.",
            "score": 28,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "f8790b30bb553397",
            "authors": [
                "Canyu Zhao",
                "Mingyu Liu",
                "Huanyi Zheng",
                "Muzhi Zhu",
                "Zhiyue Zhao",
                "Hao Chen",
                "Tong He",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17157.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#diffusion",
                    "#cv",
                    "#dataset",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DICEPTION - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. DICEPTION —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ—à–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ú–æ–¥–µ–ª—å –∫–æ–¥–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Å –ø–æ–º–æ—â—å—é —Ü–≤–µ—Ç–æ–≤–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. DICEPTION –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—É—Ç–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è."
                },
                "en": {
                    "title": "DICEPTION: Efficient Generalist Perception with Minimal Data",
                    "desc": "This paper presents DICEPTION, a versatile perception model designed to perform multiple tasks efficiently while minimizing the need for extensive computational resources and training data. By leveraging pre-trained text-to-image diffusion models, DICEPTION achieves competitive performance on various perception tasks using only a fraction of the data required by traditional models. The innovative use of color encoding for output representation enhances the model's effectiveness in both entity and semantic segmentation. Overall, DICEPTION demonstrates that it is possible to create a powerful generalist model that requires minimal fine-tuning and can adapt quickly to new tasks."
                },
                "zh": {
                    "title": "DICEPTIONÔºöÈ´òÊïàÁöÑÈÄöÁî®ÊÑüÁü•Ê®°Âûã",
                    "desc": "Êú¨ÊñáÁöÑ‰∏ªË¶ÅÁõÆÊ†áÊòØÂàõÂª∫‰∏Ä‰∏™ÈÄöÁî®ÁöÑÊÑüÁü•Ê®°ÂûãÔºåËÉΩÂ§üÂú®ËÆ°ÁÆóËµÑÊ∫êÂíåËÆ≠ÁªÉÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÂ§ÑÁêÜÂ§öÁßç‰ªªÂä°„ÄÇÊàë‰ª¨‰ΩøÁî®‰∫ÜÂú®Êï∞ÂçÅ‰∫øÂº†ÂõæÂÉè‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°Âûã„ÄÇÈÄöËøáÂÖ®Èù¢ÁöÑËØÑ‰º∞ÊåáÊ†áÔºåDICEPTIONÂú®Â§ö‰∏™ÊÑüÁü•‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫Ü‰∏éÊúÄÂÖàËøõÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂú®ÈÄÇÂ∫îÂÖ∂‰ªñ‰ªªÂä°Êó∂Ôºå‰ªÖÈúÄÂØπ50Âº†ÂõæÂÉèÂíå1%ÁöÑÂèÇÊï∞ËøõË°åÂæÆË∞ÉÔºåÊòæÁ§∫Âá∫ÂÖ∂È´òÊïàÊÄßÂíåÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17129",
            "title": "Thus Spake Long-Context Large Language Model",
            "url": "https://huggingface.co/papers/2502.17129",
            "abstract": "Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.   Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.",
            "score": 19,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "8b44dbb5e39d9b38",
            "authors": [
                "Xiaoran Liu",
                "Ruixiao Li",
                "Mianqiu Huang",
                "Zhigeng Liu",
                "Yuerong Song",
                "Qipeng Guo",
                "Siyang He",
                "Qiqi Wang",
                "Linlin Li",
                "Qun Liu",
                "Yaqian Zhou",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "School of Computer Science Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17129.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#survey",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –≥—Ä–∞–Ω–∏—Ü—ã: –ø—É—Ç—å –∫ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—è –º–µ–∂–¥—É —Å—Ç—Ä–µ–º–ª–µ–Ω–∏–µ–º —Ä–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç LLM –∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ —á–µ–ª–æ–≤–µ–∫–∞ –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Å–≤–æ—é —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç—å. –°—Ç–∞—Ç—å—è –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è 10 –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, —Å—Ç–æ—è—â–∏–º–∏ –ø–µ—Ä–µ–¥ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                },
                "en": {
                    "title": "Unlocking the Power of Long Context in Language Models",
                    "desc": "This paper discusses the importance of long context in Natural Language Processing (NLP) and its impact on Large Language Models (LLMs). It highlights the advancements in extending context length to millions of tokens, which enhances the models' capabilities. The authors explore the challenges faced by LLMs in balancing the need for longer context with their inherent limitations. Additionally, the paper provides a comprehensive overview of the lifecycle of long-context LLMs, covering aspects such as architecture, infrastructure, training, and evaluation, while also posing ten critical questions for future research."
                },
                "zh": {
                    "title": "Èïø‰∏ä‰∏ãÊñáÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÁ´û‰∫âÂäõ",
                    "desc": "Èïø‰∏ä‰∏ãÊñáÊòØËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶Å‰∏ªÈ¢òÔºåÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇÂ∞ΩÁÆ°ËøΩÊ±ÇÈïø‰∏ä‰∏ãÊñáÈù¢‰∏¥ËÆ∏Â§öÊåëÊàòÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÊòØLLMsÁöÑÊ†∏ÂøÉÁ´û‰∫â‰ºòÂäø„ÄÇËøëÂπ¥Êù•ÔºåLLMsÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶Â∑≤Á™ÅÁ†¥Âà∞Êï∞Áôæ‰∏á‰∏™Ê†áËÆ∞ÔºåÁ†îÁ©∂‰πü‰ªéÈïøÂ∫¶Âª∂Â±ïÊâ©Â±ïÂà∞Êû∂ÊûÑ„ÄÅÂü∫Á°ÄËÆæÊñΩ„ÄÅËÆ≠ÁªÉÂíåËØÑ‰º∞ÊäÄÊúØÁöÑÂÖ®Èù¢ÂÖ≥Ê≥®„ÄÇÊú¨ÊñáÂ∞Ü‰ªéÂõõ‰∏™ËßíÂ∫¶Â±ïÁ§∫Èïø‰∏ä‰∏ãÊñáLLMsÁöÑÁîüÂëΩÂë®ÊúüÔºåÂπ∂ÊèêÂá∫ÂΩìÂâçÈù¢‰∏¥ÁöÑÂçÅ‰∏™Êú™Ëß£ÈóÆÈ¢òÔºå‰ª•Êúü‰∏∫Èïø‰∏ä‰∏ãÊñáLLMsÁöÑÁ†îÁ©∂Êèê‰æõÁ≥ªÁªüÊÄßÁöÑ‰ªãÁªç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16584",
            "title": "Audio-FLAN: A Preliminary Release",
            "url": "https://huggingface.co/papers/2502.16584",
            "abstract": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
            "score": 13,
            "issue_id": 2388,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 23",
                "zh": "2Êúà23Êó•"
            },
            "hash": "67b5d61b3df1a4bc",
            "authors": [
                "Liumeng Xue",
                "Ziya Zhou",
                "Jiahao Pan",
                "Zixuan Li",
                "Shuai Fan",
                "Yinghao Ma",
                "Sitong Cheng",
                "Dongchao Yang",
                "Haohan Guo",
                "Yujia Xiao",
                "Xinsheng Wang",
                "Zixuan Shen",
                "Chuanbo Zhu",
                "Xinshen Zhang",
                "Tianchi Liu",
                "Ruibin Yuan",
                "Zeyue Tian",
                "Haohe Liu",
                "Emmanouil Benetos",
                "Ge Zhang",
                "Yike Guo",
                "Wei Xue"
            ],
            "affiliations": [
                "Beihang University",
                "Inner Mongolia University",
                "National University of Singapore",
                "Queen Mary University of London",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16584.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset"
                ],
                "emoji": "üéµ",
                "ru": {
                    "title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Audio-FLAN - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–µ —Å –∞—É–¥–∏–æ. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 80 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –∑–≤—É–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 100 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤. Audio-FLAN —Å–æ–∑–¥–∞–µ—Ç –æ—Å–Ω–æ–≤—É –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –î–∞—Ç–∞—Å–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ HuggingFace –∏ GitHub –∏ –±—É–¥–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è."
                },
                "en": {
                    "title": "Unifying Audio Understanding and Generation with Audio-FLAN",
                    "desc": "This paper presents Audio-FLAN, a large-scale dataset designed for instruction tuning in audio tasks. It addresses the challenge of integrating audio understanding and generation into unified audio-language models. By providing over 100 million instances across 80 diverse tasks, Audio-FLAN enables models to perform both comprehension and generation tasks in a zero-shot manner. The dataset aims to enhance the capabilities of large language models in handling various audio domains effectively."
                },
                "zh": {
                    "title": "Èü≥È¢ëÁêÜËß£‰∏éÁîüÊàêÁöÑÁªü‰∏Ä‰πãË∑Ø",
                    "desc": "ÊúÄËøëÈü≥È¢ëÊ†áËÆ∞ÊäÄÊúØÁöÑËøõÊ≠•ÊòæËëóÊèêÂçá‰∫ÜÈü≥È¢ëËÉΩÂäõ‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï¥Âêà„ÄÇÁÑ∂ËÄåÔºåÈü≥È¢ëÁêÜËß£ÂíåÁîüÊàêÈÄöÂ∏∏Ë¢´ËßÜ‰∏∫‰∏çÂêåÁöÑ‰ªªÂä°ÔºåËøôÈòªÁ¢ç‰∫ÜÁúüÊ≠£Áªü‰∏ÄÁöÑÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇÂ∞ΩÁÆ°Êåá‰ª§Ë∞É‰ºòÂú®ÊñáÊú¨ÂíåËßÜËßâÈ¢ÜÂüüÁöÑÊ≥õÂåñÂíåÈõ∂Ê†∑Êú¨Â≠¶‰π†‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂú®Èü≥È¢ëÈ¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜAudio-FLANÔºåËøôÊòØ‰∏Ä‰∏™Ê∂µÁõñ80‰∏™Â§öÊ†∑Âåñ‰ªªÂä°ÁöÑÂ§ßËßÑÊ®°Êåá‰ª§Ë∞É‰ºòÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá1‰∫ø‰∏™ÂÆû‰æãÔºå‰∏∫Áªü‰∏ÄÁöÑÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16614",
            "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2502.16614",
            "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
            "score": 10,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 23",
                "zh": "2Êúà23Êó•"
            },
            "hash": "c70e868ad4c1b726",
            "authors": [
                "Alexander Zhang",
                "Marcus Dong",
                "Jiaheng Liu",
                "Wei Zhang",
                "Yejie Wang",
                "Jian Yang",
                "Ge Zhang",
                "Tianyu Liu",
                "Zhongyuan Peng",
                "Yingshui Tan",
                "Yuanxing Zhang",
                "Zhexu Wang",
                "Weixun Wang",
                "Yancheng He",
                "Ken Deng",
                "Wangchunshu Zhou",
                "Wenhao Huang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "BUPT",
                "CASIA",
                "Kuaishou",
                "M-A-P",
                "NJU",
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16614.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "CodeCriticBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeCriticBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫—Ä–∏—Ç–∏–∫–æ–≤–∞—Ç—å –∫–æ–¥. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–¥–µ —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. CodeCriticBench –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –æ—Ü–µ–Ω–∫—É –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —á–µ–∫-–ª–∏—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ LLM, –ø–æ–∫–∞–∑–∞–≤—à–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞."
                },
                "en": {
                    "title": "Enhancing Code Critique with CodeCriticBench",
                    "desc": "This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities."
                },
                "zh": {
                    "title": "ÂÖ®Èù¢ËØÑ‰º∞LLMsÁöÑ‰ª£Á†ÅÊâπËØÑËÉΩÂäõ",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊâπËØÑËÉΩÂäõÂØπ‰∫éÊé®ÁêÜËÉΩÂäõËá≥ÂÖ≥ÈáçË¶ÅÔºåÂèØ‰ª•Êèê‰æõÂøÖË¶ÅÁöÑÂª∫ËÆÆÔºåÂ¶ÇËØ¶ÁªÜÂàÜÊûêÂíåÂª∫ËÆæÊÄßÂèçÈ¶à„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞LLMsÁöÑÊâπËØÑËÉΩÂäõÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊèêÂá∫‰∫ÜÂ§ö‰∏™ÊâπËØÑÂü∫ÂáÜÔºå‰ΩÜÁé∞ÊúâÂü∫ÂáÜÂ≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÊÄßÔºå‰æãÂ¶ÇÂØπ‰ª£Á†Å‰ªªÂä°ÁöÑËØÑ‰º∞‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑ‰ª£Á†ÅÊâπËØÑÂü∫ÂáÜÔºåÁß∞‰∏∫CodeCriticBenchÔºåÊ∂µÁõñ‰∫Ü‰ª£Á†ÅÁîüÊàêÂíå‰ª£Á†ÅÈóÆÁ≠î‰∏§Áßç‰∏ªÊµÅ‰ªªÂä°ÔºåÂπ∂ËÆæËÆ°‰∫ÜÁªÜËá¥ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇÈÄöËøáÂØπÁé∞ÊúâLLMsÁöÑÂπøÊ≥õÂÆûÈ™åÁªìÊûúÔºåÊàë‰ª¨È™åËØÅ‰∫ÜCodeCriticBenchÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16033",
            "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
            "url": "https://huggingface.co/papers/2502.16033",
            "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
            "score": 9,
            "issue_id": 2386,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 22",
                "zh": "2Êúà22Êó•"
            },
            "hash": "3e5fc69b8713e252",
            "authors": [
                "Qianqi Yan",
                "Yue Fan",
                "Hongquan Li",
                "Shan Jiang",
                "Yang Zhao",
                "Xinze Guan",
                "Ching-Chen Kuo",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16033.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMIR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ç–µ–∫—Å—Ç–æ–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ. MMIR –≤–∫–ª—é—á–∞–µ—Ç 534 —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏ –≤–Ω–µ–¥—Ä–µ–Ω–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏ –≤ –ø—è—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ —à–µ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM, –ø–æ–∫–∞–∑–∞–≤, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥—Ä—É–≥–∏–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ —É—è–∑–≤–∏–º—ã –∫ –æ—à–∏–±–∫–∞–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ MLLM."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content",
                    "desc": "This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÔºåËß£ÂÜ≥Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏ç‰∏ÄËá¥ÊÄß",
                    "desc": "Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏ªË¶ÅÂú®‰∏ÄËá¥ÁöÑËßÜËßâ-ÊñáÊú¨ËæìÂÖ•‰∏äËøõË°åËÆ≠ÁªÉÂíåÊµãËØïÔºåÂ∞ö‰∏çÊ∏ÖÊ•öÂÆÉ‰ª¨ËÉΩÂê¶Â§ÑÁêÜÁé∞ÂÆû‰∏ñÁïå‰∏≠Â∏ÉÂ±Ä‰∏∞ÂØåÂÜÖÂÆπÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÄÅ‰∏ç‰∏ÄËá¥ÊÄßÊé®ÁêÜÔºàMMIRÔºâÂü∫ÂáÜÔºå‰ª•ËØÑ‰º∞MLLMsÂú®Ê£ÄÊµãÂíåÊé®ÁêÜËØ≠‰πâ‰∏çÂåπÈÖçÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇMMIRÂåÖÂê´534‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊ†∑Êú¨ÔºåÊ∂µÁõñ‰∫î‰∏™Êé®ÁêÜÂØÜÈõÜÂûãÁ±ªÂà´ÁöÑÂêàÊàêÈîôËØØ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂÖ∑Â§á‰∏ìÈó®Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÊ®°ÂûãÂú®Â§ÑÁêÜ‰∏ç‰∏ÄËá¥ÊÄßÊó∂Ë°®Áé∞‰ºòÂºÇÔºåËÄåÂºÄÊ∫êÊ®°ÂûãÂàôÁâπÂà´ÂÆπÊòìÂèóÂà∞‰∏ç‰∏ÄËá¥ÊÄßÈîôËØØÁöÑÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17407",
            "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2502.17407",
            "abstract": "Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although \"thinking LLMs\" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.",
            "score": 7,
            "issue_id": 2388,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "f77ddcdca8181036",
            "authors": [
                "Guijin Son",
                "Jiwoo Hong",
                "Hyunwoo Ko",
                "James Thorne"
            ],
            "affiliations": [
                "KAIST AI",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17407.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#long_context",
                    "#multilingual",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ø–ú –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞: –≤—ã–∑–æ–≤—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MCLM - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ —Å –∑–∞–¥–∞—á–∞–º–∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–∞ 55 —è–∑—ã–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–µ—Å—Ç–∏—Ä—É—é—Ç —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –Ω–∞ –¥–≤—É—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: Qwen2.5-1.5B Math –∏ MR1-1.5B. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å '–¥—É–º–∞—é—â–∏—Ö' –Ø–ú —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ."
                },
                "en": {
                    "title": "Exploring Test-Time Scaling for Multilingual LLMs",
                    "desc": "This paper investigates the effectiveness of test-time scaling methods for multilingual large language models (LLMs) using a new benchmark called MCLM, which includes math problems in 55 languages. The authors evaluate three scaling techniques: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF) on two LLMs, Qwen2.5-1.5B Math and MR1-1.5B. Results indicate that ORM with Qwen2.5-1.5B Math achieves the highest score, while BF shows limited improvement across languages compared to English. The findings suggest that traditional scaling methods may perform similarly to advanced LLMs when inference resources are equal, and that test-time scaling may not generalize well to multilingual tasks."
                },
                "zh": {
                    "title": "Â§öËØ≠Ë®ÄÊï∞Â≠¶Âü∫ÂáÜ‰∏éÊµãËØïÊó∂Èó¥Êâ©Â±ïÁöÑÊé¢Á¥¢",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫MCLMÁöÑÂ§öËØ≠Ë®ÄÊï∞Â≠¶Âü∫ÂáÜÔºåÊ∂µÁõñ55ÁßçËØ≠Ë®ÄÁöÑÁ´û‰∫âÁ∫ßÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊµãËØï‰∫Ü‰∏âÁßçÊµãËØïÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÔºöÁªìÊûúÂ•ñÂä±Âª∫Ê®°ÔºàORMÔºâ„ÄÅËøáÁ®ãÂ•ñÂä±Âª∫Ê®°ÔºàORMÔºâÂíåÈ¢ÑÁÆóÂº∫Âà∂ÔºàBFÔºâÔºåÂπ∂Âú®‰∏§‰∏™Â§öËØ≠Ë®ÄÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏äËøõË°åÂÆûÈ™å„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®Qwen2.5-1.5B Math‰∏éORMÁªìÂêàÊó∂ÔºåÂú®MCLM‰∏äÂæóÂàÜ‰∏∫35.8ÔºåËÄåMR1-1.5BÂú®BF‰∏ãÂæóÂàÜ‰∏∫35.2„ÄÇÂ∞ΩÁÆ°‚ÄúÊÄùËÄÉÂûãÂ§ßËØ≠Ë®ÄÊ®°Âûã‚ÄùÂèóÂà∞ÂÖ≥Ê≥®Ôºå‰ΩÜÊàë‰ª¨ÂèëÁé∞ÂÖ∂ÊÄßËÉΩ‰∏é‰º†ÁªüÁöÑÊâ©Â±ïÊñπÊ≥ïÁõ∏ÂΩìÔºå‰∏îÊµãËØïÊó∂Èó¥Êâ©Â±ïÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏äÁöÑÊïàÊûú‰∏çÂ¶ÇÈ¢ÑÊúü„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16922",
            "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
            "url": "https://huggingface.co/papers/2502.16922",
            "abstract": "Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.",
            "score": 7,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "8e0389428d77685b",
            "authors": [
                "Zhenglin Wang",
                "Jialong Wu",
                "Pengfei LI",
                "Yong Jiang",
                "Deyu Zhou"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16922.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "CTM: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∏—Ç–∞–π—Å–∫–æ–π –¥–∏–Ω–∞—Å—Ç–∏—á–µ—Å–∫–æ–π —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ Chinese Time Reasoning (CTM) —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏, –ø–æ–ø–∞—Ä–Ω–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å —É—á–µ—Ç–æ–º –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π. CTM –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö –∏ –∏–º–µ—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—è–≤–ª—è—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å CTM, –∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM."
                },
                "en": {
                    "title": "Enhancing Temporal Reasoning with Chinese Time Benchmark",
                    "desc": "This paper presents the Chinese Time Reasoning (CTM) benchmark, which aims to enhance the evaluation of Large Language Models (LLMs) in the area of temporal reasoning. Unlike existing benchmarks that are rule-based and limited in scope, CTM focuses on the rich context of Chinese dynastic history, allowing for a deeper assessment of temporal relationships. It emphasizes the importance of cross-entity relationships and pairwise temporal alignment, ensuring that the reasoning is both contextualized and culturally relevant. The results from extensive experiments indicate significant challenges for LLMs in this domain, suggesting areas for future research and improvement."
                },
                "zh": {
                    "title": "‰∏≠ÊñáÊó∂Èó¥Êé®ÁêÜÔºöÊèêÂçáÊú∫Âô®Â≠¶‰π†ÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõ",
                    "desc": "Êó∂Èó¥Êé®ÁêÜÊòØ‰∫∫Á±ªËÆ§Áü•ÁöÑÂü∫Á°ÄÔºåÂØπËÆ∏Â§öÂÆûÈôÖÂ∫îÁî®Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êó∂Èó¥Êé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÁé∞ÊúâÂü∫ÂáÜ‰∏ªË¶Å‰æùËµñ‰∫éÂü∫‰∫éËßÑÂàôÁöÑÊûÑÂª∫ÔºåÁº∫‰πè‰∏ä‰∏ãÊñáÊ∑±Â∫¶ÔºåÂπ∂‰∏îÊ∂âÂèäÁöÑÊó∂Èó¥ÂÆû‰ΩìËåÉÂõ¥ÊúâÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏≠ÊñáÊó∂Èó¥Êé®ÁêÜÔºàCTMÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏≠ÂõΩÂéÜÂè≤Êó∂Èó¥Êé®ÁêÜÊñπÈù¢ÁöÑÂü∫ÂáÜ„ÄÇCTMÂº∫Ë∞ÉË∑®ÂÆû‰ΩìÂÖ≥Á≥ª„ÄÅÊàêÂØπÊó∂Èó¥ÂØπÈΩê‰ª•Âèä‰∏ä‰∏ãÊñáÂåñÂíåÊñáÂåñÂü∫Á°ÄÁöÑÊé®ÁêÜÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16894",
            "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
            "url": "https://huggingface.co/papers/2502.16894",
            "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose Great LoRA Mixture-of-Expert (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.",
            "score": 7,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "020c0f54f92a9238",
            "authors": [
                "Chenghao Fan",
                "Zhenyi Lu",
                "Sichen Liu",
                "Xiaoye Qu",
                "Wei Wei",
                "Chengfeng Gu",
                "Yu Cheng"
            ],
            "affiliations": [
                "School of Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16894.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "üêê",
                "ru": {
                    "title": "GOAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏",
                    "desc": "–≠—Ça —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GOAT (Great LoRA Mixture-of-Expert) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ (LoRA) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. GOAT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é SVD-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts –∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π LoRA –∏ –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 25 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ GOAT –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Boosting LoRA with GOAT: A New Path to Efficiency in LLMs",
                    "desc": "This paper introduces Great LoRA Mixture-of-Expert (GOAT), a new framework designed to enhance the performance of Low-Rank Adaptation (LoRA) for Large Language Models (LLMs). GOAT addresses the limitations of existing methods by integrating adaptive priors through a singular value decomposition (SVD)-structured Mixture-of-Experts (MoE) architecture. It also aligns the optimization process with that of fully fine-tuned MoE models by introducing a theoretical scaling factor. The results show that GOAT significantly improves efficiency and performance across various tasks, effectively bridging the gap between LoRA and Full Fine-Tuning."
                },
                "zh": {
                    "title": "ÊèêÂçáLoRAÊÄßËÉΩÁöÑÂÖ®Êñ∞Ê°ÜÊû∂ÔºöGOAT",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Great LoRA Mixture-of-ExpertÔºàGOATÔºâÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´ò‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇGOATÈÄöËøáËá™ÈÄÇÂ∫îÊï¥ÂêàÁõ∏ÂÖ≥ÁöÑÂÖàÈ™åÁü•ËØÜÔºå‰ΩøÁî®Â•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÁªìÊûÑÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊù•‰ºòÂåñLoRA„ÄÇËØ•Ê°ÜÊû∂ËøòÈÄöËøáÊé®ÂØºÁêÜËÆ∫Áº©ÊîæÂõ†Â≠êÔºå‰Ωø‰ºòÂåñËøáÁ®ã‰∏éÂÆåÂÖ®ÂæÆË∞ÉÔºàFull FTÔºâÁöÑMoEÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGOATÂú®25‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁº©Â∞è‰∫Ü‰∏éÂÆåÂÖ®ÂæÆË∞ÉÁöÑÂ∑ÆË∑ù„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17435",
            "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
            "url": "https://huggingface.co/papers/2502.17435",
            "abstract": "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.",
            "score": 6,
            "issue_id": 2390,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "896a996c71c167eb",
            "authors": [
                "Chen-Wei Chang",
                "Cheng-De Fan",
                "Chia-Che Chang",
                "Yi-Chen Lo",
                "Yu-Chee Tseng",
                "Jiun-Long Huang",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "MediaTek Inc.",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17435.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –¥–ª—è –ª—é–±—ã—Ö –∫–∞–º–µ—Ä",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ GCC –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. GCC –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–æ—Ä–∏—Å–æ–≤–∫–∏ —Ü–≤–µ—Ç–æ–≤—ã—Ö —à–∫–∞–ª, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö –æ—Å–≤–µ—â–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –õ–∞–ø–ª–∞—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —à–∫–∞–ª—ã –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ü–≤–µ—Ç–∞ –∫ –æ—Å–≤–µ—â–µ–Ω–∏—é. GCC –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–∞–º–µ—Ä–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –æ—à–∏–±–∫–µ –≤ –Ω–∞–∏—Ö—É–¥—à–∏—Ö 25% —Å–ª—É—á–∞–µ–≤."
                },
                "en": {
                    "title": "GCC: Robust Color Constancy Across Cameras",
                    "desc": "This paper introduces GCC, a novel method for achieving color constancy in images taken with different camera sensors. It utilizes diffusion models to inpaint color checkers, which helps in estimating the illumination of the scene. Key innovations include a single-step inference process for inpainting, a technique to maintain the structure of checkers while adapting colors based on illumination, and a data augmentation strategy to improve the handling of color checker annotations. The results show that GCC outperforms existing methods in challenging cross-camera scenarios, demonstrating its robustness and ability to generalize without needing specific training for each camera."
                },
                "zh": {
                    "title": "GCCÔºöË∑®Áõ∏Êú∫Âú∫ÊôØ‰∏≠ÁöÑÈ¢úËâ≤ÊÅíÂ∏∏ÊÄßÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢úËâ≤ÊÅíÂ∏∏ÊÄßÊñπÊ≥ïGCCÔºåÊó®Âú®Ëß£ÂÜ≥‰∏çÂêåÁõ∏Êú∫‰º†ÊÑüÂô®‰πãÈó¥ÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇGCCÂà©Áî®Êâ©Êï£Ê®°ÂûãÂ∞ÜÈ¢úËâ≤Ê£ãÁõòÊ†ºÂ°´ÂÖÖÂà∞ÂõæÂÉè‰∏≠Ôºå‰ª•‰º∞ËÆ°ÂÖâÁÖßÊù°‰ª∂„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨ÂçïÊ≠•Á°ÆÂÆöÊÄßÊé®ÁêÜ„ÄÅÊãâÊôÆÊãâÊñØÂàÜËß£ÊäÄÊúØÂíåÂü∫‰∫éÊé©Á†ÅÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÔºåÁ°Æ‰øù‰∫ÜÂú®‰∏çÂêåÂÖâÁÖß‰∏ãÁöÑÈ¢úËâ≤ÈÄÇÂ∫îÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGCCÂú®Ë∑®Áõ∏Êú∫Âú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑Êúâ‰ºòË∂äÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17110",
            "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2502.17110",
            "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks.",
            "score": 5,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "1900233d7723f824",
            "authors": [
                "Junyang Wang",
                "Haiyang Xu",
                "Xi Zhang",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Jitao Sang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17110.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#agents"
                ],
                "emoji": "üì±",
                "ru": {
                    "title": "–í–∏–¥–µ–æ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤",
                    "desc": "Mobile-Agent-V - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–¥–µ–æ–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –∏ –≤–∫–ª—é—á–∞–µ—Ç –≤–∏–¥–µ–æ–∞–≥–µ–Ω—Ç–∞ –∏ –∞–≥–µ–Ω—Ç–∞ –≥–ª—É–±–æ–∫–æ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, —á—Ç–æ –¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç 30% —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏."
                },
                "en": {
                    "title": "Empowering Mobile Automation with Video Guidance",
                    "desc": "The paper presents Mobile-Agent-V, a novel framework designed to enhance task management on mobile devices through automation. It addresses the limitations of existing AI frameworks that lack sufficient operational knowledge by utilizing video guidance to provide rich, actionable insights. The framework employs a sliding window strategy along with a video agent and deep-reflection agent to ensure that the system's actions are in line with user instructions. Experimental results demonstrate that Mobile-Agent-V significantly improves performance by 30% over traditional methods, making it a more efficient solution for mobile automation."
                },
                "zh": {
                    "title": "ÁßªÂä®Ëá™Âä®ÂåñÁöÑÊñ∞Á™ÅÁ†¥ÔºöMobile-Agent-V",
                    "desc": "ÈöèÁùÄÁßªÂä®ËÆæÂ§á‰ΩøÁî®ÁöÑÂø´ÈÄüÂ¢ûÈïøÔºå‰ªªÂä°ÁÆ°ÁêÜÁöÑËá™Âä®ÂåñÈúÄÊ±Ç‰πüÂú®Â¢ûÂä†„ÄÇËÆ∏Â§öÂü∫‰∫é‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ°ÜÊû∂Áî±‰∫éÁº∫‰πèË∂≥Â§üÁöÑÊìç‰ΩúÁü•ËØÜËÄåÈù¢‰∏¥ÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑMobile-Agent-VÊ°ÜÊû∂Âà©Áî®ËßÜÈ¢ëÊåáÂØºÊèê‰æõ‰∏∞ÂØå‰∏îÁªèÊµéÁöÑÊìç‰ΩúÁü•ËØÜÔºå‰ªéËÄåÊîπÂñÑÁßªÂä®Ëá™Âä®Âåñ„ÄÇÈÄöËøáÈõÜÊàêÊªëÂä®Á™óÂè£Á≠ñÁï•ÂíåËßÜÈ¢ë‰ª£ÁêÜÔºåMobile-Agent-VËÉΩÂ§üÈ´òÊïàÂú∞Â≠¶‰π†ÂíåÊâßË°å‰ªªÂä°ÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂ÊÄßËÉΩÊØîÁé∞ÊúâÊ°ÜÊû∂ÊèêÈ´ò‰∫Ü30%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15894",
            "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.15894",
            "abstract": "Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality 2times extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables 3times extrapolation by minimal fine-tuning without long videos. Project page and codes: https://riflex-video.github.io/{https://riflex-video.github.io/.}",
            "score": 4,
            "issue_id": 2388,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 21",
                "zh": "2Êúà21Êó•"
            },
            "hash": "3c5243aaae60e8eb",
            "authors": [
                "Min Zhao",
                "Guande He",
                "Yixiao Chen",
                "Hongzhou Zhu",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "Pazhou Laboratory (Huangpu)",
                "ShengShu",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15894.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "RIFLEx: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RIFLEx –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ–ª—å —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –∏ –≤—ã—è–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—É—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —á–∞—Å—Ç–æ—Ç—É, –≤–ª–∏—è—é—â—É—é –Ω–∞ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é. RIFLEx —É–º–µ–Ω—å—à–∞–µ—Ç —ç—Ç—É —á–∞—Å—Ç–æ—Ç—É, –ø–æ–¥–∞–≤–ª—è—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Ç—Ä–µ—Ö–∫—Ä–∞—Ç–Ω–æ–≥–æ - —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π."
                },
                "en": {
                    "title": "RIFLEx: Enhancing Video Length Extrapolation with Frequency Insights",
                    "desc": "This paper addresses the challenge of generating longer videos while maintaining temporal coherence. The authors analyze how frequency components in positional embeddings affect video extrapolation and identify a key frequency that influences this behavior. They introduce RIFLEx, a simple yet effective method that reduces this intrinsic frequency to minimize repetition and ensure consistent motion. RIFLEx achieves impressive results, allowing for 2x extrapolation without additional training and improving quality for 3x extrapolation with minimal fine-tuning."
                },
                "zh": {
                    "title": "RIFLExÔºöÈ´òÊïàËßÜÈ¢ëÂ§ñÊé®ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "ÊúÄËøëËßÜÈ¢ëÁîüÊàêÊäÄÊúØÁöÑËøõÊ≠•‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂêàÊàêÈ´òË¥®ÈáèÁöÑÈïøËææ‰∏ÄÂàÜÈíüÁöÑËßÜÈ¢ë„ÄÇÁÑ∂ËÄåÔºåÁîüÊàêÊõ¥ÈïøÁöÑËßÜÈ¢ëÂπ∂‰øùÊåÅÊó∂Èó¥‰∏ÄËá¥ÊÄß‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÊåëÊàòÔºåÁé∞ÊúâÁöÑÈïøÂ∫¶Â§ñÊé®ÊñπÊ≥ïÂæÄÂæÄÂØºËá¥Êó∂Èó¥ÈáçÂ§çÊàñËøêÂä®ÂáèÈÄü„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Á≥ªÁªüÂú∞ÂàÜÊûê‰∫Ü‰ΩçÁΩÆÂµåÂÖ•‰∏≠È¢ëÁéáÊàêÂàÜÁöÑ‰ΩúÁî®ÔºåÂπ∂ËØÜÂà´Âá∫‰∏ÄÁßç‰∏ªË¶ÅÂΩ±ÂìçÂ§ñÊé®Ë°å‰∏∫ÁöÑÂÜÖÂú®È¢ëÁéá„ÄÇÂü∫‰∫éËøô‰∏ÄËßÅËß£ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜRIFLExÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÈÄöËøáÈôç‰ΩéÂÜÖÂú®È¢ëÁéáÊù•ÊäëÂà∂ÈáçÂ§çÔºåÂêåÊó∂‰øùÊåÅËøêÂä®‰∏ÄËá¥ÊÄßÔºåÊó†ÈúÄ‰ªª‰ΩïÈ¢ùÂ§ñ‰øÆÊîπ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15814",
            "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
            "url": "https://huggingface.co/papers/2502.15814",
            "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .",
            "score": 4,
            "issue_id": 2388,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 19",
                "zh": "2Êúà19Êó•"
            },
            "hash": "735714e237272170",
            "authors": [
                "Gallil Maimon",
                "Avishai Elmakies",
                "Yossi Adi"
            ],
            "affiliations": [
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15814.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#synthetic",
                    "#data",
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#training"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Slam - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM) –Ω–∞ –æ–¥–Ω–æ–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º GPU –∑–∞ 24 —á–∞—Å–∞. –û–Ω–∏ –ø—Ä–æ–≤–æ–¥—è—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Ö–æ—Ä–æ—à–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ–¥—É—â–∏—Ö SLM –ø—Ä–∏ –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –Ω–∞–¥–µ—é—Ç—Å—è, —á—Ç–æ —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–¥–µ–ª–∞—é—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ SLM –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏."
                },
                "en": {
                    "title": "Slam: Fast and Efficient Speech Language Model Training",
                    "desc": "This paper presents Slam, a method for efficiently training high-quality Speech Language Models (SLMs) using a single academic GPU within 24 hours. The authors analyze various factors such as model initialization, architecture, and synthetic training data to optimize the training process. Their empirical results show that Slam not only achieves competitive performance compared to leading SLMs but also does so at a significantly lower computational cost. This research aims to make SLM training more accessible and demonstrates that it can exceed expected performance based on scaling laws."
                },
                "zh": {
                    "title": "SlamÔºöÈ´òÊïàËÆ≠ÁªÉËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SlamÁöÑËÆ≠ÁªÉÈ´òË¥®ÈáèËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÔºàSLMÔºâÁöÑÊñπÊ°àÔºåËØ•ÊñπÊ°àÂèØ‰ª•Âú®24Â∞èÊó∂ÂÜÖ‰ΩøÁî®Âçï‰∏™Â≠¶ÊúØGPUÂÆåÊàê„ÄÇÈÄöËøáÂØπÊ®°ÂûãÂàùÂßãÂåñ„ÄÅÊû∂ÊûÑ„ÄÅÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆÂíåÂÅèÂ•Ω‰ºòÂåñÁ≠âÊñπÈù¢ÁöÑÂÆûËØÅÂàÜÊûêÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËøô‰∏ÄËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÅÊòéÔºåËøôÁßçËÆ≠ÁªÉÊñπÊ°àÂú®ËÆ°ÁÆóËµÑÊ∫êÂ¢ûÂä†Êó∂‰πüËÉΩËâØÂ•ΩÊâ©Â±ïÔºå‰∏îÂú®ËÆ°ÁÆóÊàêÊú¨‰∏äËøú‰Ωé‰∫éÈ¢ÜÂÖàÁöÑSLM„ÄÇÊàë‰ª¨Â∏åÊúõËøô‰∫õËßÅËß£ËÉΩ‰ΩøSLMÁöÑËÆ≠ÁªÉÂíåÁ†îÁ©∂ÂèòÂæóÊõ¥Âä†ÂèØÂèä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16707",
            "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
            "url": "https://huggingface.co/papers/2502.16707",
            "abstract": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a \"reflection\" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.",
            "score": 3,
            "issue_id": 2389,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 23",
                "zh": "2Êúà23Êó•"
            },
            "hash": "e739378d26e8e7ab",
            "authors": [
                "Yunhai Feng",
                "Jiaming Han",
                "Zhuoran Yang",
                "Xiangyu Yue",
                "Sergey Levine",
                "Jianlan Luo"
            ],
            "affiliations": [
                "Cornell University",
                "The Chinese University of Hong Kong",
                "University of California, Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16707.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ VLM –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º '—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏', –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π VLM –ø—É—Ç–µ–º –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ VLM –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏."
                },
                "en": {
                    "title": "Enhancing VLMs for Better Robotic Manipulation through Reflection",
                    "desc": "This paper addresses the challenges of robotic manipulation over long time frames by enhancing vision-language models (VLMs) with improved physical reasoning. The authors propose a test-time computation framework that incorporates a 'reflection' mechanism, allowing the VLM to predict future states and refine its actions based on these predictions. This iterative process helps the model to better handle complex tasks by addressing potential errors before they occur. Experimental results show that this approach significantly outperforms existing VLMs and other advanced techniques like Monte Carlo Tree Search (MCTS)."
                },
                "zh": {
                    "title": "ÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ°ÁÆóÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§öÈò∂ÊÆµÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰∏ÄÁßç‚ÄúÂèçÊÄù‚ÄùÊú∫Âà∂ÔºåËø≠‰ª£Âú∞ÊîπËøõÈ¢ÑËÆ≠ÁªÉÁöÑVLMÔºåÂà©Áî®ÁîüÊàêÊ®°ÂûãÊÉ≥Ë±°Êú™Êù•ÁöÑ‰∏ñÁïåÁä∂ÊÄÅÔºåÂπ∂Ê†πÊçÆËøô‰∫õÈ¢ÑÊµãÊåáÂØºÂä®‰ΩúÈÄâÊã©„ÄÇÈÄöËøáÂèçÊÄùÊΩúÂú®ÁöÑÊ¨°‰ºòÂÜ≥Á≠ñÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñÊé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÂ§öÁßçÊúÄÂÖàËøõÁöÑÂïÜ‰∏öVLMÂíåÂÖ∂‰ªñÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ¶ÇËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15987",
            "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
            "url": "https://huggingface.co/papers/2502.15987",
            "abstract": "As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage.",
            "score": 3,
            "issue_id": 2388,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 21",
                "zh": "2Êúà21Êó•"
            },
            "hash": "4e2786a48c6c49ab",
            "authors": [
                "Kushal Raj Bhandari",
                "Pin-Yu Chen",
                "Jianxi Gao"
            ],
            "affiliations": [
                "Department of Computer Science, Network Science and Technology Center, Rensselaer Polytechnic Institute, Troy, NY, USA",
                "IBM Research, Yorktown Heights, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15987.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "üìà",
                "ru": {
                    "title": "–ò–∑–º–µ—Ä—è–µ–º –≤–ª–∏—è–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –Ω–∞—É—á–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç framework –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è open-weight –º–æ–¥–µ–ª–µ–π –≤ —Å—Ñ–µ—Ä–µ –ò–ò. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç –º–æ–¥–µ–ª—å –í–∞–Ω–≥–∞ –∏ –¥—Ä., –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π, –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ö–ª—é—á–µ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≤ —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ —è–≤–ª—è—é—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ adoption open-weight –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Tracking AI Model Influence: A Citation-Style Approach",
                    "desc": "This paper presents a framework to analyze the influence of open-weight AI models over time, similar to how citations are tracked in scientific research. It introduces three parameters: immediacy, longevity, and relative fitness, to measure the impact of these models based on the number of fine-tuned versions created from them. The study finds that this citation-style method can effectively illustrate the varying adoption patterns of open-weight models, highlighting both typical trends and unique outliers. This approach helps predict which models are likely to lead innovation in the AI field."
                },
                "zh": {
                    "title": "È¢ÑÊµãÂºÄÊîæÊùÉÈáçÊ®°ÂûãÁöÑÂΩ±ÂìçÂäõÊºîÂèò",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÁî®‰∫éÈ¢ÑÊµãÂºÄÊîæÊùÉÈáçÊ®°ÂûãÂú®‰∫∫Â∑•Êô∫ËÉΩÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑÂΩ±ÂìçÂäõÊºîÂèò„ÄÇÊàë‰ª¨ÂÄüÈâ¥‰∫ÜÁßëÂ≠¶ÊñáÁåÆ‰∏≠ÁöÑÂºïÁî®Âä®ÊÄÅÔºå‰ΩøÁî®‰∏â‰∏™ÂÖ≥ÈîÆÂèÇÊï∞ÔºöÂç≥Êó∂ÊÄß„ÄÅÊåÅ‰πÖÊÄßÂíåÁõ∏ÂØπÈÄÇÂ∫îÊÄßÔºåÊù•Ë∑üË∏™ÂºÄÊîæÊùÉÈáçÊ®°ÂûãÁöÑÂæÆË∞ÉÊ®°ÂûãÊï∞Èáè„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåËøôÁßçÂºïÁî®È£éÊ†ºÁöÑÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÊçïÊçâÂºÄÊîæÊùÉÈáçÊ®°ÂûãÈááÁî®ÁöÑÂ§öÊ†∑ÂåñËΩ®Ëøπ„ÄÇÂ§ßÂ§öÊï∞Ê®°ÂûãË°®Áé∞ËâØÂ•ΩÔºåËÄåÂºÇÂ∏∏ÂÄºÂàôÊòæÁ§∫Âá∫Áã¨ÁâπÁöÑÊ®°ÂºèÊàñ‰ΩøÁî®ÁöÑÁ™ÅÁÑ∂ÂèòÂåñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16701",
            "title": "Beyond Release: Access Considerations for Generative AI Systems",
            "url": "https://huggingface.co/papers/2502.16701",
            "abstract": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.",
            "score": 3,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 23",
                "zh": "2Êúà23Êó•"
            },
            "hash": "db3e8ec873d10ddb",
            "authors": [
                "Irene Solaiman",
                "Rishi Bommasani",
                "Dan Hendrycks",
                "Ariel Herbert-Voss",
                "Yacine Jernite",
                "Aviya Skowron",
                "Andrew Trask"
            ],
            "affiliations": [
                "Center for AI Safety",
                "EleutherAI",
                "Hugging Face",
                "OpenMined",
                "RunSybil",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16701.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "üîì",
                "ru": {
                    "title": "–î–æ—Å—Ç—É–ø –∫ –ò–ò: –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ä–µ–ª–∏–∑",
                    "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –æ —Ä–µ–ª–∏–∑–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø–æ —Ç—Ä–µ–º –æ—Å—è–º: —Ä–µ—Å—É—Ä—Å–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —É–¥–æ–±–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ —á–µ—Ç—ã—Ä–µ—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç—Å—è —Å—Ö–æ–∂–∏–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º –¥–æ—Å—Ç—É–ø–∞. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –º–∞—Å—à—Ç–∞–± –¥–æ—Å—Ç—É–ø–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∏—Å–∫–∞–º–∏ –∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞."
                },
                "en": {
                    "title": "Understanding Access: Key to Responsible AI Release",
                    "desc": "This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access."
                },
                "zh": {
                    "title": "Ëß£ÊûÑÁîüÊàêÊÄßAIÁöÑËÆøÈóÆ‰∏éÈ£éÈô©ÁÆ°ÁêÜ",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂèëÂ∏ÉÂÜ≥Á≠ñÂØπÁ≥ªÁªüÁªÑ‰ª∂ÂèØÁî®ÊÄßÁöÑÂΩ±Âìç„ÄÇ‰ΩúËÄÖÊåáÂá∫ÔºåÂèëÂ∏ÉÂπ∂‰∏çËÉΩËß£ÂÜ≥Áî®Êà∑ÂíåÂà©ÁõäÁõ∏ÂÖ≥ËÄÖ‰∏éÁ≥ªÁªü‰∫íÂä®ÁöÑÊâÄÊúâÈóÆÈ¢òÔºåËÆøÈóÆÁ≥ªÁªüÁªÑ‰ª∂ÁöÑÊñπÂºè‰πü‰ºöÂΩ±ÂìçÊΩúÂú®ÁöÑÈ£éÈô©ÂíåÊî∂Áõä„ÄÇËÆ∫ÊñáÂ∞ÜËÆøÈóÆÂàÜ‰∏∫‰∏â‰∏™ÊñπÈù¢ÔºöËµÑÊ∫ê„ÄÅÊäÄÊúØÂèØÁî®ÊÄßÂíåÊïàÁî®ÔºåÂπ∂Âú®ÊØè‰∏™Á±ªÂà´‰∏≠ÊòéÁ°Æ‰∫Ü‰∏çÂêåÂèòÈáèÁöÑÊùÉË°°„ÄÇÈÄöËøáÊØîËæÉÂõõÁßçÈ´òÊÄßËÉΩËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËÆøÈóÆÊÄßÔºå‰ΩúËÄÖÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáËÆøÈóÆÂèòÈáèÊù•ËØÑ‰º∞ÂíåÁÆ°ÁêÜÈ£éÈô©„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17258",
            "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
            "url": "https://huggingface.co/papers/2502.17258",
            "abstract": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/",
            "score": 2,
            "issue_id": 2389,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "aaaaf48d4594432f",
            "authors": [
                "Xiangpeng Yang",
                "Linchao Zhu",
                "Hehe Fan",
                "Yi Yang"
            ],
            "affiliations": [
                "ReLER Lab, AAII, University of Technology Sydney",
                "ReLER Lab, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17258.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
                    "desc": "VideoGrain - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç-—Ä–µ–≥–∏–æ–Ω –∏ —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Ç–µ–º –º–æ–¥—É–ª—è—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ-–≤—Ä–µ–º–µ–Ω–∏. –ú–µ—Ç–æ–¥ —É—Å–∏–ª–∏–≤–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-—Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º —Ä–µ–≥–∏–æ–Ω–∞–º –∏ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VideoGrain –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Fine-Grained Control for Enhanced Video Editing with VideoGrain",
                    "desc": "This paper introduces VideoGrain, a novel approach to enhance video generation and editing using diffusion models. It tackles the challenges of multi-grained video editing by improving the control over different levels of video content, such as class, instance, and part modifications. The method employs advanced attention mechanisms to ensure that local prompts effectively target specific regions while minimizing irrelevant interactions. Experimental results show that VideoGrain outperforms existing methods, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "VideoGrainÔºöÁ≤æÁªÜÊéßÂà∂ËßÜÈ¢ëÁºñËæëÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VideoGrainÁöÑÈõ∂-shotÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Â§öÁ≤íÂ∫¶ËßÜÈ¢ëÁºñËæë‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáË∞ÉËäÇÊó∂Á©∫Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÆûÁé∞ÂØπËßÜÈ¢ëÂÜÖÂÆπÁöÑÁ≤æÁªÜÊéßÂà∂„ÄÇÊàë‰ª¨Â¢ûÂº∫‰∫ÜÊñáÊú¨Âà∞Âå∫ÂüüÁöÑÊéßÂà∂ÔºåÁ°Æ‰øùÊØè‰∏™Â±ÄÈÉ®ÊèêÁ§∫ÁöÑÊ≥®ÊÑèÂäõÈõÜ‰∏≠Âú®Áõ∏Â∫îÁöÑÁ©∫Èó¥Âå∫ÂüüÔºåÂêåÊó∂ÂáèÂ∞ë‰∏éÊó†ÂÖ≥Âå∫ÂüüÁöÑ‰∫§‰∫í„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈÄöËøáÊèêÈ´òÂå∫ÂüüÂÜÖÁöÑÊÑèËØÜÂíåÂáèÂ∞ëÂå∫ÂüüÈó¥ÁöÑÂπ≤Êâ∞ÔºåÊîπÂñÑ‰∫ÜÁâπÂæÅÂàÜÁ¶ª„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15122",
            "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
            "url": "https://huggingface.co/papers/2502.15122",
            "abstract": "We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data.",
            "score": 1,
            "issue_id": 2389,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 21",
                "zh": "2Êúà21Êó•"
            },
            "hash": "0befd2d9ae63b48e",
            "authors": [
                "Angus Dempster",
                "Navid Mohammadi Foumani",
                "Chang Wei Tan",
                "Lynn Miller",
                "Amish Mishra",
                "Mahsa Salehi",
                "Charlotte Pelletier",
                "Daniel F. Schmidt",
                "Geoffrey I. Webb"
            ],
            "affiliations": [
                "Monash University, Melbourne, Australia",
                "Universite Bretagne Sud, IRISA, Vannes, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15122.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ü¶ñ",
                "ru": {
                    "title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤",
                    "desc": "MONSTER - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∫—Ä—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –û–Ω —Å–æ–∑–¥–∞–Ω –≤ –ø—Ä–æ—Ç–∏–≤–æ–≤–µ—Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º UCR –∏ UEA, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ–±–æ–ª—å—à–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö. MONSTER –ø—Ä–∏–∑–≤–∞–Ω —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Å—Ç–∏–º—É–ª–∏—Ä—É—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–æ–¥–µ–ª–µ–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –Ω–∞–¥–µ—é—Ç—Å—è, —á—Ç–æ —ç—Ç–æ –æ—Ç–∫—Ä–æ–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å—Ñ–µ—Ä–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö."
                },
                "en": {
                    "title": "Unlocking Potential with Large Datasets in Time Series Classification",
                    "desc": "This paper presents MONSTER, a new repository designed for evaluating time series classification using large datasets. Current benchmarks, like UCR and UEA, consist of small datasets that limit the types of models that can be effectively tested. By providing larger datasets, MONSTER aims to encourage the development of models that can handle scalability and learn from more extensive data. The authors believe that this will lead to significant advancements in the field of time series classification."
                },
                "zh": {
                    "title": "ÂºïÂÖ•Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÊé®Âä®Êó∂Èó¥Â∫èÂàóÂàÜÁ±ªËøõÊ≠•",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜMONSTER‚Äî‚ÄîËíôÁ∫≥Â£´ÂèØÊâ©Â±ïÊó∂Èó¥Â∫èÂàóËØÑ‰º∞Â∫ìÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´Â§ßÈáèÊó∂Èó¥Â∫èÂàóÂàÜÁ±ªÊï∞ÊçÆÈõÜÁöÑÈõÜÂêà„ÄÇÁé∞ÊúâÁöÑÊó∂Èó¥Â∫èÂàóÂàÜÁ±ªÂü∫ÂáÜÔºàÂ¶ÇUCRÂíåUEAÔºâ‰∏≠ÁöÑÊï∞ÊçÆÈõÜËæÉÂ∞èÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂ§öÊ†∑ÊÄß„ÄÇMONSTERÊó®Âú®ÈÄöËøáÂºïÂÖ•Êõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜÊù•‰∏∞ÂØåËøô‰∏ÄÈ¢ÜÂüüÔºå‰øÉËøõÊ®°ÂûãÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÊó∂ÁöÑÊúâÊïàÂ≠¶‰π†„ÄÇÊàë‰ª¨Áõ∏‰ø°ÔºåÈù¢ÂØπÊõ¥Â§ßÊï∞ÊçÆÈáèÁöÑÁêÜËÆ∫ÂíåÂÆûË∑µÊåëÊàòÔºåÂ∞Ü‰∏∫Êó∂Èó¥Â∫èÂàóÂàÜÁ±ªÈ¢ÜÂüüÂ∏¶Êù•Êñ∞ÁöÑËøõÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17414",
            "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
            "url": "https://huggingface.co/papers/2502.17414",
            "abstract": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.",
            "score": 1,
            "issue_id": 2389,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 24",
                "zh": "2Êúà24Êó•"
            },
            "hash": "ef6013182ae4065e",
            "authors": [
                "Zeyuan Chen",
                "Hongyi Xu",
                "Guoxian Song",
                "You Xie",
                "Chenxu Zhang",
                "Xin Chen",
                "Chao Wang",
                "Di Chang",
                "Linjie Luo"
            ],
            "affiliations": [
                "ByteDance",
                "UC San Diego",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17414.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "üíÉ",
                "ru": {
                    "title": "–¢–∞–Ω—Ü—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ò–ò –æ–∂–∏–≤–ª—è–µ—Ç —Ñ–æ—Ç–æ –ø–æ–¥ –º—É–∑—ã–∫—É",
                    "desc": "X-Dancer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Ç–∞–Ω—Ü–µ–≤ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–¥ –º—É–∑—ã–∫—É –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑ —Ç–µ–ª–∞, –≥–æ–ª–æ–≤—ã –∏ —Ä—É–∫, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —É–ø—Ä–∞–≤–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –≤–∏–¥–µ–æ. X-Dancer –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä 2D –¥–≤–∏–∂–µ–Ω–∏–π —Ç–∞–Ω—Ü–∞, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—è –∏—Ö –Ω—é–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–º –±–∏—Ç–∞–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ X-Dancer —Å–ø–æ—Å–æ–±–Ω–∞ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Ç–∞–Ω—Ü–µ–≤–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é, –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Transforming Static Images into Dynamic Dance Videos with Music",
                    "desc": "X-Dancer is a new system that creates realistic dance videos from just one image, using music as a guide. It combines a transformer model that generates dance poses with a diffusion model that turns these poses into video frames. This approach allows it to produce a wide variety of 2D dance movements that match the rhythm of the music, overcoming limitations of traditional 3D methods. The results show that X-Dancer excels in creating diverse and expressive dance videos, outperforming existing techniques."
                },
                "zh": {
                    "title": "X-DancerÔºö‰ªéÈùôÊÄÅÂõæÂÉèÁîüÊàêÈü≥‰πêÈ©±Âä®ÁöÑËàûËπàËßÜÈ¢ë",
                    "desc": "X-DancerÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈõ∂Ê†∑Êú¨Èü≥‰πêÈ©±Âä®ÂõæÂÉèÂä®ÁîªÁÆ°ÈÅìÔºåÂèØ‰ª•‰ªéÂçï‰∏ÄÈùôÊÄÅÂõæÂÉèÁîüÊàêÂ§öÊ†∑Âåñ‰∏îÈïøÊó∂Èó¥ÁöÑÈÄºÁúü‰∫∫Á±ªËàûËπàËßÜÈ¢ë„ÄÇÂÆÉÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂèòÊç¢Âô®-Êâ©Êï£Ê°ÜÊû∂ÔºåÂà©Áî®Ëá™ÂõûÂΩíÂèòÊç¢Âô®Ê®°ÂûãÂêàÊàê‰∏éÈü≥‰πêÂêåÊ≠•ÁöÑ2DË∫´‰Ωì„ÄÅÂ§¥ÈÉ®ÂíåÊâãÈÉ®ÂßøÂäøÁöÑÊâ©Â±ïÂ∫èÂàóÔºåÊåáÂØºÊâ©Êï£Ê®°ÂûãÁîüÊàêËøûË¥Ø‰∏îÁúüÂÆûÁöÑËàûËπàËßÜÈ¢ëÂ∏ß„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏ªË¶ÅÁîüÊàê3D‰∫∫Á±ªËøêÂä®‰∏çÂêåÔºåX-DancerÈÄöËøáÂª∫Ê®°ÂπøÊ≥õÁöÑ2DËàûËπàÂä®‰ΩúÔºåÊçïÊçâ‰∏éÈü≥‰πêËäÇÊãçÁöÑÁªÜÂæÆÂØπÈΩêÔºåËß£ÂÜ≥‰∫ÜÊï∞ÊçÆÈôêÂà∂Âπ∂Â¢ûÂº∫‰∫ÜÂèØÊâ©Â±ïÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåX-DancerÂú®Â§öÊ†∑ÊÄß„ÄÅË°®Áé∞ÂäõÂíåÁúüÂÆûÊÑüÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-02-24.html",
    "link_next": "2025-02-26.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2Êúà24Êó•"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2Êúà26Êó•"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 4,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SurveyX ÁöÑËá™Âä®ÂåñÈóÆÂç∑ÁîüÊàêÁ≥ªÁªü„ÄÇSurveyX ÈÄöËøáÂºïÂÖ•Âú®Á∫øÂèÇËÄÉÊ£ÄÁ¥¢„ÄÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ï AttributeTree ÂíåÈáçÊñ∞Ê∂¶Ëâ≤ËøáÁ®ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈóÆÂç∑ÁîüÊàêÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSurveyX Âú®ÂÜÖÂÆπË¥®ÈáèÂíåÂºïÁî®Ë¥®ÈáèÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁ≥ªÁªüÔºåÊé•Ëøë‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑË°®Áé∞„ÄÇÊñáÁ´†ËøòÊèêÂà∞‰∫Ü SurveyX ÁîüÊàêÁöÑÈóÆÂç∑Á§∫‰æãÂèØ‰ª•Âú® www.surveyx.cn ‰∏äÊâæÂà∞„ÄÇ",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SurveyX ÁöÑËá™Âä®ÂåñÈóÆÂç∑ÁîüÊàêÁ≥ªÁªü„ÄÇSurveyX ÈÄöËøáÂºïÂÖ•Âú®Á∫øÂèÇËÄÉÊ£ÄÁ¥¢„ÄÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ï AttributeTree ÂíåÈáçÊñ∞Ê∂¶Ëâ≤ËøáÁ®ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈóÆÂç∑ÁîüÊàêÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSurveyX Âú®ÂÜÖÂÆπË¥®ÈáèÂíåÂºïÁî®Ë¥®ÈáèÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁ≥ªÁªüÔºåÊé•Ëøë‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑË°®Áé∞„ÄÇÊñáÁ´†ËøòÊèêÂà∞‰∫Ü SurveyX ÁîüÊàêÁöÑÈóÆÂç∑Á§∫‰æãÂèØ‰ª•Âú® www.surveyx.cn ‰∏äÊâæÂà∞„ÄÇ\n\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le yƒ´zh«íng m√≠ngw√®i SurveyX de z√¨d√≤nghu√† w√®nju√†n shƒìngch√©ng x√¨t«íng. SurveyX t≈çnggu√≤ y«ênr√π z√†ixi√†n cƒÅnk«éo ji«énsu«í, y√πch«îl«ê fƒÅngf«é AttributeTree h√© ch√≥ngxƒ´n r√πns√® gu√≤ch√©ng, xi«énzh√π tƒ´gƒÅo le w√®nju√†n shƒìngch√©ng de xi√†ogu«í. Sh√≠y√†n ji√©gu«í bi«éom√≠ng, SurveyX z√†i n√®ir√≥ng zh√¨li√†ng h√© y«êny√≤ng zh√¨li√†ng fƒÅngmi√†n y≈çuy√∫ xi√†ny«íu x√¨t«íng, jiƒìj√¨n r√©nl√®i zhuƒÅnjiƒÅ de bi«éoxi√†n. W√©nzhƒÅng h√°i t√≠ d√†o le SurveyX shƒìngch√©ng de w√®nju√†n sh√¨l√¨ kƒõy«ê z√†i www.surveyx.cn sh√†ng zh«éo d√†o.",
        "vocab": "[{'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨d√≤nghu√†', 'trans': 'automated'},\n{'word': 'ÈóÆÂç∑', 'pinyin': 'w√®nju√†n', 'trans': 'questionnaire'},\n{'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'},\n{'word': 'Á≥ªÁªü', 'pinyin': 'x√¨t«íng', 'trans': 'system'},\n{'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'},\n{'word': 'Âú®Á∫ø', 'pinyin': 'z√†ixi√†n', 'trans': 'online'},\n{'word': 'ÂèÇËÄÉ', 'pinyin': 'cƒÅnk«éo', 'trans': 'reference'},\n{'word': 'Ê£ÄÁ¥¢', 'pinyin': 'ji«énsu«í', 'trans': 'retrieval'},\n{'word': 'È¢ÑÂ§ÑÁêÜ', 'pinyin': 'y√πch«îl«ê', 'trans': 'preprocessing'},\n{'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': 'Ê∂¶Ëâ≤', 'pinyin': 'r√πns√®', 'trans': 'polish'},\n{'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ch√©ng', 'trans': 'process'},\n{'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'},\n{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'},\n{'word': 'ÊïàÊûú', 'pinyin': 'xi√†ogu«í', 'trans': 'effect'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},\n{'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'},\n{'word': 'Ë°®Êòé', 'pinyin': 'bi«éom√≠ng', 'trans': 'indicate'},\n{'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®ir√≥ng', 'trans': 'content'},\n{'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'},\n{'word': 'ÂºïÁî®', 'pinyin': 'y«êny√≤ng', 'trans': 'citation'},\n{'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'},\n{'word': 'Êé•Ëøë', 'pinyin': 'jiƒìj√¨n', 'trans': 'close to'},\n{'word': '‰∫∫Á±ª', 'pinyin': 'r√©nl√®i', 'trans': 'human'},\n{'word': '‰∏ìÂÆ∂', 'pinyin': 'zhuƒÅnjiƒÅ', 'trans': 'expert'},\n{'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'},\n{'word': 'ÊèêÂà∞', 'pinyin': 't√≠d√†o', 'trans': 'mention'},\n{'word': 'Á§∫‰æã', 'pinyin': 'sh√¨l√¨', 'trans': 'example'},\n{'word': 'ÊâæÂà∞', 'pinyin': 'zh«éod√†o', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}