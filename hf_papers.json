{
    "date": {
        "ru": "28 февраля",
        "en": "February 28",
        "zh": "2月28日"
    },
    "time_utc": "2025-02-28 03:19",
    "weekday": 4,
    "issue_id": 2455,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.19613",
            "title": "Self-rewarding correction for mathematical reasoning",
            "url": "https://huggingface.co/papers/2502.19613",
            "abstract": "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.",
            "score": 2,
            "issue_id": 2455,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 февраля",
                "en": "February 26",
                "zh": "2月26日"
            },
            "hash": "e2535efc8aadcc9d",
            "authors": [
                "Wei Xiong",
                "Hanning Zhang",
                "Chenlu Ye",
                "Lichang Chen",
                "Nan Jiang",
                "Tong Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19613.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самокорректирующиеся языковые модели: новый шаг к автономному ИИ",
                    "desc": "Исследователи изучают языковые модели с самовознаграждением, способные генерировать пошаговые рассуждения и оценивать их корректность без внешней обратной связи. Предложен двухэтапный алгоритмический подход для создания таких моделей, использующий только самогенерируемые данные. На первом этапе применяется последовательная выборка с отклонением для синтеза длинных цепочек рассуждений, включающих механизмы самовознаграждения и самокоррекции. Второй этап усиливает способность моделей оценивать точность ответов и улучшать выходные данные с помощью обучения с подкреплением."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Rewarding Reasoning and Self-Correction",
                    "desc": "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."
                },
                "zh": {
                    "title": "自我奖励推理：模型的独立思考与修正",
                    "desc": "我们研究了自我奖励推理的大型语言模型（LLMs），这些模型能够在推理过程中同时生成逐步推理并评估输出的正确性，而无需外部反馈。这种集成方法使得单一模型能够独立引导其推理过程，为模型部署提供了计算优势。我们特别关注自我修正的任务，模型能够自主检测响应中的错误，修正输出，并决定何时终止迭代优化循环。为此，我们提出了一种两阶段的算法框架，利用自生成的数据构建自我奖励推理模型。"
                }
            }
        }
    ],
    "link_prev": "2025-02-27.html",
    "link_next": "2025-03-03.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2月27日"
    },
    "short_date_next": {
        "ru": "03.03",
        "en": "03/03",
        "zh": "3月3日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。",
        "title": "Kanana: Compute-efficient Bilingual Language Models",
        "pinyin": "Wǒmen jièshào le Kanana, yī xìliè zài hányǔ hé yīngyǔ zhōng biǎoxiàn chūsè de shuāngyǔ móxíng. Kanana de jìsuàn chéngběn xiǎnzhù dīyú lèisì guīmó de dǐngjí móxíng. Bàogào xiángxì jièshào le yùxùnliàn zhōng shǐyòng de jìshù, bāokuò gāo zhìliàng shùjù guòlǜ, fēn jiēduàn yùxùnliàn, shēndù kuòzhǎn, jiǎnzhī hé zhēngliú. Cǐwài, bàogào hái gàikuàng le Kanana móxíng zài xùnliàn hòu shǐyòng de fāngfǎ, bāokuò jiàndū wēitiáo hé piānhǎo yōuhuà, yǐ tígāo qí yǔ yònghù hùdòng de nénglì. Zùihòu, bàogào hái tǎolùn le yǔyán móxíng shìyìng tèdìng chǎngjīng de kěnéng fāngfǎ, rú qiànrù, jiǎnsuǒ zēngqiáng shēngchéng hé hánshù diàoyòng. Kanana móxíng xìliè cóng 2.1B dào 32.5B cānshù bùděng, 2.1B móxíng (jīchǔ, zhǐlìng, qiànrù) yǐ gōngkāi fābù, yǐ cùjìn hányǔ móxíng yánjiū.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'},\n{'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'},\n{'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'},\n{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'},\n{'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'},\n{'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'},\n{'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'},\n{'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'},\n{'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'},\n{'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'},\n{'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'},\n{'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'},\n{'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'},\n{'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'},\n{'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'},\n{'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'},\n{'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'},\n{'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]",
        "trans": "We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models.",
        "update_ts": "2025-02-27 09:11"
    }
}