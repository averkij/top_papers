{
    "date": {
        "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 24",
        "zh": "4æœˆ24æ—¥"
    },
    "time_utc": "2025-04-24 02:25",
    "weekday": 3,
    "issue_id": 3403,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15843",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "url": "https://huggingface.co/papers/2504.15843",
            "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.",
            "score": 3,
            "issue_id": 3403,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "e8cb4456a20efe2a",
            "authors": [
                "Junshu Pan",
                "Wei Shen",
                "Shulin Huang",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Engineering, Westlake University",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15843.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Pre-DPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pre-DPO - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Pre-DPO ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ DPO Ğ¸ SimPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AlpacaEval 2.0 Ğ¸ Arena-Hard v0.1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Pre-DPO Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Preference Learning with Pre-DPO",
                    "desc": "This paper introduces Pre-DPO, a new training method for large language models that enhances Direct Preference Optimization (DPO) by using a guiding reference model. The reference model helps adjust the importance of training data, allowing the model to focus on more relevant samples and improve learning efficiency. The authors highlight that traditional methods can lead to poor performance due to identical initialization of models and lack of robustness in simpler approaches. Through experiments, they show that Pre-DPO outperforms existing methods without needing extra data or external models."
                },
                "zh": {
                    "title": "æå‡åå¥½ä¼˜åŒ–æ€§èƒ½çš„Pre-DPOæ–¹æ³•",
                    "desc": "ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šè¿‡ç›´æ¥ä¼˜åŒ–äººç±»åå¥½ï¼Œç®€åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨DPOè®­ç»ƒä¸­ï¼Œå‚è€ƒæ¨¡å‹å……å½“æ•°æ®æƒé‡è°ƒæ•´å™¨ï¼Œä½†å¸¸è§çš„å°†ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åˆå§‹åŒ–ä¸ºç›¸åŒçš„åšæ³•å¯èƒ½å¯¼è‡´æ•°æ®åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºçš„Pre-DPOè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨æŒ‡å¯¼æ€§å‚è€ƒæ¨¡å‹ï¼Œå¢å¼ºäº†åå¥½ä¼˜åŒ–çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¸ºæ›´é€‚åˆæ¨¡å‹çš„æ ·æœ¬åˆ†é…æ›´é«˜çš„æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPre-DPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡äº†DPOå’ŒSimPOçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-23.html",
    "link_next": "2025-04-25.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†æ–°è¯­è¨€æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•æˆåŠŸåœ°å°†ä¹‹å‰æœªè§çš„ç›®æ ‡è¯­è¨€åŠ å…¥ç°æœ‰æ¨¡å‹ï¼Œä¸å½±å“å…¶åŸæœ‰çŸ¥è¯†ã€‚ç ”ç©¶å›¢é˜Ÿç”¨15äº¿å‚æ•°è®­ç»ƒäº†ä¸€ä¸ªåä¸ºKuwainçš„å°æ¨¡å‹ï¼Œå°†é˜¿æ‹‰ä¼¯è¯­åŠ å…¥ä¸»è¦ç”¨è‹±è¯­è®­ç»ƒçš„å¼€æºæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œé˜¿æ‹‰ä¼¯è¯­æ€§èƒ½æé«˜äº†8%ï¼ŒåŒæ—¶ä¿ç•™äº†åŸæœ‰çŸ¥è¯†ã€‚è¿™æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…äº†å¤§è§„æ¨¡é‡æ–°è®­ç»ƒã€‚",
        "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†æ–°è¯­è¨€æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n fÄngfÇ, jiÄng xÄ«n yÇ”yÃ¡n zhÄ›nghÃ© dÃ o dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng zhÅng.\n\nè¯¥æ–¹æ³•æˆåŠŸåœ°å°†ä¹‹å‰æœªè§çš„ç›®æ ‡è¯­è¨€åŠ å…¥ç°æœ‰æ¨¡å‹ï¼Œä¸å½±å“å…¶åŸæœ‰çŸ¥è¯†ã€‚\nGÇi fÄngfÇ chÃ©nggÅng de jiÄng zhÄ«qiÃ¡n wÃ¨ijiÃ n de mÃ¹biÄo yÇ”yÃ¡n jiÄrÃ¹ xiÃ nyÇ’u mÃ³xÃ­ng, bÃ¹ yÇngxiÇng qÃ­ yuÃ¡nyÇ’u zhÄ«shi.\n\nç ”ç©¶å›¢é˜Ÿç”¨15äº¿å‚æ•°è®­ç»ƒäº†ä¸€ä¸ªåä¸ºKuwainçš„å°æ¨¡å‹ï¼Œå°†é˜¿æ‹‰ä¼¯è¯­åŠ å…¥ä¸»è¦ç”¨è‹±è¯­è®­ç»ƒçš„å¼€æºæ¨¡å‹ã€‚\nYÃ¡njiÅ« tuÃ¡nduÃ¬ yÃ²ng 15 yÃ¬ cÄnshÇ” xÃ¹nliÃ n le yÄ«gÃ¨ mÃ­ngwÃ¨i Kuwain de xiÇo mÃ³xÃ­ng, jiÄng Ä€lÄbÃ³yÇ” jiÄrÃ¹ zhÇ”yÃ o yÃ²ng YÄ«ngyÇ” xÃ¹nliÃ n de kÄiyuÃ¡n mÃ³xÃ­ng.\n\nç»“æœæ˜¾ç¤ºï¼Œé˜¿æ‹‰ä¼¯è¯­æ€§èƒ½æé«˜äº†8%ï¼ŒåŒæ—¶ä¿ç•™äº†åŸæœ‰çŸ¥è¯†ã€‚\nJiÃ©gÇ”o xiÇnshÃ¬, Ä€lÄbÃ³yÇ” xÃ¬ngnÃ©ng tÃ­gÄo le 8%, tÃ³ngshÃ­ bÇoliÃº le yuÃ¡nyÇ’u zhÄ«shi.\n\nè¿™æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…äº†å¤§è§„æ¨¡é‡æ–°è®­ç»ƒã€‚\nZhÃ¨ tÃ­gÅng le yÄ« zhÇ’ng chÃ©ngbÄ›n xiÃ oyÃ¬ gÄo de tÃ¬dÃ i fÄng'Ã n, bÃ¬miÇn le dÃ guÄ«mÃ³ chÃ³ngxÄ«n xÃ¹nliÃ n.",
        "vocab": "[\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›nghÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"æœªè§\", \"pinyin\": \"wÃ¨ijiÃ n\", \"trans\": \"unseen\"},\n    {\"word\": \"ç›®æ ‡\", \"pinyin\": \"mÃ¹biÄo\", \"trans\": \"target\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"å½±å“\", \"pinyin\": \"yÇngxiÇng\", \"trans\": \"affect\"},\n    {\"word\": \"åŸæœ‰\", \"pinyin\": \"yuÃ¡nyÇ’u\", \"trans\": \"original\"},\n    {\"word\": \"çŸ¥è¯†\", \"pinyin\": \"zhÄ«shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡njiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å›¢é˜Ÿ\", \"pinyin\": \"tuÃ¡nduÃ¬\", \"trans\": \"team\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÇ”\", \"trans\": \"parameters\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹nliÃ n\", \"trans\": \"train\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ngwÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"å°æ¨¡å‹\", \"pinyin\": \"xiÇo mÃ³xÃ­ng\", \"trans\": \"small model\"},\n    {\"word\": \"åŠ å…¥\", \"pinyin\": \"jiÄrÃ¹\", \"trans\": \"add\"},\n    {\"word\": \"ä¸»è¦\", \"pinyin\": \"zhÇ”yÃ o\", \"trans\": \"main\"},\n    {\"word\": \"ç”¨è‹±è¯­\", \"pinyin\": \"yÃ²ng yÄ«ngyÇ”\", \"trans\": \"using English\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄiyuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"ä¿ç•™\", \"pinyin\": \"bÇoliÃº\", \"trans\": \"retain\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ngbÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"æ•ˆç›Š\", \"pinyin\": \"xiÃ oyÃ¬\", \"trans\": \"benefit\"},\n    {\"word\": \"é«˜\", \"pinyin\": \"gÄo\", \"trans\": \"high\"},\n    {\"word\": \"æ›¿ä»£\", \"pinyin\": \"tÃ¬dÃ i\", \"trans\": \"alternative\"},\n    {\"word\": \"æ–¹æ¡ˆ\", \"pinyin\": \"fÄngÃ n\", \"trans\": \"solution\"},\n    {\"word\": \"é¿å…\", \"pinyin\": \"bÃ¬miÇn\", \"trans\": \"avoid\"},\n    {\"word\": \"å¤§è§„æ¨¡\", \"pinyin\": \"dÃ guÄ«mÃ³\", \"trans\": \"large-scale\"},\n    {\"word\": \"é‡æ–°\", \"pinyin\": \"chÃ³ngxÄ«n\", \"trans\": \"re-\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹nliÃ n\", \"trans\": \"train\"}\n]",
        "trans": "This article introduces a new method for integrating new languages into large language models. The method successfully incorporates previously unseen target languages into existing models without affecting their original knowledge. The research team trained a small model named Kuwain with 1.5 billion parameters, adding Arabic to a primarily English-trained open-source model. The results showed an 8% improvement in Arabic performance while retaining the original knowledge. This provides a cost-effective alternative, avoiding the need for large-scale retraining.",
        "update_ts": "2025-04-23 09:13"
    }
}