{
    "date": {
        "ru": "18 февраля",
        "en": "February 18",
        "zh": "2月18日"
    },
    "time_utc": "2025-02-18 04:12",
    "weekday": 1,
    "issue_id": 2264,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.11438",
            "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
            "url": "https://huggingface.co/papers/2502.11438",
            "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.",
            "score": 3,
            "issue_id": 2264,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "93b545df707b1538",
            "authors": [
                "Jimin Lee",
                "Ingeol Baek",
                "Byeongjeong Kim",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11438.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самоусиление ИИ в преобразовании текста в SQL",
                    "desc": "SAFE-SQL - это новый подход к преобразованию естественного языка в SQL-запросы. Он использует большие языковые модели для генерации и фильтрации релевантных примеров, улучшая процесс обучения в контексте. SAFE-SQL превосходит предыдущие методы в задачах zero-shot и few-shot, достигая более высокой точности выполнения запросов. Особенно эффективен в сложных и ранее не встречавшихся сценариях, где традиционные методы часто не справляются."
                },
                "en": {
                    "title": "Transforming Language to SQL with Self-Augmented Learning",
                    "desc": "This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches."
                },
                "zh": {
                    "title": "自我增强，提升Text-to-SQL的准确性",
                    "desc": "本文提出了一种新的框架SAFE-SQL，用于将自然语言问题转换为可执行的SQL查询。该框架通过自我增强的上下文学习和细粒度示例选择来提高SQL生成的质量。SAFE-SQL首先生成多个与测试输入相关的Text-to-SQL示例，然后通过三种相关性评估对这些示例进行过滤，从而构建高质量的学习示例。与传统的零样本和少样本方法相比，SAFE-SQL在执行准确性上取得了显著提升，尤其在困难和未见过的场景中表现更佳。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 3,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "🐣",
                "ru": {
                    "title": "Извлечение информации на плечах гигантов: как IE модели могут использовать ресурсы LLM",
                    "desc": "Исследователи представили новый подход к извлечению информации (IE) с использованием ресурсов больших языковых моделей (LLM). Метод под названием 'извлечение следующих токенов' (NTE) позволяет переформулировать задачу предсказания следующего токена в задачу извлечения уже присутствующих в контексте токенов. Модель Cuckoo, обученная на 102,6 млн примеров извлекательных данных, показывает лучшие результаты в условиях малого количества обучающих примеров по сравнению с существующими предобученными IE моделями. Этот подход позволяет IE моделям автоматически развиваться вместе с улучшениями в подготовке данных для LLM без дополнительных ручных усилий."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "利用LLM提升信息提取模型的性能",
                    "desc": "本文探讨了如何利用大型语言模型（LLM）来提升信息提取（IE）模型的性能。我们提出了一种新的提取方法，称为下一标记提取（NTE），通过将下一个标记预测转化为对上下文中已存在标记的提取，从而使IE模型能够利用LLM的资源。我们开发的Cuckoo模型在少量样本的情况下，能够有效适应传统和复杂的指令跟随IE任务，并且表现优于现有的预训练IE模型。Cuckoo作为一个“搭便车者”，能够随着LLM数据准备的进步而自然演变，无需额外的人工努力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09061",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "url": "https://huggingface.co/papers/2502.09061",
            "abstract": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.",
            "score": 2,
            "issue_id": 2264,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4a44947deeb14cf4",
            "authors": [
                "Debangshu Banerjee",
                "Tarun Suresh",
                "Shubham Ugare",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс между ограничениями и рассуждением в языковых моделях",
                    "desc": "Эта статья исследует проблему генерации синтаксически и семантически корректных выходных данных языковыми моделями (LLM) для задач, требующих формального рассуждения. Авторы предлагают теоретическое объяснение, почему строгое ограничение выходных данных LLM может снижать их способности к рассуждению. Они представляют алгоритм CRANE, который балансирует между корректностью ограниченной генерации и гибкостью неограниченной генерации. Эксперименты показывают, что CRANE значительно превосходит существующие методы ограниченного и неограниченного декодирования на сложных задачах символьного рассуждения."
                },
                "en": {
                    "title": "Balancing Correctness and Reasoning in LLM Outputs with CRANE",
                    "desc": "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."
                },
                "zh": {
                    "title": "平衡推理能力与生成正确性的创新解码算法",
                    "desc": "本研究探讨了如何在生成代码和符号数学推理等任务中，确保大型语言模型（LLM）输出的语法和语义正确性。我们发现，过于严格的语法约束会降低模型的推理能力。为了解决这个问题，我们提出了一种新的解码算法CRANE，通过增加精心设计的额外规则，既能保持输出的正确性，又能增强推理能力。实验结果表明，CRANE在多个基准测试中显著优于现有的解码策略，提升了符号推理任务的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 2,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты в доказательном программировании",
                    "desc": "Статья посвящена проблеме обучения языковых моделей программированию, ориентированному на доказательства. Авторы предлагают метод синтетического расширения данных для решения проблемы нехватки корпусов на языках доказательного программирования. Они создают модель PoPilot, которая превосходит GPT-4 на 64% в задачах проектного уровня. Метод также позволяет улучшить результаты GPT-4 на 54% путем исправления его выходных данных."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "合成数据增强，提升证明编程能力！",
                    "desc": "现有的语言模型在面向证明的编程中面临数据稀缺的问题，主要体现在两个方面：缺乏足够的面向证明编程语言（如F*）的语料库，以及缺少大规模的项目级证明实现，无法教会模型复杂的推理过程。我们提出了一种基于合成数据增强的方法，专注于项目级的面向证明编程，既用于生成也用于修复。该方法通过合成基本的面向证明编程问题来解决数据稀缺问题，并结合多样化的编码数据以提高推理能力，同时在现有代码库中创建新的证明和修复数据。我们的14B参数模型PoPilot经过微调后，在项目级面向证明编程中超越了GPT-4o模型64%的性能，并通过修复其输出提高了54%的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2月19日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。\n\nkuò sàn mó xíng (DMs) zài gè zhǒng lǐng yù de shēng chéng rèn wù zhōng chéng wéi shǒu xuǎn. rán ér, tā men yī lài duō cì shùn xù qián xiāng chuán dì, xiǎn zhù xiàn zhì le shí shí xìng néng. yǐ qián de jiā sù fāng fǎ zhǔ yào jī zhōng zài jiǎn shǎo cǎi yàng bù zhòu huò chóng yòng zhōng jiān jié guǒ, wèi néng lì yòng tú xiàng nèi bù kōng jiān qū yù de biàn huà. tōng guò lì yòng kuò sàn biàn shū zhǔ (DiTs) chǔ lǐ kě biàn shù liàng de biāo jì de líng huó xìng, wǒ men yǐn rù le RAS, yī zhǒng xīn de wú xū xùn liàn de cǎi yàng cè lüè, gēn jù DiT mó xíng de guān zhù diǎn dòng tài fēn pèi tú xiàng nèi bù tōng qū yù de cǎi yàng bǐ lǜ. wǒ men de guǎn jiàn guān chá shì, zài měi gè cǎi yàng bù zhòu zhōng, mó xíng jí zhōng zài yǔ yì shàng yǒu yì yì de qū yù, zhè xiē guān zhù qū yù zài lián xù bù zhòu zhōng biǎo xiàn chū qiáng dà de lián xù xìng. lì yòng zhè yī dòng chá, RAS jǐn gēng xīn shǐ dāng qián guān zhù de qū yù, ér qí tā qū yù shǐ yòng shàng yī bù de huǎn cùn zào shēng gēng xīn. mó xíng de guān zhù diǎn gēn jù qián yī bù de shū chū què dìng, lì yòng le wǒ men guān chá dào de shí jiān yī zhì xìng. wǒ men zài Stable Diffusion 3 hé Lumina-Next-T2I shàng píng guǎ RAS, fēn bié shí xiàn le zuì gāo 2.36 bèi hé 2.51 bèi de jiā sù, shēng chéng zhì liàng jǐn qīng wēi xià jiàng. cǐ wài, yòng hù yán jiū biǎo míng, RAS zài rén lèi píng jià xià tí gōng le xiāng sì de zhì liàng, tóng shí shí xiàn le 1.6 bèi de jiā sù. wǒ men de fāng fǎ zài gèng gāo xiào de kuò sàn biàn shū zhǔ fāng miàn zhǔ dé dào le zhòng yào jìn zhǎn, zēng qiáng le tā men zài shí shí yìng yòng zhōng de qián lì.",
        "vocab": "[\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"首选\", \"pinyin\": \"shǒu xuǎn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"顺序\", \"pinyin\": \"shùn xù\", \"trans\": \"sequential\"},\n    {\"word\": \"前向传递\", \"pinyin\": \"qián xiàng chuán dì\", \"trans\": \"forward pass\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"实时性能\", \"pinyin\": \"shí shí xìng néng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus on\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"采样步骤\", \"pinyin\": \"cǎi yàng bù zhòu\", \"trans\": \"sampling steps\"},\n    {\"word\": \"重用\", \"pinyin\": \"chóng yòng\", \"trans\": \"reuse\"},\n    {\"word\": \"中间结果\", \"pinyin\": \"zhōng jiān jié guǒ\", \"trans\": \"intermediate results\"},\n    {\"word\": \"利用\", \"pinyin\": \"lì yòng\", \"trans\": \"utilize\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"内部空间区域\", \"pinyin\": \"nèi bù kōng jiān qū yù\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"变化\", \"pinyin\": \"biàn huà\", \"trans\": \"change\"},\n    {\"word\": \"扩散变压器\", \"pinyin\": \"kuò sàn biàn yā qì\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"采样策略\", \"pinyin\": \"cǎi yàng cè lüè\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"动态分配\", \"pinyin\": \"dòng tài fēn pèi\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"关注点\", \"pinyin\": \"guān zhù diǎn\", \"trans\": \"focus points\"},\n    {\"word\": \"关键观察\", \"pinyin\": \"guǎn jiàn guān chá\", \"trans\": \"key observation\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"有意义\", \"pinyin\": \"yǒu yì yì\", \"trans\": \"meaningful\"},\n    {\"word\": \"连续步骤\", \"pinyin\": \"lián xù bù zhòu\", \"trans\": \"continuous steps\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"连续性\", \"pinyin\": \"lián xù xìng\", \"trans\": \"continuity\"},\n    {\"word\": \"洞察\", \"pinyin\": \"dòng chá\", \"trans\": \"insight\"},\n    {\"word\": \"更新\", \"pinyin\": \"gēng xīn\", \"trans\": \"update\"},\n    {\"word\": \"缓存噪声\", \"pinyin\": \"huǎn cún zào shēng\", \"trans\": \"cached noise\"},\n    {\"word\": \"确定\", \"pinyin\": \"què dìng\", \"trans\": \"determine\"},\n    {\"word\": \"时间一致性\", \"pinyin\": \"shí jiān yī zhì xìng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"acceleration\"},\n    {\"word\": \"生成质量\", \"pinyin\": \"shēng chéng zhì liàng\", \"trans\": \"generation quality\"},\n    {\"word\": \"轻微下降\", \"pinyin\": \"qīng wēi xià jiàng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"用户研究\", \"pinyin\": \"yòng hù yán jiū\", \"trans\": \"user study\"},\n    {\"word\": \"人类评估\", \"pinyin\": \"rén lèi píng gū\", \"trans\": \"human evaluation\"},\n    {\"word\": \"相似\", \"pinyin\": \"xiāng sì\", \"trans\": \"similar\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"potential\"},\n    {\"word\": \"重要进展\", \"pinyin\": \"zhòng yào jìn zhǎn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}