{
    "date": {
        "ru": "20 января",
        "en": "January 20",
        "zh": "1月20日"
    },
    "time_utc": "2025-01-20 08:14",
    "weekday": 0,
    "issue_id": 1754,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.09891",
            "title": "Evolving Deeper LLM Thinking",
            "url": "https://huggingface.co/papers/2501.09891",
            "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
            "score": 23,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "f2f5bbede5781334",
            "authors": [
                "Kuang-Huei Lee",
                "Ian Fischer",
                "Yueh-Hua Wu",
                "Dave Marwood",
                "Shumeet Baluja",
                "Dale Schuurmans",
                "Xinyun Chen"
            ],
            "affiliations": [
                "Google DeepMind",
                "UC San Diego",
                "University of Alberta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09891.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эволюция мышления: новый подход к оптимизации вывода в языковых моделях",
                    "desc": "Статья представляет эволюционную стратегию поиска для масштабирования вычислений во время вывода в больших языковых моделях. Метод, названный Mind Evolution, использует языковую модель для генерации, рекомбинации и уточнения кандидатов-ответов. Этот подход устраняет необходимость формализации исходной задачи вывода, если доступен оценщик решений. При контроле за стоимостью вычислений, Mind Evolution значительно превосходит другие стратегии вывода в задачах планирования на естественном языке."
                },
                "en": {
                    "title": "Mind Evolution: Revolutionizing Inference in Large Language Models",
                    "desc": "This paper presents Mind Evolution, an innovative evolutionary search strategy designed to enhance the inference time of Large Language Models (LLMs). By leveraging a language model, Mind Evolution generates, recombines, and refines potential responses without needing to define the inference problem formally, as long as a solution evaluator is available. The results demonstrate that Mind Evolution significantly outperforms traditional inference methods like Best-of-N and Sequential Revision in natural language planning tasks. In benchmarks such as TravelPlanner and Natural Plan, Mind Evolution successfully solves over 98% of instances using Gemini 1.5 Pro, showcasing its effectiveness without relying on a formal solver."
                },
                "zh": {
                    "title": "Mind Evolution：推理效率的新突破",
                    "desc": "本文探讨了一种用于大语言模型推理时间计算的进化搜索策略，称为Mind Evolution。该方法利用语言模型生成、重组和优化候选响应，避免了在有解决方案评估器的情况下需要形式化推理问题。通过控制推理成本，我们发现Mind Evolution在自然语言规划任务中显著优于其他推理策略，如Best-of-N和Sequential Revision。在TravelPlanner和Natural Plan基准测试中，Mind Evolution在不使用正式求解器的情况下，解决了超过98%的问题实例。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10020",
            "title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
            "url": "https://huggingface.co/papers/2501.10020",
            "abstract": "The 2D cartoon style is a prominent art form in digital character creation, particularly popular among younger audiences. While advancements in digital human technology have spurred extensive research into photorealistic digital humans and 3D characters, interactive 2D cartoon characters have received comparatively less attention. Unlike 3D counterparts, which require sophisticated construction and resource-intensive rendering, Live2D, a widely-used format for 2D cartoon characters, offers a more efficient alternative, which allows to animate 2D characters in a manner that simulates 3D movement without the necessity of building a complete 3D model. Furthermore, Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility and efficiency. In this technical report, we introduce Textoon, an innovative method for generating diverse 2D cartoon characters in the Live2D format based on text descriptions. The Textoon leverages cutting-edge language and vision models to comprehend textual intentions and generate 2D appearance, capable of creating a wide variety of stunning and interactive 2D characters within one minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.",
            "score": 6,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "828788f94bccbdc9",
            "authors": [
                "Chao He",
                "Jianqiang Ren",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10020.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Textoon: ИИ создает 2D мультперсонажей по текстовому описанию",
                    "desc": "В статье представлен метод Textoon для создания 2D мультипликационных персонажей в формате Live2D на основе текстовых описаний. Textoon использует современные языковые и визуальные модели для понимания текстовых намерений и генерации 2D внешнего вида персонажей. Метод способен создавать разнообразных интерактивных 2D персонажей менее чем за минуту. Live2D предлагает эффективную альтернативу 3D моделям, позволяя анимировать 2D персонажей, имитируя 3D движение, без необходимости создания полной 3D модели."
                },
                "en": {
                    "title": "Transforming Text into 2D Cartoon Characters with Textoon!",
                    "desc": "This paper presents Textoon, a novel approach for generating diverse 2D cartoon characters using the Live2D format. By utilizing advanced language and vision models, Textoon interprets text descriptions to create visually appealing and interactive characters efficiently. Unlike traditional 3D character models, Textoon allows for quick generation of 2D characters that simulate 3D movement without extensive resources. The method enhances accessibility and efficiency in digital character creation, catering especially to younger audiences."
                },
                "zh": {
                    "title": "Textoon：快速生成多样化2D卡通角色的创新方法",
                    "desc": "这篇论文介绍了一种名为Textoon的方法，用于根据文本描述生成多样化的2D卡通角色。与3D角色相比，2D卡通角色的动画制作更为高效，Textoon利用先进的语言和视觉模型来理解文本意图，并生成2D外观。该方法使用Live2D格式，使得角色动画能够模拟3D运动，而无需构建完整的3D模型。Textoon能够在一分钟内创建出多种令人惊叹和互动的2D角色，提升了数字角色创作的效率和可访问性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10120",
            "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
            "url": "https://huggingface.co/papers/2501.10120",
            "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholarly queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",
            "score": 6,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "bf3bfc73e6d5b31d",
            "authors": [
                "Yichen He",
                "Guanhua Huang",
                "Peiyuan Feng",
                "Yuan Lin",
                "Yuchen Zhang",
                "Hang Li",
                "Weinan E"
            ],
            "affiliations": [
                "ByteDance Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10120.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "PaSa: ИИ-агент для эффективного поиска научных статей",
                    "desc": "PaSa - это продвинутый агент для поиска научных статей, основанный на больших языковых моделях. Он способен автономно принимать решения, включая использование поисковых инструментов, чтение статей и выбор релевантных ссылок для получения комплексных и точных результатов по сложным научным запросам. PaSa оптимизирован с помощью обучения с подкреплением на синтетическом наборе данных AutoScholarQuery, содержащем 35 тысяч детализированных академических запросов и соответствующих статей из ведущих конференций по ИИ. Несмотря на обучение на синтетических данных, PaSa значительно превосходит существующие базовые модели на реальном тестовом наборе RealScholarQuery, включая Google и ChatGPT."
                },
                "en": {
                    "title": "Revolutionizing Academic Search with PaSa!",
                    "desc": "The paper presents PaSa, a sophisticated Paper Search agent that utilizes large language models to enhance academic research. PaSa autonomously navigates the search process by making decisions such as invoking search tools, analyzing papers, and selecting pertinent references to deliver thorough and precise results for complex queries. It is optimized through reinforcement learning using a synthetic dataset called AutoScholarQuery, which contains 35,000 detailed academic queries and related papers from leading AI conferences. The performance of PaSa is evaluated against real-world queries using the RealScholarQuery benchmark, demonstrating significant improvements over existing search tools, including Google and various GPT models."
                },
                "zh": {
                    "title": "PaSa：智能论文搜索的新纪元",
                    "desc": "本文介绍了一种名为PaSa的先进论文搜索代理，利用大型语言模型进行自主决策。PaSa能够调用搜索工具、阅读论文并选择相关参考文献，以获取复杂学术查询的全面和准确结果。我们通过强化学习优化PaSa，使用了一个包含35,000个细粒度学术查询的合成数据集AutoScholarQuery。尽管在合成数据上训练，PaSa在真实学术查询基准RealScholarQuery上的表现显著优于现有的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10021",
            "title": "X-Dyna: Expressive Dynamic Human Image Animation",
            "url": "https://huggingface.co/papers/2501.10021",
            "abstract": "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna.",
            "score": 3,
            "issue_id": 1752,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "4163d7e5ec4b04ce",
            "authors": [
                "Di Chang",
                "Hongyi Xu",
                "You Xie",
                "Yipeng Gao",
                "Zhengfei Kuang",
                "Shengqu Cai",
                "Chenxu Zhang",
                "Guoxian Song",
                "Chao Wang",
                "Yichun Shi",
                "Zeyuan Chen",
                "Shijie Zhou",
                "Linjie Luo",
                "Gordon Wetzstein",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "ByteDance",
                "Stanford University",
                "University of California Los Angeles",
                "University of California San Diego",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10021.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#diffusion",
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Оживление статичных изображений с помощью ИИ: реалистичная анимация человека и окружения",
                    "desc": "X-Dyna - это новый подход к анимации изображений человека с нуля, основанный на диффузионных моделях. Он использует выражения лица и движения тела из видео-драйвера для создания реалистичной динамики как субъекта, так и окружающей среды. В основе X-Dyna лежит модуль Dynamics-Adapter, который интегрирует контекст внешнего вида в пространственное внимание диффузионной модели. Система также включает локальный модуль управления для передачи выражений лица, что повышает реалистичность анимированных сцен."
                },
                "en": {
                    "title": "X-Dyna: Realistic Animation from a Single Image",
                    "desc": "X-Dyna is a new method for animating a single human image by using expressions and movements from a video. It improves on previous techniques by maintaining dynamic details, making animations look more realistic. The key part of X-Dyna is the Dynamics-Adapter, which helps blend the appearance of the subject with their movements while keeping the animation smooth. Additionally, it includes a module for accurately transferring facial expressions, resulting in more lifelike and expressive animations."
                },
                "zh": {
                    "title": "X-Dyna：真实感动画的新突破",
                    "desc": "X-Dyna是一种新颖的零样本扩散基础管道，能够通过驱动视频中的面部表情和身体动作为单个人物图像生成动画。该方法解决了以往人类姿态控制方法中的动态细节丢失问题，增强了视频动画的真实感。X-Dyna的核心是Dynamics-Adapter模块，它有效地将参考外观上下文整合到扩散模型的空间注意力中，同时保持运动模块合成流畅动态细节的能力。通过连接局部控制模块，X-Dyna能够捕捉与身份无关的面部表情，实现更真实的动画场景中的表情转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10045",
            "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
            "url": "https://huggingface.co/papers/2501.10045",
            "abstract": "The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio).",
            "score": 3,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "8d8cd8e70ad62b51",
            "authors": [
                "Shengkui Zhao",
                "Kun Zhou",
                "Zexu Pan",
                "Yukun Ma",
                "Chong Zhang",
                "Bin Ma"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10045.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#optimization"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "HiFi-SR: Единая сеть для сверхчеткой речи",
                    "desc": "Статья представляет HiFi-SR - унифицированную нейронную сеть для высококачественного повышения разрешения речи. Модель использует единую архитектуру трансформер-сверточной сети для обработки мел-спектрограмм и генерации высококачественных аудиосигналов. Для улучшения качества высоких частот применяется многополосный дискриминатор и многомасштабная функция потерь реконструкции мел-спектрограмм. Экспериментальные результаты показывают превосходство HiFi-SR над существующими методами как по объективным метрикам, так и по субъективным тестам."
                },
                "en": {
                    "title": "HiFi-SR: Elevating Speech Quality with Unified GANs",
                    "desc": "This paper introduces HiFi-SR, a novel approach to speech super-resolution using generative adversarial networks (GANs). Unlike traditional methods that use separate networks, HiFi-SR employs a unified transformer-convolutional architecture for end-to-end training, improving the consistency and quality of generated speech. The transformer encodes low-resolution mel-spectrograms into latent representations, while the convolutional network converts these into high-resolution audio waveforms. The model also integrates a multi-band discriminator and a mel-reconstruction loss to enhance high-frequency details, achieving superior performance in various scenarios."
                },
                "zh": {
                    "title": "HiFi-SR：高保真语音超分辨率的新方法",
                    "desc": "本研究提出了一种名为HiFi-SR的统一网络，用于语音超分辨率（SR），通过端到端的对抗训练实现高保真语音重建。该模型结合了变换器和卷积网络，能够有效地将低分辨率的mel谱图转换为高分辨率的时域波形。为了提高高频细节的保真度，我们在对抗训练中引入了多带宽、多尺度的时频判别器和多尺度mel重建损失。实验结果表明，HiFi-SR在目标指标和ABX偏好测试中显著优于现有的语音超分辨率方法，适用于不同的输入语音信号。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09978",
            "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
            "url": "https://huggingface.co/papers/2501.09978",
            "abstract": "We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: [Project Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).",
            "score": 2,
            "issue_id": 1751,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "e5b8603f26a902f9",
            "authors": [
                "Xiangyue Liu",
                "Kunming Luo",
                "Heng Li",
                "Qi Zhang",
                "Yuan Liu",
                "Li Yi",
                "Ping Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Tencent AI Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09978.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в редактировании анимируемых 3D-аватаров с помощью гауссовых моделей",
                    "desc": "Статья представляет GaussianAvatar-Editor - инновационную систему для редактирования анимируемых гауссовых аватаров головы на основе текстовых инструкций. Авторы предлагают функцию Weighted Alpha Blending Equation (WABE) для решения проблем, связанных с окклюзией при движении и пространственно-временной несогласованностью. Система использует условное состязательное обучение для улучшения качества редактирования и обеспечения согласованности в 4D. Эксперименты показывают превосходство предложенного подхода над существующими методами в создании фотореалистичных и согласованных результатов редактирования анимируемых 4D гауссовых аватаров."
                },
                "en": {
                    "title": "Revolutionizing 4D Avatar Editing with GaussianAvatar-Editor",
                    "desc": "GaussianAvatar-Editor is a new framework designed for editing animated Gaussian head avatars using text inputs. It tackles challenges like motion occlusion and maintaining spatial-temporal consistency, which are common in 4D animations. The framework introduces the Weighted Alpha Blending Equation (WABE) to improve the blending of visible elements while minimizing the impact of non-visible ones. Additionally, it employs conditional adversarial learning to enhance the quality of edits and ensure consistency throughout the animation process, resulting in photorealistic outputs."
                },
                "zh": {
                    "title": "高斯头像编辑的创新之路",
                    "desc": "我们介绍了GaussianAvatar-Editor，这是一个创新的框架，用于基于文本驱动的可动画高斯头像编辑。与静态3D高斯编辑不同，编辑可动画的4D高斯头像面临运动遮挡和时空不一致等挑战。为了解决这些问题，我们提出了加权阿尔法混合方程（WABE），该函数增强了可见高斯的混合权重，同时抑制了对不可见高斯的影响。通过结合条件对抗学习，我们提高了编辑质量并确保了4D一致性，从而实现了逼真且一致的可动画4D高斯编辑结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10132",
            "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
            "url": "https://huggingface.co/papers/2501.10132",
            "abstract": "Enhancing large language models (LLMs) with real-time APIs can help generate more accurate and up-to-date responses. However, evaluating the function calling abilities of LLMs in real-world scenarios remains under-explored due to the complexity of data collection and evaluation. In this work, we introduce ComplexFuncBench, a benchmark for complex function calling across five real-world scenarios. Compared to existing benchmarks, ComplexFuncBench encompasses multi-step and constrained function calling, which requires long-parameter filing, parameter value reasoning, and 128k long context. Additionally, we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks. Through comprehensive experiments, we demonstrate the deficiencies of state-of-the-art LLMs in function calling and suggest future directions for optimizing these capabilities. The data and code are available at https://github.com/THUDM/ComplexFuncBench.",
            "score": 2,
            "issue_id": 1749,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "de405dcc4bfc8efc",
            "authors": [
                "Lucen Zhong",
                "Zhengxiao Du",
                "Xiaohan Zhang",
                "Haiyi Hu",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10132.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Новый бенчмарк для оценки сложных вызовов функций в больших языковых моделях",
                    "desc": "Данная статья представляет новый бенчмарк ComplexFuncBench для оценки способностей больших языковых моделей (LLM) вызывать сложные функции в реальных сценариях. Бенчмарк включает в себя многошаговые и ограниченные вызовы функций, требующие заполнения длинных параметров и рассуждений о значениях параметров. Авторы также предлагают автоматическую систему ComplexEval для количественной оценки задач сложного вызова функций. Эксперименты показывают недостатки современных LLM в вызове функций и предлагают направления для оптимизации этих возможностей."
                },
                "en": {
                    "title": "Benchmarking Complex Function Calling in LLMs",
                    "desc": "This paper presents ComplexFuncBench, a new benchmark designed to evaluate the function calling abilities of large language models (LLMs) in real-world scenarios. It focuses on complex tasks that involve multi-step and constrained function calling, which require advanced reasoning and handling of long contexts. The authors also introduce ComplexEval, an automatic framework for quantitatively assessing these complex function calling tasks. Through their experiments, they highlight the limitations of current state-of-the-art LLMs and propose directions for improving their performance in this area."
                },
                "zh": {
                    "title": "提升LLMs函数调用能力的基准与评估",
                    "desc": "本论文提出了ComplexFuncBench，这是一个用于评估大型语言模型（LLMs）在复杂函数调用方面的基准测试。该基准涵盖了五种真实场景，涉及多步骤和受限的函数调用，要求模型进行长参数填写和参数值推理。我们还提出了ComplexEval，一个自动化框架，用于定量评估复杂函数调用任务的能力。通过实验，我们展示了当前最先进的LLMs在函数调用方面的不足，并提出了未来优化的方向。"
                }
            }
        }
    ],
    "link_prev": "2025-01-17.html",
    "link_next": "2025-01-21.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "short_date_next": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了生成模型在各个领域的影响。研究发现，大语言模型在推理时增加计算量可以提高性能。扩散模型也可以通过增加去噪步骤来调整计算量，但收益通常在几十步后趋于平缓。作者探讨了扩散模型在推理时的计算行为，并通过实验发现，增加计算量可以显著提高生成图像的质量。不同的组件组合可以适应不同的应用场景。",
        "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
        "pinyin": "这篇文章讨论了生成模型在各个领域的影响。\nZhè piān wénzhāng tǎolùn le shēngchéng móxíng zài gègè lǐngyù de yǐngxiǎng.\n\n研究发现，大语言模型在推理时增加计算量可以提高性能。\nYánjiū fāxiàn, dà yǔyán móxíng zài tuīlǐ shí zēngjiā jìsuàn liàng kěyǐ tígāo xìngnéng.\n\n扩散模型也可以通过增加去噪步骤来调整计算量，但收益通常在几十步后趋于平缓。\nKuòsàn móxíng yě kěyǐ tōngguò zēngjiā qùzào bùzhòu lái tiáozhěng jìsuàn liàng, dàn shōuyì tōngcháng zài jǐshí bù hòu qūyú píngchuǎn.\n\n作者探讨了扩散模型在推理时的计算行为，并通过实验发现，增加计算量可以显著提高生成图像的质量。\nZuòzhě tàntǎo le kuòsàn móxíng zài tuīlǐ shí de jìsuàn xíngwéi, bìng tōngguò shíyàn fāxiàn, zēngjiā jìsuàn liàng kěyǐ xiǎnzhù tígāo shēngchéng túxiàng de zhìliàng.\n\n不同的组件组合可以适应不同的应用场景。\nBùtóng de zǔjiàn zǔhé kěyǐ shìyìng bùtóng de yìngyòng chǎngjǐng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐng xiǎng\", \"trans\": \"influence\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"计算量\", \"pinyin\": \"jì suàn liàng\", \"trans\": \"computational load\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"去噪\", \"pinyin\": \"qù zào\", \"trans\": \"denoise\"},\n    {\"word\": \"步骤\", \"pinyin\": \"bù zhòu\", \"trans\": \"step\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"},\n    {\"word\": \"收益\", \"pinyin\": \"shōu yì\", \"trans\": \"benefit\"},\n    {\"word\": \"趋于\", \"pinyin\": \"qū yú\", \"trans\": \"tend towards\"},\n    {\"word\": \"平缓\", \"pinyin\": \"píng huǎn\", \"trans\": \"gentle\"},\n    {\"word\": \"作者\", \"pinyin\": \"zuò zhě\", \"trans\": \"author\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"行为\", \"pinyin\": \"xíng wéi\", \"trans\": \"behavior\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔ jiàn\", \"trans\": \"component\"},\n    {\"word\": \"组合\", \"pinyin\": \"zǔ hé\", \"trans\": \"combination\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scenario\"}\n]",
        "trans": "This article discusses the impact of generative models across various domains. Research has found that increasing the computational load during inference can enhance the performance of large language models. Diffusion models can also adjust their computational load by increasing the number of denoising steps, although the benefits typically plateau after a few dozen steps. The authors explore the computational behavior of diffusion models during inference and, through experiments, discover that increasing the computational load can significantly improve the quality of generated images. Different combinations of components can be adapted to suit different application scenarios.",
        "update_ts": "2025-01-19 12:36"
    }
}