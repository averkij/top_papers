{
    "date": {
        "ru": "23 Ğ¸ÑĞ½Ñ",
        "en": "June 23",
        "zh": "6æœˆ23æ—¥"
    },
    "time_utc": "2025-06-23 10:13",
    "weekday": 0,
    "issue_id": 4432,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.16406",
            "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
            "url": "https://huggingface.co/papers/2506.16406",
            "abstract": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
            "score": 56,
            "issue_id": 4426,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "7ec3cdc6c01d1d79",
            "authors": [
                "Zhiyuan Liang",
                "Dongwen Tang",
                "Yuhao Zhou",
                "Xuanlei Zhao",
                "Mingjia Shi",
                "Wangbo Zhao",
                "Zekai Li",
                "Peihao Wang",
                "Konstantin SchÃ¼rholt",
                "Damian Borth",
                "Michael M. Bronstein",
                "Yang You",
                "Zhangyang Wang",
                "Kai Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Oxford University",
                "UT Austin",
                "University of St. Gallen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16406.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¯Ğœ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Drag-and-Drop LLMs (DnD) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DnD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² LoRA Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. DnD Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼ĞµĞ¶Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Efficient Task-Specific Adaptation with Drag-and-Drop LLMs",
                    "desc": "The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels."
                },
                "zh": {
                    "title": "æ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®šå‚æ•°ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºæ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼ˆDnDï¼‰ï¼Œå®ƒé€šè¿‡æç¤ºæ¡ä»¶ç”Ÿæˆå‚æ•°ï¼Œæ¶ˆé™¤äº†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒéœ€æ±‚ã€‚DnDä½¿ç”¨è½»é‡çº§æ–‡æœ¬ç¼–ç å™¨å°†æç¤ºæ‰¹æ¬¡æç‚¼ä¸ºæ¡ä»¶åµŒå…¥ï¼Œå¹¶é€šè¿‡çº§è”è¶…å·ç§¯è§£ç å™¨è½¬æ¢ä¸ºå®Œæ•´çš„LoRAçŸ©é˜µã€‚ä¸ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDnDåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æç¤ºæ¡ä»¶å‚æ•°ç”Ÿæˆæ˜¯å¿«é€Ÿä¸“é—¨åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16054",
            "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models",
            "url": "https://huggingface.co/papers/2506.16054",
            "abstract": "PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.",
            "score": 35,
            "issue_id": 4428,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "141661e70ce4b65f",
            "authors": [
                "Tianchen Zhao",
                "Ke Hong",
                "Xinhao Yang",
                "Xuefeng Xiao",
                "Huixia Li",
                "Feng Ling",
                "Ruiqi Xie",
                "Siqi Chen",
                "Hongyu Zhu",
                "Yichong Zhang",
                "Yu Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16054.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ PAROAttention Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.9-2.7 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (20-30%) Ğ¸ 8/4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Visual Attention for Efficient AI",
                    "desc": "PAROAttention is a novel approach that reorganizes visual attention patterns to improve the efficiency of sparsification and quantization in machine learning models. By addressing the challenges posed by the irregular characteristics of attention mechanisms, this method simplifies the process of reducing memory and computational costs. The technique, known as Pattern-Aware token ReOrdering (PARO), consolidates diverse attention patterns into a more manageable block-wise format. As a result, PAROAttention achieves high-quality image and video generation with significantly reduced resource requirements, maintaining performance comparable to full-precision models."
                },
                "zh": {
                    "title": "PAROAttentionï¼šé«˜æ•ˆçš„è§†è§‰æ³¨æ„åŠ›é‡ç»„",
                    "desc": "PAROAttentionæ˜¯ä¸€ç§é€šè¿‡é‡æ–°ç»„ç»‡è§†è§‰æ³¨æ„åŠ›æ¨¡å¼æ¥æé«˜ç¨€ç–åŒ–å’Œé‡åŒ–æ•ˆç‡çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå°½é‡ä¸å½±å“æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰æ³¨æ„åŠ›æ¨¡å¼çš„åˆ†æ•£å’Œä¸è§„åˆ™ç‰¹æ€§æ˜¯é€ æˆé«˜è®¡ç®—æˆæœ¬çš„ä¸»è¦åŸå› ã€‚PAROæŠ€æœ¯é€šè¿‡å°†å¤šæ ·çš„æ³¨æ„åŠ›æ¨¡å¼ç»Ÿä¸€ä¸ºç¡¬ä»¶å‹å¥½çš„å—çŠ¶æ¨¡å¼ï¼Œæ˜¾è‘—ç®€åŒ–äº†ç¨€ç–åŒ–å’Œé‡åŒ–è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16035",
            "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
            "url": "https://huggingface.co/papers/2506.16035",
            "abstract": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",
            "score": 33,
            "issue_id": 4427,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "08abcd6ec6f0d9e1",
            "authors": [
                "Vishesh Tripathi",
                "Tanmay Odapally",
                "Indraneel Das",
                "Uday Allu",
                "Biddwan Ahmed"
            ],
            "affiliations": [
                "AI Research Team, Yellow.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16035.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#optimization",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing RAG with Multimodal Document Chunking",
                    "desc": "This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—ï¼Œæå‡ä¿¡æ¯æ£€ç´¢æ–°é«˜åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥å¤„ç†å¤æ‚çš„PDFæ–‡æ¡£ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šé¡µè¡¨æ ¼å’ŒåµŒå…¥è§†è§‰å…ƒç´ ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡é…ç½®é¡µé¢æ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè·¨æ‰¹æ¬¡ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜äº†åˆ†å—è´¨é‡å’Œåç»­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ–‡æ¡£ç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09049",
            "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.09049",
            "abstract": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
            "score": 24,
            "issue_id": 4425,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "91caa1a0b1cb2b54",
            "authors": [
                "Li Kang",
                "Xiufeng Song",
                "Heng Zhou",
                "Yiran Qin",
                "Jie Yang",
                "Xiaohong Liu",
                "Philip Torr",
                "Lei Bai",
                "Zhenfei Yin"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09049.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#games",
                    "#rl",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "VIKI-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VIKI-R - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VIKI-R Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑŒÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R",
                    "desc": "This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI."
                },
                "zh": {
                    "title": "VIKI-Benchä¸VIKI-Rï¼šæ¨åŠ¨å…·èº«æ™ºèƒ½ä½“çš„è§†è§‰é©±åŠ¨åˆä½œ",
                    "desc": "VIKI-Benchå’ŒVIKI-Ræ˜¯ç”¨äºè¯„ä¼°å’Œæ”¹å–„å¤šæ ·åŒ–å…·èº«æ™ºèƒ½ä½“ä¹‹é—´è§†è§‰é©±åŠ¨åˆä½œçš„åŸºå‡†å’Œæ¡†æ¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼ŒåŒ…å«ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–å±‚æ¬¡ã€‚VIKI-Ræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ³¨é‡Šç¤ºä¾‹å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡å±‚æ¬¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”å¼ºåŒ–å­¦ä¹ ä¿ƒè¿›äº†å¼‚æ„æ™ºèƒ½ä½“ä¹‹é—´çš„ç»„åˆåˆä½œæ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17206",
            "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
            "url": "https://huggingface.co/papers/2506.17206",
            "abstract": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.",
            "score": 13,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "bbec38fbe4e9ddd8",
            "authors": [
                "Yukun Huang",
                "Yanning Zhou",
                "Jianan Wang",
                "Kaiyi Huang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Astribot",
                "Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17206.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 3D Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamCube, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "DreamCube: Bridging 2D and 3D for Stunning Panoramas",
                    "desc": "This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation."
                },
                "zh": {
                    "title": "å¤šå¹³é¢åŒæ­¥ï¼Œå¼€å¯ä¸‰ç»´å…¨æ™¯æ–°è§†ç•Œ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamCubeçš„å¤šå¹³é¢åŒæ­¥æ–¹æ³•ï¼Œæ—¨åœ¨å°†äºŒç»´åŸºç¡€æ¨¡å‹æ‰©å±•åˆ°ä¸‰ç»´å…¨æ™¯ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ¨¡å‹çš„ä¸°å¯Œå›¾åƒå…ˆéªŒï¼Œå…‹æœäº†ä¸‰ç»´å…¨æ™¯æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„è§†è§‰æ•ˆæœå’Œå‡†ç¡®çš„å‡ ä½•å½¢çŠ¶ï¼ŒåŒæ—¶ä¿æŒå¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯åœ¨å…¨æ™¯å›¾åƒç”Ÿæˆã€æ·±åº¦ä¼°è®¡å’Œä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17201",
            "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
            "url": "https://huggingface.co/papers/2506.17201",
            "abstract": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.",
            "score": 11,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "def0daf064c03fe6",
            "authors": [
                "Jiaqi Li",
                "Junshu Tang",
                "Zhiyong Xu",
                "Longhuang Wu",
                "Yuan Zhou",
                "Shuai Shao",
                "Tianbao Yu",
                "Zhiguo Cao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17201.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Hunyuan-GameCraft - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 100 ĞĞĞ-Ğ¸Ğ³Ñ€, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hunyuan-GameCraft Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Interactive Video Generation in Gaming",
                    "desc": "Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios."
                },
                "zh": {
                    "title": "Hunyuan-GameCraftï¼šæå‡äº’åŠ¨æ¸¸æˆè§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿä¸å¯ç©æ€§",
                    "desc": "Hunyuan-GameCraftæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¸¸æˆç¯å¢ƒä¸­é«˜åŠ¨æ€äº’åŠ¨è§†é¢‘ç”Ÿæˆçš„å±€é™æ€§ã€‚å®ƒé€šè¿‡ç»Ÿä¸€è¾“å…¥è¡¨ç¤ºã€æ··åˆå†å²æ¡ä»¶è®­ç»ƒå’Œæ¨¡å‹è’¸é¦æ¥æé«˜åŠ¨æ€æ€§ã€é€šç”¨æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°ç²¾ç»†çš„åŠ¨ä½œæ§åˆ¶ï¼Œå¹¶åœ¨å¤æ‚çš„äº’åŠ¨ç¯å¢ƒä¸­ä¿æŒé•¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒï¼ŒHunyuan-GameCraftåœ¨ç”Ÿæˆäº’åŠ¨æ¸¸æˆè§†é¢‘çš„çœŸå®æ„Ÿå’Œå¯ç©æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16504",
            "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
            "url": "https://huggingface.co/papers/2506.16504",
            "abstract": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
            "score": 9,
            "issue_id": 4424,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "7ea16d5712b67fcc",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Haolin Liu",
                "Zibo Zhao",
                "Qingxiang Lin",
                "Huiwen Shi",
                "Xianghui Yang",
                "Mingxin Yang",
                "Shuhui Yang",
                "Yifei Feng",
                "Sheng Zhang",
                "Xin Huang",
                "Di Luo",
                "Fan Yang",
                "Fang Yang",
                "Lifu Wang",
                "Sicong Liu",
                "Yixuan Tang",
                "Yulin Cai",
                "Zebin He",
                "Tian Liu",
                "Yuhong Liu",
                "Jie Jiang",
                "Linus",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16504.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Hunyuan3D 2.5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LATTICE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ (PBR) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, Hunyuan3D 2.5 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Generation with Hunyuan3D 2.5",
                    "desc": "Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions."
                },
                "zh": {
                    "title": "Hunyuan3D 2.5ï¼šå½¢çŠ¶ä¸çº¹ç†ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Hunyuan3D 2.5 æ˜¯ä¸€å¥—å…ˆè¿›çš„ 3D æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œç»†è‡´çš„çº¹ç† 3D èµ„äº§ã€‚å®ƒåœ¨å½¢çŠ¶ç”Ÿæˆæ–¹é¢å¼•å…¥äº†æ–°çš„åŸºç¡€æ¨¡å‹ LATTICEï¼Œç»è¿‡å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ä¸”ç»†è‡´çš„ 3D å½¢çŠ¶ã€‚çº¹ç†ç”Ÿæˆæ–¹é¢ï¼Œé‡‡ç”¨äº†åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰æŠ€æœ¯ï¼Œç»“åˆäº†å¤šè§†è§’æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†çº¹ç†è´¨é‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒHunyuan3D 2.5 åœ¨å½¢çŠ¶å’Œçº¹ç†ç”Ÿæˆæ–¹é¢å‡ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15745",
            "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
            "url": "https://huggingface.co/papers/2506.15745",
            "abstract": "InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.",
            "score": 4,
            "issue_id": 4431,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "f6babb4a5e89bb3f",
            "authors": [
                "Minsoo Kim",
                "Kyuhong Shim",
                "Jungwook Choi",
                "Simyung Chang"
            ],
            "affiliations": [
                "Hanyang University",
                "Qualcomm AI Research, Qualcomm Korea",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15745.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "InfiniPot-V - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (TaR) Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ (VaN) Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². InfiniPot-V ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ´Ğ¾ 94%, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ°."
                },
                "en": {
                    "title": "Streamline Video Understanding with InfiniPot-V!",
                    "desc": "InfiniPot-V is a novel framework designed to optimize memory usage during video encoding for real-time video understanding. It operates without the need for training or prior knowledge of user queries, making it flexible and efficient. The framework compresses the key-value cache by removing redundant information while preserving important data, ensuring that memory usage remains constant regardless of video length. This approach significantly reduces GPU memory requirements while maintaining high accuracy in video processing tasks."
                },
                "zh": {
                    "title": "InfiniPot-Vï¼šå®æ—¶è§†é¢‘ç†è§£çš„å†…å­˜å‹ç¼©æ–°æ–¹æ¡ˆ",
                    "desc": "InfiniPot-V æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”ä¸æŸ¥è¯¢æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨å‹ç¼©è§†é¢‘ç¼–ç ä¸­çš„å…³é”®å€¼ç¼“å­˜ï¼Œä»¥ä¿æŒå›ºå®šçš„å†…å­˜é™åˆ¶ï¼Œä»è€Œæå‡å®æ—¶æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›‘æ§ç¼“å­˜ï¼Œå½“è¾¾åˆ°ç”¨æˆ·è®¾å®šçš„é˜ˆå€¼æ—¶ï¼Œæ‰§è¡Œè½»é‡çº§å‹ç¼©ï¼Œå»é™¤æ—¶é—´ä¸Šå†—ä½™çš„æ ‡è®°ï¼Œå¹¶ä¿ç•™è¯­ä¹‰ä¸Šé‡è¦çš„æ ‡è®°ã€‚ä¸ä»¥å¾€çš„å‹ç¼©æ–¹æ¡ˆä¸åŒï¼ŒInfiniPot-V ä¸éœ€è¦ç¦»çº¿è·å–æ•´ä¸ªè§†é¢‘æˆ–ç”¨æˆ·æŸ¥è¯¢ï¼Œå› æ­¤èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡åœ¨å¤šä¸ªå¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éªŒè¯ï¼ŒInfiniPot-V æ˜¾è‘—é™ä½äº† GPU å†…å­˜å³°å€¼ï¼ŒåŒæ—¶ä¿æŒå®æ—¶ç”Ÿæˆå’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17213",
            "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation",
            "url": "https://huggingface.co/papers/2506.17213",
            "abstract": "InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.  \t\t\t\t\tAI-generated summary \t\t\t\t An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",
            "score": 2,
            "issue_id": 4431,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "0940c34932f5181b",
            "authors": [
                "Xiuyu Yang",
                "Shuhan Tan",
                "Philipp KrÃ¤henbÃ¼hl"
            ],
            "affiliations": [
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17213.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "InfGen: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "InfGen - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½. InfGen Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ (9 ÑĞµĞºÑƒĞ½Ğ´), Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ (30 ÑĞµĞºÑƒĞ½Ğ´) Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºĞ°."
                },
                "en": {
                    "title": "InfGen: Revolutionizing Long-Term Traffic Simulation with Unified Prediction",
                    "desc": "InfGen is a novel model designed for traffic simulation that predicts the next movement of vehicles in a scene. It combines two key processes: closed-loop motion simulation, which tracks the movement of vehicles, and scene generation, which creates the environment around them. This interleaving allows InfGen to maintain stability during long-term simulations, addressing the challenges faced by previous models that struggled with dynamic agent interactions. The model demonstrates superior performance in both short-term and long-term traffic simulations, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "InfGenï¼šç¨³å®šçš„é•¿æœŸäº¤é€šä»¿çœŸæ–°æ–¹æ³•",
                    "desc": "InfGenæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡äº¤æ›¿è¿›è¡Œé—­ç¯è¿åŠ¨ä»¿çœŸå’Œåœºæ™¯ç”Ÿæˆï¼Œå®ç°ç¨³å®šçš„é•¿æœŸäº¤é€šä»¿çœŸã€‚ä¼ ç»Ÿæ¨¡å‹ä¸»è¦å…³æ³¨åˆå§‹ä»£ç†çš„é—­ç¯è¿åŠ¨ä»¿çœŸï¼Œè¿™åœ¨é•¿æœŸä»¿çœŸä¸­å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºä»£ç†åœ¨è‡ªé©¾è½¦è¿›å…¥æ–°åŒºåŸŸæ—¶ä¼šè¿›å‡ºåœºæ™¯ã€‚InfGenèƒ½å¤Ÿè‡ªåŠ¨åˆ‡æ¢é—­ç¯è¿åŠ¨ä»¿çœŸå’Œåœºæ™¯ç”Ÿæˆæ¨¡å¼ï¼Œä»è€Œå®ç°ç¨³å®šçš„é•¿æœŸä»¿çœŸã€‚è¯¥æ¨¡å‹åœ¨çŸ­æœŸï¼ˆ9ç§’ï¼‰äº¤é€šä»¿çœŸä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨é•¿æœŸï¼ˆ30ç§’ï¼‰ä»¿çœŸä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17202",
            "title": "UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2506.17202",
            "abstract": "A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.",
            "score": 2,
            "issue_id": 4432,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "3796f42ead32837a",
            "authors": [
                "Teng Li",
                "Quanfeng Lu",
                "Lirui Zhao",
                "Hao Li",
                "Xizhou Zhu",
                "Yu Qiao",
                "Jun Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "HKUST",
                "SJTU",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17202.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ´",
                "ru": {
                    "title": "UniFork: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "UniFork - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Y-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. UniFork Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "UniFork: Balancing Shared Learning and Task Specialization in Image AI",
                    "desc": "The paper introduces UniFork, a Y-shaped architecture designed to improve unified image understanding and generation in machine learning. It addresses the challenge of balancing shared learning and task specialization by analyzing how different tasks align modalities at various network depths. The findings show that understanding tasks benefit from deeper alignment, while generation tasks require a different approach, leading to conflicts in traditional models. UniFork resolves these issues by sharing shallow layers for common learning and using specialized branches for deeper layers, resulting in superior performance compared to fully shared Transformer models."
                },
                "zh": {
                    "title": "UniForkï¼šå¹³è¡¡å…±äº«å­¦ä¹ ä¸ä»»åŠ¡ä¸“é—¨åŒ–çš„åˆ›æ–°æ¶æ„",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Yå½¢æ¶æ„UniForkï¼Œæ—¨åœ¨å¹³è¡¡å…±äº«å­¦ä¹ å’Œä»»åŠ¡ä¸“é—¨åŒ–ï¼Œä»¥å®ç°ç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆã€‚ç ”ç©¶å‘ç°ï¼Œç†è§£ä»»åŠ¡åœ¨ç½‘ç»œæ·±åº¦ä¸Šéœ€è¦é€æ¸å¢åŠ çš„æ¨¡æ€å¯¹é½ï¼Œè€Œç”Ÿæˆä»»åŠ¡åˆ™åœ¨æ—©æœŸå±‚å¢åŠ æ¨¡æ€å¯¹é½ä½†åœ¨æ·±å±‚å‡å°‘ï¼Œä»¥æ¢å¤ç©ºé—´ç»†èŠ‚ã€‚è¿™ç§å¯¹é½æ¨¡å¼çš„å·®å¼‚åœ¨ä¼ ç»Ÿçš„å®Œå…¨å…±äº«Transformeræ¨¡å‹ä¸­é€ æˆäº†æ€§èƒ½å¦¥åã€‚UniForké€šè¿‡åœ¨æµ…å±‚å…±äº«è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨æ·±å±‚é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„åˆ†æ”¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15442",
            "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material",
            "url": "https://huggingface.co/papers/2506.15442",
            "abstract": "The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.",
            "score": 2,
            "issue_id": 4431,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "c5c9b452dd58395e",
            "authors": [
                "Team Hunyuan3D",
                "Shuhui Yang",
                "Mingxin Yang",
                "Yifei Feng",
                "Xin Huang",
                "Sheng Zhang",
                "Zebin He",
                "Di Luo",
                "Haolin Liu",
                "Yunfei Zhao",
                "Qingxiang Lin",
                "Zeqiang Lai",
                "Xianghui Yang",
                "Huiwen Shi",
                "Zibo Zhao",
                "Bowen Zhang",
                "Hongyu Yan",
                "Lifu Wang",
                "Sicong Liu",
                "Jihong Zhang",
                "Meng Chen",
                "Liang Dong",
                "Yiwen Jia",
                "Yulin Cai",
                "Jiaao Yu",
                "Yixuan Tang",
                "Dongyuan Guo",
                "Junlin Yu",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Shida Wei",
                "Chao Zhang",
                "Yonghao Tan",
                "Yifu Sun",
                "Lin Niu",
                "Shirui Huang",
                "Bojian Zheng",
                "Shu Liu",
                "Shilin Chen",
                "Xiang Yuan",
                "Xiaofeng Yang",
                "Kai Liu",
                "Jianchen Zhu",
                "Peng Chen",
                "Tian Liu",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15442.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#data"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Hunyuan3D 2.1",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Hunyuan3D 2.1 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Hunyuan3D 2.1 ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Hunyuan3D-DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Hunyuan3D-Paint Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Ğ¦ĞµĞ»ÑŒ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° - ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking 3D Model Creation with Hunyuan3D 2.1",
                    "desc": "This paper presents a tutorial on Hunyuan3D 2.1, a system designed for generating high-resolution, textured 3D models. It addresses the complexities of 3D model creation by providing a detailed guide on data preparation, model architecture, training, evaluation, and deployment. The system features two main components: Hunyuan3D-DiT for shape generation and Hunyuan3D-Paint for texture synthesis. By following this tutorial, users will learn how to finetune or create effective 3D generative models for various applications such as gaming and virtual reality."
                },
                "zh": {
                    "title": "æŒæ¡Hunyuan3D 2.1ï¼Œè½»æ¾ç”Ÿæˆé«˜è´¨é‡3Dæ¨¡å‹",
                    "desc": "æœ¬æ•™ç¨‹å…¨é¢ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨Hunyuan3D 2.1ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€å¸¦çº¹ç†çš„3Dæ¨¡å‹ï¼Œæ¶µç›–äº†æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²ç­‰æ–¹é¢ã€‚å°½ç®¡3D AIç”Ÿæˆå†…å®¹é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºæ”¶é›†ã€å¤„ç†å’Œè®­ç»ƒ3Dæ¨¡å‹çš„å¤æ‚æ€§ï¼Œä»ç„¶ä¸»è¦é¢å‘ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ã€‚Hunyuan3D 2.1ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæä¾›äº†é€æ­¥æŒ‡å¯¼ï¼Œå¸®åŠ©ç”¨æˆ·å¤„ç†3Dæ•°æ®ã€è®­ç»ƒç”Ÿæˆæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†æŒæ¡å¾®è°ƒæˆ–å¼€å‘é€‚ç”¨äºæ¸¸æˆã€è™šæ‹Ÿç°å®å’Œå·¥ä¸šè®¾è®¡çš„å¼ºå¤§3Dç”Ÿæˆæ¨¡å‹çš„çŸ¥è¯†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15925",
            "title": "Reranking-based Generation for Unbiased Perspective Summarization",
            "url": "https://huggingface.co/papers/2506.15925",
            "abstract": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.",
            "score": 1,
            "issue_id": 4425,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "8cb1a06d9566bcfc",
            "authors": [
                "Narutatsu Ri",
                "Nicholas Deas",
                "Kathleen McKeown"
            ],
            "affiliations": [
                "Department of Computer Science, Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15925.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¾ÑÑÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²."
                },
                "en": {
                    "title": "Enhancing Perspective Summaries with Reranking and Preference Tuning",
                    "desc": "This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries."
                },
                "zh": {
                    "title": "æå‡è§‚ç‚¹æ‘˜è¦è´¨é‡çš„å…³é”®åœ¨äºé‡æ’åºä¸åå¥½è°ƒä¼˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„è§‚ç‚¹æ‘˜è¦çš„è´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡åœ¨æµ‹é‡æ‘˜è¦çš„è¦†ç›–ç‡å’Œå¿ å®åº¦æ—¶æ•ˆæœä¸ä½³ï¼Œè€ŒåŸºäºè¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†å¹¶è¿›è¡Œäººç±»æ ‡æ³¨ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›æ–°æŒ‡æ ‡çš„å¯é æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†é‡æ’åºå’Œåå¥½è°ƒä¼˜çš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‘˜è¦ç”Ÿæˆçš„æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-20.html",
    "link_next": "2025-06-24.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 3,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}