{
    "date": {
        "ru": "30 апреля",
        "en": "April 30",
        "zh": "4月30日"
    },
    "time_utc": "2025-04-30 22:10",
    "weekday": 2,
    "issue_id": 3522,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20571",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "url": "https://huggingface.co/papers/2504.20571",
            "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "score": 45,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "5392cdfe5ab1de59",
            "authors": [
                "Yiping Wang",
                "Qing Yang",
                "Zhiyuan Zeng",
                "Liliang Ren",
                "Lucas Liu",
                "Baolin Peng",
                "Hao Cheng",
                "Xuehai He",
                "Kuan Wang",
                "Jianfeng Gao",
                "Weizhu Chen",
                "Shuohang Wang",
                "Simon Shaolei Du",
                "Yelong Shen"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Microsoft",
                "University of California, Santa Cruz",
                "University of Southern California",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#math",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Один пример - огромный скачок в математических способностях ИИ",
                    "desc": "Исследование показывает эффективность обучения с подкреплением с верифицируемым вознаграждением (RLVR) на одном примере для улучшения математических рассуждений больших языковых моделей. Применение этого метода к базовой модели Qwen2.5-Math-1.5B значительно повысило ее производительность на различных математических тестах. Авторы наблюдали интересные явления, такие как межобластная генерализация и улучшение тестовых показателей после насыщения точности обучения. Исследование также подчеркивает важность поощрения исследования в процессе обучения и отличие этого метода от феномена 'грокинга'."
                },
                "en": {
                    "title": "Boosting Math Skills in LLMs with 1-Shot RLVR",
                    "desc": "This paper presents a method called 1-shot Reinforcement Learning with Verifiable Reward (RLVR) that significantly enhances the mathematical reasoning abilities of large language models (LLMs). By using just one training example, the authors demonstrate a remarkable increase in performance on the MATH500 benchmark, achieving a score of 73.6% compared to 36.0% before. The study also reveals that this approach leads to improvements across various models and algorithms, highlighting the importance of exploration in training. Additionally, the authors introduce the concept of post-saturation generalization, where performance continues to improve even after training accuracy levels off, suggesting new avenues for research in RLVR."
                },
                "zh": {
                    "title": "一例强化学习，提升数学推理能力！",
                    "desc": "本文展示了使用可验证奖励的强化学习（1-shot RLVR）在提升大型语言模型（LLMs）数学推理能力方面的有效性。通过将RLVR应用于基础模型Qwen2.5-Math-1.5B，我们发现一个单一示例可以将模型在MATH500上的表现从36.0%提升至73.6%。此外，本文还观察到在1-shot RLVR过程中出现了一些有趣现象，如跨领域泛化和自我反思频率增加。我们验证了1-shot RLVR的有效性主要源于策略梯度损失，并强调了促进探索在训练中的关键作用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20734",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "url": "https://huggingface.co/papers/2504.20734",
            "abstract": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.",
            "score": 42,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "53427aff6a4d7ed5",
            "authors": [
                "Woongyeong Yeo",
                "Kangsan Kim",
                "Soyeong Jeong",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20734.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "UniversalRAG: Универсальное извлечение знаний из разнородных источников",
                    "desc": "UniversalRAG - это новая система извлечения и интеграции знаний из разнородных источников с различными модальностями и уровнями детализации. В отличие от существующих подходов RAG, ограниченных одномодальными корпусами, UniversalRAG использует механизм маршрутизации с учетом модальности для динамического выбора наиболее подходящего корпуса. Система также организует каждую модальность на несколько уровней детализации, позволяя точно настраивать извлечение информации в соответствии со сложностью и объемом запроса. Эксперименты на 8 тестовых наборах данных показали превосходство UniversalRAG над одномодальными и унифицированными базовыми моделями."
                },
                "en": {
                    "title": "UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval",
                    "desc": "The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations."
                },
                "zh": {
                    "title": "多模态知识整合的全新框架",
                    "desc": "本文介绍了一种新的检索增强生成框架，称为UniversalRAG，旨在从多种异构知识源中检索和整合信息，以提高模型的事实准确性。现有的检索增强生成方法通常仅限于文本数据，而UniversalRAG能够处理多种模态的信息，如图像和视频。该框架采用了一种模态感知的路由机制，能够动态识别最合适的模态特定语料库，从而进行针对性的检索。此外，UniversalRAG还将每种模态组织为多个粒度级别，以便根据查询的复杂性和范围进行精细化检索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20595",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "url": "https://huggingface.co/papers/2504.20595",
            "abstract": "We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.",
            "score": 35,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "244cff2e64afeaa0",
            "authors": [
                "Rulin Shao",
                "Rui Qiao",
                "Varsha Kishore",
                "Niklas Muennighoff",
                "Xi Victoria Lin",
                "Daniela Rus",
                "Bryan Kian Hsiang Low",
                "Sewon Min",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Luke Zettlemoyer"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "FAIR at Meta",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Singapore-MIT Alliance for Research and Technology",
                "Stanford University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20595.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#rag",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ReasonIR-8B: Прорыв в информационном поиске для задач рассуждения",
                    "desc": "ReasonIR-8B - это первая модель для поиска информации, специально обученная для задач рассуждения. Авторы разработали конвейер генерации синтетических данных, создающий сложные и релевантные запросы для каждого документа. Модель, обученная на смеси синтетических и существующих публичных данных, достигает нового уровня производительности на бенчмарке BRIGHT. При применении к задачам RAG, ReasonIR-8B значительно улучшает результаты на MMLU и GPQA по сравнению с baseline без доступа к внешней информации."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with ReasonIR-8B",
                    "desc": "ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications."
                },
                "zh": {
                    "title": "推理任务的专属检索器：ReasonIR-8B",
                    "desc": "我们提出了ReasonIR-8B，这是第一个专门为一般推理任务训练的检索器。现有的检索器在推理任务上的表现有限，部分原因是现有的训练数据集主要集中在与文档直接相关的短小事实查询上。我们开发了一种合成数据生成管道，为每个文档创建具有挑战性和相关性的查询，以及一个看似相关但实际上无用的困难负样本。通过在合成数据和现有公共数据的混合上进行训练，ReasonIR-8B在BRIGHT基准上达到了29.9的nDCG@10（不使用重排序器）和36.9的nDCG@10（使用重排序器），并在RAG任务中显著提高了MMLU和GPQA的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20879",
            "title": "The Leaderboard Illusion",
            "url": "https://huggingface.co/papers/2504.20879",
            "abstract": "Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field",
            "score": 32,
            "issue_id": 3505,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "c1d3b5cc6840e6e6",
            "authors": [
                "Shivalika Singh",
                "Yiyang Nan",
                "Alex Wang",
                "Daniel D'Souza",
                "Sayash Kapoor",
                "Ahmet Üstün",
                "Sanmi Koyejo",
                "Yuntian Deng",
                "Shayne Longpre",
                "Noah Smith",
                "Beyza Ermis",
                "Marzieh Fadaee",
                "Sara Hooker"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Cohere",
                "Cohere Labs",
                "Massachusetts Institute of Technology",
                "Princeton University",
                "Stanford University",
                "University of Washington",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20879.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Неравенство в оценке ИИ: проблемы и решения для Chatbot Arena",
                    "desc": "Статья анализирует проблемы в методологии оценки чат-ботов на платформе Chatbot Arena. Авторы выявили, что некоторые компании имеют преимущество из-за возможности приватного тестирования и выборочного раскрытия результатов. Исследование показало значительную асимметрию в доступе к данным между закрытыми и открытыми моделями. Авторы предлагают рекомендации по реформированию системы оценки для обеспечения более справедливого и прозрачного бенчмаркинга в области машинного обучения."
                },
                "en": {
                    "title": "Towards Fairer AI Benchmarking: Reforming the Chatbot Arena",
                    "desc": "This paper discusses the challenges in measuring progress in AI through benchmarks, specifically focusing on the Chatbot Arena leaderboard. It highlights how undisclosed private testing practices create an uneven playing field, favoring certain providers who can selectively disclose their best scores. The authors reveal that proprietary models receive more testing opportunities and data access compared to open-weight models, leading to biased performance evaluations. They propose reforms to enhance transparency and fairness in the benchmarking process, ensuring that all AI systems are evaluated on a level playing field."
                },
                "zh": {
                    "title": "推动公平透明的AI基准测试",
                    "desc": "本文探讨了在人工智能领域中，基准测试的重要性及其潜在的扭曲问题。我们发现，某些提供者通过私下测试和选择性披露成绩，导致了Chatbot Arena的评分偏差。特别是，Meta在Llama-4发布前测试了27个私有LLM变体，这使得其评分更具优势。我们建议对Chatbot Arena的评估框架进行改革，以促进更公平和透明的基准测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20157",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "url": "https://huggingface.co/papers/2504.20157",
            "abstract": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.",
            "score": 28,
            "issue_id": 3504,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "18f16590c380c078",
            "authors": [
                "Zae Myung Kim",
                "Chanwoo Park",
                "Vipul Raheja",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Grammarly",
                "MIT",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20157.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rl",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное обучение с подкреплением для больших языковых моделей",
                    "desc": "Статья представляет новый метод под названием Meta Policy Optimization (MPO) для обучения с подкреплением больших языковых моделей. MPO использует мета-модель вознаграждения, которая динамически корректирует промпт модели вознаграждения в процессе обучения. Это позволяет решить проблемы эксплуатации сигнала вознаграждения и зависимости от ручной разработки промптов. Метод показывает результаты на уровне или лучше моделей с тщательно разработанными вручную промптами вознаграждения."
                },
                "en": {
                    "title": "Dynamic Reward Adjustment for Robust LLM Alignment",
                    "desc": "This paper presents Meta Policy Optimization (MPO), a new framework designed to improve reward-based alignment methods for large language models (LLMs). MPO tackles two main issues: the risk of reward hacking and the need for complex prompt engineering. By using a meta-reward model, MPO dynamically adjusts the reward prompts during training, ensuring that the reward signal remains effective and less exploitable. The results show that MPO achieves comparable or superior performance to traditional methods while simplifying the alignment process across various tasks."
                },
                "zh": {
                    "title": "元策略优化：提升大型语言模型的对齐能力",
                    "desc": "本文介绍了一种名为元策略优化（MPO）的新框架，旨在解决大型语言模型（LLMs）在基于奖励的对齐方法中面临的两个主要问题：奖励黑客和脆弱的提示工程。MPO通过引入一个动态调整奖励模型提示的元奖励模型，来提高对齐的稳定性和适应性。该方法能够在训练过程中监控环境变化，持续优化奖励信号，从而减少手动设计奖励提示的需求。实验结果表明，MPO在多种任务上表现优异，且不需要专门的奖励设计，具有更强的适应性和鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20995",
            "title": "TesserAct: Learning 4D Embodied World Models",
            "url": "https://huggingface.co/papers/2504.20995",
            "abstract": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.",
            "score": 11,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "6339aba982c02561",
            "authors": [
                "Haoyu Zhen",
                "Qiao Sun",
                "Hongxin Zhang",
                "Junyan Li",
                "Siyuan Zhou",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20995.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "4D-модели мира: новый уровень предсказания динамики воплощенных агентов",
                    "desc": "В статье представлен эффективный подход к обучению новых 4D-моделей воплощенного мира, которые предсказывают динамическую эволюцию 3D-сцен во времени в ответ на действия воплощенного агента. Авторы предлагают обучать 4D-модель мира на видео RGB-DN (RGB, глубина и нормали), что превосходит традиционные 2D-модели. Метод включает дополнение существующих наборов данных видео роботизированных манипуляций информацией о глубине и нормалях, а также тонкую настройку модели генерации видео. Предложенный алгоритм позволяет напрямую преобразовывать сгенерированные RGB-DN видео в высококачественную 4D-сцену мира."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Prediction for Embodied Agents",
                    "desc": "This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments."
                },
                "zh": {
                    "title": "学习四维世界模型，提升具身智能的预测能力",
                    "desc": "本文提出了一种有效的方法，用于学习新颖的四维具身世界模型，该模型能够预测三维场景在具身智能体动作下的动态演变，确保时空一致性。我们通过训练RGB-DN（RGB、深度和法线）视频来学习四维世界模型，这种方法超越了传统的二维模型，能够将详细的形状、配置和时间变化纳入预测中。具体而言，我们首先利用现成模型扩展现有的机器人操作视频数据集，加入深度和法线信息。接着，我们在这个标注数据集上微调视频生成模型，联合预测每一帧的RGB-DN，最终将生成的RGB、深度和法线视频直接转换为高质量的四维场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20998",
            "title": "YoChameleon: Personalized Vision and Language Generation",
            "url": "https://huggingface.co/papers/2504.20998",
            "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting.",
            "score": 9,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "21fed074912b3e2f",
            "authors": [
                "Thao Nguyen",
                "Krishna Kumar Singh",
                "Jing Shi",
                "Trung Bui",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🦎",
                "ru": {
                    "title": "Персонализация мультимодальных ИИ-моделей на основе нескольких примеров",
                    "desc": "Статья представляет Yo'Chameleon - первую попытку персонализации больших мультимодальных моделей. Система использует метод soft-prompt tuning для встраивания информации о конкретном объекте на основе 3-5 изображений. Yo'Chameleon обучается отвечать на вопросы об объекте и воссоздавать его детали на новых изображениях. В процессе обучения применяются механизм самопромптинга и подход 'soft-positive' для улучшения качества генерации изображений."
                },
                "en": {
                    "title": "Personalizing Multimodal Models with Yo'Chameleon",
                    "desc": "This paper presents Yo'Chameleon, a novel approach to personalize large multimodal models like GPT-4 for specific user concepts. By using 3-5 images of a subject, Yo'Chameleon applies soft-prompt tuning to incorporate personalized information, enabling the model to answer questions and generate contextually relevant images. The training involves a self-prompting optimization mechanism that ensures balanced performance across different modalities, as well as a 'soft-positive' image generation technique to improve image quality in few-shot scenarios. This work addresses the gap in adapting personalization methods for image generation within multimodal frameworks."
                },
                "zh": {
                    "title": "个性化多模态模型的创新探索",
                    "desc": "大型多模态模型（如GPT-4、Gemini、Chameleon）已经发展成为强大的工具，拥有数百万用户。然而，这些模型仍然是通用的，缺乏对特定用户概念的个性化知识。本文介绍了Yo'Chameleon，这是首次研究大型多模态模型个性化的方法。Yo'Chameleon通过软提示调优，将特定主题的信息嵌入模型，以回答关于该主题的问题并在新环境中重建图像的像素级细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20630",
            "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
            "url": "https://huggingface.co/papers/2504.20630",
            "abstract": "Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.",
            "score": 9,
            "issue_id": 3510,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "e4f8da880ab13ca8",
            "authors": [
                "Yu Zhang",
                "Wenxiang Guo",
                "Changhao Pan",
                "Zhiyuan Zhu",
                "Tao Jin",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Zhejiang Univeristy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20630.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#story_generation",
                    "#dataset"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Погружение в виртуальную драму: мультимодальная генерация пространственного аудио",
                    "desc": "Статья представляет новый подход к генерации иммерсивной пространственной драмы на основе мультимодальных подсказок. Авторы создали первый датасет MRSDrama, содержащий бинауральные аудио драмы, сценарии, видео и другие модальности. Они разработали модель ISDrama, включающую мультимодальный кодировщик поз и иммерсивный драма-трансформер на основе Mamba. Экспериментальные результаты показывают превосходство ISDrama над базовыми моделями по объективным и субъективным метрикам."
                },
                "en": {
                    "title": "Revolutionizing Drama Generation with Multimodal AI",
                    "desc": "This paper introduces a novel approach to generating immersive spatial drama using machine learning techniques. It presents MRSDrama, a unique dataset that includes various multimodal inputs such as binaural audio, scripts, and videos, which are essential for training the model. The proposed ISDrama model utilizes a Multimodal Pose Encoder and an Immersive Drama Transformer to effectively capture spatial dynamics and dramatic prosody. Experimental results demonstrate that ISDrama significantly improves the quality of generated drama compared to existing models, showcasing its potential for applications in augmented and virtual reality."
                },
                "zh": {
                    "title": "多模态沉浸式戏剧生成的创新之路",
                    "desc": "这篇论文关注于多模态沉浸式空间戏剧生成，旨在基于多模态提示创建连续的多语者双耳语音，具有戏剧性的语调。该任务需要同时建模空间信息和戏剧性语调，但数据收集成本较高。我们构建了MRSDrama，这是第一个多模态录制的空间戏剧数据集，包含双耳戏剧音频、剧本、视频、几何姿态和文本提示。我们提出的ISDrama模型通过多模态提示生成沉浸式空间戏剧，采用了多模态姿态编码器和沉浸式戏剧变换器，实验结果表明ISDrama在客观和主观指标上均优于基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16046",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "url": "https://huggingface.co/papers/2504.16046",
            "abstract": "The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of \"copyright takedown\" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.",
            "score": 9,
            "issue_id": 3504,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "3a2630d279485a85",
            "authors": [
                "Jingyu Zhang",
                "Jiacan Yu",
                "Marc Marone",
                "Benjamin Van Durme",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16046.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#ethics",
                    "#leakage"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "BloomScrub: защита языковых моделей от нарушения авторских прав",
                    "desc": "Статья представляет метод BloomScrub для снижения риска нарушения авторских прав крупными языковыми моделями. Этот подход использует обнаружение цитат и техники переписывания текста во время вывода модели, чтобы трансформировать потенциально нарушающие авторские права сегменты. BloomScrub применяет эффективные структуры данных (фильтры Блума) для масштабируемой проверки авторских прав. Метод показывает высокую эффективность в снижении риска нарушений при сохранении полезности модели."
                },
                "en": {
                    "title": "BloomScrub: Smart Copyright Protection for Language Models",
                    "desc": "This paper addresses the issue of copyright infringement by large language models (LLMs) that may unintentionally generate content similar to copyrighted material. It introduces BloomScrub, a novel method that detects and rewrites long quotes from copyrighted sources during the model's inference phase. By using Bloom filters for efficient quote detection, BloomScrub can effectively screen large datasets while maintaining the model's utility. The approach not only reduces the risk of copyright infringement but also allows for flexible enforcement levels through adaptive abstention when necessary."
                },
                "zh": {
                    "title": "BloomScrub：有效的版权保护方法",
                    "desc": "本文讨论了大型语言模型在预训练过程中接触到版权材料所带来的版权侵权风险。为了解决这一问题，研究者们开发了\"版权撤销\"方法，旨在防止模型生成与版权内容相似的文本。我们提出了一种名为BloomScrub的方法，通过在推理时检测引用并进行重写，来有效地减少版权侵权风险。实验结果表明，BloomScrub在降低侵权风险的同时，保持了模型的实用性，并能够适应不同的执行严格性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20996",
            "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
            "url": "https://huggingface.co/papers/2504.20996",
            "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
            "score": 7,
            "issue_id": 3507,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "2427da9ba7d7d3f4",
            "authors": [
                "Sicheng Mo",
                "Thao Nguyen",
                "Xun Huang",
                "Siddharth Srinivasan Iyer",
                "Yijun Li",
                "Yuchen Liu",
                "Abhishek Tandon",
                "Eli Shechtman",
                "Krishna Kumar Singh",
                "Yong Jae Lee",
                "Bolei Zhou",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of California, Los Angeles",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20996.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#small_models",
                    "#agi",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "X-Fusion: Мультимодальное расширение языковых моделей без ущерба для языковых способностей",
                    "desc": "X-Fusion - это фреймворк, расширяющий возможности предобученных больших языковых моделей (LLM) для мультимодальных задач. Он использует архитектуру с двумя башнями и модальностно-специфичными весами, сохраняя параметры LLM неизменными. X-Fusion превосходит альтернативные архитектуры в задачах преобразования изображения в текст и текста в изображение. Исследования показали, что включение данных для понимания улучшает качество генерации, а уменьшение шума в данных изображений повышает общую производительность."
                },
                "en": {
                    "title": "X-Fusion: Uniting Language and Vision for Superior Multimodal Performance",
                    "desc": "The paper introduces X-Fusion, a new framework that enhances pretrained Large Language Models (LLMs) for tasks involving both text and images. It uses a dual-tower architecture that allows the model to maintain its language processing abilities while integrating visual information. The results show that X-Fusion outperforms other models in tasks that convert images to text and vice versa. Additionally, the study highlights the importance of using understanding-focused data and feature alignment to improve model performance and training efficiency."
                },
                "zh": {
                    "title": "X-Fusion：高效的多模态模型框架",
                    "desc": "我们提出了X-Fusion框架，旨在扩展预训练的大型语言模型（LLMs）以处理多模态任务，同时保持其语言能力。X-Fusion采用双塔设计，使用特定于模态的权重，保持LLM的参数不变，同时整合视觉特定信息以进行理解和生成。实验结果表明，X-Fusion在图像到文本和文本到图像任务上始终优于其他架构。我们的研究发现，结合以理解为重点的数据可以提高生成质量，减少图像数据噪声可以增强整体性能，而特征对齐加速了小模型的收敛，但对大模型的影响较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20073",
            "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.20073",
            "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.",
            "score": 7,
            "issue_id": 3518,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "80fac0e4c856acbd",
            "authors": [
                "Zihan Wang",
                "Kangrui Wang",
                "Qineng Wang",
                "Pingyue Zhang",
                "Linjie Li",
                "Zhengyuan Yang",
                "Kefan Yu",
                "Minh Nhat Nguyen",
                "Licheng Liu",
                "Eli Gottlieb",
                "Monica Lam",
                "Yiping Lu",
                "Kyunghyun Cho",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Lijuan Wang",
                "Yejin Choi",
                "Manling Li"
            ],
            "affiliations": [
                "Imperial College London",
                "Microsoft",
                "New York University",
                "Northwestern University",
                "Singapore Management University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20073.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#hallucinations",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "StarPO: новый подход к обучению мыслящих языковых агентов",
                    "desc": "Эта статья представляет StarPO - новую систему для обучения с подкреплением языковых моделей-агентов. Авторы разработали RAGEN - модульную систему для тренировки и оценки LLM-агентов. Исследование выявило проблему 'Echo Trap' при обучении агентов и предложило способы ее решения. Также обнаружено, что без детальных сигналов награды, учитывающих рассуждения, у агентов сложно развить глубокие стратегии мышления."
                },
                "en": {
                    "title": "Empowering LLMs with StarPO for Enhanced Interactive Learning",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) as interactive agents using reinforcement learning (RL). It introduces StarPO, a framework designed for trajectory-level RL that enhances decision-making over multiple interactions. The authors also present RAGEN, a modular system for training and evaluating these agents, revealing key insights about reward structures and the importance of diverse initial states. Their findings suggest that without detailed reward signals, LLMs struggle to develop deep reasoning capabilities, often resorting to simplistic strategies."
                },
                "zh": {
                    "title": "提升代理推理能力的强化学习新框架",
                    "desc": "本文探讨了训练大型语言模型（LLMs）作为交互代理的挑战，特别是在长时间决策和随机环境反馈的交互中。我们提出了StarPO（状态-思考-行动-奖励策略优化），这是一个用于轨迹级代理强化学习的通用框架，并引入了RAGEN，一个用于训练和评估LLM代理的模块化系统。研究发现，代理强化学习训练中存在回声陷阱模式，奖励方差和梯度波动显著，我们通过StarPO-S进行稳定化处理。最后，我们指出缺乏细致的、基于推理的奖励信号会导致代理推理能力不足，表现出浅层策略或幻觉思维。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20690",
            "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.20690",
            "abstract": "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.",
            "score": 5,
            "issue_id": 3515,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "857739e05043be6d",
            "authors": [
                "Zechuan Zhang",
                "Ji Xie",
                "Yu Lu",
                "Zongxin Yang",
                "Yi Yang"
            ],
            "affiliations": [
                "DBMI, HMS, Harvard University",
                "ReLER, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20690.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективное редактирование изображений с помощью ИИ: точность без компромиссов",
                    "desc": "Эта статья представляет новый подход к редактированию изображений на основе текстовых инструкций. Авторы предлагают использовать Диффузионный Трансформер (DiT) для улучшения качества генерации и понимания контекста. Метод включает в себя фреймворк редактирования в контексте, гибридную стратегию обучения LoRA-MoE и масштабирование начального шума с помощью VLM. Результаты показывают превосходство предложенного метода над существующими подходами при значительно меньших требованиях к данным и вычислительным ресурсам."
                },
                "en": {
                    "title": "Efficient and Precise Instruction-Based Image Editing",
                    "desc": "This paper presents a novel approach to instruction-based image editing that balances precision and efficiency. It introduces a framework that allows for zero-shot editing compliance using in-context prompting, which avoids the need for structural changes in images. The authors also propose a hybrid tuning strategy that combines LoRA and MoE techniques to enhance flexibility and reduce the need for extensive retraining. Finally, they implement an inference-time scaling method that improves edit quality by selecting better initial noise using vision-language models, demonstrating superior performance with minimal training data and parameters."
                },
                "zh": {
                    "title": "高效精准的指令引导图像编辑新范式",
                    "desc": "本论文提出了一种新的图像编辑方法，利用大型扩散变换器（DiT）来提高图像编辑的精度和效率。我们引入了一个上下文编辑框架，能够在零-shot情况下遵循指令，同时避免结构性变化。通过结合LoRA-MoE混合调优策略，我们实现了灵活的适应性和动态专家路由，而无需大量重新训练。此外，我们还提出了一种早期过滤推理时间缩放方法，利用视觉-语言模型（VLMs）选择更好的初始噪声，从而提高编辑质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18087",
            "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
            "url": "https://huggingface.co/papers/2504.18087",
            "abstract": "Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.",
            "score": 4,
            "issue_id": 3509,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 апреля",
                "en": "April 25",
                "zh": "4月25日"
            },
            "hash": "c6ed690774bd93ba",
            "authors": [
                "Weipeng Tan",
                "Chuming Lin",
                "Chengming Xu",
                "FeiFan Xu",
                "Xiaobin Hu",
                "Xiaozhong Ji",
                "Junwei Zhu",
                "Chengjie Wang",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University",
                "Youtu Lab, Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18087.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#emotion",
                    "#video",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DICE-Talk: реалистичные эмоции в генерации говорящих голов",
                    "desc": "Статья представляет DICE-Talk - новый подход к генерации эмоциональных говорящих голов. Авторы решают проблемы недостаточного использования эмоциональных сигналов из аудио, утечки идентичности в представлениях эмоций и изолированного обучения корреляций эмоций. DICE-Talk использует разделение идентичности и эмоций, а также объединение схожих эмоций для более точной генерации выражений лица. Эксперименты показывают превосходство метода над существующими подходами в точности передачи эмоций при сохранении качества синхронизации губ."
                },
                "en": {
                    "title": "DICE-Talk: Emotionally Expressive Talking Heads with Identity Preservation",
                    "desc": "This paper presents DICE-Talk, a new framework for Talking Head Generation (THG) that enhances emotional expressiveness while maintaining speaker identity. It addresses key issues in current methods, such as the underutilization of audio emotional cues and identity leakage in emotion representations. The framework employs a disentangled emotion embedder to model emotional cues and a correlation-enhanced emotion conditioning module to capture relationships between emotions. Experimental results show that DICE-Talk outperforms existing methods in emotion accuracy and preserves identity in generated portraits."
                },
                "zh": {
                    "title": "解耦身份与情感，生成丰富的说话头像",
                    "desc": "本文提出了一种新的框架DICE-Talk，用于生成情感丰富的说话头像，同时保持说话者的身份。我们识别了当前情感生成方法的三个主要限制，包括对音频情感线索的利用不足、情感表示中的身份泄漏以及情感关联的孤立学习。DICE-Talk通过解耦身份与情感，并结合相似特征的情感来解决这些问题。实验结果表明，我们的方法在情感准确性上优于现有的最先进方法，同时保持了竞争力的口型同步性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16272",
            "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
            "url": "https://huggingface.co/papers/2504.16272",
            "abstract": "Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward.",
            "score": 4,
            "issue_id": 3519,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "4e342702553fd2c1",
            "authors": [
                "Ryan Koo",
                "Ian Yang",
                "Vipul Raheja",
                "Mingyi Hong",
                "Kwang-Sung Jun",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Grammarly",
                "University of Arizona",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16272.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точное распределение вознаграждений для улучшения обучения языковых моделей",
                    "desc": "Статья предлагает новый подход к обучению с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Авторы разработали функцию формирования вознаграждения, использующую методы объяснимого ИИ для оценки вознаграждений на уровне отдельных токенов. Предложенный метод применяет двухуровневую оптимизацию, сочетающую байесовскую оптимизацию и обучение политики. Эксперименты показывают, что данный подход улучшает производительность модели и ускоряет процесс обучения оптимальной политики."
                },
                "en": {
                    "title": "Enhancing Token-Level Rewards for Better Language Model Training",
                    "desc": "This paper addresses the limitations of current reinforcement learning from human feedback (RLHF) methods in training large language models (LLMs). It highlights the issue of sparse feedback when using scalar rewards based on the final token, which hampers effective token-level credit assignment. The authors propose a novel reward-shaping function that utilizes explainability techniques like SHAP and LIME to provide more granular, per-token rewards. By employing a bilevel optimization framework that combines Bayesian Optimization with policy training, the approach improves performance on downstream tasks and accelerates the discovery of optimal policies during training."
                },
                "zh": {
                    "title": "优化标记级奖励，提升模型性能",
                    "desc": "当前的基于人类反馈的强化学习（RLHF）方法在大型语言模型（LLM）对齐中，通常将标量奖励分配给序列，使用最后一个标记作为整个序列质量的替代指标。这种方法导致反馈稀疏和标记级别的信用分配不理想。本文将奖励塑造视为一个优化问题，专注于标记级别的信用分配。我们提出了一种利用可解释性方法（如SHAP和LIME）来估计每个标记奖励的奖励塑造函数，并通过双层优化框架来学习该函数的参数，从而在训练过程中更快地找到最佳策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20114",
            "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
            "url": "https://huggingface.co/papers/2504.20114",
            "abstract": "Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional \"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined \"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\\%-0.4\\% of the model parameter size and reducing the query latency by approximately 99\\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop.",
            "score": 3,
            "issue_id": 3511,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "56755beaa151585a",
            "authors": [
                "Zhonghao Li",
                "Kunpeng Zhang",
                "Jinghuai Ou",
                "Shuliang Liu",
                "Xuming Hu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20114.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "TreeHop: Эффективный многоэтапный поиск без языковых моделей",
                    "desc": "TreeHop - это новый подход к многоэтапному вопросно-ответному поиску в системах генерации с дополнением извлечением (RAG). Он использует обновление векторных представлений запросов на уровне эмбеддингов, избегая необходимости в языковых моделях для переформулировки запросов. TreeHop значительно снижает вычислительные затраты, заменяя традиционный цикл 'Извлечение-Переписывание-Векторизация-Извлечение' на оптимизированный цикл 'Извлечение-Эмбеддинг-Извлечение'. Экспериментальные результаты показывают, что TreeHop сопоставим по эффективности с передовыми методами RAG, при этом используя всего 5%-0.4% от размера параметров модели и сокращая задержку запросов примерно на 99%."
                },
                "en": {
                    "title": "Streamlining Multi-Hop Question Answering with TreeHop",
                    "desc": "The paper introduces TreeHop, a new framework designed to improve multi-hop question answering (MHQA) in retrieval-augmented generation (RAG) systems. Unlike traditional methods that rely on large language models (LLMs) for query rewriting, TreeHop operates at the embedding level, dynamically updating query embeddings by integrating information from previous queries and retrieved documents. This approach simplifies the retrieval process by replacing the complex cycle of retrieving, rewriting, and vectorizing with a more efficient 'Retrieve-Embed-Retrieve' loop. Experimental results demonstrate that TreeHop achieves competitive performance with significantly lower computational costs and faster query processing times, making it a practical solution for knowledge-intensive applications."
                },
                "zh": {
                    "title": "TreeHop：高效的多跳问答解决方案",
                    "desc": "本论文提出了一种名为TreeHop的框架，旨在解决多跳问答中的高计算成本问题。与传统的基于大语言模型（LLM）的查询重写方法不同，TreeHop通过融合先前查询和检索文档的语义信息，动态更新查询嵌入。该方法简化了检索过程，采用了“检索-嵌入-检索”的循环，显著降低了计算开销。实验结果表明，TreeHop在多个开放领域的多跳问答数据集上表现出色，且模型参数量仅为传统方法的5%-0.4%，查询延迟减少约99%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18942",
            "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes",
            "url": "https://huggingface.co/papers/2504.18942",
            "abstract": "Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).",
            "score": 3,
            "issue_id": 3520,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 апреля",
                "en": "April 26",
                "zh": "4月26日"
            },
            "hash": "045a82785fe0f501",
            "authors": [
                "Debarati Das",
                "Khanh Chi Le",
                "Ritik Sachin Parkar",
                "Karin De Langis",
                "Brendan Madson",
                "Chad M. Berryman",
                "Robin M. Willis",
                "Daniel H. Moses",
                "Brett McDonnell",
                "Daniel Schwarcz",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Computer Science and Engineering, University of Minnesota",
                "Law School, University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18942.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#data",
                    "#alignment",
                    "#reasoning"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "LawFlow: На пути к более эффективному ИИ в юриспруденции",
                    "desc": "Эта статья представляет LawFlow - набор данных, отражающий полные юридические рабочие процессы, собранные у студентов-юристов в реальных сценариях формирования бизнес-структур. В отличие от существующих наборов данных, LawFlow охватывает динамичные, модульные и итеративные процессы рассуждений, характерные для юридической практики. Сравнение рабочих процессов, созданных людьми и языковыми моделями (ЯМ), выявило систематические различия в структуре, гибкости рассуждений и выполнении планов. На основе результатов авторы предлагают рекомендации по разработке систем искусственного интеллекта (ИИ) для поддержки юридической деятельности, согласованных с человеческими целями ясности, полноты, креативности и эффективности."
                },
                "en": {
                    "title": "Empowering Legal Practice with Adaptive AI Workflows",
                    "desc": "This paper introduces LawFlow, a new dataset designed to capture complete legal workflows as performed by trained law students in real-world scenarios. Unlike existing datasets that focus on isolated tasks, LawFlow emphasizes the dynamic and iterative nature of legal reasoning, reflecting the complexities of actual legal practice. The study compares human-generated workflows with those produced by large language models (LLMs), revealing that human workflows are more modular and adaptable, while LLMs tend to follow a more linear and exhaustive approach. The authors suggest that AI should assist legal professionals in supportive roles rather than attempting to execute complex workflows independently, and they provide design recommendations to enhance AI's alignment with human legal reasoning."
                },
                "zh": {
                    "title": "提升法律AI的适应性与协作性",
                    "desc": "本论文介绍了LawFlow数据集，旨在填补现有法律AI模型在复杂法律工作流中的不足。该数据集收集了经过培训的法学生在真实商业实体成立场景中的完整法律工作流，强调了动态、模块化和迭代推理过程。研究发现，人类的工作流更具适应性和模块化，而大型语言模型（LLM）的工作流则更为线性和全面。基于这些发现，论文提出了一系列设计建议，以提高AI在法律实践中的支持能力，强调清晰性、完整性、创造性和效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18738",
            "title": "A Review of 3D Object Detection with Vision-Language Models",
            "url": "https://huggingface.co/papers/2504.18738",
            "abstract": "This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI",
            "score": 2,
            "issue_id": 3515,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 апреля",
                "en": "April 25",
                "zh": "4月25日"
            },
            "hash": "991de2200cc55a55",
            "authors": [
                "Ranjan Sapkota",
                "Konstantinos I Roumeliotis",
                "Rahul Harsha Cheppally",
                "Marco Flores Calero",
                "Manoj Karkee"
            ],
            "affiliations": [
                "Cornell University, USA",
                "Kansas State University, USA",
                "Universidad de las Fuerzas Armadas, Ecuador",
                "University of Peloponnese, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18738.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#survey",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новые горизонты 3D-зрения: VLM на страже объектов",
                    "desc": "Эта статья представляет собой систематический обзор 3D-обнаружения объектов с использованием моделей зрения-языка (VLM). Авторы анализируют более 100 исследовательских работ, рассматривая уникальные проблемы этой области, такие как пространственное мышление и сложность данных. Сравниваются традиционные подходы с использованием облаков точек и воксельных сеток с современными фреймворками, такими как CLIP и 3D LLM, которые позволяют выполнять обнаружение с открытым словарем и обобщение без примеров. Обсуждаются ключевые архитектуры, стратегии предварительного обучения и методы инженерии промптов для эффективного 3D-обнаружения объектов с помощью VLM."
                },
                "en": {
                    "title": "Advancing 3D Object Detection with Vision-Language Models",
                    "desc": "This paper reviews the field of 3D object detection using vision-language models (VLMs), which combine visual and textual information. It analyzes over 100 research papers to identify the unique challenges faced in 3D detection compared to 2D detection, particularly in spatial reasoning and data complexity. The authors compare traditional methods, like point clouds, with modern VLM approaches that allow for open-vocabulary detection and zero-shot generalization. They also discuss key architectures, pretraining strategies, and the importance of prompt engineering in aligning textual and 3D features for improved detection performance."
                },
                "zh": {
                    "title": "推动3D物体检测的视觉-语言模型研究",
                    "desc": "这篇综述文章系统分析了使用视觉-语言模型（VLMs）进行3D物体检测的研究进展。通过审查超过100篇研究论文，文章首次提供了专门针对3D物体检测的系统分析，强调了与2D检测在空间推理和数据复杂性方面的不同挑战。文章比较了传统的点云和体素网格方法与现代的视觉-语言框架，如CLIP和3D LLMs，这些框架支持开放词汇检测和零样本泛化。最后，文章讨论了当前的挑战和未来的研究方向，以推动3D物体检测技术的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17838",
            "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
            "url": "https://huggingface.co/papers/2504.17838",
            "abstract": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.",
            "score": 1,
            "issue_id": 3519,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "2a2f148d75eae81c",
            "authors": [
                "Bernhard Jaeger",
                "Daniel Dauner",
                "Jens Beißwenger",
                "Simon Gerstenecker",
                "Kashyap Chitta",
                "Andreas Geiger"
            ],
            "affiliations": [
                "University of Tübingen, Tübingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17838.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Простое вознаграждение - ключ к эффективному обучению автономного вождения",
                    "desc": "Исследование посвящено применению обучения с подкреплением (RL) для планирования движения автономных транспортных средств. Авторы предлагают новый подход к разработке функции вознаграждения, основанный на оптимизации единственного интуитивно понятного параметра - завершения маршрута. Использование этого метода позволяет эффективно масштабировать алгоритм PPO с увеличением размера мини-батча, что обеспечивает более эффективное распределенное обучение. Результаты экспериментов на бенчмарках CARLA и nuPlan показывают превосходство предложенного подхода над существующими методами RL с более сложными функциями вознаграждения."
                },
                "en": {
                    "title": "Simplifying Rewards for Scalable Reinforcement Learning in Autonomous Driving",
                    "desc": "This paper explores the use of reinforcement learning (RL) for planning in autonomous driving, highlighting its advantages over traditional rule-based methods. The authors identify that existing RL techniques often struggle with complex reward structures, particularly when using larger mini-batch sizes. They propose a simplified reward system focused on route completion, which enhances the scalability and performance of the Proximal Policy Optimization (PPO) algorithm. Their approach demonstrates significant improvements in training efficiency and model performance across large datasets, outperforming other methods with more complicated reward designs."
                },
                "zh": {
                    "title": "强化学习助力自动驾驶的高效规划",
                    "desc": "本文研究了在自动驾驶中使用强化学习（RL）进行特权规划的问题。现有的基于规则的方法在处理长尾问题时效果不佳，而RL方法具有可扩展性，并且不会像模仿学习那样出现累积错误。我们提出了一种新的奖励设计，主要优化单一的直观奖励项：路线完成度，这样在增加小批量大小时，PPO的优化效果得到了提升。通过在CARLA和nuPlan上进行大规模训练，我们的模型在多个基准测试中表现优异，显示了RL在自动驾驶中的潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-29.html",
    "link_next": "2025-05-01.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4月29日"
    },
    "short_date_next": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5月1日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 3,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#emotion": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的检索增强生成（RAG）框架，称为UniversalRAG。它能从不同模态和粒度的知识源中检索和整合信息。现有的RAG方法主要局限于单一模态的语料库，而UniversalRAG通过模态感知路由机制动态选择最合适的模态特定语料库进行检索。此外，它还根据查询的复杂性和范围，对每种模态进行多级粒度的组织。研究在8个多模态基准上验证了UniversalRAG的优越性。",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "pinyin": "这篇文章介绍了一种新的检索增强生成（RAG）框架，称为UniversalRAG。它能从不同模态和粒度的知识源中检索和整合信息。现有的RAG方法主要局限于单一模态的语料库，而UniversalRAG通过模态感知路由机制动态选择最合适的模态特定语料库进行检索。此外，它还根据查询的复杂性和范围，对每种模态进行多级粒度的组织。研究在8个多模态基准上验证了UniversalRAG的优越性。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de jiǎn suǒ zēng qiáng shēng chéng (RAG) kuàng jià, chēng wéi UniversalRAG. tā néng cóng bù tóng mó tài hé lì dù de zhī shi yuán zhōng jiǎn suǒ hé zhěng hé xìn xī. xiàn yǒu de RAG fāng fǎ zhǔ yào jú xiàn yī dàn yī mó tài de yǔ liào kù, ér UniversalRAG tōng guò mó tài gǎn jué lù yóu jī zhì dòng tài xuǎn zé zuì hé shì de mó tài tè dìng yǔ liào kù jìn xíng jiǎn suǒ. cǐ wài, tā hái gēn jù chá xún de fú zà xìng hé fàn wéi, duì měi zhǒng mó tài jìn xíng duō jí lì dù de zǔ zhī. yán jiū zài 8 gè duō mó tài bǐ zhǔn shàng yàn zhèng le UniversalRAG de yōu yuè xìng.",
        "vocab": "[\n    {\"word\": \"检索\", \"pinyin\": \"jiǎnsuǒ\", \"trans\": \"retrieval\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēngqiáng\", \"trans\": \"enhancement\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generation\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"粒度\", \"pinyin\": \"lì dù\", \"trans\": \"granularity\"},\n    {\"word\": \"知识源\", \"pinyin\": \"zhīshi yuán\", \"trans\": \"knowledge source\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integration\"},\n    {\"word\": \"局限于\", \"pinyin\": \"jú xiàn yú\", \"trans\": \"limited to\"},\n    {\"word\": \"单一\", \"pinyin\": \"dān yī\", \"trans\": \"single\"},\n    {\"word\": \"语料库\", \"pinyin\": \"yǔ liào kù\", \"trans\": \"corpus\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎnzhī\", \"trans\": \"perception\"},\n    {\"word\": \"路由\", \"pinyin\": \"lù yóu\", \"trans\": \"routing\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamic\"},\n    {\"word\": \"选择\", \"pinyin\": \"xuǎnzé\", \"trans\": \"selection\"},\n    {\"word\": \"合适\", \"pinyin\": \"héshì\", \"trans\": \"appropriate\"},\n    {\"word\": \"特定\", \"pinyin\": \"tèdìng\", \"trans\": \"specific\"},\n    {\"word\": \"查询\", \"pinyin\": \"cháxún\", \"trans\": \"query\"},\n    {\"word\": \"复杂性\", \"pinyin\": \"fùzáxìng\", \"trans\": \"complexity\"},\n    {\"word\": \"范围\", \"pinyin\": \"fànwéi\", \"trans\": \"scope\"},\n    {\"word\": \"多级\", \"pinyin\": \"duōjí\", \"trans\": \"multi-level\"},\n    {\"word\": \"组织\", \"pinyin\": \"zǔzhī\", \"trans\": \"organization\"},\n    {\"word\": \"研究\", \"pinyin\": \"yánjiū\", \"trans\": \"research\"},\n    {\"word\": \"验证\", \"pinyin\": \"yànzhèng\", \"trans\": \"validation\"},\n    {\"word\": \"优越性\", \"pinyin\": \"yōuyuèxìng\", \"trans\": \"superiority\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"}\n]",
        "trans": "This article introduces a new Retrieval-Augmented Generation (RAG) framework called UniversalRAG. It can retrieve and integrate information from knowledge sources of different modalities and granularities. Existing RAG methods are mainly limited to single-modality corpora, while UniversalRAG dynamically selects the most suitable modality-specific corpus for retrieval through a modality-aware routing mechanism. Additionally, it organizes each modality at multiple granularity levels based on the complexity and scope of the query. The superiority of UniversalRAG has been validated on 8 multimodal benchmarks.",
        "update_ts": "2025-04-30 09:12"
    }
}