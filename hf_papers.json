{
    "date": {
        "ru": "7 апреля",
        "en": "April 7",
        "zh": "4月7日"
    },
    "time_utc": "2025-04-07 04:13",
    "weekday": 0,
    "issue_id": 3095,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02605",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "url": "https://huggingface.co/papers/2504.02605",
            "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.",
            "score": 6,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "bcf7d7c20685c914",
            "authors": [
                "Daoguang Zan",
                "Zhirong Huang",
                "Wei Liu",
                "Hanwu Chen",
                "Linhao Zhang",
                "Shulin Xin",
                "Lu Chen",
                "Qi Liu",
                "Xiaojian Zhong",
                "Aoyan Li",
                "Siyao Liu",
                "Yongsheng Xiao",
                "Liangqiang Chen",
                "Yuyu Zhang",
                "Jing Su",
                "Tianyu Liu",
                "Rui Long",
                "Kai Shen",
                "Liang Xiang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02605.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычный бенчмарк для оценки ИИ в решении программных задач",
                    "desc": "Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок."
                },
                "en": {
                    "title": "Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages",
                    "desc": "This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "多语言问题解决基准，推动强化学习研究",
                    "desc": "本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03641",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "url": "https://huggingface.co/papers/2504.03641",
            "abstract": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.",
            "score": 1,
            "issue_id": 3095,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "45da77ffd9c21caf",
            "authors": [
                "Wulin Xie",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Yang Shi",
                "Bingyan Nie",
                "Hongkai Chen",
                "Zhang Zhang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "CASIA",
                "M-M-E Project",
                "NJU",
                "PKU",
                "Vivo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый стандарт оценки мультимодальных языковых моделей",
                    "desc": "Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью."
                },
                "en": {
                    "title": "Enhancing Evaluation for Unified Multimodal Language Models",
                    "desc": "This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs."
                },
                "zh": {
                    "title": "全面评估统一多模态大语言模型的必要性",
                    "desc": "现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02949",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.02949",
            "abstract": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "score": 1,
            "issue_id": 3095,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "71423989b2bed2d8",
            "authors": [
                "Xianwei Zhuang",
                "Yuxin Xie",
                "Yufan Deng",
                "Dongchao Yang",
                "Liming Liang",
                "Jinghan Ru",
                "Yuguo Yin",
                "Yuexian Zou"
            ],
            "affiliations": [
                "School of Electronic and Computer Engineering, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#open_source",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, генерации и редактирования изображений",
                    "desc": "VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям."
                },
                "en": {
                    "title": "Unifying Visual Understanding and Generation with VARGPT-v1.1",
                    "desc": "VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation."
                },
                "zh": {
                    "title": "统一视觉自回归模型的突破性进展",
                    "desc": "本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。"
                }
            }
        }
    ],
    "link_prev": "2025-04-04.html",
    "link_next": "2025-04-08.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4月4日"
    },
    "short_date_next": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "大型语言模型（LLMs）的出现推动了人工智能的变革，产生了具有复杂推理、强大感知和多样行动能力的智能代理。这些代理在AI研究和实际应用中的设计、评估和改进面临复杂的挑战。本文综合概述了智能代理的模块化、脑启发式架构，结合认知科学、神经科学和计算研究的原则。文章分为四个部分，探讨智能代理的模块基础、自我增强机制、多代理系统和安全可靠的AI系统。",
        "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
        "pinyin": "Dàxíng yǔyán móxíng (LLMs) de chūxiàn tuīdòngle réngōng zhìnéng de biànge, chǎnshēngle jùyǒu fùzá xīnglǐ, qiángdà gǎnjué hé duōyàng xíngdòng nénglì de zhìnéng dàilǐ. Zhèxiē dàilǐ zài AI yánjiū hé shíjì yìngyòng zhōng de shèjì, pínggū hé gǎijìn miànlín fùzá de tiǎozhàn. Běnwén zònghé gàishùle zhìnéng dàilǐ de mókuàihuà, nǎo qǐfāshì jiàgòu, jiéhé rénzhī kēxué, shénjīng kēxué hé jìsuàn yánjiū de yuánzé. Wénzhāng fēnwéi sì gè bùfēn, tuàntào zhìnéng dàilǐ de mókuài jīchǔ, zìwǒ zēngqiáng jīzhì, duō dàilǐ xìtǒng hé ānquán kěkào de AI xìtǒng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔyán móxíng\", \"trans\": \"language model\"},\n    {\"word\": \"变革\", \"pinyin\": \"biàngé\", \"trans\": \"transformation\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎnzhī\", \"trans\": \"perception\"},\n    {\"word\": \"行动\", \"pinyin\": \"xíngdòng\", \"trans\": \"action\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"代理\", \"pinyin\": \"dàilǐ\", \"trans\": \"agent\"},\n    {\"word\": \"面临\", \"pinyin\": \"miànlín\", \"trans\": \"face\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎozhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"综合\", \"pinyin\": \"zònghé\", \"trans\": \"comprehensive\"},\n    {\"word\": \"概述\", \"pinyin\": \"gàishù\", \"trans\": \"overview\"},\n    {\"word\": \"模块化\", \"pinyin\": \"mókuàihuà\", \"trans\": \"modularization\"},\n    {\"word\": \"脑启发式\", \"pinyin\": \"nǎo qǐfāshì\", \"trans\": \"brain-inspired\"},\n    {\"word\": \"架构\", \"pinyin\": \"jiàgòu\", \"trans\": \"architecture\"},\n    {\"word\": \"认知科学\", \"pinyin\": \"rènzhī kēxué\", \"trans\": \"cognitive science\"},\n    {\"word\": \"神经科学\", \"pinyin\": \"shénjīng kēxué\", \"trans\": \"neuroscience\"},\n    {\"word\": \"计算\", \"pinyin\": \"jìsuàn\", \"trans\": \"computation\"},\n    {\"word\": \"原则\", \"pinyin\": \"yuánzé\", \"trans\": \"principle\"},\n    {\"word\": \"部分\", \"pinyin\": \"bùfen\", \"trans\": \"part\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàntǎo\", \"trans\": \"discuss\"},\n    {\"word\": \"基础\", \"pinyin\": \"jīchǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"自我增强\", \"pinyin\": \"zìwǒ zēngqiáng\", \"trans\": \"self-enhancement\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"多代理\", \"pinyin\": \"duō dàilǐ\", \"trans\": \"multi-agent\"},\n    {\"word\": \"系统\", \"pinyin\": \"xìtǒng\", \"trans\": \"system\"},\n    {\"word\": \"安全\", \"pinyin\": \"ānquán\", \"trans\": \"safe\"},\n    {\"word\": \"可靠\", \"pinyin\": \"kěkào\", \"trans\": \"reliable\"}\n]",
        "trans": "The emergence of large language models (LLMs) has driven a transformation in artificial intelligence, giving rise to intelligent agents with complex reasoning, powerful perception, and diverse action capabilities. The design, evaluation, and improvement of these agents in AI research and practical applications face complex challenges. This article provides a comprehensive overview of the modular, brain-inspired architecture of intelligent agents, integrating principles from cognitive science, neuroscience, and computational research. The article is divided into four sections, discussing the modular foundations of intelligent agents, self-enhancement mechanisms, multi-agent systems, and secure and reliable AI systems.",
        "update_ts": "2025-04-06 12:41"
    }
}