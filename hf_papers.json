{
    "date": {
        "ru": "20 –º–∞—Ä—Ç–∞",
        "en": "March 20",
        "zh": "3Êúà20Êó•"
    },
    "time_utc": "2025-03-20 04:13",
    "weekday": 3,
    "issue_id": 2801,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.15475",
            "title": "Cube: A Roblox View of 3D Intelligence",
            "url": "https://huggingface.co/papers/2503.15475",
            "abstract": "Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.",
            "score": 10,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 –º–∞—Ä—Ç–∞",
                "en": "March 19",
                "zh": "3Êúà19Êó•"
            },
            "hash": "89037dc780448ff8",
            "authors": [
                "Foundation AI Team",
                "Kiran Bhat",
                "Nishchaie Khanna",
                "Karun Channa",
                "Tinghui Zhou",
                "Yiheng Zhu",
                "Xiaoxia Sun",
                "Charles Shang",
                "Anirudh Sudarshan",
                "Maurice Chu",
                "Daiqing Li",
                "Kangle Deng",
                "Jean-Philippe Fauconnier",
                "Tijmen Verhulsdonck",
                "Maneesh Agrawala",
                "Kayvon Fatahalian",
                "Alexander Weiss",
                "Christian Reiser",
                "Ravi Kiran Chirravuri",
                "Ravali Kandur",
                "Alejandro Pelaez",
                "Akash Garg",
                "Michael Palleschi",
                "Jessica Wang",
                "Skylar Litz",
                "Leon Liu",
                "Anying Li",
                "David Harmon",
                "Derek Liu",
                "Liangjun Feng",
                "Denis Goupil",
                "Lukas Kuczynski",
                "Jihyun Yoon",
                "Naveen Marri",
                "Peiye Zhuang",
                "Yinan Zhang",
                "Brian Yin",
                "Haomiao Jiang",
                "Marcel van Workum",
                "Thomas Lane",
                "Bryce Erickson",
                "Salil Pathare",
                "Kyle Price",
                "Anupam Singh",
                "David Baszucki"
            ],
            "affiliations": [
                "Foundation AI team, Roblox",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15475.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "üßä",
                "ru": {
                    "title": "3D-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è 3D-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ Roblox. –ú–æ–¥–µ–ª—å –ø—Ä–∏–∑–≤–∞–Ω–∞ –ø–æ–º–æ—á—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –∏–≥—Ä–æ–≤–æ–≥–æ –æ–ø—ã—Ç–∞, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∞–Ω–∏–º–∞—Ü–∏—é –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ 3D-—Ñ–æ—Ä–º –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ—Ä–º –∏–∑ —Ç–µ–∫—Å—Ç–∞, —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–æ—Ä–º –∏ —Å—Ü–µ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞. –¢–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ —Å—Ü–µ–Ω–∞—Ö."
                },
                "en": {
                    "title": "Building the Future of 3D Intelligence in Roblox",
                    "desc": "This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation."
                },
                "zh": {
                    "title": "ÊûÑÂª∫3DÊô∫ËÉΩÁöÑÂü∫Á°ÄÊ®°Âûã",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰Ωï‰∏∫3DÊô∫ËÉΩÊûÑÂª∫Âü∫Á°ÄÊ®°ÂûãÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊîØÊåÅÂºÄÂèëËÄÖÂú®RobloxÂπ≥Âè∞‰∏äÁîüÊàê3DÂØπË±°„ÄÅÂú∫ÊôØÂíåÂä®ÁîªËßíËâ≤„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆËÆæËÆ°Ë¶ÅÊ±ÇÔºåÂπ∂‰ªãÁªç‰∫ÜÊûÑÂª∫3DÂΩ¢Áä∂Ê†áËÆ∞Âô®ÁöÑÂàùÊ≠•Ê≠•È™§„ÄÇÊàë‰ª¨ÁöÑÊ†áËÆ∞ÂåñÊñπÊ°àÂèØ‰ª•Â∫îÁî®‰∫éÊñáÊú¨Âà∞ÂΩ¢Áä∂ÁîüÊàê„ÄÅÂΩ¢Áä∂Âà∞ÊñáÊú¨ÁîüÊàêÂíåÊñáÊú¨Âà∞Âú∫ÊôØÁîüÊàêÁ≠â‰ªªÂä°„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜÂ¶Ç‰Ωï‰∏éÁé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂçè‰ΩúÔºå‰ª•ÂÆûÁé∞Âú∫ÊôØÂàÜÊûêÂíåÊé®ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15417",
            "title": "Temporal Regularization Makes Your Video Generator Stronger",
            "url": "https://huggingface.co/papers/2503.15417",
            "abstract": "Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.",
            "score": 8,
            "issue_id": 2801,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 –º–∞—Ä—Ç–∞",
                "en": "March 19",
                "zh": "3Êúà19Êó•"
            },
            "hash": "8eb262eda880d162",
            "authors": [
                "Harold Haodong Chen",
                "Haojian Huang",
                "Xianfeng Wu",
                "Yexin Liu",
                "Yajing Bai",
                "Wen-Jie Shu",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "HKU",
                "HKUST",
                "UCF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15417.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "FluxFlow: –£–ª—É—á—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FluxFlow, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–æ–∑–º—É—â–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FluxFlow –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–µ–π –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã."
                },
                "en": {
                    "title": "Enhancing Video Generation with Temporal Augmentation",
                    "desc": "This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÊó∂Èó¥Ë¥®Èáè",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊó∂Èó¥Ë¥®ÈáèÈóÆÈ¢òÔºåÂº∫Ë∞É‰∫ÜÂú®Â∏ß‰πãÈó¥‰øùÊåÅ‰∏ÄËá¥ËøêÂä®ÂíåÁúüÂÆûÂä®ÊÄÅÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨È¶ñÊ¨°ÂºïÂÖ•‰∫ÜÊó∂Èó¥Â¢ûÂº∫ÊäÄÊúØÔºåÂπ∂ÊèêÂá∫‰∫ÜFluxFlowÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òËßÜÈ¢ëÁîüÊàêÁöÑÊó∂Èó¥Ë¥®Èáè„ÄÇFluxFlowÂú®Êï∞ÊçÆÂ±ÇÈù¢ËøõË°åÊìç‰ΩúÔºåÈÄöËøáÊéßÂà∂Êó∂Èó¥Êâ∞Âä®Êù•Â¢ûÂº∫Êó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÂ§öÊ†∑ÊÄßÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÊû∂ÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFluxFlowÂú®Â§ö‰∏™ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏äÊòæËëóÊîπÂñÑ‰∫ÜÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁ©∫Èó¥‰øùÁúüÂ∫¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15485",
            "title": "TULIP: Towards Unified Language-Image Pretraining",
            "url": "https://huggingface.co/papers/2503.15485",
            "abstract": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io",
            "score": 7,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 –º–∞—Ä—Ç–∞",
                "en": "March 19",
                "zh": "3Êúà19Êó•"
            },
            "hash": "d4b870742a020d5a",
            "authors": [
                "Zineng Tang",
                "Long Lian",
                "Seun Eisape",
                "XuDong Wang",
                "Roei Herzig",
                "Adam Yala",
                "Alane Suhr",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15485.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#architecture",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "üå∑",
                "ru": {
                    "title": "TULIP: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –¥–µ—Ç–∞–ª—å–Ω—ã–º –∑—Ä–µ–Ω–∏–µ–º –∏ —è–∑—ã–∫–æ–≤—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TULIP - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. TULIP —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Å —Ç–µ–∫—Å—Ç–æ–º. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∏ —è–∑—ã–∫–æ–≤—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º. TULIP –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–π state-of-the-art –≤ –∑–∞–¥–∞—á–∞—Ö zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ few-shot –æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "TULIP: Bridging Vision and Language for Enhanced Image Understanding",
                    "desc": "This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks."
                },
                "zh": {
                    "title": "TULIPÔºöÊèêÂçáÂõæÂÉèÁêÜËß£ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Â∞ΩÁÆ°ÂÉèCLIPÂíåSigLIPËøôÊ†∑ÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπÊØîÊ®°ÂûãÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂÆÉ‰ª¨Âú®ÈúÄË¶ÅÈ´ò‰øùÁúüÂõæÂÉèÁêÜËß£ÁöÑËßÜËßâ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜTULIPÔºåËøôÊòØ‰∏ÄÁßçÂºÄÊ∫êÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊó®Âú®ÈÄöËøáÁîüÊàêÊï∞ÊçÆÂ¢ûÂº∫ÂíåÂØπÊØîÂ≠¶‰π†Êù•ÊèêÈ´òÂõæÂÉèÁêÜËß£ËÉΩÂäõ„ÄÇTULIPËÉΩÂ§üÂ≠¶‰π†ÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâÁâπÂæÅÔºåÂêåÊó∂‰øùÊåÅÂÖ®Â±ÄËØ≠‰πâÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåTULIPÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåÊòæËëóÊèêÂçá‰∫ÜÈõ∂-shotÊÄßËÉΩÂíåÂ∞ë-shotÂàÜÁ±ªÁöÑÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12769",
            "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
            "url": "https://huggingface.co/papers/2503.12769",
            "abstract": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.",
            "score": 4,
            "issue_id": 2800,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "0913c8e386f3aae5",
            "authors": [
                "Shenghao Fu",
                "Qize Yang",
                "Yuan-Ming Li",
                "Yi-Xing Peng",
                "Kun-Yu Lin",
                "Xihan Wei",
                "Jian-Fang Hu",
                "Xiaohua Xie",
                "Wei-Shi Zheng"
            ],
            "affiliations": [
                "Guangdong Province Key Laboratory of Information Security Technology, China",
                "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
                "Pazhou Laboratory (Huangpu), China",
                "Peng Cheng Laboratory, China",
                "School of Computer Science and Engineering, Sun Yat-sen University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12769.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#agents"
                ],
                "emoji": "üëÅÔ∏è",
                "ru": {
                    "title": "ViSpeak: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å' –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å ViSpeak, —Å–ø–æ—Å–æ–±–Ω—É—é —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–∏—Ö –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö ViSpeak-Instruct –∏ ViSpeak-Bench. –ú–æ–¥–µ–ª—å ViSpeak –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ GPT-4 –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Enhancing User-Agent Interaction through Visual Instruction Feedback",
                    "desc": "This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions."
                },
                "zh": {
                    "title": "ÊèêÂçáÁî®Êà∑‰∏é‰ª£ÁêÜ‰∫íÂä®ÁöÑËßÜËßâÊåá‰ª§ÂèçÈ¶à",
                    "desc": "ËøëÂπ¥Êù•ÔºåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÂú®Á¶ªÁ∫øËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÊµÅÂ™í‰ΩìËßÜÈ¢ëÁêÜËß£Áî±‰∫éÂÖ∂Êó∂Èó¥ÊïèÊÑüÊÄß„ÄÅÂÖ®Ê®°ÊÄÅÂíå‰∫§‰∫íÁâπÊÄßÔºåÂØπÁé∞ÊúâÊ®°ÂûãÊèêÂá∫‰∫ÜÂ∑®Â§ßÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞‰ªªÂä°ÔºåÁß∞‰∏∫ËßÜËßâÊåá‰ª§ÂèçÈ¶àÔºåÊ®°ÂûãÈúÄË¶ÅÁêÜËß£ËßÜËßâÂÜÖÂÆπÂπ∂‰ªé‰∏≠ÊèêÂèñÊåá‰ª§Ôºå‰ªéËÄåÂ¢ûÂº∫Áî®Êà∑‰∏é‰ª£ÁêÜ‰πãÈó¥ÁöÑ‰∫íÂä®„ÄÇÊàë‰ª¨ÂÆö‰πâ‰∫Ü‰∏É‰∏™‰∏éËßÜËßâÊ®°ÊÄÅÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÂ≠ê‰ªªÂä°ÔºåÂπ∂Êî∂ÈõÜ‰∫ÜViSpeak-InstructÊï∞ÊçÆÈõÜÁî®‰∫éËÆ≠ÁªÉÔºåViSpeak-BenchÁî®‰∫éËØÑ‰º∞ÔºåÂêåÊó∂ÊèêÂá∫‰∫ÜViSpeakÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂú®ÊµÅÂ™í‰ΩìËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜ‰∏äÁöÑÂÖàËøõÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12532",
            "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
            "url": "https://huggingface.co/papers/2503.12532",
            "abstract": "Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.",
            "score": 4,
            "issue_id": 2800,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "185728a70d3b80d0",
            "authors": [
                "Fanbin Lu",
                "Zhisheng Zhong",
                "Ziqin Wei",
                "Shu Liu",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12532.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#agents",
                    "#training",
                    "#cv"
                ],
                "emoji": "üñ•Ô∏è",
                "ru": {
                    "title": "STEVE: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç STEVE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Å—É–±–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é GPT-4. –ù–∞ –æ—Å–Ω–æ–≤–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ö–∞–Ω–µ–º–∞–Ω–∞-–¢–≤–µ—Ä—Å–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ —Å–ª–æ–∂–Ω–æ–π —Å—Ä–µ–¥–µ —Ä–∞–±–æ—á–µ–≥–æ —Å—Ç–æ–ª–∞."
                },
                "en": {
                    "title": "STEVE: Optimizing AI Agents for GUI Manipulation Efficiently",
                    "desc": "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."
                },
                "zh": {
                    "title": "Êô∫ËÉΩ‰ª£ÁêÜËÆ≠ÁªÉÁöÑÊñ∞Á™ÅÁ†¥ÔºöSTEVE",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫STEVEÁöÑÊ≠•È™§È™åËØÅÁÆ°ÈÅìÔºåÁî®‰∫éËÆ≠ÁªÉËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜ„ÄÇÊàë‰ª¨È¶ñÂÖàÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Â§ßÂûãÊåá‰ª§ÈõÜÔºåÂπ∂Êî∂ÈõÜ‰∫Ü‰∏Ä‰∫õÊ¨°‰ºò‰ª£ÁêÜÁöÑËΩ®ËøπÊï∞ÊçÆ„ÄÇÈÄöËøá‰ΩøÁî®GPT-4oÈ™åËØÅÊØè‰∏™Ê≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂπ∂‰∏∫ÊØè‰∏™Ê≠•È™§ÂàÜÈÖç‰∫åÂÖÉÊ†áÁ≠æÔºåÊúÄÂêéÈááÁî®Âç°Â∞ºÊõºÂíåÁâπÊ≤ÉÊñØ‰ºòÂåñÊñπÊ≥ïÊù•‰ºòÂåñ‰ª£ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑ‰ª£ÁêÜÂú®Â§çÊùÇÁöÑÊ°åÈù¢ÁéØÂ¢É‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÔºå‰∏îËÆ≠ÁªÉÊïàÁéáÈ´ò„ÄÅÊàêÊú¨‰Ωé„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14505",
            "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
            "url": "https://huggingface.co/papers/2503.14505",
            "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.",
            "score": 1,
            "issue_id": 2801,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "61ef56ac402491c0",
            "authors": [
                "Susung Hong",
                "Ira Kemelmacher-Shlizerman",
                "Brian Curless",
                "Steven M. Seitz"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.14505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "üíÉ",
                "ru": {
                    "title": "–¢–∞–Ω—Ü—É–π –ø–æ–¥ –º—É–∑—ã–∫—É: –ò–ò-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–∏—Ç–º–µ –º–µ–ª–æ–¥–∏–∏",
                    "desc": "MusicInfuser - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–∞–Ω—Ü–µ–≤–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –∑–∞–¥–∞–Ω–Ω–æ–π –º—É–∑—ã–∫–∞–ª—å–Ω–æ–π –¥–æ—Ä–æ–∂–∫–æ–π. –û–Ω –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Å –º—É–∑—ã–∫–∞–ª—å–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–µ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É –º—É–∑—ã–∫–æ–π –∏ –≤–∏–¥–µ–æ –∏ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–π –∞–¥–∞–ø—Ç–µ—Ä. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç, MusicInfuser –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–∞–Ω—Ü–µ–≤–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–Ω—Ü–µ–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–¥–µ–æ-LLM."
                },
                "en": {
                    "title": "Syncing Dance with Music: Introducing MusicInfuser",
                    "desc": "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."
                },
                "zh": {
                    "title": "Èü≥‰πê‰∏éËàûËπàÁöÑÂÆåÁæéËûçÂêà",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜMusicInfuserÔºåËøôÊòØ‰∏ÄÁßçÁîüÊàêÈ´òË¥®ÈáèËàûËπàËßÜÈ¢ëÁöÑÊñπÊ≥ïÔºåËÉΩÂ§ü‰∏éÊåáÂÆöÁöÑÈü≥‰πêËΩ®ÈÅìÂêåÊ≠•„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ËΩªÈáèÁ∫ßÁöÑÈü≥‰πê-ËßÜÈ¢ë‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Âíå‰ΩéÁß©ÈÄÇÈÖçÂô®ÔºåÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïË∞ÉÊï¥Áé∞ÊúâÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰ª•ÈÄÇÂ∫îÈü≥‰πêËæìÂÖ•„ÄÇ‰∏é‰πãÂâçÈúÄË¶ÅËøêÂä®ÊçïÊçâÊï∞ÊçÆÁöÑÂ∑•‰Ωú‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ªÖÂú®ËàûËπàËßÜÈ¢ë‰∏äËøõË°åÂæÆË∞É„ÄÇMusicInfuserÂú®‰øùÊåÅÂ∫ïÂ±ÇÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåÁîüÊàêËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ´òË¥®ÈáèÁöÑÈü≥‰πêÈ©±Âä®ËßÜÈ¢ëÁîüÊàê„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-19.html",
    "link_next": "2025-03-21.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3Êúà19Êó•"
    },
    "short_date_next": {
        "ru": "21.03",
        "en": "03/21",
        "zh": "3Êúà21Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü RWKV-7 \"Goose\"Ôºå‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂª∫Ê®°Êû∂ÊûÑ„ÄÇÂÆÉÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏äËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÁöÑ‰ª§ÁâåÊï∞ÈáèÂ∞ë„ÄÇRWKV-7 Ê®°ÂûãÊØè‰∏™‰ª§ÁâåÁöÑÂÜÖÂ≠òÂíåÊé®ÁêÜÊó∂Èó¥ÊòØÂ∏∏Êï∞„ÄÇÂÆÉÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ delta ËßÑÂàôÂíåÊîæÊùæÁöÑÂÄºÊõøÊç¢ËßÑÂàô„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü RWKV-7 ÁöÑËØ≠Ë®ÄÂª∫Ê®°ËÉΩÂäõÔºåÂπ∂ÂèëÂ∏É‰∫ÜÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜ„ÄÇ",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "pinyin": "W«ímen ji√®sh√†o le RWKV-7 \"Goose\", yƒ´ zh«íng xƒ´n de x√πli√® ji√†nm√≥ ji√†g√≤u. TƒÅ z√†i du≈çy«îy√°n r√®nw√π sh√†ng d√°d√†o le xƒ´n de zu√¨jiƒÅ x√¨ngn√©ng, j«êngu«én x√πnli√†n de l√¨ngp√°i sh√πli√†ng sh«éo. RWKV-7 m√≥x√≠ng mƒõi ge l√¨ngp√°i de n√®ic√∫n h√© tuƒ´l«ê sh√≠jiƒÅn sh√¨ ch√°ngsh√π. TƒÅ y«ênr√π le xƒ´n de delta guƒ´z√© h√© f√†ngs≈çng de zh√≠ t√¨hu√†n guƒ´z√©. W«ímen zh«énsh√¨ le RWKV-7 de y«îy√°n ji√†nm√≥ n√©ngl√¨, b√¨ng fƒÅb√π le m√≥x√≠ng h√© sh√πj√πj√≠.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"Â∫èÂàó\", \"pinyin\": \"x√π li√®\", \"trans\": \"sequence\"},\n    {\"word\": \"Âª∫Ê®°\", \"pinyin\": \"ji√†n m√≥\", \"trans\": \"modeling\"},\n    {\"word\": \"Êû∂ÊûÑ\", \"pinyin\": \"ji√† g√≤u\", \"trans\": \"architecture\"},\n    {\"word\": \"Â§öËØ≠Ë®Ä\", \"pinyin\": \"du≈ç y«î y√°n\", \"trans\": \"multilingual\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n wu\", \"trans\": \"task\"},\n    {\"word\": \"ËææÂà∞\", \"pinyin\": \"d√° d√†o\", \"trans\": \"achieve\"},\n    {\"word\": \"ÊúÄ‰Ω≥\", \"pinyin\": \"zu√¨ jiƒÅ\", \"trans\": \"best\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"Â∞ΩÁÆ°\", \"pinyin\": \"j√¨n gu«én\", \"trans\": \"although\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"training\"},\n    {\"word\": \"‰ª§Áâå\", \"pinyin\": \"l√¨ng p√†i\", \"trans\": \"token\"},\n    {\"word\": \"Êï∞Èáè\", \"pinyin\": \"sh√π li√†ng\", \"trans\": \"quantity\"},\n    {\"word\": \"Â∞ë\", \"pinyin\": \"sh«éo\", \"trans\": \"few\"},\n    {\"word\": \"ÂÜÖÂ≠ò\", \"pinyin\": \"n√®i c√∫n\", \"trans\": \"memory\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"inference\"},\n    {\"word\": \"Êó∂Èó¥\", \"pinyin\": \"sh√≠ jiƒÅn\", \"trans\": \"time\"},\n    {\"word\": \"Â∏∏Êï∞\", \"pinyin\": \"ch√°ng sh√π\", \"trans\": \"constant\"},\n    {\"word\": \"ÂºïÂÖ•\", \"pinyin\": \"y«ên r√π\", \"trans\": \"introduce\"},\n    {\"word\": \"ËßÑÂàô\", \"pinyin\": \"guƒ´ z√©\", \"trans\": \"rule\"},\n    {\"word\": \"ÊîæÊùæ\", \"pinyin\": \"f√†ng s≈çng\", \"trans\": \"relax\"},\n    {\"word\": \"ÂÄº\", \"pinyin\": \"zh√≠\", \"trans\": \"value\"},\n    {\"word\": \"ÊõøÊç¢\", \"pinyin\": \"t√¨ hu√†n\", \"trans\": \"replace\"},\n    {\"word\": \"Â±ïÁ§∫\", \"pinyin\": \"zh«én sh√¨\", \"trans\": \"demonstrate\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÂèëÂ∏É\", \"pinyin\": \"fƒÅ b√π\", \"trans\": \"release\"},\n    {\"word\": \"Êï∞ÊçÆÈõÜ\", \"pinyin\": \"sh√π j√π j√≠\", \"trans\": \"dataset\"}\n]",
        "trans": "We introduced RWKV-7 \"Goose,\" a new sequence modeling architecture. It achieved new best performance on multilingual tasks despite being trained with fewer tokens. The RWKV-7 model has constant memory and inference time per token. It introduces new delta rules and relaxed value substitution rules. We demonstrated the language modeling capabilities of RWKV-7 and released the model and dataset.",
        "update_ts": "2025-03-19 09:12"
    }
}