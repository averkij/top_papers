{
    "date": {
        "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 11",
        "zh": "9æœˆ11æ—¥"
    },
    "time_utc": "2025-09-11 02:19",
    "weekday": 3,
    "issue_id": 5829,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.08826",
            "title": "RewardDance: Reward Scaling in Visual Generation",
            "url": "https://huggingface.co/papers/2509.08826",
            "abstract": "RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
            "score": 2,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "773c62b4801465a6",
            "authors": [
                "Jie Wu",
                "Yu Gao",
                "Zilyu Ye",
                "Ming Li",
                "Liang Li",
                "Hanzhong Guo",
                "Jie Liu",
                "Zeyue Xue",
                "Xiaoxia Hou",
                "Wei Liu",
                "Yan Zeng",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.08826.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ’ƒ",
                "ru": {
                    "title": "RewardDance: Ğ¢Ğ°Ğ½Ñ†ÑƒÑ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RewardDance - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ°Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ VLM. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. RewardDance Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ 'Ğ´Ğ°', Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RewardDance Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "RewardDance: Scaling Reward Models for Better AI Generation",
                    "desc": "RewardDance is a new framework designed to improve reward modeling in visual generation tasks by aligning with Vision-Language Models (VLMs). It addresses the limitations of existing reward models, which often struggle with architectural constraints and misalignment with next-token predictions. The framework introduces a novel generative reward paradigm that reformulates reward scoring, allowing for effective scaling of reward models up to 26 billion parameters. Importantly, RewardDance mitigates the issue of reward hacking, ensuring that models produce diverse and high-quality outputs during reinforcement learning fine-tuning."
                },
                "zh": {
                    "title": "RewardDanceï¼šè§£å†³å¥–åŠ±é»‘å®¢çš„å¯æ‰©å±•å¥–åŠ±å»ºæ¨¡æ¡†æ¶",
                    "desc": "RewardDanceæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°æ‰©å±•å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¹¶è§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­çš„æ‰©å±•æ€§å—åˆ°æ¶æ„å’Œè¾“å…¥æ¨¡æ€çš„é™åˆ¶ï¼Œè€Œæµè¡Œçš„Bradley-TerryæŸå¤±ä¸VLMçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ä¸åŒ¹é…ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ‰©å±•ã€‚é€šè¿‡å°†å¥–åŠ±åˆ†æ•°é‡æ–°å®šä¹‰ä¸ºæ¨¡å‹é¢„æµ‹â€œæ˜¯â€æ ‡è®°çš„æ¦‚ç‡ï¼ŒRewardDanceä½¿å¥–åŠ±ç›®æ ‡ä¸VLMæ¶æ„å†…åœ¨å¯¹é½ï¼Œä»è€Œåœ¨æ¨¡å‹å’Œä¸Šä¸‹æ–‡ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°æ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒRewardDanceåœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆè§£å†³äº†å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08827",
            "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
            "url": "https://huggingface.co/papers/2509.08827",
            "abstract": "Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "score": 0,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "ea01091be58aef4b",
            "authors": [
                "Kaiyan Zhang",
                "Yuxin Zuo",
                "Bingxiang He",
                "Youbang Sun",
                "Runze Liu",
                "Che Jiang",
                "Yuchen Fan",
                "Kai Tian",
                "Guoli Jia",
                "Pengfei Li",
                "Yu Fu",
                "Xingtai Lv",
                "Yuchen Zhang",
                "Sihang Zeng",
                "Shang Qu",
                "Haozhan Li",
                "Shijie Wang",
                "Yuru Wang",
                "Xinwei Long",
                "Fangfu Liu",
                "Xiang Xu",
                "Jiaze Ma",
                "Xuekai Zhu",
                "Ermo Hua",
                "Yihao Liu",
                "Zonglin Li",
                "Huayu Chen",
                "Xiaoye Qu",
                "Yafu Li",
                "Weize Chen",
                "Zhenzhao Yuan",
                "Junqi Gao",
                "Dong Li",
                "Zhiyuan Ma",
                "Ganqu Cui",
                "Zhiyuan Liu",
                "Biqing Qi",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huazhong University of Science and Technology",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University College London",
                "University of Science and Technology of China",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08827.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#survey",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑ…Ğ¸ RL Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RL Ğ´Ğ»Ñ LRM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Scaling Reinforcement Learning for Advanced Reasoning in Language Models",
                    "desc": "This paper reviews the integration of Reinforcement Learning (RL) with Large Language Models (LLMs) to improve their reasoning capabilities. It highlights the success of RL in enhancing LLMs for complex tasks like mathematics and coding, positioning RL as a key method for evolving LLMs into more advanced reasoning models (LRMs). The authors discuss the challenges of scaling RL, including the need for better computational resources, algorithm design, and training data. They aim to identify future research directions to further develop RL applications in reasoning tasks, especially in the context of achieving Artificial SuperIntelligence (ASI)."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡",
                    "desc": "æœ¬è®ºæ–‡è°ƒæŸ¥äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æå‡LLMèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³å¤æ‚çš„é€»è¾‘ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒRLåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•é¢ä¸´ç€è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰åŸºç¡€æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹ç»¼è¿°ä¿ƒè¿›æœªæ¥åœ¨æ›´å¹¿æ³›æ¨ç†æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-10.html",
    "link_next": "2025-09-12.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}