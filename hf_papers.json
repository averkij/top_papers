{
    "date": {
        "ru": "13 ноября",
        "en": "November 13",
        "zh": "11月13日"
    },
    "time_utc": "2024-11-13 20:10",
    "weekday": 2,
    "issue_id": 557,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 16,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "SAMPart3D: гибкая сегментация 3D-объектов без предварительного обучения",
                    "desc": "Статья представляет SAMPart3D - масштабируемый фреймворк для сегментации частей 3D-объектов без предварительного обучения. Авторы используют безтекстовые модели компьютерного зрения для извлечения признаков из 3D-данных, что позволяет обучаться на больших наборах неразмеченных 3D-объектов. Метод способен сегментировать объекты на части с разной степенью детализации, а затем присваивать семантические метки с помощью мультимодальных языковых моделей. SAMPart3D превосходит существующие методы и может применяться для редактирования и интерактивной сегментации 3D-объектов."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3D：无文本提示的3D部件分割新框架",
                    "desc": "3D部件分割是3D感知中的一项重要且具有挑战性的任务，广泛应用于机器人技术、3D生成和3D编辑等领域。本文提出了SAMPart3D框架，它能够在不依赖预定义文本提示的情况下，对任意3D对象进行多粒度的语义部件分割。该框架利用无文本依赖的视觉基础模型，从大规模未标记的3D数据集中提取丰富的3D特征，并通过条件化的部件感知特征实现灵活的分割。实验结果表明，SAMPart3D在处理复杂对象时显著优于现有的零样本3D部件分割方法，并能支持多种应用，如部件级编辑和交互式分割。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07133",
            "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
            "url": "https://huggingface.co/papers/2411.07133",
            "abstract": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.",
            "score": 11,
            "issue_id": 546,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "be2fc1cdad8aa9f3",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Парадокс больших моделей: не всегда лучшие учителя",
                    "desc": "В статье рассматривается проблема настройки больших языковых моделей (LLMs) с помощью синтетических наборов инструкций. Авторы выявили парадокс, заключающийся в том, что более крупные модели не всегда являются лучшими учителями для более мелких моделей. Для решения этой проблемы они разработали новый метрик, называемый Compatibility-Adjusted Reward (CAR), который учитывает совместимость между учителем и базовой моделью. Эксперименты показали, что CAR превосходит почти все существующие подходы."
                },
                "en": {
                    "title": "Rethinking Instruction Tuning: Size Isn't Everything!",
                    "desc": "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."
                },
                "zh": {
                    "title": "大模型不一定是好教师！",
                    "desc": "本论文探讨了指令调优在大型语言模型（LLMs）中的应用，强调了指令数据集对模型性能的重要性。我们发现，较大或较强的模型并不一定是较小模型的更好教师，这一现象被称为“大模型悖论”。此外，现有的评估指标无法准确预测响应生成器的有效性，因为它们忽略了教师模型与被调优基础模型之间的兼容性。为此，我们提出了一种新的评估指标——兼容性调整奖励（CAR），并通过实验验证了其优越性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07975",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2411.07975",
            "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.",
            "score": 11,
            "issue_id": 544,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "294dc65a01cd1218",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#architecture",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Единая модель для понимания и генерации изображений",
                    "desc": "JanusFlow - это новая архитектура, объединяющая понимание и генерацию изображений в одной модели. Она интегрирует авторегрессионные языковые модели с методом rectified flow для генеративного моделирования. Ключевое преимущество - возможность обучать rectified flow в рамках больших языковых моделей без сложных модификаций архитектуры. Для улучшения производительности используется разделение энкодеров понимания и генерации, а также выравнивание их представлений при обучении."
                },
                "en": {
                    "title": "JanusFlow: Unifying Image Understanding and Generation Efficiently",
                    "desc": "JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches."
                },
                "zh": {
                    "title": "JanusFlow：图像理解与生成的统一模型",
                    "desc": "JanusFlow是一个强大的框架，将图像理解和生成统一在一个模型中。它采用了简约的架构，将自回归语言模型与修正流结合，这是生成建模中的一种先进方法。研究发现，修正流可以在大型语言模型框架内轻松训练，无需复杂的架构修改。通过解耦理解和生成编码器以及在统一训练中对齐它们的表示，JanusFlow在标准基准测试中表现出色，超越了现有的统一方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07461",
            "title": "BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions",
            "url": "https://huggingface.co/papers/2411.07461",
            "abstract": "We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale",
            "score": 5,
            "issue_id": 552,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "08fb959148999629",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "KALE: Обогащение мультимодальных моделей фактическими знаниями",
                    "desc": "В статье представлен новый набор данных BLIP3-KALE, состоящий из 218 миллионов пар изображений и текстов. Этот датасет объединяет синтетические описательные подписи с фактическими альтернативными текстами из веб-источников. Авторы используют двухэтапный подход с применением крупных мультимодальных и языковых моделей для создания подписей, обогащенных знаниями. Эксперименты показывают, что обучение на KALE улучшает результаты в задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Bridging Descriptive and Factual Captions with KALE",
                    "desc": "The paper presents BLIP3-KALE, a new dataset containing 218 million image-text pairs that enhances the quality of image captions by combining synthetic captions with factual web-based alt-text. This dataset is created using a two-stage method that employs large vision-language models and language models to produce captions that are both descriptive and factually accurate. By training specialized vision-language models on the KALE dataset, the authors demonstrate significant improvements in various vision-language tasks. The findings highlight the effectiveness of KALE in developing more advanced multimodal models that can better understand and generate content across different modalities."
                },
                "zh": {
                    "title": "知识增强的图像标题生成",
                    "desc": "我们介绍了BLIP3-KALE，这是一个包含2.18亿对图像-文本的数据集，旨在弥合描述性合成标题与事实性网络规模替代文本之间的差距。KALE通过将合成的密集图像标题与网络规模的替代文本相结合，生成基于事实的图像标题。我们采用两阶段的方法，利用大型视觉-语言模型和语言模型创建知识增强的标题，并用这些标题训练专门的视觉语言模型，以扩大数据集规模。实验结果表明，KALE在训练更强大和更具知识性的多模态模型方面具有重要价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08017",
            "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings",
            "url": "https://huggingface.co/papers/2411.08017",
            "abstract": "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.",
            "score": 4,
            "issue_id": 547,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "0af1f4c0dc38cc5b",
            "data": {
                "categories": [
                    "#inference",
                    "#open_source",
                    "#3d",
                    "#dataset",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективная генерация 3D-моделей с помощью вейвлет-сжатия",
                    "desc": "Статья представляет новый подход к генерации 3D-моделей под названием Wavelet Latent Diffusion (WaLa). Метод использует вейвлет-кодирование для создания компактных латентных представлений 3D-форм, достигая степени сжатия 2427x. Это позволяет эффективно обучать крупномасштабные генеративные нейронные сети без увеличения времени вывода. WaLa демонстрирует улучшенное качество генерации, разнообразие и вычислительную эффективность по сравнению с существующими методами."
                },
                "en": {
                    "title": "Efficient 3D Shape Generation with Wavelet Latent Diffusion",
                    "desc": "This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public."
                },
                "zh": {
                    "title": "高效压缩，快速生成3D形状的创新方法",
                    "desc": "本论文提出了一种新的方法，称为Wavelet Latent Diffusion（WaLa），旨在提高大规模3D生成模型的效率。通过将3D形状编码为基于小波的紧凑潜在编码，WaLa实现了高达2427倍的压缩比，同时保持了细节的完整性。该方法使得训练大型生成网络变得更加高效，并且在推理时不会显著增加时间。我们的模型在多个数据集上表现出色，生成高质量的3D形状，并且开源了代码，推动了3D生成模型的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08034",
            "title": "Scaling Properties of Diffusion Models for Perceptual Tasks",
            "url": "https://huggingface.co/papers/2411.08034",
            "abstract": "In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute. To use our code and models, see https://scaling-diffusion-perception.github.io .",
            "score": 3,
            "issue_id": 557,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "4be3b9af89108627",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Диффузионные модели: новый взгляд на задачи визуального восприятия",
                    "desc": "В этой статье исследуются возможности итеративных вычислений с диффузионными моделями для задач визуального восприятия. Авторы объединяют такие задачи как оценка глубины, оптический поток и сегментация в рамках преобразования изображения в изображение. Они демонстрируют, как диффузионные модели выигрывают от увеличения вычислительных ресурсов на этапах обучения и вывода. Предложенные методы позволяют эффективно обучать диффузионные модели для задач визуального восприятия, достигая улучшенных результатов при меньшем объеме данных и вычислений."
                },
                "en": {
                    "title": "Scaling Diffusion Models for Enhanced Visual Perception",
                    "desc": "This paper explores how diffusion models can be effectively used for both generating images and performing visual perception tasks like depth estimation and segmentation. It presents a unified approach that treats these tasks as image-to-image translation problems, highlighting the advantages of scaling in training and testing. The authors analyze the scaling behaviors of diffusion models and propose techniques to enhance their efficiency in visual perception tasks. As a result, their models demonstrate competitive performance compared to leading methods while requiring less data and computational resources."
                },
                "zh": {
                    "title": "扩散模型：视觉感知的新力量",
                    "desc": "本文提出了扩散模型在生成和视觉感知任务中的强大能力。我们将深度估计、光流和分割等任务统一为图像到图像的转换，并展示了扩散模型在这些感知任务中如何通过扩展训练和测试计算来获益。通过对这些扩展行为的仔细分析，我们提出了多种高效训练扩散模型的技术。我们的模型在使用显著更少的数据和计算的情况下，达到了与最先进方法相当或更好的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05197",
            "title": "Hardware and Software Platform Inference",
            "url": "https://huggingface.co/papers/2411.05197",
            "abstract": "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.",
            "score": 3,
            "issue_id": 550,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "7685c8e74f6dbc6b",
            "data": {
                "categories": [
                    "#leakage",
                    "#security",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Разоблачение обмана: как определить реальное оборудование для языковой модели",
                    "desc": "Статья представляет метод HSPI (идентификация аппаратной и программной платформы), позволяющий определить архитектуру и программный стек модели машинного обучения только на основе ее поведения при вводе-выводе. Метод использует присущие различным архитектурам и компиляторам отличия для распознавания типов оборудования и программных конфигураций. Анализируя числовые паттерны в выводе модели, предложена классификационная система, способная точно идентифицировать оборудование, используемое для вывода модели. Результаты показывают возможность определения типа оборудования для черного ящика модели с точностью до 100% в белом ящике и до трех раз выше случайного угадывания в черном ящике."
                },
                "en": {
                    "title": "Verify Your Model: Uncovering the Truth Behind LLM Inference",
                    "desc": "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."
                },
                "zh": {
                    "title": "验证大型语言模型的真实性",
                    "desc": "本文介绍了一种名为硬件和软件平台推理（HSPI）的方法，用于识别机器学习模型的底层架构和软件堆栈。该方法通过分析模型的输入输出行为，利用不同架构和编译器的固有差异来区分不同类型的硬件和软件配置。研究表明，在白盒环境下，我们可以以83.9%到100%的准确率区分不同的硬件，而在黑盒环境下，准确率也能达到随机猜测的三倍以上。此方法为验证大型语言模型的真实性提供了一种有效的手段。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06307",
            "title": "Acoustic Volume Rendering for Neural Impulse Response Fields",
            "url": "https://huggingface.co/papers/2411.06307",
            "abstract": "Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener's position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.",
            "score": 2,
            "issue_id": 554,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 ноября",
                "en": "November 9",
                "zh": "11月9日"
            },
            "hash": "f6935a265562d416",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "Реалистичный синтез звука с помощью объемного рендеринга",
                    "desc": "Статья представляет новый метод синтеза акустических импульсных характеристик под названием Acoustic Volume Rendering (AVR). AVR адаптирует технику рендеринга объема для моделирования распространения звука в пространстве. Метод использует рендеринг в частотной области и сферическую интеграцию для точного воспроизведения измеренных импульсных характеристик. Эксперименты показывают, что AVR значительно превосходит существующие методы в синтезе импульсных характеристик для новых позиций."
                },
                "en": {
                    "title": "Revolutionizing Sound: Acoustic Volume Rendering for Immersive Audio Experiences",
                    "desc": "This paper introduces Acoustic Volume Rendering (AVR), a new method for synthesizing realistic audio in virtual and augmented reality. AVR uses volume rendering techniques to model acoustic impulse responses (IRs), which describe how sound travels in a scene. The authors tackle the unique challenges of IRs as time-series signals by employing frequency-domain volume rendering and spherical integration. Their approach not only improves the accuracy of sound synthesis but also outperforms existing methods, demonstrating significant advancements in acoustic simulation with their platform, AcoustiX."
                },
                "zh": {
                    "title": "声学体积渲染：提升虚拟现实中的音频体验",
                    "desc": "本文提出了一种新的声学体积渲染（AVR）方法，用于合成真实的声学脉冲响应（IR），以增强虚拟和增强现实中的沉浸体验。AVR通过适应体积渲染技术，解决了声波传播的独特挑战，特别是将时间序列信号建模为脉冲响应场。我们引入了频域体积渲染和球面积分技术，以更准确地拟合IR测量。实验结果表明，AVR在合成新姿态的脉冲响应方面超越了现有的领先方法，并且我们还开发了一个声学模拟平台AcoustiX，提供更准确的IR模拟。"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11月12日"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11月14日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。",
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "pinyin": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。\n\nPinyin transcription:\n\nZhè piān wénzhāng tǎolùn le 3D bùjiàn fēngé de zhòngyàoxìng jí qí zài jīqìrén, 3D shēngchéng hé 3D biānjí zhōng de yìngyòng. Xiànyǒu fāngfǎ lìyòng shìjué yǔyán móxíng jìnxíng 2D dào 3D de zhīshi zhēngliú, shíxiàn líng yàngběn 3D bùjiàn fēngé, dàn yīlài wénběn tíshì, xiànzhì le kuòzhǎnxìng hé chǔlǐ bùjiàn jíyì de línghuóxìng. Wénzhāng jièshào le SAMPart3D, yīgè kě kuòzhǎn de líng yàngběn 3D bùjiàn fēngé kuàngjià, bù xūyào yùdìngyì de bùjiàn biǎoqián jí. Tā shǐyòng yǔ wénběn wúguān de shìjué jīchǔ móxíng jìnxíng zhēngliú, bìng tīquǎn duō chǐdù de bùjiàn gǎnjué 3D tèzhēng. Shíyàn xiǎnshì, SAMPart3D zài dà guīmó 3D shùjùjí shàng biǎoxiàn yōuyuè, bìng chāoyué xiànyǒu fāngfǎ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"3D部件分割\", \"pinyin\": \"3D bù jiàn fēn gē\", \"trans\": \"3D part segmentation\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòng yào xìng\", \"trans\": \"importance\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"现有方法\", \"pinyin\": \"xiàn yǒu fāng fǎ\", \"trans\": \"existing methods\"},\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"知识蒸馏\", \"pinyin\": \"zhī shi zhēng liú\", \"trans\": \"knowledge distillation\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"文本提示\", \"pinyin\": \"wén běn tí shì\", \"trans\": \"textual prompts\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"扩展性\", \"pinyin\": \"kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"handle\"},\n    {\"word\": \"部件歧义\", \"pinyin\": \"bù jiàn qí yì\", \"trans\": \"part ambiguity\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"SAMPart3D\", \"pinyin\": \"SAMPart3D\", \"trans\": \"SAMPart3D\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"预定义\", \"pinyin\": \"yù dìng yì\", \"trans\": \"predefined\"},\n    {\"word\": \"标签集\", \"pinyin\": \"biāo qiān jí\", \"trans\": \"label set\"},\n    {\"word\": \"视觉基础模型\", \"pinyin\": \"shì jué jī chǔ mó xíng\", \"trans\": \"vision foundation model\"},\n    {\"word\": \"多尺度\", \"pinyin\": \"duō chǐ dù\", \"trans\": \"multi-scale\"},\n    {\"word\": \"部件感知\", \"pinyin\": \"bù jiàn gǎn zhī\", \"trans\": \"part-aware\"},\n    {\"word\": \"3D特征\", \"pinyin\": \"3D tè zhēng\", \"trans\": \"3D features\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses the importance of 3D part segmentation and its applications in robotics, 3D generation, and 3D editing. Existing methods utilize vision-language models for knowledge distillation from 2D to 3D, achieving zero-shot 3D part segmentation, but they rely on textual prompts, which limits their scalability and flexibility in handling part ambiguities. The article introduces SAMPart3D, a scalable zero-shot 3D part segmentation framework that does not require predefined part labels. It uses a text-agnostic visual foundation model for distillation and extracts multi-scale part-aware 3D features. Experiments show that SAMPart3D performs excellently on large-scale 3D datasets and outperforms existing methods.",
        "update_ts": "2024-11-13 09:10"
    }
}