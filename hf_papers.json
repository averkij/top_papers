{
    "date": {
        "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 26",
        "zh": "8æœˆ26æ—¥"
    },
    "time_utc": "2025-08-26 03:37",
    "weekday": 1,
    "issue_id": 5540,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.18265",
            "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
            "url": "https://huggingface.co/papers/2508.18265",
            "abstract": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
            "score": 11,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "f9e73f2c4171c881",
            "authors": [
                "Weiyun Wang",
                "Zhangwei Gao",
                "Lixin Gu",
                "Hengjun Pu",
                "Long Cui",
                "Xingguang Wei",
                "Zhaoyang Liu",
                "Linglin Jing",
                "Shenglong Ye",
                "Jie Shao",
                "Zhaokai Wang",
                "Zhe Chen",
                "Hongjie Zhang",
                "Ganlin Yang",
                "Haomin Wang",
                "Qi Wei",
                "Jinhui Yin",
                "Wenhao Li",
                "Erfei Cui",
                "Guanzhou Chen",
                "Zichen Ding",
                "Changyao Tian",
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Yuchen Duan",
                "Xuehui Wang",
                "Songze Li",
                "Xiangyu Zhao",
                "Haodong Duan",
                "Nianchen Deng",
                "Bin Fu",
                "Yinan He",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Junjun He",
                "Yingtong Xiong",
                "Han Lv",
                "Lijun Wu",
                "Wenqi Shao",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Biqing Qi",
                "Jiaye Ge",
                "Qipeng Guo",
                "Wenwei Zhang",
                "Wanli Ouyang",
                "Limin Wang",
                "Min Dou",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Jifeng Dai",
                "Bowen Zhou",
                "Weijie Su",
                "Kai Chen",
                "Yu Qiao",
                "Wenhai Wang",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18265.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "InternVL 3.5: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "InternVL 3.5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Cascade Reinforcement Learning (Cascade RL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Visual Resolution Router (ViR), Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Reasoning with InternVL 3.5",
                    "desc": "InternVL 3.5 is a new set of open-source multimodal models that improve reasoning, efficiency, and performance. It introduces Cascade Reinforcement Learning (Cascade RL), which uses a two-stage training process to enhance reasoning capabilities. The Visual Resolution Router (ViR) optimizes the resolution of visual inputs dynamically, while the Decoupled Vision-Language Deployment (DvD) strategy balances the computational load across GPUs. These innovations lead to significant performance gains in reasoning tasks and faster inference speeds compared to previous models."
                },
                "zh": {
                    "title": "æå‡æ¨ç†ä¸æ•ˆç‡çš„å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "InternVL 3.5 æ˜¯ä¸€æ¬¾æ–°å‹çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å…¶å¤šæ ·æ€§ã€æ¨ç†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯çº§è”å¼ºåŒ–å­¦ä¹ ï¼ˆCascade RLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ä¼˜åŒ–æ•ˆç‡ï¼Œæå‡ºäº†è§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼ˆViRï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¸å½±å“æ€§èƒ½ã€‚ç»“åˆè§£è€¦è§†è§‰-è¯­è¨€éƒ¨ç½²ï¼ˆDvDï¼‰ç­–ç•¥ï¼ŒInternVL 3.5 åœ¨æ¨ç†æ€§èƒ½ä¸Šæå‡äº† 16.0%ï¼Œå¹¶å®ç°äº† 4.05 å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17580",
            "title": "UQ: Assessing Language Models on Unsolved Questions",
            "url": "https://huggingface.co/papers/2508.17580",
            "abstract": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.",
            "score": 3,
            "issue_id": 5539,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "9f8a8ba45f2a8eca",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#survey",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "UQ: ĞÑ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "UQ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Stack Exchange, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ğ½Ñ‚Ğ°ÑÑ‚Ğ¸ĞºĞ¸. UQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "UQ: Evaluating AI on Real-World Unsolved Questions",
                    "desc": "The paper introduces UQ, a new benchmark for evaluating AI models on unsolved questions, which combines difficulty and realism to better assess capabilities like reasoning and factuality. Unlike traditional benchmarks that often present artificially difficult questions, UQ focuses on real-world challenges that arise from genuine human inquiries. The UQ framework includes a dataset of 500 diverse questions, validation strategies, and a collaborative platform for community verification. This innovative approach aims to push the boundaries of AI performance by addressing open-ended challenges that reflect actual knowledge gaps."
                },
                "zh": {
                    "title": "UQï¼šè¯„ä¼°AIæ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "UQæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æœªè§£å†³é—®é¢˜ä¸Šçš„åŸºå‡†ï¼Œç»“åˆäº†éš¾åº¦å’Œç°å®æ€§ï¼Œä»¥è¯„ä¼°æ¨ç†ã€äº‹å®æ€§å’Œæµè§ˆç­‰èƒ½åŠ›ã€‚å½“å‰çš„åŸºå‡†å¾€å¾€é¢ä¸´éš¾åº¦ä¸ç°å®æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œè€ŒUQé€šè¿‡è¯„ä¼°æœªè§£å†³çš„é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°èŒƒå¼ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«500ä¸ªæ¥è‡ªStack Exchangeçš„æŒ‘æˆ˜æ€§é—®é¢˜çš„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†éªŒè¯è€…è¾…åŠ©ç­›é€‰å’Œç¤¾åŒºéªŒè¯çš„æ–¹æ³•ã€‚UQä¸ºè¯„ä¼°å‰æ²¿æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°æä¾›äº†æ–°çš„è·¯å¾„ï¼Œæ¨åŠ¨äººç±»çŸ¥è¯†çš„å‰æ²¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18032",
            "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2508.18032",
            "abstract": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.",
            "score": 1,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "19b061daf44e7f58",
            "authors": [
                "Yaqi Li",
                "Peng Chen",
                "Mingyang Han",
                "Bu Pi",
                "Haoxiang Shi",
                "Runzhou Zhao",
                "Yang Yao",
                "Xuan Zhang",
                "Jun Song"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18032.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Visual-Chain of Guidance (Visual-CoG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Stage-Aware Rewards for Enhanced Image Generation",
                    "desc": "The Visual-Chain of Guidance (Visual-CoG) paradigm improves text-to-image generation by introducing stage-aware rewards that enhance the model's performance. Traditional models often provide feedback only at the end of the generation process, making it hard to understand which parts of the process are effective. Visual-CoG breaks down the generation into three stages: semantic reasoning, process refining, and outcome evaluation, allowing for immediate feedback at each stage. This approach leads to significant performance improvements across various benchmarks, demonstrating its effectiveness in handling complex prompts."
                },
                "zh": {
                    "title": "è§†è§‰å¼•å¯¼é“¾ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰å¼•å¯¼é“¾ï¼ˆVisual-CoGï¼‰çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡æä¾›é˜¶æ®µæ€§å¥–åŠ±ï¼Œæ”¹å–„äº†æ¨¡å‹åœ¨å¤„ç†å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºæ—¶çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„ä»…åœ¨ç”Ÿæˆç»“æŸæ—¶æä¾›å¥–åŠ±çš„æ–¹å¼ä¸åŒï¼ŒVisual-CoGåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¯ä¸ªé˜¶æ®µéƒ½ç»™äºˆå³æ—¶æŒ‡å¯¼ï¼Œä»è€Œä¼˜åŒ–ç”Ÿæˆç­–ç•¥ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼ŒVisual-CoGæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17188",
            "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
            "url": "https://huggingface.co/papers/2508.17188",
            "abstract": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.",
            "score": 1,
            "issue_id": 5539,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "fb29b368892da03a",
            "authors": [
                "Zhilin Zhang",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "New York University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17188.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#agi",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PosterGen: Ğ˜Ğ˜-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²",
                    "desc": "PosterGen - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¼Ğ°ĞºĞµÑ‚, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚ĞµÑ€. PosterGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ñ‹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM)."
                },
                "en": {
                    "title": "Automating Academic Posters with PosterGen: Design Meets Efficiency",
                    "desc": "PosterGen is a multi-agent framework that automates the process of creating academic posters from research papers using large language models. It consists of four specialized agents that work together: Parser and Curator agents extract and organize content, the Layout agent arranges this content spatially, the Stylist agents enhance the visual design, and the Renderer composes the final poster. This system not only ensures that the posters are semantically accurate but also visually appealing, addressing the shortcomings of previous automation methods. The framework is evaluated using a vision-language model to ensure high design quality, demonstrating superior performance in generating ready-to-present posters with minimal manual adjustments."
                },
                "zh": {
                    "title": "PosterGenï¼šè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å­¦æœ¯æµ·æŠ¥çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "PosterGenæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å­¦æœ¯æµ·æŠ¥ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å››ä¸ªåä½œçš„ä¸“é—¨ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£å†…å®¹æå–ã€å¸ƒå±€è®¾è®¡ã€è§†è§‰é£æ ¼åº”ç”¨å’Œæœ€ç»ˆæµ·æŠ¥åˆæˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPosterGenåœ¨è®¾è®¡ç¾å­¦å’Œå†…å®¹å‡†ç¡®æ€§ä¸Šè¡¨ç°æ›´ä½³ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡ ä¹æ— éœ€äººå·¥ä¿®æ”¹çš„æµ·æŠ¥ã€‚é€šè¿‡å¼•å…¥è§†è§‰-è¯­è¨€æ¨¡å‹è¯„ä¼°è®¾è®¡è´¨é‡ï¼ŒPosterGenç¡®ä¿äº†æµ·æŠ¥çš„å¯è¯»æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18190",
            "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
            "url": "https://huggingface.co/papers/2508.18190",
            "abstract": "ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "e7ede69164051787",
            "authors": [
                "Zirui Tang",
                "Boyu Niu",
                "Xuanhe Zhou",
                "Boxiu Li",
                "Wei Zhou",
                "Jiannan Wang",
                "Guoliang Li",
                "Xinyi Zhang",
                "Fan Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai Jiao Tong University",
                "Simon Fraser University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18190.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#data",
                    "#interpretability"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ST-Raptor: Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "ST-Raptor - ÑÑ‚Ğ¾ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞÑ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ”ĞµÑ€ĞµĞ²Ğ¾ (HO-Tree) Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ñ€ĞµĞ²ĞµÑĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ST-Raptor Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸: Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ST-Raptor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾ 20% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "ST-Raptor: Revolutionizing Table Question Answering with Hierarchical Trees",
                    "desc": "ST-Raptor is a novel framework designed to enhance question answering from semi-structured tables using large language models (LLMs). It introduces a Hierarchical Orthogonal Tree (HO-Tree) to effectively represent complex table layouts, allowing for better interpretation of the data. The framework employs a two-stage verification mechanism to ensure the accuracy of answers by validating execution steps and reconstructing queries. Experimental results demonstrate that ST-Raptor significantly improves answer accuracy compared to existing methods, making it a valuable tool for automating table-based question answering."
                },
                "zh": {
                    "title": "ST-Raptorï¼šåŠç»“æ„åŒ–è¡¨æ ¼é—®ç­”çš„æ–°çªç ´",
                    "desc": "ST-Raptoræ˜¯ä¸€ä¸ªåŸºäºæ ‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»åŠç»“æ„åŒ–è¡¨æ ¼ä¸­å›ç­”é—®é¢˜çš„æŒ‘æˆ˜ã€‚å®ƒå¼•å…¥äº†å±‚æ¬¡æ­£äº¤æ ‘ï¼ˆHO-Treeï¼‰æ¥æ•æ‰å¤æ‚çš„è¡¨æ ¼å¸ƒå±€ï¼Œå¹¶é€šè¿‡åŸºæœ¬æ ‘æ“ä½œæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡Œå¸¸è§çš„é—®ç­”ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨äº†ä¸¤é˜¶æ®µéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿æ‰§è¡Œæ­¥éª¤çš„æ­£ç¡®æ€§å’Œç­”æ¡ˆçš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒST-Raptoråœ¨ç­”æ¡ˆå‡†ç¡®æ€§ä¸Šæ¯”ä¹ä¸ªåŸºçº¿æ–¹æ³•æé«˜äº†å¤šè¾¾20%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18159",
            "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
            "url": "https://huggingface.co/papers/2508.18159",
            "abstract": "SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visually-guided image editing, where edits are conditioned on both visual cues and textual prompts, has emerged as a powerful paradigm for fine-grained, controllable content generation. Although recent generative models have shown remarkable capabilities, existing evaluations remain simple and insufficiently representative of real-world editing challenges. We present SpotEdit, a comprehensive benchmark designed to systematically assess visually-guided image editing methods across diverse diffusion, autoregressive, and hybrid generative models, uncovering substantial performance disparities. To address a critical yet underexplored challenge, our benchmark includes a dedicated component on hallucination, highlighting how leading models, such as GPT-4o, often hallucinate the existence of a visual cue and erroneously perform the editing task. Our code and benchmark are publicly released at https://github.com/SaraGhazanfari/SpotEdit.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "7e9e7873459fbab7",
            "authors": [
                "Sara Ghazanfari",
                "Wei-An Lin",
                "Haitong Tian",
                "Ersin Yumer"
            ],
            "affiliations": [
                "Adobe Inc.",
                "New York University, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18159.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#hallucinations",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "SpotEdit: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "SpotEdit â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. ĞĞ½ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. SpotEdit Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "SpotEdit: A New Standard for Evaluating Image Editing Models",
                    "desc": "SpotEdit is a new benchmark created to evaluate how well different image editing methods work when guided by visual cues and text prompts. It highlights the differences in performance among various generative models, including diffusion, autoregressive, and hybrid types. The benchmark also focuses on a significant issue called hallucination, where models mistakenly believe a visual cue exists and make incorrect edits. By providing a more thorough evaluation framework, SpotEdit aims to improve the understanding and development of visually-guided image editing techniques."
                },
                "zh": {
                    "title": "SpotEditï¼šè¯„ä¼°è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘çš„åŸºå‡†",
                    "desc": "SpotEditæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘æ–¹æ³•çš„åŸºå‡†ï¼Œæ­ç¤ºäº†æ‰©æ•£ã€è‡ªåŠ¨å›å½’å’Œæ··åˆç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å’Œå¹»è§‰é—®é¢˜ã€‚è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘ç»“åˆäº†è§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æç¤ºï¼Œæˆä¸ºä¸€ç§å¼ºå¤§çš„ç»†ç²’åº¦å¯æ§å†…å®¹ç”Ÿæˆæ–¹å¼ã€‚å°½ç®¡æœ€è¿‘çš„ç”Ÿæˆæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¿‡äºç®€å•ï¼Œæ— æ³•å……åˆ†ä»£è¡¨ç°å®ä¸–ç•Œçš„ç¼–è¾‘æŒ‘æˆ˜ã€‚SpotEditåŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨å¹»è§‰é—®é¢˜ï¼ŒæŒ‡å‡ºä¸€äº›é¢†å…ˆæ¨¡å‹åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­å¸¸å¸¸é”™è¯¯åœ°å‡è®¾å­˜åœ¨è§†è§‰çº¿ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18076",
            "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
            "url": "https://huggingface.co/papers/2508.18076",
            "abstract": "The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "0e751250907ca130",
            "authors": [
                "Khaoula Chehbouni",
                "Mohammed Haddou",
                "Jackie Chi Kit Cheung",
                "Golnoosh Farnadi"
            ],
            "affiliations": [
                "McGill University",
                "Mila - Quebec AI Institute",
                "Statistics Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18076.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#alignment",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑÑƒĞ´ÑŒĞ¸: Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ½Ñ‚ÑƒĞ·Ğ¸Ğ°Ğ·Ğ¼?",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ñ… Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM ĞºĞ°Ğº ÑÑƒĞ´ĞµĞ¹: ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Rethinking Large Language Models as Evaluators in NLG",
                    "desc": "This paper critiques the use of large language models (LLMs) as judges for evaluating natural language generation (NLG) systems. It questions their reliability, capabilities, scalability, and cost-effectiveness, suggesting that the excitement around using LLMs as evaluators may be premature. The authors analyze four key assumptions about LLMs acting as proxies for human judgment and their effectiveness in evaluation tasks. They call for more responsible evaluation practices to ensure that the integration of LLMs in NLG evaluation supports meaningful progress in the field."
                },
                "zh": {
                    "title": "å®¡æ…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç”Ÿæˆç³»ç»Ÿ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ‰¹è¯„äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆç³»ç»Ÿçš„è¯„åˆ¤è€…çš„åšæ³•ï¼Œè´¨ç–‘å…¶å¯é æ€§ã€èƒ½åŠ›ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ä½œè€…è®¤ä¸ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLJsï¼‰è¢«è§†ä¸ºä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡çš„æœ‰å¸Œæœ›æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚è®ºæ–‡åˆ†æäº†LLJsä½¿ç”¨çš„å››ä¸ªæ ¸å¿ƒå‡è®¾ï¼ŒåŒ…æ‹¬å…¶ä½œä¸ºäººç±»åˆ¤æ–­ä»£ç†çš„èƒ½åŠ›ã€è¯„ä¼°èƒ½åŠ›ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œå¹¶æŒ‡å‡ºè¿™äº›å‡è®¾å¯èƒ½å—åˆ°LLMsçš„å›ºæœ‰é™åˆ¶çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒéœ€è¦æ›´è´Ÿè´£ä»»çš„è¯„ä¼°å®è·µï¼Œä»¥ç¡®ä¿LLJsåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä½œç”¨èƒ½å¤Ÿä¿ƒè¿›è€Œä¸æ˜¯é˜»ç¢è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16790",
            "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
            "url": "https://huggingface.co/papers/2508.16790",
            "abstract": "TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 1) dependence on multi-layer residual vector quantization structures or high frame rates, 2) reliance on auxiliary pre-trained models for semantic distillation, and 3) requirements for complex two-stage training processes. In this work, we introduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a novel approach designed to overcome these challenges. TaDiCodec employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression. TaDiCodec achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS). Notably, TaDiCodec employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models. We also validate the compatibility of TaDiCodec in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small reconstruction-generation gap. We will open source our code and model checkpoints. Audio samples are are available at https:/tadicodec.github.io/. We release code and model checkpoints at https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "566e852623f03412",
            "authors": [
                "Yuancheng Wang",
                "Dekun Chen",
                "Xueyao Zhang",
                "Junan Zhang",
                "Jiaqi Li",
                "Zhizheng Wu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16790.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "TaDiCodec: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "TaDiCodec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² (6,25 Ğ“Ñ†) Ğ¸ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ° (0,0875 ĞºĞ±Ğ¸Ñ‚/Ñ) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. TaDiCodec Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, ĞºĞ°Ğº Word Error Rate, ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Speech Generation with TaDiCodec",
                    "desc": "TaDiCodec is a new speech codec that uses a Text-aware Diffusion Transformer to improve speech generation while keeping low frame rates and bitrates. It addresses limitations of existing speech tokenizers by using end-to-end optimization for quantization and reconstruction, eliminating the need for complex training processes and auxiliary models. The codec achieves a low frame rate of 6.25 Hz and a bitrate of 0.0875 kbps, while still performing well on important metrics like Word Error Rate and speech quality. Additionally, TaDiCodec supports zero-shot text-to-speech applications, showcasing its versatility in speech language modeling."
                },
                "zh": {
                    "title": "æ–‡æœ¬æ„ŸçŸ¥çš„é«˜æ•ˆè¯­éŸ³ç¼–è§£ç å™¨",
                    "desc": "TaDiCodecæ˜¯ä¸€ç§æ–‡æœ¬æ„ŸçŸ¥çš„æ‰©æ•£å˜æ¢å™¨è¯­éŸ³ç¼–è§£ç å™¨ï¼Œæ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å’Œæ–‡æœ¬æŒ‡å¯¼æ¥å®ç°ä½å¸§ç‡å’Œä½æ¯”ç‰¹ç‡çš„ä¼˜è¶Šè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚è¯¥æ¨¡å‹å…‹æœäº†ç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨å¤šå±‚æ®‹å·®å‘é‡é‡åŒ–ã€ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹å’Œå¤æ‚çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚TaDiCodecé‡‡ç”¨æ‰©æ•£è‡ªç¼–ç å™¨è¿›è¡Œé‡åŒ–å’Œé‡å»ºï¼Œå¹¶é€šè¿‡æ‰©æ•£è§£ç å™¨é›†æˆæ–‡æœ¬æŒ‡å¯¼ï¼Œä»¥æé«˜é‡å»ºè´¨é‡å’Œå‹ç¼©æ•ˆç‡ã€‚æœ€ç»ˆï¼ŒTaDiCodecåœ¨è¯­éŸ³ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å®ç°äº†æä½çš„å¸§ç‡å’Œæ¯”ç‰¹ç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-25.html",
    "link_next": "2025-08-27.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}