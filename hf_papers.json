{
    "date": {
        "ru": "27 августа",
        "en": "August 27",
        "zh": "8月27日"
    },
    "time_utc": "2025-08-27 02:22",
    "weekday": 2,
    "issue_id": 5562,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.19247",
            "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
            "url": "https://huggingface.co/papers/2508.19247",
            "abstract": "VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.",
            "score": 3,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "1b1c0dd833778383",
            "authors": [
                "Lin Li",
                "Zehuan Huang",
                "Haoran Feng",
                "Gengxiong Zhuang",
                "Rui Chen",
                "Chunchao Guo",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19247.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🔨",
                "ru": {
                    "title": "Точное 3D-редактирование без компромиссов",
                    "desc": "VoxHammer - это метод редактирования 3D-моделей в латентном пространстве без дополнительного обучения. Он обеспечивает точное и согласованное редактирование указанных областей, сохраняя при этом неизмененные части модели. VoxHammer предсказывает траекторию инверсии 3D-модели и использует инвертированные латентные представления и токены для сохранения контекстуальных особенностей. Эксперименты показывают, что метод превосходит существующие подходы по качеству и согласованности результатов редактирования."
                },
                "en": {
                    "title": "VoxHammer: Precision and Coherence in 3D Latent Space Editing",
                    "desc": "VoxHammer is a novel method for editing 3D models in latent space without the need for training. It focuses on maintaining the consistency of unedited regions while ensuring high-quality results in the edited areas. By predicting an inversion trajectory and utilizing key-value tokens, VoxHammer effectively integrates changes while preserving contextual features. This approach outperforms existing techniques, making it valuable for applications in the gaming industry and robotics."
                },
                "zh": {
                    "title": "VoxHammer：无训练的精确3D编辑新方法",
                    "desc": "VoxHammer是一种无需训练的方法，能够在潜在空间中进行精确且连贯的3D编辑。该方法确保了保留区域的一致性和整体结果的高质量。与传统方法不同，VoxHammer直接在3D潜在空间中进行编辑，避免了多视图图像渲染和重建模型的复杂性。实验结果表明，VoxHammer在保留区域的3D一致性和整体质量方面显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19205",
            "title": "VibeVoice Technical Report",
            "url": "https://huggingface.co/papers/2508.19205",
            "abstract": "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.",
            "score": 1,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "fc5dc3d2ea656b66",
            "authors": [
                "Zhiliang Peng",
                "Jianwei Yu",
                "Wenhui Wang",
                "Yaoyao Chang",
                "Yutao Sun",
                "Li Dong",
                "Yi Zhu",
                "Weijiang Xu",
                "Hangbo Bao",
                "Zehua Wang",
                "Shaohan Huang",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19205.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "VibeVoice: революция в синтезе длительной многоголосой речи",
                    "desc": "VibeVoice - это новая модель для синтеза длительной многоголосой речи, использующая диффузию следующего токена и эффективный непрерывный токенизатор речи. Модель способна генерировать до 90 минут речи с участием до 4 говорящих, сохраняя естественность разговора. По сравнению с популярной моделью Encodec, токенизатор VibeVoice улучшает сжатие данных в 80 раз при сопоставимом качестве. VibeVoice превосходит как открытые, так и проприетарные модели диалогов по качеству и достоверности синтезированной речи."
                },
                "en": {
                    "title": "VibeVoice: Revolutionizing Multi-Speaker Speech Synthesis",
                    "desc": "VibeVoice is a new model that creates long speeches with multiple speakers using a technique called next-token diffusion. This method generates speech by predicting the next part of the audio in a smart way, making it efficient for continuous data. The model also introduces a special speech tokenizer that compresses data much better than existing models, allowing it to maintain high audio quality while being faster. With VibeVoice, users can generate up to 90 minutes of realistic multi-speaker conversations, outperforming other dialogue systems."
                },
                "zh": {
                    "title": "VibeVoice：高效合成多说话人长语音的创新模型",
                    "desc": "VibeVoice是一种新型模型，旨在合成长时间的多说话人语音。它采用了下一步扩散的方法，通过自回归生成潜在向量来建模连续数据。为了实现这一点，我们引入了一种新型的连续语音标记器，与流行的Encodec模型相比，数据压缩提高了80倍，同时保持了相似的性能。VibeVoice能够合成最长可达90分钟的语音，捕捉真实的对话氛围，超越了开源和专有的对话模型。"
                }
            }
        }
    ],
    "link_prev": "2025-08-26.html",
    "link_next": "2025-08-28.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "26.08",
        "en": "08/26",
        "zh": "8月26日"
    },
    "short_date_next": {
        "ru": "28.08",
        "en": "08/28",
        "zh": "8月28日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}