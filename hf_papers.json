{
    "date": {
        "ru": "4 апреля",
        "en": "April 4",
        "zh": "4月4日"
    },
    "time_utc": "2025-04-04 03:26",
    "weekday": 4,
    "issue_id": 3064,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 7,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прозрачное обучение с подкреплением для визуально-языковых моделей",
                    "desc": "Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "建立可重复的强化学习框架",
                    "desc": "强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 3,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Контролируемая генерация видео из отдельных элементов",
                    "desc": "SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2：可控视频生成的新突破",
                    "desc": "本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02826",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "url": "https://huggingface.co/papers/2504.02826",
            "abstract": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.",
            "score": 2,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "dbb1c07cd5a01838",
            "authors": [
                "Xiangyu Zhao",
                "Peiyuan Zhang",
                "Kexian Tang",
                "Hao Li",
                "Zicheng Zhang",
                "Guangtao Zhai",
                "Junchi Yan",
                "Hua Yang",
                "Xue Yang",
                "Haodong Duan"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02826.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RISEBench: Новый рубеж в оценке визуального редактирования с рассуждениями",
                    "desc": "RISEBench - это новый эталонный тест для оценки визуального редактирования с учетом рассуждений в мультимодальных моделях. Он фокусируется на четырех типах рассуждений: временном, причинно-следственном, пространственном и логическом. Тест оценивает понимание инструкций, сохранение внешнего вида и визуальную правдоподобность с помощью как человеческих оценщиков, так и LLM-судей. Эксперименты показали, что даже современные модели, такие как GPT-4, испытывают трудности с задачами логического рассуждения."
                },
                "en": {
                    "title": "RISEBench: Advancing Reasoning in Visual Editing",
                    "desc": "This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area."
                },
                "zh": {
                    "title": "推理驱动的视觉编辑新基准",
                    "desc": "大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑中仍面临挑战，尤其是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一问题，我们引入了RISEBench，这是第一个用于评估推理驱动视觉编辑（RISE）的基准。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们的实验表明，尽管GPT-4o-Native在性能上显著优于其他模型，但在逻辑推理任务上仍然存在困难，显示出这一领域仍需深入探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02542",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "url": "https://huggingface.co/papers/2504.02542",
            "abstract": "Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.",
            "score": 2,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "fa93ea3aeacd0dbc",
            "authors": [
                "Fa-Ting Hong",
                "Zunnan Xu",
                "Zixiang Zhou",
                "Jun Zhou",
                "Xiu Li",
                "Qin Lin",
                "Qinglin Lu",
                "Dan Xu"
            ],
            "affiliations": [
                "HKUST",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02542.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Гибкий синтез говорящей головы с множественным контролем",
                    "desc": "ACTalker - это новая модель для синтеза видео с говорящей головой, использующая диффузионный подход. Она поддерживает как мультимодальное, так и одномодальное управление генерацией видео. Модель использует параллельную структуру mamba с несколькими ветвями для обработки различных управляющих сигналов. ACTalker применяет механизм маскирования для предотвращения конфликтов между разными модальностями управления."
                },
                "en": {
                    "title": "ACTalker: Multi-Signal Control for Natural Talking Head Synthesis",
                    "desc": "This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs."
                },
                "zh": {
                    "title": "多信号控制的对话头像生成新方法",
                    "desc": "本文介绍了一种名为ACTalker的端到端视频扩散框架，旨在生成虚拟头像的对话视频。该方法支持多信号和单信号控制，克服了现有方法的局限性。通过设计并行的mamba结构，允许不同的驱动信号控制面部的特定区域，并使用门控机制实现灵活控制。实验结果表明，ACTalker能够生成自然的面部视频，并且能够无冲突地整合多种驱动信号。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02782",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.02782",
            "abstract": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.",
            "score": 1,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "5346697bd326eed4",
            "authors": [
                "Zhiyuan Yan",
                "Junyan Ye",
                "Weijia Li",
                "Zilong Huang",
                "Shenghai Yuan",
                "Xiangyang He",
                "Kaiqing Lin",
                "Jun He",
                "Conghui He",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "Shanghai AI Laboratory",
                "Shenzhen University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#diffusion",
                    "#interpretability",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GPT-4o: Новый рубеж в генерации и редактировании изображений с помощью ИИ",
                    "desc": "Статья представляет первый оценочный бенчмарк (GPT-ImgEval) для модели GPT-4o от OpenAI, анализирующий ее способности в генерации и редактировании изображений. Исследование оценивает качество генерации, мастерство редактирования и семантический синтез на основе мировых знаний, демонстрируя превосходство GPT-4o над существующими методами. Авторы также предполагают, что архитектура GPT-4o включает авторегрессионную модель в сочетании с диффузионной головкой для декодирования изображений. Кроме того, статья анализирует ограничения GPT-4o, сравнивает ее с Gemini 2.0 Flash и обсуждает вопросы безопасности, связанные с выходными данными модели."
                },
                "en": {
                    "title": "Unleashing the Power of GPT-4o in Image Generation and Editing",
                    "desc": "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."
                },
                "zh": {
                    "title": "GPT-4o：图像生成与编辑的新突破",
                    "desc": "本论文介绍了OpenAI的GPT-4o模型在图像生成和编辑方面的最新突破。我们提出了一个名为GPT-ImgEval的评估基准，定量和定性地分析了GPT-4o在生成质量、编辑能力和知识推理等三个关键维度的表现。研究表明，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，我们还探讨了GPT-4o的架构，并识别了其在图像生成中常见的合成伪影和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 1,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов",
                    "desc": "Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "利用大型语言模型优化时间序列预测的模型选择",
                    "desc": "本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "pinyin": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de túxiàng shēngchéng móxíng, jiàozuò MergeVQ. Tā jiēhé le xiàngliàng liànggéhuà hé token hébìng jìshù, zhǐyú jiějué túxiàng shēngchéng zhìliàng hé biǎoshì xuéxí xiàoyì zhījiān de pínghéng wèntí. MergeVQ zài yùxùnliàn jiēduàn tōngguò token hébìng mókuài tíqǔ yǔyì xìnxī, bìng zài jiěmǎ jiēduàn huīfù xìjiè. Shíyàn jiéguǒ xiǎnshì, MergeVQ zài túxiàng shēngchéng hé biǎoshì xuéxí rènwù zhōng dōu biǎoxiàn chūsè, qiě xiàoyì gāo. Dàimǎ hé móxíng jiāng zài wǎngshàng gōngkāi.",
        "vocab": "[\n    {\"word\": \"向量量化\", \"pinyin\": \"xiàngliàng liàngzhì\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"tōukèn\", \"trans\": \"token\"},\n    {\"word\": \"合并\", \"pinyin\": \"hébìng\", \"trans\": \"merge\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"平衡\", \"pinyin\": \"pínghéng\", \"trans\": \"balance\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"解码\", \"pinyin\": \"jiěmǎ\", \"trans\": \"decode\"},\n    {\"word\": \"恢复\", \"pinyin\": \"huīfù\", \"trans\": \"recover\"},\n    {\"word\": \"细节\", \"pinyin\": \"xìjié\", \"trans\": \"detail\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōngkāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.",
        "update_ts": "2025-04-03 09:11"
    }
}