{
    "date": {
        "ru": "15 апреля",
        "en": "April 15",
        "zh": "4月15日"
    },
    "time_utc": "2025-04-15 02:25",
    "weekday": 1,
    "issue_id": 3236,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 5,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях",
                    "desc": "FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель использует текстово-управляемое унифицированное кодирование изображений и контекстно-зависимое рекурсивное декодирование для глубокой интеграции модальностей. Авторы разработали специальную функцию потерь и синтетический набор данных для оптимизации процесса обучения. FUSION превосходит существующие методы, демонстрируя эффективность подхода полной интеграции модальностей."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSION：深度集成的多模态语言模型",
                    "desc": "我们介绍了FUSION，这是一种多模态大型语言模型（MLLM），采用完全的视觉-语言对齐和集成范式。与现有方法主要依赖于LLM解码过程中的后期模态交互不同，我们的方法在整个处理流程中实现了深度、动态的集成。我们提出了文本引导的统一视觉编码，将文本信息融入视觉编码，实现像素级的集成。此外，我们设计了上下文感知的递归对齐解码，能够在解码过程中根据文本上下文递归聚合视觉特征，从而实现细粒度的问题级语义集成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 3,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное постобучение LLM с учетом разнородности данных",
                    "desc": "Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают адаптивную стратегию обучения, учитывающую разнородность обучающих данных по сложности и источникам. Ключевая идея заключается в использовании величины преимущества политики для оценки пользы дальнейшего обучения на конкретном распределении данных. Метод применяет принцип Upper Confidence Bound для динамической корректировки вероятностей выборки из разных распределений, что позволяет оптимизировать процесс обучения."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "优化学习效率的分布级课程学习框架",
                    "desc": "本文提出了一种基于分布级学习能力的课程学习框架，旨在优化强化学习（RL）后训练的大型语言模型（LLM）。现有方法通常将训练数据视为统一整体，忽视了数据分布的多样性和复杂性。我们的方法通过动态调整不同分布的采样概率，优先考虑高平均优势或低样本数量的分布，从而提高学习效率。实验结果表明，该框架在逻辑推理数据集上显著提高了收敛速度和最终性能，展示了分布感知课程策略的价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 3,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации",
                    "desc": "Исследование оценивает способности мультимодальной модели GPT-4o от OpenAI в области семантического синтеза с использованием мировых знаний. Авторы анализируют три ключевых аспекта: глобальное следование инструкциям, точность детального редактирования и постгенерационное рассуждение. Результаты показывают, что модель часто интерпретирует инструкции буквально, непоследовательно применяет ограничения, основанные на знаниях, и испытывает трудности с задачами условного рассуждения. Исследование призывает к разработке более надежных методов оценки и стратегий обучения для улучшения мультимодальной генерации."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "提升多模态生成的上下文理解与推理能力",
                    "desc": "本研究评估了OpenAI的多模态模型GPT-4o在图像生成和编辑方面的能力，特别关注其在全球指令遵循、精细编辑精度和生成后推理三个维度的表现。尽管现有基准显示GPT-4o在图像处理上表现强劲，但我们的评估揭示了其在指令理解和知识应用上的局限性。模型常常对指令进行字面解释，知识约束应用不一致，并且在条件推理任务中表现不佳。这些发现挑战了对GPT-4o统一理解和生成能力的普遍假设，强调了需要更强大的基准和训练策略，以实现更具上下文意识和推理基础的多模态生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 1,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "Продвинутые языковые модели теперь доступны на домашних устройствах",
                    "desc": "Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковые модели (до 70 миллиардов параметров) на обычных домашних устройствах. Система использует комбинацию CPU/GPU, низкие требования к RAM/VRAM и поддержку Wi-Fi для эффективной работы. Prima.cpp применяет технологию mmap для управления весами модели и вводит конвейерный кольцевой параллелизм с предвыборкой для скрытия загрузки с диска. Авторы предлагают алгоритм Halda для оптимального распределения слоев модели между устройствами, учитывая их гетерогенность."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "让家庭设备也能运行大型语言模型",
                    "desc": "本文介绍了一种名为prima.cpp的分布式推理系统，能够在普通家庭设备上运行70B规模的语言模型。该系统通过混合使用CPU和GPU，优化内存和带宽的使用，解决了传统方案对高性能硬件的依赖。它采用了mmap管理模型权重，并引入了管道环并行和预取技术，以减少磁盘加载时间。通过优化计算、通信和内存管理，prima.cpp显著降低了延迟，使得先进的AI模型能够在家庭助手中普及。"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇技术报告介绍了训练视频生成基础模型的成本效益策略。报告展示了一个名为 Seaweed-7B 的中型研究模型，拥有约70亿参数，使用665,000个H100 GPU小时从头训练。尽管使用的计算资源适中，Seaweed-7B 仍表现出与更大模型相媲美的性能。设计选择在资源受限的环境中尤为重要。报告强调了提升中型扩散模型性能的关键设计决策。经验上，有两个观察结果：(1) Seaweed-7B 的性能与或超过使用更多GPU资源训练的大型模型；(2) 该模型展示出强大的泛化能力，可通过轻量微调或继续训练有效适应多种下游应用。项目页面见 https://seaweed.video/。",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "pinyin": "Zhè piān jìshù bàogào jièshào le xùnliàn shìpǐn shēngchéng jīchǔ móxíng de chéngběn xiàoyì cèlüè. Bàogào zhǎnshì le yīgè míngwèi Seaweed-7B de zhōngxíng yánjiū móxíng, yǒngyǒu yuē 70 yì cānshù, shǐyòng 665,000 gè H100 GPU xiǎoshí cóngtóu xùnliàn. Jīnrán shǐyòng de jìsuàn zīyuán shìguì zhōngděng, Seaweed-7B réng biǎoxiàn chū yǔ gèng dà móxíng xiàng bǐměi de xíngnéng. Shèjì xuǎnzé zài zīyuán shòuxiàn de huánjìng zhōng yóuwèi zhòngyào. Bàogào qiángdiǎo le tíshēng zhōngxíng kuòsàn móxíng xíngnéng de guǎnjiàn shèjì juécè. Jīngyàn shàng, yǒu liǎng gè guānchá jiéguǒ: (1) Seaweed-7B de xíngnéng yǔ huò chāoguò shǐyòng gèng duō GPU zīyuán xùnliàn de dàxíng móxíng; (2) gè móxíng zhǎnshì chū qiángdà de fànhuà nénglì, kě tōngguò qīngliàng wēitiáo huò jìxù xùnliàn yǒuxiào shìyìng duōzhǒng xiàyóu yìngyòng. Xiàngmù yèmiàn jiàn https://seaweed.video/。",
        "vocab": "[\n    {\"word\": \"技术报告\", \"pinyin\": \"jìshù bàogào\", \"trans\": \"technical report\"},\n    {\"word\": \"视频生成\", \"pinyin\": \"shìpín shēngchéng\", \"trans\": \"video generation\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jīchǔ móxíng\", \"trans\": \"foundational model\"},\n    {\"word\": \"成本效益\", \"pinyin\": \"chéngběn xiàoyì\", \"trans\": \"cost-effectiveness\"},\n    {\"word\": \"策略\", \"pinyin\": \"cèlüè\", \"trans\": \"strategy\"},\n    {\"word\": \"中型\", \"pinyin\": \"zhōngxíng\", \"trans\": \"medium-sized\"},\n    {\"word\": \"研究模型\", \"pinyin\": \"yánjiū móxíng\", \"trans\": \"research model\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"从头训练\", \"pinyin\": \"cóngtóu xùnliàn\", \"trans\": \"train from scratch\"},\n    {\"word\": \"计算资源\", \"pinyin\": \"jìsuàn zīyuán\", \"trans\": \"computational resources\"},\n    {\"word\": \"适中\", \"pinyin\": \"shìzhōng\", \"trans\": \"moderate\"},\n    {\"word\": \"媲美\", \"pinyin\": \"pìměi\", \"trans\": \"rival\"},\n    {\"word\": \"设计选择\", \"pinyin\": \"shèjì xuǎnzé\", \"trans\": \"design choices\"},\n    {\"word\": \"受限\", \"pinyin\": \"shòuxiàn\", \"trans\": \"constrained\"},\n    {\"word\": \"环境\", \"pinyin\": \"huánjìng\", \"trans\": \"environment\"},\n    {\"word\": \"尤为\", \"pinyin\": \"yóuwéi\", \"trans\": \"especially\"},\n    {\"word\": \"重要\", \"pinyin\": \"zhòngyào\", \"trans\": \"important\"},\n    {\"word\": \"强调\", \"pinyin\": \"qiángdiào\", \"trans\": \"emphasize\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"enhance\"},\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuòsàn móxíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"性能\", \"pinyin\": \"xíngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"设计决策\", \"pinyin\": \"shèjì juécè\", \"trans\": \"design decisions\"},\n    {\"word\": \"经验\", \"pinyin\": \"jīngyàn\", \"trans\": \"experience\"},\n    {\"word\": \"观察结果\", \"pinyin\": \"guānchá jiéguǒ\", \"trans\": \"observation results\"},\n    {\"word\": \"泛化能力\", \"pinyin\": \"fànhuà nénglì\", \"trans\": \"generalization capability\"},\n    {\"word\": \"轻量微调\", \"pinyin\": \"qīngliàng wēitiáo\", \"trans\": \"lightweight fine-tuning\"},\n    {\"word\": \"继续训练\", \"pinyin\": \"jìxù xùnliàn\", \"trans\": \"continued training\"},\n    {\"word\": \"下游应用\", \"pinyin\": \"xiàyóu yìngyòng\", \"trans\": \"downstream applications\"},\n    {\"word\": \"项目页面\", \"pinyin\": \"xiàngmù yèmiàn\", \"trans\": \"project page\"}\n]",
        "trans": "This technical report introduces cost-effective strategies for training foundational video generation models. The report presents a mid-sized research model named Seaweed-7B, which has approximately 7 billion parameters and was trained from scratch using 665,000 H100 GPU hours. Despite the moderate computational resources used, Seaweed-7B demonstrates performance comparable to larger models. Design choices are particularly important in resource-constrained environments. The report emphasizes key design decisions that enhance the performance of mid-sized diffusion models. Empirically, there are two observations: (1) Seaweed-7B's performance matches or exceeds that of larger models trained with more GPU resources; (2) The model demonstrates strong generalization capabilities and can effectively adapt to various downstream applications through lightweight fine-tuning or continued training. For more information, visit the project page at https://seaweed.video/.",
        "update_ts": "2025-04-14 09:12"
    }
}