{
    "date": {
        "ru": "5 июня",
        "en": "June 5",
        "zh": "6月5日"
    },
    "time_utc": "2025-06-05 02:41",
    "weekday": 3,
    "issue_id": 4133,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.24500",
            "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
            "url": "https://huggingface.co/papers/2505.24500",
            "abstract": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.",
            "score": 9,
            "issue_id": 4133,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "ee580986393d0b7e",
            "authors": [
                "Guiyang Hou",
                "Xing Gao",
                "Yuchuan Wu",
                "Xiang Huang",
                "Wenqi Zhang",
                "Zhe Zheng",
                "Yongliang Shen",
                "Jialu Du",
                "Fei Huang",
                "Yongbin Li",
                "Weiming Lu"
            ],
            "affiliations": [
                "Nanjing University",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24500.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый метод обучения для повышения социального интеллекта языковых моделей",
                    "desc": "Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения социального интеллекта больших языковых моделей (LLM). Этот подход учитывает временную составляющую и иерархию когнитивных процессов, характерных для социальных взаимодействий. Метод TimeHC-RL показал превосходство над широко используемым методом обучения с подкреплением System 2 RL. Эксперименты продемонстрировали, что применение TimeHC-RL позволяет моделям с 7 миллиардами параметров достигать производительности передовых моделей в задачах социального интеллекта."
                },
                "en": {
                    "title": "Boosting LLMs' Social Intelligence with TimeHC-RL",
                    "desc": "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."
                },
                "zh": {
                    "title": "提升社交智能的时间感知强化学习",
                    "desc": "本文提出了一种新的方法，称为时间感知层次认知强化学习（TimeHC-RL），旨在提升大型语言模型（LLMs）在社交领域的智能。与数学等依赖系统2认知的领域不同，社交领域需要更丰富的认知模式，包括直觉反应和表层思维。通过对八个不同数据集的实验，我们验证了TimeHC-RL方法的有效性，结果显示其在社交智能方面优于传统的系统2强化学习方法。该方法使得7B基础模型的表现接近于更先进的模型，如DeepSeek-R1和OpenAI-O3。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03569",
            "title": "MiMo-VL Technical Report",
            "url": "https://huggingface.co/papers/2506.03569",
            "abstract": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
            "score": 8,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "cb568276c7e799cb",
            "authors": [
                "Xiaomi LLM-Core Team",
                ":",
                "Zihao Yue",
                "Zhenru Lin",
                "Yifan Song",
                "Weikun Wang",
                "Shuhuai Ren",
                "Shuhao Gu",
                "Shicheng Li",
                "Peidian Li",
                "Liang Zhao",
                "Lei Li",
                "Kainan Bao",
                "Hao Tian",
                "Hailin Zhang",
                "Gang Wang",
                "Dawei Zhu",
                "Cici",
                "Chenhong He",
                "Bowen Ye",
                "Bowen Shen",
                "Zihan Zhang",
                "Zihan Jiang",
                "Zhixian Zheng",
                "Zhichao Song",
                "Zhenbo Luo",
                "Yue Yu",
                "Yudong Wang",
                "Yuanyuan Tian",
                "Yu Tu",
                "Yihan Yan",
                "Yi Huang",
                "Xu Wang",
                "Xinzhe Xu",
                "Xingchen Song",
                "Xing Zhang",
                "Xing Yong",
                "Xin Zhang",
                "Xiangwei Deng",
                "Wenyu Yang",
                "Wenhan Ma",
                "Weiwei Lv",
                "Weiji Zhuang",
                "Wei Liu",
                "Sirui Deng",
                "Shuo Liu",
                "Shimao Chen",
                "Shihua Yu",
                "Shaohui Liu",
                "Shande Wang",
                "Rui Ma",
                "Qiantong Wang",
                "Peng Wang",
                "Nuo Chen",
                "Menghang Zhu",
                "Kangyang Zhou",
                "Kang Zhou",
                "Kai Fang",
                "Jun Shi",
                "Jinhao Dong",
                "Jiebao Xiao",
                "Jiaming Xu",
                "Huaqiu Liu",
                "Hongshen Xu",
                "Heng Qu",
                "Haochen Zhao",
                "Hanglong Lv",
                "Guoan Wang",
                "Duo Zhang",
                "Dong Zhang",
                "Di Zhang",
                "Chong Ma",
                "Chang Liu",
                "Can Cai",
                "Bingquan Xia"
            ],
            "affiliations": [
                "Xiaomi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03569.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты",
                    "desc": "Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT и MiMo-VL-7B-RL, демонстрирующие передовые результаты в задачах визуального понимания и мультимодальных рассуждений. Модель MiMo-VL-7B-RL превосходит Qwen2.5-VL-7B в 35 из 40 оцениваемых задач и достигает 59.4 баллов на бенчмарке OlympiadBench. Обучение моделей включало четырехэтапное предобучение на 2.4 триллионах токенов и применение смешанного обучения с подкреплением (MORL). Авторы подчеркивают важность включения качественных данных для рассуждений с длинной цепочкой мыслей в этапы предобучения."
                },
                "en": {
                    "title": "Revolutionizing Vision-Language Models with MiMo-VL",
                    "desc": "The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research."
                },
                "zh": {
                    "title": "开创视觉-语言模型的新标准",
                    "desc": "我们开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL，这两个强大的视觉-语言模型在一般视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在40个评估任务中有35个超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数高达78B的模型。在GUI定位应用中，它在OSWorld-G上以56.1的分数设定了新标准，甚至超越了专门模型UI-TARS。我们的训练结合了四阶段的预训练（24万亿个标记）和混合在线强化学习（MORL），并强调了在预训练阶段融入高质量推理数据和长链思维的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04180",
            "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
            "url": "https://huggingface.co/papers/2506.04180",
            "abstract": "Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.",
            "score": 4,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "3f52b337c5fa3683",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqiang Hu",
                "Juanzi Li",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04180.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#long_context",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#story_generation"
                ],
                "emoji": "✍️",
                "ru": {
                    "title": "Структурированное мышление для улучшения генерации длинных текстов",
                    "desc": "SuperWriter-Agent - это новая система для улучшения качества генерации длинных текстов с помощью больших языковых моделей (LLM). Она вводит этапы планирования и уточнения в процесс генерации, имитируя подход профессионального писателя. Авторы обучили 7B-параметровую модель SuperWriter-LM на специально созданном наборе данных и разработали иерархическую процедуру оптимизации предпочтений (DPO) с использованием метода Монте-Карло. Эмпирические результаты показывают, что SuperWriter-LM превосходит более крупные базовые модели по автоматическим и человеческим оценкам."
                },
                "en": {
                    "title": "Elevating Long-Form Text Generation with Structured Thinking",
                    "desc": "This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks."
                },
                "zh": {
                    "title": "提升长文本生成质量的智能代理",
                    "desc": "长文本生成是大型语言模型（LLMs）面临的重要挑战，尤其是在保持连贯性、逻辑一致性和文本质量方面。为了解决这些问题，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。该框架通过规划和精炼阶段引入明确的结构化思维，指导模型遵循更有意识和认知基础的过程，类似于专业作家的写作方式。实验结果表明，SuperWriter-LM在多个基准测试中表现出色，超越了更大规模的基线模型，证明了分层直接偏好优化（DPO）和结构化思维步骤的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04158",
            "title": "Image Editing As Programs with Diffusion Models",
            "url": "https://huggingface.co/papers/2506.04158",
            "abstract": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.",
            "score": 2,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "e5a32d484bb427f3",
            "authors": [
                "Yujia Hu",
                "Songhua Liu",
                "Zhenxiong Tan",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04158.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента",
                    "desc": "Исследователи представили новый подход к редактированию изображений с использованием искусственного интеллекта, названный IEAP (Image Editing As Programs). Эта система основана на архитектуре Diffusion Transformer и разбивает сложные инструкции по редактированию на последовательность простых операций. Каждая операция реализуется с помощью специализированного адаптера, использующего общий базовый DiT. IEAP значительно превосходит современные методы в различных задачах редактирования, особенно при сложных многоэтапных инструкциях."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Programmatic Precision",
                    "desc": "This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions."
                },
                "zh": {
                    "title": "图像编辑的新方法：将复杂指令转化为简单操作",
                    "desc": "本研究提出了一种新的图像编辑框架，称为图像编辑作为程序（IEAP），旨在解决扩散模型在指令驱动的图像编辑中面临的挑战。IEAP基于扩散变换器（DiT）架构，通过将复杂的编辑指令分解为一系列原子操作来实现。每个操作由轻量级适配器实现，专门针对特定类型的编辑，能够支持任意和结构不一致的变换。实验结果表明，IEAP在各种编辑场景中显著优于现有的最先进方法，尤其在处理复杂的多步骤指令时表现出更高的准确性和语义保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04133",
            "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2506.04133",
            "abstract": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "a0a258935ed39508",
            "authors": [
                "Shaina Raza",
                "Ranjan Sapkota",
                "Manoj Karkee",
                "Christos Emmanouilidis"
            ],
            "affiliations": [
                "Cornell University, USA",
                "University of Groningen, Netherlands",
                "Vector Institute, Toronto, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04133.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#survey",
                    "#agents",
                    "#multimodal",
                    "#security",
                    "#alignment",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Безопасность и доверие в эпоху агентного ИИ",
                    "desc": "Статья представляет структурированный анализ управления доверием, рисками и безопасностью (TRiSM) в контексте агентных мультиагентных систем на основе больших языковых моделей (LLM). Рассматриваются четыре основных аспекта: управление, объяснимость, ModelOps и конфиденциальность/безопасность. Авторы идентифицируют уникальные векторы угроз и представляют комплексную таксономию рисков для приложений агентного ИИ. Статья также исследует механизмы построения доверия, методы обеспечения прозрачности и надзора, а также современные стратегии объяснимости в распределенных системах агентов LLM."
                },
                "en": {
                    "title": "Navigating Trust and Security in Agentic AI Systems",
                    "desc": "This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment."
                },
                "zh": {
                    "title": "构建安全透明的代理人工智能系统",
                    "desc": "本文回顾了基于大型语言模型（LLM）的代理多智能体系统中的信任、风险和安全管理（TRiSM）。我们分析了代理人工智能的概念基础及其与传统人工智能代理的架构差异，并探讨了支持可扩展自主性的系统设计。文章详细阐述了TRiSM的四个支柱：治理、可解释性、模型操作和隐私/安全，并为代理LLM提供了具体的背景。最后，提出了负责任的代理人工智能的路线图，建议研究方向以确保新兴多智能体系统的安全、透明和负责任的部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04034",
            "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2506.04034",
            "abstract": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "9d3dcbdd5158f101",
            "authors": [
                "Qing Jiang",
                "Xingyu Chen",
                "Zhaoyang Zeng",
                "Junzhi Yu",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "Peking University",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04034.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#hallucinations",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Интерпретируемое объектное реферирование через пошаговые рассуждения",
                    "desc": "Статья представляет новый подход к задаче объектного реферирования в компьютерном зрении, названный Rex-Thinker. Модель использует пошаговое рассуждение для оценки соответствия объектов заданному описанию, что повышает интерпретируемость и надежность предсказаний. Авторы создали датасет HumanRef-CoT для обучения модели структурированным рассуждениям. Rex-Thinker обучается в два этапа: контролируемая тонкая настройка и обучение с подкреплением, что улучшает точность и обобщающую способность модели."
                },
                "en": {
                    "title": "Rex-Thinker: Grounded Object Referring with Explainable Reasoning",
                    "desc": "This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios."
                },
                "zh": {
                    "title": "Rex-Thinker：可解释的物体指代模型",
                    "desc": "本文提出了一种新的物体指代模型Rex-Thinker，旨在通过明确的链式推理任务来检测与自然语言描述匹配的图像中的所有物体。该模型强调可验证性和可信性，确保其预测能够解释并与视觉证据相连。Rex-Thinker通过逐步推理候选物体实例，判断其是否符合给定的描述，从而做出最终预测。实验结果表明，该方法在精确度和可解释性方面优于传统基线，并在拒绝虚假输出和跨领域泛化能力上表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02945",
            "title": "Quantitative LLM Judges",
            "url": "https://huggingface.co/papers/2506.02945",
            "abstract": "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "de4ea9c8e4abb76a",
            "authors": [
                "Aishwarya Sahoo",
                "Jeevana Kruthi Karnuthala",
                "Tushar Parmanand Budhwani",
                "Pranchal Agarwal",
                "Sankaran Vaidyanathan",
                "Alexa Siu",
                "Franck Dernoncourt",
                "Jennifer Healey",
                "Nedim Lipka",
                "Ryan Rossi",
                "Uttaran Bhattacharya",
                "Branislav Kveton"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02945.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии",
                    "desc": "Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой модели. Авторы предлагают количественных LLM-судей, которые согласуют оценки существующих судей с человеческими оценками в заданной области с помощью регрессионных моделей. Представлены четыре количественных судьи для различных типов абсолютной и относительной обратной связи. Эксперименты показывают, что количественные судьи могут эффективно улучшить предсказательную силу существующих судей через постобработку."
                },
                "en": {
                    "title": "Enhancing LLM Evaluation with Quantitative Judges",
                    "desc": "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."
                },
                "zh": {
                    "title": "利用LLM提升评估效率的创新框架",
                    "desc": "本文提出了一种名为LLM-as-a-judge的框架，利用大型语言模型（LLM）自动评估另一个LLM的输出。我们引入了定量LLM评估者，通过回归模型将现有评估者的评分与人类评分对齐。该模型通过使用评估者的文本评价和评分来提高原始评估者的评分。我们的框架在计算效率上优于监督微调，并且在人工反馈有限的情况下，统计效率更高，适用于大多数应用场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01344",
            "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
            "url": "https://huggingface.co/papers/2506.01344",
            "abstract": "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 июня",
                "en": "June 2",
                "zh": "6月2日"
            },
            "hash": "788495117e4bf1d7",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Vivek Gupta",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "Adobe",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01344.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#cv",
                    "#reasoning",
                    "#agents",
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Точная интерпретация блок-схем с помощью нейросимволического агента",
                    "desc": "Статья представляет задачу точной атрибуции блок-схем и агента FlowPathAgent для ее решения. Авторы разработали нейросимволический подход, который сегментирует блок-схему, преобразует ее в структурированный символьный граф и использует агентный метод для генерации путей атрибуции. Также представлен новый бенчмарк FlowExplainBench для оценки атрибуций блок-схем. Результаты показывают, что FlowPathAgent снижает визуальные галлюцинации в ответах языковых моделей на вопросы по блок-схемам, превосходя базовые методы на 10-14%."
                },
                "en": {
                    "title": "Enhancing Flowchart Interpretation with Fine-grained Attribution",
                    "desc": "This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark."
                },
                "zh": {
                    "title": "提升流程图解析的可靠性与可解释性",
                    "desc": "本论文介绍了一种新的任务，称为细粒度流程图归因，旨在提高大型语言模型（LLM）在处理流程图时的可靠性和可解释性。我们提出了FlowPathAgent，这是一种神经符号代理，通过图形推理进行细粒度的后期归因。该代理首先对流程图进行分割，然后将其转换为结构化的符号图，并动态与图进行交互，以生成归因路径。实验结果表明，FlowPathAgent在流程图问答中减少了视觉幻觉，相较于强基线提高了10-14%的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23807",
            "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
            "url": "https://huggingface.co/papers/2505.23807",
            "abstract": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "a817afc0cdd35d8d",
            "authors": [
                "Yuli Chen",
                "Bo Cheng",
                "Jiale Han",
                "Yingying Zhang",
                "Yingting Li",
                "Shuhao Zhang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology, Hong Kong, China",
                "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23807.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умная обрезка слоев для эффективных языковых моделей",
                    "desc": "Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбинируя информацию о весах модели и активациях. Это позволяет сохранить производительность модели при высоком уровне разреженности. Эксперименты показали, что DLP превосходит существующие методы обрезки для различных языковых моделей."
                },
                "en": {
                    "title": "Dynamic Layerwise Pruning: Smart Sparsity for Language Models",
                    "desc": "This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques."
                },
                "zh": {
                    "title": "动态剪枝，智能保持性能！",
                    "desc": "动态层级剪枝方法通过结合模型权重和激活信息，自适应地确定每一层的重要性，从而在高稀疏性下保持大型语言模型的性能。传统的剪枝技术通常采用均匀层级剪枝策略，这可能导致在高稀疏性水平下性能显著下降。动态层级剪枝（DLP）方法克服了这一限制，能够根据输入激活信息动态调整剪枝率。实验结果表明，DLP在多个大型语言模型中有效地保持了高稀疏性下的模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21541",
            "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
            "url": "https://huggingface.co/papers/2505.21541",
            "abstract": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "cda6015909393ad0",
            "authors": [
                "Zitong Wang",
                "Hang Zhao",
                "Qianyu Zhou",
                "Xuequan Lu",
                "Xiangtai Li",
                "Yiren Song"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.21541.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Умное разделение изображений на слои с помощью ИИ",
                    "desc": "DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблемы разделения полупрозрачных и прозрачных наложений, с которыми не справлялись предыдущие методы. Авторы создали датасет AlphaBlend для обучения модели работе с различными типами прозрачности. DiffDecompose использует условное генерирование и позиционное кодирование слоев для точного восстановления составляющих изображения."
                },
                "en": {
                    "title": "Revolutionizing Image Layer Decomposition with DiffDecompose",
                    "desc": "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."
                },
                "zh": {
                    "title": "透明层分解的新突破：DiffDecompose",
                    "desc": "DiffDecompose 是一个基于扩散 Transformer 的框架，能够有效地将图像分解为组成层，并使用语义提示来解决透明层分解中的挑战。该方法针对半透明和透明图层的非线性遮挡问题，提出了一种新的任务：逐层分解 alpha 合成图像。为了解决层模糊、泛化能力和数据稀缺的问题，研究者们首次引入了 AlphaBlend 数据集，支持多种实际应用场景。DiffDecompose 通过上下文分解的方法，能够在没有逐层监督的情况下预测一个或多个层，展示了其在图像分解任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02294",
            "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
            "url": "https://huggingface.co/papers/2506.02294",
            "abstract": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines",
            "score": 0,
            "issue_id": 4133,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 июня",
                "en": "June 2",
                "zh": "6月2日"
            },
            "hash": "740d99ccc158d514",
            "authors": [
                "Niclas Popp",
                "Kevin Alexander Laube",
                "Matthias Hein",
                "Lukas Schott"
            ],
            "affiliations": [
                "Bosch Center for Artificial Intelligence",
                "University of Tübingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02294.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Повышение устойчивости моделей через генерацию сложных примеров",
                    "desc": "Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения робастности в процессе дистилляции знаний. Метод генерирует сложные образцы, максимизируя разногласие между учителем и учеником, что помогает преодолеть проблему ковариационного сдвига. Эксперименты показывают значительное улучшение точности на наихудших группах и средней точности по группам на датасетах CelebA и SpuCo Birds. Подход превосходит современные методы аугментации данных на основе диффузии."
                },
                "en": {
                    "title": "Boosting Student Robustness with Diffusion Data Augmentation",
                    "desc": "This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods."
                },
                "zh": {
                    "title": "基于扩散的数据增强提升知识蒸馏鲁棒性",
                    "desc": "本文提出了一种基于扩散的数据增强策略，以提高知识蒸馏中的鲁棒性。该方法通过生成具有挑战性的样本，增强了学生网络对虚假特征的抵抗力。实验结果表明，在CelebA和SpuCo Birds数据集上，该策略显著提高了最差组和平均组的准确率。通过最大化教师和学生之间的分歧，本文有效地解决了知识蒸馏中的协变量偏移问题。"
                }
            }
        }
    ],
    "link_prev": "2025-06-04.html",
    "link_next": "2025-06-06.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "04.06",
        "en": "06/04",
        "zh": "6月4日"
    },
    "short_date_next": {
        "ru": "06.06",
        "en": "06/06",
        "zh": "6月6日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "VS-Bench是一个多模态基准，用于评估视觉语言模型在复杂多智能体环境中的策略推理和决策能力。现有的基准大多局限于单智能体或仅文本环境，而VS-Bench包含八个基于视觉的环境，涵盖合作、竞争和混合动机的互动。研究发现，现有模型在预测准确性和归一化回报方面存在显著差距。VS-Bench旨在标准化评估并指出现有模型的局限性，推动未来研究。代码和数据可在https://vs-bench.github.io获取。",
        "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
        "pinyin": "以下是您提供的文本的拼音转写：\n\nVS-Bench shì yīgè duō mó tài jīzhǔn, yòngyú pínggǔ shìjué yǔyán móxíng zài fùzá dōu zhìnéngtǐ huánjìng zhōng de cèlüè tuīlǐ hé juécè nénglì. Xiàn yǒu de jīzhǔn dàduō júxìan zài dān zhìnéngtǐ huò jǐn wénběn huánjìng, ér VS-Bench hán yǒu bā gè jīchǔ shìjué de huánjìng, hánfù hézuò, jìngzhēng hé hùnhé dòngjī de hùdòng. Yánjiū fāxiàn, xiàn yǒu móxíng zài yùcè zhǔnquèxìng hé guīyīhuà huíbào fāngmiàn cúnzài xiǎnzhù chājù. VS-Bench zhǐ yú biāozhǔnhuà pínggǔ ér zhǐchū xiàn yǒu móxíng de júxìanxìng, tuīdòng wèilái yánjiū. Dàimǎ hé shùjù kě zài https://vs-bench.github.io huòqǔ.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '视觉语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '多智能体', 'pinyin': 'duō zhì néng tǐ', 'trans': 'multi-agent'}, {'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'}, {'word': '策略推理', 'pinyin': 'cè lüè tuī lǐ', 'trans': 'strategic reasoning'}, {'word': '决策', 'pinyin': 'jué cè', 'trans': 'decision-making'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '局限于', 'pinyin': 'jú xiàn yú', 'trans': 'limited to'}, {'word': '单智能体', 'pinyin': 'dān zhì néng tǐ', 'trans': 'single-agent'}, {'word': '仅', 'pinyin': 'jǐn', 'trans': 'only'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '涵盖', 'pinyin': 'hán gài', 'trans': 'cover'}, {'word': '合作', 'pinyin': 'hé zuò', 'trans': 'cooperation'}, {'word': '竞争', 'pinyin': 'jìng zhēng', 'trans': 'competition'}, {'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'}, {'word': '动机', 'pinyin': 'dòng jī', 'trans': 'motivation'}, {'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interaction'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'find'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'prediction'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '归一化', 'pinyin': 'guī yī huà', 'trans': 'normalization'}, {'word': '回报', 'pinyin': 'huí bào', 'trans': 'reward'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '标准化', 'pinyin': 'biāo zhǔn huà', 'trans': 'standardize'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '指出', 'pinyin': 'zhǐ chū', 'trans': 'point out'}, {'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitation'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'}, {'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'}]",
        "trans": "VS-Bench is a multimodal benchmark designed to evaluate the strategic reasoning and decision-making capabilities of vision-language models in complex multi-agent environments. Existing benchmarks are largely limited to single-agent or text-only environments, whereas VS-Bench includes eight visually-based environments that encompass cooperative, competitive, and mixed-motive interactions. Research has revealed significant gaps in the predictive accuracy and normalized returns of existing models. VS-Bench aims to standardize evaluations and highlight the limitations of current models, driving future research. The code and data are available at https://vs-bench.github.io.",
        "update_ts": "2025-06-04 09:12"
    }
}