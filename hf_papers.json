{
    "date": {
        "ru": "19 –º–∞—Ä—Ç–∞",
        "en": "March 19",
        "zh": "3Êúà19Êó•"
    },
    "time_utc": "2025-03-19 09:12",
    "weekday": 2,
    "issue_id": 2783,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.14456",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "url": "https://huggingface.co/papers/2503.14456",
            "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
            "score": 59,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "0cd796cef6fa6475",
            "authors": [
                "Bo Peng",
                "Ruichong Zhang",
                "Daniel Goldstein",
                "Eric Alcaide",
                "Haowen Hou",
                "Janna Lu",
                "William Merrill",
                "Guangyu Song",
                "Kaifeng Tan",
                "Saiteja Utpala",
                "Nathan Wilce",
                "Johan S. Wind",
                "Tianyi Wu",
                "Daniel Wuttke",
                "Christian Zhou-Zheng"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "Denigma",
                "EleutherAI",
                "George Mason University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "New York University",
                "RWKV Project (under Linux Foundation AI & Data)",
                "Recursal AI",
                "Shenzhen University",
                "Tano Labs",
                "Tsinghua University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14456.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ü¶¢",
                "ru": {
                    "title": "RWKV-7: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π",
                    "desc": "RWKV-7 'Goose' - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π state-of-the-art –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ —Ç–æ–∫–µ–Ω, –≤–≤–æ–¥–∏—Ç –æ–±–æ–±—â–µ–Ω–Ω—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –ø—Ä–∞–≤–∏–ª–∞ –¥–µ–ª—å—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –≥–µ–π—Ç–∏–Ω–≥–æ–º –∏ –æ–±—É—á–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. RWKV-7 —Å–ø–æ—Å–æ–±–Ω–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≤—Å–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —è–∑—ã–∫–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –∫–æ—Ä–ø—É—Å –∏–∑ 3,1 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±—É—á–∞—é—Ç –Ω–∞ –Ω–µ–º —á–µ—Ç—ã—Ä–µ –º–æ–¥–µ–ª–∏ RWKV-7 —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 0,19 –¥–æ 2,9 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
                },
                "en": {
                    "title": "RWKV-7: Efficient Multilingual Mastery with Fewer Parameters",
                    "desc": "RWKV-7 \"Goose\" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development."
                },
                "zh": {
                    "title": "RWKV-7ÔºöÂ§öËØ≠Ë®Ä‰ªªÂä°ÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "RWKV-7 \"Goose\" ÊòØ‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂª∫Ê®°Êû∂ÊûÑÔºåÂÖ∑Êúâ3‰∫øÂèÇÊï∞ÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏≠ËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ‰∏éÂÖ∂‰ªñÈ°∂Á∫ß3BÊ®°ÂûãÁõ∏ÊØîÔºåRWKV-7Âú®ËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÊ†áËÆ∞Êï∞ÈáèÊòæËëóÂáèÂ∞ëÔºå‰ΩÜ‰ªçËÉΩ‰∏éÂΩìÂâçÁöÑËã±ËØ≠ËØ≠Ë®ÄÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇËØ•Ê®°ÂûãÈááÁî®‰∫ÜÊñ∞ÁöÑÂπø‰πâÂ¢ûÈáèËßÑÂàôÔºåÁªìÂêà‰∫ÜÂêëÈáèÂÄºÈó®ÊéßÂíå‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËÆ≠ÁªÉÁöÑÂπ∂Ë°åÊÄß„ÄÇRWKV-7ËÉΩÂ§üËøõË°åÁä∂ÊÄÅË∑üË∏™Âπ∂ËØÜÂà´ÊâÄÊúâÊ≠£ËßÑËØ≠Ë®ÄÔºåË∂ÖË∂ä‰∫ÜÊ†áÂáÜÂ§çÊùÇÊÄßÁåúÊÉ≥‰∏ãÁöÑTransformerÁöÑËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14378",
            "title": "Impossible Videos",
            "url": "https://huggingface.co/papers/2503.14378",
            "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
            "score": 36,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "3334aa74b743ac8d",
            "authors": [
                "Zechen Bai",
                "Hai Ci",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14378.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "IPV-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∏–¥–µ–æ",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IPV-Bench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–π 4 –¥–æ–º–µ–Ω–∞ –∏ 14 –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ü–µ–Ω—ã, –Ω–∞—Ä—É—à–∞—é—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ, –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ, –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–±–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Video-LLM –ø–æ–Ω–∏–º–∞—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∏–¥–µ–æ. –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—è–≤–ª—è–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–¥–µ–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ä–∞–∑–≤–∏—Ç–∏—è –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Exploring the Impossible: Advancing Video Generation and Understanding",
                    "desc": "This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field."
                },
                "zh": {
                    "title": "Êé¢Á¥¢‰∏çÂèØËÉΩËßÜÈ¢ëÁöÑÁîüÊàê‰∏éÁêÜËß£",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂêàÊàêËßÜÈ¢ëÂú®Êï∞ÊçÆÁ®ÄÁº∫ÂíåÂ§öÊ†∑ÊÄßÊñπÈù¢ÁöÑÂ∫îÁî®„ÄÇÂΩìÂâçÁöÑÂêàÊàêÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂ§çÂà∂Áé∞ÂÆûÂú∫ÊôØÔºåËÄåÂØπ‰∏çÂèØËÉΩ„ÄÅÂèç‰∫ãÂÆûÂíåÂèçÁé∞ÂÆûÁöÑËßÜÈ¢ëÊ¶ÇÂøµÁ†îÁ©∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜIPV-BenchÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Âíå‰øÉËøõËßÜÈ¢ëÁêÜËß£‰∏éÁîüÊàêÁöÑËøõÂ±ï„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫ÜÂ§öÁßçËøùÂèçÁâ©ÁêÜ„ÄÅÁîüÁâ©„ÄÅÂú∞ÁêÜÊàñÁ§æ‰ºöÊ≥ïÂàôÁöÑÂú∫ÊôØÔºåÂπ∂ÈÄöËøáÊûÑÂª∫ÊèêÁ§∫Â•ó‰ª∂Êù•ÊåëÊàòËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÂàõÈÄ†ÂäõÂíåÊèêÁ§∫Ë∑üÈöèËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14478",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "url": "https://huggingface.co/papers/2503.14478",
            "abstract": "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.",
            "score": 32,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "120f1d8ec2eb88a8",
            "authors": [
                "Xinyu Fang",
                "Zhijian Chen",
                "Kai Lan",
                "Shengyuan Ding",
                "Yingji Liang",
                "Xiangyu Zhao",
                "Farong Wen",
                "Zicheng Zhang",
                "Guofeng Zhang",
                "Haodong Duan",
                "Kai Chen",
                "Dahua Lin"
            ],
            "affiliations": [
                "East China Normal University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Tongji University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14478.jpg",
            "data": {
                "categories": [
                    "#creativity",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–ò–∑–º–µ—Ä—è—è —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Creation-MMBench - –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∑–∞–¥–∞—á–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 765 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 51 –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É, —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª—É—á–∞—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ MLLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Unlocking Creativity in Multimodal AI with Creation-MMBench",
                    "desc": "This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÈÄ†Âäõ",
                    "desc": "ÂàõÈÄ†ÂäõÊòØÊô∫ËÉΩÁöÑ‰∏Ä‰∏™Âü∫Êú¨ÊñπÈù¢ÔºåÊ∂âÂèäÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠ÁîüÊàêÊñ∞È¢ñ‰∏îÈÄÇÂΩìÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇËôΩÁÑ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂàõÈÄ†ËÉΩÂäõÊñπÈù¢ÂæóÂà∞‰∫ÜÂπøÊ≥õËØÑ‰º∞Ôºå‰ΩÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑËØÑ‰º∞‰ªçÁÑ∂Áõ∏ÂØπÁº∫‰πè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜCreation-MMBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞MLLMsÂú®Âü∫‰∫éÂõæÂÉèÁöÑÂÆûÈôÖ‰ªªÂä°‰∏≠ÂàõÈÄ†ËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÁöÑÂºÄÊ∫êMLLMsÂú®ÂàõÈÄ†ÊÄß‰ªªÂä°‰∏≠ÊòæËëó‰Ωé‰∫é‰∏ìÊúâÊ®°ÂûãÔºåËÄåËßÜËßâÂæÆË∞ÉÂèØËÉΩ‰ºöÂØπÂü∫Á°ÄLLMÁöÑÂàõÈÄ†ËÉΩÂäõ‰∫ßÁîüË¥üÈù¢ÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14476",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "url": "https://huggingface.co/papers/2503.14476",
            "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
            "score": 20,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "5b4841d2845817e8",
            "authors": [
                "Qiying Yu",
                "Zheng Zhang",
                "Ruofei Zhu",
                "Yufeng Yuan",
                "Xiaochen Zuo",
                "Yu Yue",
                "Tiantian Fan",
                "Gaohong Liu",
                "Lingjun Liu",
                "Xin Liu",
                "Haibin Lin",
                "Zhiqi Lin",
                "Bole Ma",
                "Guangming Sheng",
                "Yuxuan Tong",
                "Chi Zhang",
                "Mofan Zhang",
                "Wang Zhang",
                "Hang Zhu",
                "Jinhua Zhu",
                "Jiaze Chen",
                "Jiangjie Chen",
                "Chengyi Wang",
                "Hongli Yu",
                "Weinan Dai",
                "Yuxuan Song",
                "Xiangpeng Wei",
                "Hao Zhou",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Lin Yan",
                "Mu Qiao",
                "Yonghui Wu",
                "Mingxuan Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14476.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ RL –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º DAPO –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã—Ç–æ –ø—É–±–ª–∏–∫—É—é—Ç —Å–∏—Å—Ç–µ–º—É, –¥–æ—Å—Ç–∏–≥–∞—é—â—É—é 50 –±–∞–ª–ª–æ–≤ –Ω–∞ AIME 2024 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen2.5-32B. –í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç—Å—è —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏, –¥–µ–ª–∞—é—â–∏–µ —É—Å–ø–µ—à–Ω—ã–º RL –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ø–ú. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Open-Source Reinforcement Learning",
                    "desc": "This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research."
                },
                "zh": {
                    "title": "Ëß£ÈîÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊΩúÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁÆóÊ≥ïÔºåÁß∞‰∏∫Ëß£ËÄ¶Ââ™ËæëÂíåÂä®ÊÄÅÈááÊ†∑Á≠ñÁï•‰ºòÂåñÔºàDAPOÔºâÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊäÄÊúØÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÂú®AIME 2024‰∏äËé∑Âæó50ÂàÜÁöÑÊàêÁª©Ôºå‰ΩøÁî®ÁöÑÊòØQwen2.5-32BÂü∫Á°ÄÊ®°Âûã„ÄÇ‰∏é‰ª•ÂæÄÁ†îÁ©∂‰∏çÂêåÔºåÊàë‰ª¨ÂÖ¨ÂºÄ‰∫ÜÂõõ‰∏™ÂÖ≥ÈîÆÊäÄÊúØÁªÜËäÇÔºåÂ∏ÆÂä©Á§æÂå∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§çÁé∞Êàë‰ª¨ÁöÑËÆ≠ÁªÉÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºÄÊ∫ê‰∫ÜËÆ≠ÁªÉ‰ª£Á†ÅÂíåÁªèËøáÁ≤æÂøÉÂ§ÑÁêÜÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•‰øÉËøõÂ§ßÂûãLLMÂº∫ÂåñÂ≠¶‰π†ÁöÑÂèØÈáçÂ§çÊÄßÂíåÊú™Êù•Á†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12797",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
            "url": "https://huggingface.co/papers/2503.12797",
            "abstract": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.",
            "score": 19,
            "issue_id": 2777,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "c682a086aaac0fa1",
            "authors": [
                "Xinyu Ma",
                "Ziyang Ding",
                "Zhicong Luo",
                "Chi Chen",
                "Zonghao Guo",
                "Derek F. Wong",
                "Xiaoyi Feng",
                "Maosong Sun"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Shandong University",
                "Tsinghua University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12797.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#reasoning",
                    "#synthetic",
                    "#benchmark",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "DeepPerception: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ò–ò —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∑–∞–¥–∞—á—É knowledge-intensive visual grounding (KVG), —Ç—Ä–µ–±—É—é—â—É—é —Ç–æ–Ω–∫–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å DeepPerception, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π MLLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ KVG-Bench –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Enhancing Visual Perception in MLLMs with DeepPerception",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâÊÑüÁü•ÁöÑËÆ§Áü•Êï¥ÂêàËÉΩÂäõ",
                    "desc": "‰∫∫Á±ª‰∏ìÂÆ∂Âú®ÁªÜÁ≤íÂ∫¶ËßÜËßâËæ®Âà´ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂà©Áî®È¢ÜÂüüÁü•ËØÜÊù•‰ºòÂåñÊÑüÁü•ÁâπÂæÅÔºåËÄåÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËøôÊñπÈù¢‰ªçÊòæ‰∏çË∂≥„ÄÇÂ∞ΩÁÆ°Êã•Êúâ‰∏∞ÂØåÁöÑ‰∏ìÂÆ∂Á∫ßÁü•ËØÜÔºåMLLMsÂú®ËßÜËßâÊÑüÁü•‰∏≠Êï¥ÂêàÊé®ÁêÜÁöÑËÉΩÂäõËæÉÂº±ÔºåÂ∏∏Â∏∏Áõ¥Êé•ÁîüÊàêÂìçÂ∫îËÄåÁº∫‰πèÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁü•ËØÜÂØÜÈõÜÂûãËßÜËßâÂÆö‰ΩçÔºàKVGÔºâÔºåËøôÊòØ‰∏ÄÈ°πÊñ∞È¢ñÁöÑËßÜËßâÂÆö‰Ωç‰ªªÂä°ÔºåË¶ÅÊ±ÇÂêåÊó∂ÂÖ∑Â§áÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÊï¥Âêà„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑDeepPerceptionÊ®°ÂûãÂ¢ûÂº∫‰∫ÜËÆ§Áü•ËßÜËßâÊÑüÁü•ËÉΩÂäõÔºåÈÄöËøáËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÂíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®KVG-BenchÊï∞ÊçÆÈõÜ‰∏äÁöÑÂáÜÁ°ÆÊÄßÂíåË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12329",
            "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
            "url": "https://huggingface.co/papers/2503.12329",
            "abstract": "Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.",
            "score": 17,
            "issue_id": 2778,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "c97a8c730bfcbfa8",
            "authors": [
                "Kanzhi Cheng",
                "Wenpo Song",
                "Jiaxin Fan",
                "Zheng Ma",
                "Qiushi Sun",
                "Fangzhi Xu",
                "Chenyang Yan",
                "Nuo Chen",
                "Jianbing Zhang",
                "Jiajun Chen"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "üì∏",
                "ru": {
                    "title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: –ò–ò –¥–æ–≥–æ–Ω—è–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å –ø–æ–º–æ—â—å—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É CapArena –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–µ–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è–º–∏ –∏ –ª—é–¥—å–º–∏, –ø–æ–∫–∞–∑–∞–≤, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-4o) –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–¥–ø–∏—Å–µ–π, –≤—ã—è–≤–ª—è—è –∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ CapArena-Auto - –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º."
                },
                "en": {
                    "title": "Elevating Image Captioning: Human-Level Performance and Reliable Metrics",
                    "desc": "This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking."
                },
                "zh": {
                    "title": "ÂõæÂÉèÊèèËø∞ÁöÑÊñ∞Ê†áÂáÜÔºöVLMÁöÑÂ¥õËµ∑‰∏éËØÑ‰º∞",
                    "desc": "ÂõæÂÉèÊèèËø∞‰∏ÄÁõ¥ÊòØËßÜËßâ‰∏éËØ≠Ë®ÄÁ†îÁ©∂‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàò„ÄÇÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºåÁé∞‰ª£ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËÉΩÂ§üÁîüÊàêËØ¶ÁªÜ‰∏îÂÖ®Èù¢ÁöÑÂõæÂÉèÊèèËø∞„ÄÇÊú¨ÊñáÈÄöËøáÂª∫Á´ãCapArenaÂπ≥Âè∞ÔºåËØÑ‰º∞ÂΩìÂâçVLMÂú®ÂõæÂÉèÊèèËø∞‰ªªÂä°‰∏äÁöÑË°®Áé∞ÔºåÂèëÁé∞È¢ÜÂÖàÊ®°ÂûãÂ¶ÇGPT-4oÁöÑË°®Áé∞ÁîöËá≥Ë∂ÖËøá‰∫Ü‰∫∫Á±ª„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËá™Âä®ËØÑ‰º∞ÊåáÊ†áÁöÑÂèØÈù†ÊÄßÔºåÁªìÊûúË°®ÊòéVLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÂú®ÊèèËø∞Ë¥®ÈáèËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ´òÊïàÂü∫ÂáÜÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13424",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "url": "https://huggingface.co/papers/2503.13424",
            "abstract": "Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility",
            "score": 13,
            "issue_id": 2776,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "20793013b58dba36",
            "authors": [
                "Xinyu Lian",
                "Zichao Yu",
                "Ruiming Liang",
                "Yitong Wang",
                "Li Ray Luo",
                "Kaixu Chen",
                "Yuanzhen Zhou",
                "Qihong Tang",
                "Xudong Xu",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Artificial Intelligence Laboratory",
                "South China University of Technology",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13424.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ü—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ—á–ª–µ–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Infinite Mobility –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–æ—á–ª–µ–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∏—Ö –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ —Å—Ä–∞–≤–Ω–∏–º —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –≤—Ä—É—á–Ω—É—é, –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —Å–≤–æ–π—Å—Ç–≤–∞–º –∏ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–ª–∏–≥–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–æ–∫. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Å–∏–º—É–ª—è—Ü–∏–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Infinite Mobility, –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Revolutionizing Articulated Object Creation with Infinite Mobility",
                    "desc": "This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI."
                },
                "zh": {
                    "title": "Êó†ÈôêÁßªÂä®ÔºöÂêàÊàêÈ´ò‰øùÁúüÂÖ≥ËäÇÁâ©‰ΩìÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Êó†ÈôêÁßªÂä®ÔºàInfinite MobilityÔºâÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÈÄöËøáÁ®ãÂ∫èÁîüÊàêÂêàÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑÂÖ≥ËäÇÁâ©‰Ωì„ÄÇËøôÁßçÊñπÊ≥ïÂÖãÊúç‰∫ÜÁé∞ÊúâÊï∞ÊçÆÈ©±Âä®ÊàñÊ®°ÊãüÊñπÊ≥ïÂú®ËßÑÊ®°ÂíåË¥®Èáè‰∏äÁöÑÈôêÂà∂„ÄÇÁî®Êà∑Á†îÁ©∂ÂíåÂÆöÈáèËØÑ‰º∞Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Áâ©ÁêÜÂ±ûÊÄßÂíåÁΩëÊ†ºË¥®Èáè‰∏äË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂπ∂‰∏î‰∏é‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÁõ∏ÂΩì„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÂêàÊàêÊï∞ÊçÆÂèØ‰ª•‰Ωú‰∏∫ÁîüÊàêÊ®°ÂûãÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÊîØÊåÅÂêéÁª≠ÁöÑÊâ©Â±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14125",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "url": "https://huggingface.co/papers/2503.14125",
            "abstract": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.",
            "score": 10,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "936ea0aa4972c382",
            "authors": [
                "Defa Zhu",
                "Hongzhi Huang",
                "Jundong Zhou",
                "Zihao Huang",
                "Yutao Zeng",
                "Banggu Wu",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "Frac-Connections: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Frac-Connections. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –∏–¥–µ—é –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (residual connections), —Ä–∞–∑–¥–µ–ª—è—è —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π –≤–º–µ—Å—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏—Ö —à–∏—Ä–∏–Ω—ã. Frac-Connections —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Hyper-Connections, –ø—Ä–∏ —ç—Ç–æ–º —Å–Ω–∏–∂–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MoE —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ 3 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Frac-Connections: Efficient Memory for Deep Learning",
                    "desc": "This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections."
                },
                "zh": {
                    "title": "Frac-ConnectionsÔºö‰ºòÂåñÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂÜÖÂ≠ò‰ΩøÁî®",
                    "desc": "ÊÆãÂ∑ÆËøûÊé•ÊòØÁé∞‰ª£Ê∑±Â∫¶Â≠¶‰π†Êû∂ÊûÑÁöÑÊ†∏ÂøÉÔºåËÉΩÂ§üÈÄöËøáÂáèËΩªÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÊù•ËÆ≠ÁªÉÈùûÂ∏∏Ê∑±ÁöÑÁΩëÁªú„ÄÇË∂ÖËøûÊé•ÊúÄËøëÈÄöËøáÂú®‰∏çÂêåÊ∑±Â∫¶ÂºïÂÖ•Â§ö‰∏™ËøûÊé•Âº∫Â∫¶Êù•Êé®ÂπøÊÆãÂ∑ÆËøûÊé•Ôºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÊ¢ØÂ∫¶Ê∂àÂ§±‰∏éË°®Á§∫Â¥©Ê∫É‰πãÈó¥ÁöÑÊëáÊëÜÊïàÂ∫î„ÄÇÁÑ∂ËÄåÔºåË∂ÖËøûÊé•ÈÄöËøáÊâ©Â±ïÈöêËóèÁä∂ÊÄÅÁöÑÂÆΩÂ∫¶Â¢ûÂä†‰∫ÜÂÜÖÂ≠òËÆøÈóÆÊàêÊú¨„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFrac-ConnectionsÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÈöêËóèÁä∂ÊÄÅÂàíÂàÜ‰∏∫Â§ö‰∏™ÈÉ®ÂàÜËÄå‰∏çÊòØÊâ©Â±ïÂÖ∂ÂÆΩÂ∫¶Ôºå‰øùÁïô‰∫ÜË∂ÖËøûÊé•ÁöÑÈÉ®ÂàÜ‰ºòÂäøÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12505",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "url": "https://huggingface.co/papers/2503.12505",
            "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
            "score": 6,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "0ac4fdd3411855ac",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojiang Peng",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "MPBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MPBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. MPBench –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ—Ü–µ–Ω–∫–∏: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤, –∞–≥—Ä–µ–≥–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏—Ç—å PRM –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –∏ –≤—ã—è–≤–ª–µ–Ω–∏—é –æ—à–∏–±–æ–∫ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with MPBench",
                    "desc": "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."
                },
                "zh": {
                    "title": "ÂÖ®Èù¢ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜ",
                    "desc": "Êé®ÁêÜÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§ÑÁêÜÂ§çÊùÇ‰ªªÂä°ÁöÑÈáçË¶ÅËÉΩÂäõÔºåËÄåËØÜÂà´ËøáÁ®ãÈîôËØØÂØπ‰∫éÊèêÂçáËøô‰∏ÄËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊúÄËøëÊèêÂá∫ÁöÑËøáÁ®ãÁ∫ßÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÈÄöËøáÊèê‰æõÈÄêÊ≠•Â•ñÂä±Ôºå‰øÉËøõ‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÂíåÊï∞ÊçÆÁîüÊàêÔºå‰ªéËÄåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂºïÂØºLLMsËµ∞ÂêëÊ≠£Á°ÆÁöÑÊ≠•È™§ÔºåÊèêÈ´òÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑPRMsÂü∫ÂáÜ‰∏ªË¶ÅÂü∫‰∫éÊñáÊú¨Ôºå‰∏ìÊ≥®‰∫éÈîôËØØÊ£ÄÊµãÔºåÂøΩËßÜ‰∫ÜÊé®ÁêÜÊêúÁ¥¢Á≠âÂÖ∂‰ªñÂú∫ÊôØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜMPBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§ö‰ªªÂä°Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÊó®Âú®Á≥ªÁªüËØÑ‰º∞PRMsÂú®‰∏çÂêåÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14504",
            "title": "Aligning Multimodal LLM with Human Preference: A Survey",
            "url": "https://huggingface.co/papers/2503.14504",
            "abstract": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
            "score": 5,
            "issue_id": 2778,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "b33bcba515cfa942",
            "authors": [
                "Tao Yu",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Junkang Wu",
                "Jinda Lu",
                "Kun Wang",
                "Xingyu Lu",
                "Yunhang Shen",
                "Guibin Zhang",
                "Dingjie Song",
                "Yibo Yan",
                "Tianlong Xu",
                "Qingsong Wen",
                "Zhang Zhang",
                "Yan Huang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Institute of automation, Chinese academy of science",
                "Lehigh University",
                "Nanjing University",
                "Nanyang Technological University",
                "National University of Singapore",
                "Shenzhen International Graduate School, Tsinghua University",
                "Squirrel Ai Learning",
                "Tencent Youtu Lab",
                "The Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14504.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò: –ø—É—Ç—å –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ. –û–Ω–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ –æ–±—Å—É–∂–¥–∞—é—Ç –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Aligning MLLMs: Bridging Gaps for Better Understanding",
                    "desc": "This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs."
                },
                "zh": {
                    "title": "ÂØπÈΩêÁÆóÊ≥ïÂä©ÂäõÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÊú™Êù•",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üÈÄöËøáÁÆÄÂçïÁöÑÊèêÁ§∫Â§ÑÁêÜÂêÑÁßçÈÄöÁî®‰ªªÂä°ÔºåËÄåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉ„ÄÇÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÊ∂âÂèäËßÜËßâ„ÄÅÂê¨ËßâÂíåÊñáÊú¨Êï∞ÊçÆÁöÑÂ§çÊùÇ‰ªªÂä°ÊñπÈù¢Â±ïÁé∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÖ≥‰∫éÁúüÂÆûÊÄß„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÁ±ªo1Êé®ÁêÜÂíå‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÁ≠âÂÖ≥ÈîÆÈóÆÈ¢ò‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜËß£ÂÜ≥„ÄÇÊú¨ÊñáÊó®Âú®Á≥ªÁªüÊÄßÂú∞ÂõûÈ°æMLLMsÁöÑÂØπÈΩêÁÆóÊ≥ïÔºåÊé¢ËÆ®ÂÖ∂Â∫îÁî®Âú∫ÊôØ„ÄÅÊï∞ÊçÆÈõÜÊûÑÂª∫Ê†∏ÂøÉÂõ†Á¥†„ÄÅËØÑ‰º∞Âü∫ÂáÜ‰ª•ÂèäÊú™Êù•ÂèëÂ±ïÊñπÂêë„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14499",
            "title": "Measuring AI Ability to Complete Long Tasks",
            "url": "https://huggingface.co/papers/2503.14499",
            "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.",
            "score": 4,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "c31aeeed7f6139af",
            "authors": [
                "Thomas Kwa",
                "Ben West",
                "Joel Becker",
                "Amy Deng",
                "Katharyn Garcia",
                "Max Hasin",
                "Sami Jawhar",
                "Megan Kinniment",
                "Nate Rush",
                "Sydney Von Arx",
                "Ryan Bloom",
                "Thomas Broadley",
                "Haoxing Du",
                "Brian Goodrich",
                "Nikola Jurkovic",
                "Luke Harold Miles",
                "Seraphina Nix",
                "Tao Lin",
                "Neev Parikh",
                "David Rein",
                "Lucas Jun Koba Sato",
                "Hjalmar Wijk",
                "Daniel M. Ziegler",
                "Elizabeth Barnes",
                "Lawrence Chan"
            ],
            "affiliations": [
                "Anthropic",
                "Model Evaluation & Threat Research (METR)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14499.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "–í—Ä–µ–º—è –Ω–∞ –≤–∞—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ: –ò–ò –¥–æ–≥–æ–Ω—è–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò-—Å–∏—Å—Ç–µ–º - –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ 50%-–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –≠—Ç–æ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∏–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª—é–¥—è–º –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –ò–ò –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å 50%-–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —É—Å–ø–µ—Ö–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ò–ò, —Ç–∞–∫–∏–µ –∫–∞–∫ Claude 3.7 Sonnet, –∏–º–µ—é—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –æ–∫–æ–ª–æ 50 –º–∏–Ω—É—Ç. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –ò–ò —É–¥–≤–∞–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –∫–∞–∂–¥—ã–µ —Å–µ–º—å –º–µ—Å—è—Ü–µ–≤ —Å 2019 –≥–æ–¥–∞, –∏ –æ–±—Å—É–∂–¥–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏ –ò–ò-—Å–∏—Å—Ç–µ–º."
                },
                "en": {
                    "title": "Measuring AI Progress: The 50%-Task-Completion Time Horizon",
                    "desc": "This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort."
                },
                "zh": {
                    "title": "AIËÉΩÂäõÁöÑÊñ∞Â∫¶ÈáèÔºö50%‰ªªÂä°ÂÆåÊàêÊó∂Èó¥",
                    "desc": "Â∞ΩÁÆ°‰∫∫Â∑•Êô∫ËÉΩÂú®Âü∫ÂáÜÊµãËØï‰∏äÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºå‰ΩÜÂÖ∂ÂÆûÈôÖË°®Áé∞ÁöÑÊÑè‰πâ‰ªç‰∏çÊòéÁ°Æ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ∫¶ÈáèÊ†áÂáÜÔºö50%‰ªªÂä°ÂÆåÊàêÊó∂Èó¥ËåÉÂõ¥ÔºåÊó®Âú®ÈáèÂåñAIÁ≥ªÁªü‰∏é‰∫∫Á±ªËÉΩÂäõÁöÑÂØπÊØî„ÄÇÈÄöËøáÂØπ‰∫∫Á±ª‰∏ìÂÆ∂Âú®Â§ö‰∏™‰ªªÂä°‰∏äÁöÑÂÆåÊàêÊó∂Èó¥ËøõË°åÊµãÈáèÔºåÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÁöÑÂâçÊ≤øAIÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑ50%Êó∂Èó¥ËåÉÂõ¥Á∫¶‰∏∫50ÂàÜÈíü„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåAIÊ®°ÂûãÁöÑÊó∂Èó¥ËåÉÂõ¥Ëá™2019Âπ¥‰ª•Êù•Â§ßÁ∫¶ÊØè‰∏É‰∏™ÊúàÁøªÂÄçÔºåÊú™Êù•‰∫îÂπ¥ÂÜÖÔºåAIÁ≥ªÁªüÂèØËÉΩËÉΩÂ§üËá™Âä®ÂåñËÆ∏Â§öÁõÆÂâçÈúÄË¶Å‰∫∫Á±ª‰∏Ä‰∏™ÊúàÊâçËÉΩÂÆåÊàêÁöÑËΩØ‰ª∂‰ªªÂä°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14495",
            "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
            "url": "https://huggingface.co/papers/2503.14495",
            "abstract": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency",
            "score": 4,
            "issue_id": 2779,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "590d8cdf2ae26151",
            "authors": [
                "Jiacheng Guo",
                "Yue Wu",
                "Jiahao Qiu",
                "Kaixuan Huang",
                "Xinzhe Juan",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14495.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ –ø–æ–¥—Ö–æ–¥–æ–≤ —Å —É—á–∞—Å—Ç–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –≤—ã—è–≤–ª–µ–Ω–∏—é –æ—à–∏–±–æ–∫ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ü—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –∫ –Ω–µ–¥–∞–≤–Ω–æ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º DeepSeek R1 –º–µ—Ç–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–æ–∑–≤–æ–ª–∏–≤ –º–æ–¥–µ–ª—è–º 7B/8B –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ 70B/72B –∏ GPT-4o –Ω–∞ —Ç–µ—Å—Ç–µ ProcessBench."
                },
                "en": {
                    "title": "Enhancing Verification Accuracy through Temporal Consistency",
                    "desc": "This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications."
                },
                "zh": {
                    "title": "ÊèêÂçáÊï∞Â≠¶È™åËØÅÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÈ™åËØÅÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜÁöÑÊúâÊïàÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËø≠‰ª£Âú∞Ê†πÊçÆ‰πãÂâçÁöÑËØÑ‰º∞Êù•ÁªÜÂåñÂà§Êñ≠ÔºåÂÖãÊúç‰∫ÜÂçïËΩÆÈ™åËØÅÂíåÂ§öÊ®°ÂûãËæ©ËÆ∫ÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êï∞Â≠¶ËøáÁ®ãÈîôËØØËØÜÂà´Âü∫ÂáÜÔºàÂ¶ÇMathcheck„ÄÅProcessBenchÂíåPRM800KÔºâ‰∏äÁöÑÂÆûËØÅËØÑ‰º∞ÔºåÊòæÁ§∫Âá∫Áõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÁöÑ‰∏ÄËá¥ÊÄßÊÄßËÉΩÊèêÂçá„ÄÇÂ∫îÁî®‰∫éÊúÄÊñ∞ÁöÑDeepSeek R1Ëí∏È¶èÊ®°ÂûãÊó∂ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÂæó7B/8BËí∏È¶èÊ®°ÂûãÂú®ProcessBench‰∏äË∂ÖË∂ä‰∫ÜÊâÄÊúâ70B/72BÊ®°ÂûãÂíåGPT-4o„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14492",
            "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
            "url": "https://huggingface.co/papers/2503.14492",
            "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.",
            "score": 4,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "5098c043161888fa",
            "authors": [
                "NVIDIA",
                ":",
                "Hassan Abu Alhaija",
                "Jose Alvarez",
                "Maciej Bala",
                "Tiffany Cai",
                "Tianshi Cao",
                "Liz Cha",
                "Joshua Chen",
                "Mike Chen",
                "Francesco Ferroni",
                "Sanja Fidler",
                "Dieter Fox",
                "Yunhao Ge",
                "Jinwei Gu",
                "Ali Hassani",
                "Michael Isaev",
                "Pooya Jannaty",
                "Shiyi Lan",
                "Tobias Lasser",
                "Huan Ling",
                "Ming-Yu Liu",
                "Xian Liu",
                "Yifan Lu",
                "Alice Luo",
                "Qianli Ma",
                "Hanzi Mao",
                "Fabio Ramos",
                "Xuanchi Ren",
                "Tianchang Shen",
                "Shitao Tang",
                "Ting-Chun Wang",
                "Jay Wu",
                "Jiashu Xu",
                "Stella Xu",
                "Kevin Xie",
                "Yuchong Ye",
                "Xiaodong Yang",
                "Xiaohui Zeng",
                "Yu Zeng"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14492.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#transfer_learning",
                    "#open_source",
                    "#inference",
                    "#3d"
                ],
                "emoji": "üåå",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏—Ä–æ–≤ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º",
                    "desc": "Cosmos-Transfer - —ç—Ç–æ —É—Å–ª–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∏—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∏–º—É–ª—è—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≤–µ—Å–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–Ω—ã—Ö –≤—Ö–æ–¥–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏—è—Ö. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∏—Ä–æ–≤ –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–µ–∂–¥—É –º–∏—Ä–∞–º–∏, –≤–∫–ª—é—á–∞—è Sim2Real. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ò–ò, –≤ —Ç–æ–º —á–∏—Å–ª–µ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ Sim2Real –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π."
                },
                "en": {
                    "title": "Empowering World Generation with Adaptive Control",
                    "desc": "Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field."
                },
                "zh": {
                    "title": "ÂèØÊéßÁöÑ‰∏ñÁïåÁîüÊàêÔºåÂä©ÂäõÁâ©ÁêÜ‰∫∫Â∑•Êô∫ËÉΩ",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜCosmos-TransferÔºåËøôÊòØ‰∏ÄÁßçÊù°‰ª∂‰∏ñÁïåÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•Ê†πÊçÆÂ§öÁßçÁ©∫Èó¥ÊéßÂà∂ËæìÂÖ•ÔºàÂ¶ÇÂàÜÂâ≤„ÄÅÊ∑±Â∫¶ÂíåËæπÁºòÔºâÁîüÊàê‰∏ñÁïåÊ®°Êãü„ÄÇÂú®ËÆæËÆ°‰∏äÔºåÁ©∫Èó¥Êù°‰ª∂ÊñπÊ°àÊòØËá™ÈÄÇÂ∫îÂíåÂèØÂÆöÂà∂ÁöÑÔºåÂÖÅËÆ∏Âú®‰∏çÂêåÁ©∫Èó¥‰ΩçÁΩÆÂØπ‰∏çÂêåÊù°‰ª∂ËæìÂÖ•ËøõË°åÂä†ÊùÉ„ÄÇËøô‰ΩøÂæó‰∏ñÁïåÁîüÊàêÂÖ∑ÊúâÈ´òÂ∫¶ÂèØÊéßÊÄßÔºåÂπ∂Âú®Â§öÁßç‰∏ñÁïåÂà∞‰∏ñÁïåÁöÑËΩ¨ÁßªÂ∫îÁî®‰∏≠ÊâæÂà∞Áî®ÈÄîÔºåÂåÖÊã¨Sim2Real„ÄÇÊàë‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞ÔºåÂàÜÊûê‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®Áâ©ÁêÜ‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÁöÑÂ∫îÁî®ÔºåÂåÖÊã¨Êú∫Âô®‰∫∫Sim2RealÂíåËá™Âä®È©æÈ©∂ËΩ¶ËæÜÊï∞ÊçÆÂ¢ûÂº∫„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12545",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2503.12545",
            "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.",
            "score": 4,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "8a908fbc8ce24853",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Weidong Tang",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Xiaojiang Peng",
                "Kai Wang",
                "Yang You",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12545.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üîí",
                "ru": {
                    "title": "PEBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PEBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è (MU) –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). PEBench –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ª–∏—á–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –æ–±—â–∏–º–∏ —Å—Ü–µ–Ω–∞–º–∏ —Å–æ–±—ã—Ç–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ 6 –º–µ—Ç–æ–¥–æ–≤ MU, –≤—ã—è–≤–∏–≤ –∏—Ö —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing Privacy in Multimodal Models with Machine Unlearning",
                    "desc": "This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems."
                },
                "zh": {
                    "title": "Êé®Âä®Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂÆâÂÖ®‰∏éÈöêÁßÅ‰øùÊä§",
                    "desc": "ËøëÂπ¥Êù•ÔºåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâÈóÆÁ≠î„ÄÅËßÜËßâÁêÜËß£ÂíåÊé®ÁêÜÁ≠â‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õËøõÂ±ï‰æùËµñ‰∫é‰ªé‰∫íËÅîÁΩëÊî∂ÈõÜÁöÑÂ§ßÈáèÊï∞ÊçÆÔºåËøôÂºïÂèë‰∫ÜÈöêÁßÅÂíåÂÆâÂÖ®ÊñπÈù¢ÁöÑÈáçÂ§ßÊãÖÂøß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú∫Âô®ÈÅóÂøòÔºàMUÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊúâÂâçÊôØÁöÑËß£ÂÜ≥ÊñπÊ°àÂ∫îËøêËÄåÁîüÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶Å‰ªéÂ§¥ÂºÄÂßãÈáçÊñ∞ËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªéÂ∑≤ËÆ≠ÁªÉÁöÑÊ®°Âûã‰∏≠Âà†Èô§ÁâπÂÆöÁü•ËØÜ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïPEBenchÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞MUÂú®MLLMs‰∏≠ÁöÑË°®Áé∞ÔºåÊé®Âä®ÂÆâÂÖ®ÂíåÈöêÁßÅ‰øùÊä§ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14151",
            "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
            "url": "https://huggingface.co/papers/2503.14151",
            "abstract": "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.",
            "score": 1,
            "issue_id": 2783,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "ba38580b0d116018",
            "authors": [
                "Yong Zhong",
                "Zhuoyi Yang",
                "Jiayan Teng",
                "Xiaotao Gu",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Gaoling School of AI, Renmin University of China, Beijing, China",
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14151.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "Concat-ID: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏",
                    "desc": "Concat-ID - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –ª–∏—Ü. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Concat-ID –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –æ–¥–Ω–∏–º –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏."
                },
                "en": {
                    "title": "Identity-Preserving Video Generation Made Easy with Concat-ID",
                    "desc": "Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds."
                },
                "zh": {
                    "title": "Concat-IDÔºöË∫´‰ªΩ‰∏ÄËá¥ÊÄßËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "Concat-ID ÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàê‰øùÊåÅË∫´‰ªΩ‰∏ÄËá¥ÊÄßÁöÑËßÜÈ¢ë„ÄÇÂÆÉ‰ΩøÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õÁâπÂæÅ‰∏éËßÜÈ¢ëÊΩúÂú®ÂèòÈáèÂú®Â∫èÂàóÁª¥Â∫¶‰∏äËøõË°åËøûÊé•ÔºåÂÆåÂÖ®‰æùËµñ‰∫é 3D Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÊ®°Âùó„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑË∑®ËßÜÈ¢ëÈÖçÂØπÁ≠ñÁï•ÂíåÂ§öÈò∂ÊÆµËÆ≠ÁªÉÊñπÊ°àÔºå‰ª•Âπ≥Ë°°Ë∫´‰ªΩ‰∏ÄËá¥ÊÄßÂíåÈù¢ÈÉ®ÂèØÁºñËæëÊÄßÔºåÂêåÊó∂ÊèêÈ´òËßÜÈ¢ëÁöÑËá™ÁÑ∂ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåConcat-ID Âú®Âçï‰∏ÄÂíåÂ§öË∫´‰ªΩÁîüÊàêÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂‰∏îËÉΩÂ§üÊó†ÁºùÊâ©Â±ïÂà∞Â§ö‰∏ª‰ΩìÂú∫ÊôØÔºåÂåÖÊã¨ËôöÊãüËØïÁ©øÂíåËÉåÊôØÂèØÊéßÁîüÊàê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12271",
            "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
            "url": "https://huggingface.co/papers/2503.12271",
            "abstract": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.",
            "score": 1,
            "issue_id": 2778,
            "pub_date": "2025-03-15",
            "pub_date_card": {
                "ru": "15 –º–∞—Ä—Ç–∞",
                "en": "March 15",
                "zh": "3Êúà15Êó•"
            },
            "hash": "2f560e2ec0839955",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Arsh Koneru",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "Salesforce AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12271.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#inference",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "Reflect-DiT: –£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Reflect-DiT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ best-of-N, Reflect-DiT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º Diffusion Transformer –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–Ω–µ–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ GenEval, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–æ–≤–æ–≥–æ —Ä–µ–∫–æ—Ä–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 0.81. Reflect-DiT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ inference-time scaling –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Refining Image Generation with Reflective Learning",
                    "desc": "This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models."
                },
                "zh": {
                    "title": "ÂèçÊÄùÁîüÊàêÔºåÊèêÂçáÂõæÂÉèË¥®ÈáèÔºÅ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊñπÊ≥ïÔºåÁß∞‰∏∫Reflect-DiT„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÂºïÂÖ•‰∏ä‰∏ãÊñáÂèçÊÄùËÉΩÂäõÔºåÂ∏ÆÂä©Diffusion TransformersÊ†πÊçÆ‰πãÂâçÁîüÊàêÁöÑÂõæÂÉèÂíåÊñáÊú¨ÂèçÈ¶àËøõË°åÊîπËøõ„ÄÇ‰∏é‰º†ÁªüÁöÑÊúÄ‰Ω≥NÈááÊ†∑ÊñπÊ≥ï‰∏çÂêåÔºåReflect-DiTËÉΩÂ§üÈíàÂØπÁâπÂÆöÁöÑÊîπËøõÈúÄÊ±ÇËøõË°åË∞ÉÊï¥Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReflect-DiTÂú®GenEvalÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÂàÜÊï∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10546",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.10546",
            "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 –º–∞—Ä—Ç–∞",
                "en": "March 13",
                "zh": "3Êúà13Êó•"
            },
            "hash": "f856affbe8bcf064",
            "authors": [
                "Zixian Liu",
                "Mingtong Zhang",
                "Yunzhu Li"
            ],
            "affiliations": [
                "Columbia University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10546.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#robotics",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "KUDA: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º",
                    "desc": "KUDA - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (VLM) –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–∞. KUDA —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –Ω–∞ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç VLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π. –ó–∞—Ç–µ–º —ç—Ç–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏."
                },
                "en": {
                    "title": "KUDA: Bridging Language and Dynamics for Robotic Manipulation",
                    "desc": "This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types."
                },
                "zh": {
                    "title": "KUDAÔºöÂä®ÊÄÅÂ≠¶‰π†‰∏éËßÜËßâÊèêÁ§∫ÁöÑÂºÄÊîæËØçÊ±áÊìç‰ΩúÁ≥ªÁªü",
                    "desc": "ÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÂºÄÊîæËØçÊ±áÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≥ªÁªüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËÆ∏Â§öÁé∞ÊúâÊñπÊ≥ïÂøΩËßÜ‰∫ÜÁâ©‰ΩìÂä®ÊÄÅÁöÑÈáçË¶ÅÊÄßÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êõ¥Â§çÊùÇÂä®ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜKUDAÔºå‰∏Ä‰∏™ÈõÜÊàê‰∫ÜÂä®ÊÄÅÂ≠¶‰π†ÂíåÈÄöËøáÂÖ≥ÈîÆÁÇπËøõË°åËßÜËßâÊèêÁ§∫ÁöÑÂºÄÊîæËØçÊ±áÊìç‰ΩúÁ≥ªÁªüÔºåÂà©Áî®‰∫ÜVLMsÂíåÂü∫‰∫éÂ≠¶‰π†ÁöÑÁ•ûÁªèÂä®ÊÄÅÊ®°Âûã„ÄÇKUDAÈÄöËøáÂ∞ÜÂÖ≥ÈîÆÁÇπÂàÜÈÖçÁªôRGBÂõæÂÉèÔºåÂπ∂Êü•ËØ¢VLMÁîüÊàêÁõÆÊ†áËßÑËåÉÔºåÂ∞ÜÊäΩË±°ÁöÑÂÖ≥ÈîÆÁÇπË°®Á§∫ËΩ¨Êç¢‰∏∫ÊàêÊú¨ÂáΩÊï∞Ôºå‰ªéËÄå‰ºòÂåñÊú∫Âô®‰∫∫ËΩ®Ëøπ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10410",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "url": "https://huggingface.co/papers/2503.10410",
            "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 –º–∞—Ä—Ç–∞",
                "en": "March 13",
                "zh": "3Êúà13Êó•"
            },
            "hash": "44150f611040e79d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "RoCo-Sim: –ø—Ä–æ—Ä—ã–≤ –≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
                    "desc": "RoCo-Sim - —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–æ—Ä–æ–∂–Ω–æ–π –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—É—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. RoCo-Sim –≤–∫–ª—é—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤–Ω–µ—à–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä, –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã–π –≤—ã–±–æ—Ä—â–∏–∫ —Å —É—á–µ—Ç–æ–º –æ–∫–∫–ª—é–∑–∏–π, –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –ø–µ—Ä–µ–¥–Ω–∏–π –ø–ª–∞–Ω-—Ñ–æ–Ω –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. –°–∏—Å—Ç–µ–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç 3D-–¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Enhancing Roadside Awareness with Collaborative Perception",
                    "desc": "This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "ÊèêÂçáË∑ØËæπÊÑüÁü•ÁöÑÂçèÂêåÂäõÈáè",
                    "desc": "Ë∑ØËæπÂçèÂêåÊÑüÁü•ÊòØ‰∏ÄÁßçÁ≥ªÁªüÔºåÂ§ö‰∏™Ë∑ØËæπÂçïÂÖÉÂçè‰ΩúÊ±áËÅöÊÑüÁü•Êï∞ÊçÆÔºåÂ∏ÆÂä©ËΩ¶ËæÜÊèêÈ´òÁéØÂ¢ÉÊÑèËØÜ„ÄÇÁé∞ÊúâÁöÑË∑ØËæπÊÑüÁü•ÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Ê®°ÂûãËÆæËÆ°Ôºå‰ΩÜÂøΩËßÜ‰∫ÜÊï∞ÊçÆÈóÆÈ¢òÔºåÂ¶ÇÊ†°ÂáÜËØØÂ∑Æ„ÄÅ‰ø°ÊÅØÁ®ÄÁñèÂíåÂ§öËßÜÂõæ‰∏ÄËá¥ÊÄßÔºåÂØºËá¥Âú®ÊúÄÊñ∞Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫ÊòæËëóÊèêÂçáË∑ØËæπÂçèÂêåÊÑüÁü•Âπ∂Ëß£ÂÜ≥ÂÖ≥ÈîÆÊï∞ÊçÆÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™Ë∑ØËæπÂçèÂêåÊÑüÁü•Ê®°ÊãüÊ°ÜÊû∂RoCo-Sim„ÄÇRoCo-SimËÉΩÂ§üÈÄöËøáÂä®ÊÄÅÂâçÊôØÁºñËæëÂíåÂçïÂõæÂÉèÁöÑÂÖ®Âú∫ÊôØÈ£éÊ†ºËøÅÁßªÁîüÊàêÂ§öÊ†∑ÂåñÁöÑ„ÄÅÂ§öËßÜÂõæ‰∏ÄËá¥ÁöÑÊ®°ÊãüË∑ØËæπÊï∞ÊçÆ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-18.html",
    "link_next": "2025-03-20.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3Êúà18Êó•"
    },
    "short_date_next": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3Êúà20Êó•"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 4,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#creativity": 1
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü RWKV-7 \"Goose\"Ôºå‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂª∫Ê®°Êû∂ÊûÑ„ÄÇÂÆÉÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏äËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÁöÑ‰ª§ÁâåÊï∞ÈáèÂ∞ë„ÄÇRWKV-7 Ê®°ÂûãÊØè‰∏™‰ª§ÁâåÁöÑÂÜÖÂ≠òÂíåÊé®ÁêÜÊó∂Èó¥ÊòØÂ∏∏Êï∞„ÄÇÂÆÉÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ delta ËßÑÂàôÂíåÊîæÊùæÁöÑÂÄºÊõøÊç¢ËßÑÂàô„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü RWKV-7 ÁöÑËØ≠Ë®ÄÂª∫Ê®°ËÉΩÂäõÔºåÂπ∂ÂèëÂ∏É‰∫ÜÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜ„ÄÇ",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "pinyin": "W«ímen ji√®sh√†o le RWKV-7 \"Goose\", yƒ´ zh«íng xƒ´n de x√πli√® ji√†nm√≥ ji√†g√≤u. TƒÅ z√†i du≈çy«îy√°n r√®nw√π sh√†ng d√°d√†o le xƒ´n de zu√¨jiƒÅ x√¨ngn√©ng, j«êngu«én x√πnli√†n de l√¨ngp√°i sh√πli√†ng sh«éo. RWKV-7 m√≥x√≠ng mƒõi ge l√¨ngp√°i de n√®ic√∫n h√© tuƒ´l«ê sh√≠jiƒÅn sh√¨ ch√°ngsh√π. TƒÅ y«ênr√π le xƒ´n de delta guƒ´z√© h√© f√†ngs≈çng de zh√≠ t√¨hu√†n guƒ´z√©. W«ímen zh«énsh√¨ le RWKV-7 de y«îy√°n ji√†nm√≥ n√©ngl√¨, b√¨ng fƒÅb√π le m√≥x√≠ng h√© sh√πj√πj√≠.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"Â∫èÂàó\", \"pinyin\": \"x√π li√®\", \"trans\": \"sequence\"},\n    {\"word\": \"Âª∫Ê®°\", \"pinyin\": \"ji√†n m√≥\", \"trans\": \"modeling\"},\n    {\"word\": \"Êû∂ÊûÑ\", \"pinyin\": \"ji√† g√≤u\", \"trans\": \"architecture\"},\n    {\"word\": \"Â§öËØ≠Ë®Ä\", \"pinyin\": \"du≈ç y«î y√°n\", \"trans\": \"multilingual\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n wu\", \"trans\": \"task\"},\n    {\"word\": \"ËææÂà∞\", \"pinyin\": \"d√° d√†o\", \"trans\": \"achieve\"},\n    {\"word\": \"ÊúÄ‰Ω≥\", \"pinyin\": \"zu√¨ jiƒÅ\", \"trans\": \"best\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"Â∞ΩÁÆ°\", \"pinyin\": \"j√¨n gu«én\", \"trans\": \"although\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"training\"},\n    {\"word\": \"‰ª§Áâå\", \"pinyin\": \"l√¨ng p√†i\", \"trans\": \"token\"},\n    {\"word\": \"Êï∞Èáè\", \"pinyin\": \"sh√π li√†ng\", \"trans\": \"quantity\"},\n    {\"word\": \"Â∞ë\", \"pinyin\": \"sh«éo\", \"trans\": \"few\"},\n    {\"word\": \"ÂÜÖÂ≠ò\", \"pinyin\": \"n√®i c√∫n\", \"trans\": \"memory\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"inference\"},\n    {\"word\": \"Êó∂Èó¥\", \"pinyin\": \"sh√≠ jiƒÅn\", \"trans\": \"time\"},\n    {\"word\": \"Â∏∏Êï∞\", \"pinyin\": \"ch√°ng sh√π\", \"trans\": \"constant\"},\n    {\"word\": \"ÂºïÂÖ•\", \"pinyin\": \"y«ên r√π\", \"trans\": \"introduce\"},\n    {\"word\": \"ËßÑÂàô\", \"pinyin\": \"guƒ´ z√©\", \"trans\": \"rule\"},\n    {\"word\": \"ÊîæÊùæ\", \"pinyin\": \"f√†ng s≈çng\", \"trans\": \"relax\"},\n    {\"word\": \"ÂÄº\", \"pinyin\": \"zh√≠\", \"trans\": \"value\"},\n    {\"word\": \"ÊõøÊç¢\", \"pinyin\": \"t√¨ hu√†n\", \"trans\": \"replace\"},\n    {\"word\": \"Â±ïÁ§∫\", \"pinyin\": \"zh«én sh√¨\", \"trans\": \"demonstrate\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÂèëÂ∏É\", \"pinyin\": \"fƒÅ b√π\", \"trans\": \"release\"},\n    {\"word\": \"Êï∞ÊçÆÈõÜ\", \"pinyin\": \"sh√π j√π j√≠\", \"trans\": \"dataset\"}\n]",
        "trans": "We introduced RWKV-7 \"Goose,\" a new sequence modeling architecture. It achieved new best performance on multilingual tasks despite being trained with fewer tokens. The RWKV-7 model has constant memory and inference time per token. It introduces new delta rules and relaxed value substitution rules. We demonstrated the language modeling capabilities of RWKV-7 and released the model and dataset.",
        "update_ts": "2025-03-19 09:12"
    }
}