{
    "date": {
        "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 15",
        "zh": "4æœˆ15æ—¥"
    },
    "time_utc": "2025-04-15 21:10",
    "weekday": 1,
    "issue_id": 3254,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10479",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "url": "https://huggingface.co/papers/2504.10479",
            "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
            "score": 164,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "51475893ef3c1d8b",
            "authors": [
                "Jinguo Zhu",
                "Weiyun Wang",
                "Zhe Chen",
                "Zhaoyang Liu",
                "Shenglong Ye",
                "Lixin Gu",
                "Yuchen Duan",
                "Hao Tian",
                "Weijie Su",
                "Jie Shao",
                "Zhangwei Gao",
                "Erfei Cui",
                "Yue Cao",
                "Yangzhou Liu",
                "Weiye Xu",
                "Hao Li",
                "Jiahao Wang",
                "Han Lv",
                "Dengnian Chen",
                "Songze Li",
                "Yinan He",
                "Tan Jiang",
                "Jiapeng Luo",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Wenqi Shao",
                "Junjun He",
                "Yingtong Xiong",
                "Wenwen Qu",
                "Peng Sun",
                "Penglong Jiao",
                "Lijun Wu",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Limin Wang",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10479.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "InternVL3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "InternVL3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. InternVL3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ÑĞ´ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (V2PE) Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (MPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 72.2 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMMU Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning with InternVL3",
                    "desc": "InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community."
                },
                "zh": {
                    "title": "InternVL3ï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒçš„æ–°æ ‡æ†",
                    "desc": "InternVL3æ˜¯InternVLç³»åˆ—çš„é‡è¦è¿›å±•ï¼Œé‡‡ç”¨äº†åŸç”Ÿçš„å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„å°†æ–‡æœ¬æ¨¡å‹è½¬å˜ä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¸åŒï¼ŒInternVL3åœ¨å•ä¸€é¢„è®­ç»ƒé˜¶æ®µåŒæ—¶å­¦ä¹ å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ç»è¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼ŒInternVL3åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†72.2çš„åˆ†æ•°ï¼Œæˆä¸ºå¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 91,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ prima.cpp - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ´Ğ¾ 70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ CPU/GPU, Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº RAM/VRAM Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Wi-Fi Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Prima.cpp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ mmap Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ»ÑŒÑ†ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ğ´Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Halda Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "è®©å®¶åº­è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºprima.cppçš„åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ™®é€šå®¶åº­è®¾å¤‡ä¸Šè¿è¡Œ70Bè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ··åˆä½¿ç”¨CPUå’ŒGPUï¼Œä¼˜åŒ–å†…å­˜å’Œå¸¦å®½çš„ä½¿ç”¨ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ¡ˆå¯¹é«˜æ€§èƒ½ç¡¬ä»¶çš„ä¾èµ–ã€‚å®ƒé‡‡ç”¨äº†mmapç®¡ç†æ¨¡å‹æƒé‡ï¼Œå¹¶å¼•å…¥äº†ç®¡é“ç¯å¹¶è¡Œå’Œé¢„å–æŠ€æœ¯ï¼Œä»¥å‡å°‘ç£ç›˜åŠ è½½æ—¶é—´ã€‚é€šè¿‡ä¼˜åŒ–è®¡ç®—ã€é€šä¿¡å’Œå†…å­˜ç®¡ç†ï¼Œprima.cppæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä½¿å¾—å…ˆè¿›çš„AIæ¨¡å‹èƒ½å¤Ÿåœ¨å®¶åº­åŠ©æ‰‹ä¸­æ™®åŠã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 34,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FUSION: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "FUSION - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. FUSION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSIONï¼šæ·±åº¦é›†æˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†FUSIONï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨å®Œå…¨çš„è§†è§‰-è¯­è¨€å¯¹é½å’Œé›†æˆèŒƒå¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºLLMè§£ç è¿‡ç¨‹ä¸­çš„åæœŸæ¨¡æ€äº¤äº’ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä¸ªå¤„ç†æµç¨‹ä¸­å®ç°äº†æ·±åº¦ã€åŠ¨æ€çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¼•å¯¼çš„ç»Ÿä¸€è§†è§‰ç¼–ç ï¼Œå°†æ–‡æœ¬ä¿¡æ¯èå…¥è§†è§‰ç¼–ç ï¼Œå®ç°åƒç´ çº§çš„é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é€’å½’å¯¹é½è§£ç ï¼Œèƒ½å¤Ÿåœ¨è§£ç è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬ä¸Šä¸‹æ–‡é€’å½’èšåˆè§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„é—®é¢˜çº§è¯­ä¹‰é›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08837",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.08837",
            "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.",
            "score": 34,
            "issue_id": 3237,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "e73823d36c951e4e",
            "authors": [
                "Haozhe Wang",
                "Chao Qu",
                "Zuming Huang",
                "Wei Chu",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "INF.AI",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08837.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#math",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ (SSR) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VL-Rethinker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning",
                    "desc": "This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œç§°ä¸ºé€‰æ‹©æ€§æ ·æœ¬é‡æ”¾ï¼ˆSSRï¼‰ï¼Œä»¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶ç»“åˆå¼ºåˆ¶é‡æ–°æ€è€ƒçš„æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹çš„è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚é€šè¿‡è¿™ä¸¤ç§æŠ€æœ¯çš„ç»“åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹VL-Rethinkeråœ¨å¤šä¸ªæ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚æœ€ç»ˆï¼ŒVL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†å¼€æºçš„æœ€æ–°æ°´å¹³ï¼Œç¼©å°äº†ä¸ç°æœ‰æœ€ä½³æ¨¡å‹çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 34,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GPT-4o: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç†è§£ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„å¤šæ¨¡æ€æ¨¡å‹GPT-4oåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨å…¨çƒæŒ‡ä»¤éµå¾ªã€ç²¾ç»†ç¼–è¾‘ç²¾åº¦å’Œç”Ÿæˆåæ¨ç†ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚å°½ç®¡ç°æœ‰åŸºå‡†æ˜¾ç¤ºGPT-4oåœ¨å›¾åƒå¤„ç†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å…¶åœ¨æŒ‡ä»¤ç†è§£å’ŒçŸ¥è¯†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚æ¨¡å‹å¸¸å¸¸å¯¹æŒ‡ä»¤è¿›è¡Œå­—é¢è§£é‡Šï¼ŒçŸ¥è¯†çº¦æŸåº”ç”¨ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ¡ä»¶æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¯¹GPT-4oç»Ÿä¸€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„æ™®éå‡è®¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„åŸºå‡†å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œæ¨ç†åŸºç¡€çš„å¤šæ¨¡æ€ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09643",
            "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
            "url": "https://huggingface.co/papers/2504.09643",
            "abstract": "Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.",
            "score": 29,
            "issue_id": 3245,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "9f5e20f45a50902d",
            "authors": [
                "Nikita Sorokin",
                "Ivan Sedykh",
                "Valentin Malykh"
            ],
            "affiliations": [
                "International IT University",
                "MTS AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09643.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#plp"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Proximal Policy Optimization (PPO). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MultiPL-E Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ 13.4B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ 33B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ GPT-4."
                },
                "en": {
                    "title": "Enhancing Code Generation with Smart Reranking",
                    "desc": "This paper addresses the challenges of generating high-quality code using decoder-based models, which often produce unpredictable outputs. It introduces a novel approach that combines a code generation model with a reranker model to select the best solutions from multiple generated samples. The authors propose an iterative self-training method using Proximal Policy Optimization (PPO) to enhance the reranking accuracy and overall code generation process. Their results show that their model, with 13.4 billion parameters, outperforms larger models in both quality and speed, achieving results comparable to GPT-4 in certain programming languages."
                },
                "zh": {
                    "title": "æå‡ä»£ç ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é«˜è´¨é‡ä»£ç ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å½“å‰åŸºäºè§£ç å™¨çš„æ¨¡å‹ä¸­ï¼Œè¾“å‡ºç»“æœå…·æœ‰é«˜åº¦éšæœºæ€§ã€‚é€šè¿‡ç»“åˆä»£ç ç”Ÿæˆæ¨¡å‹å’Œé‡æ’åºæ¨¡å‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆä»£ç çš„è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥æå‡é‡æ’åºæ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•´ä½“ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºæ›´å¤§å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10068",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
            "url": "https://huggingface.co/papers/2504.10068",
            "abstract": "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.",
            "score": 25,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "bbb7251e84f61649",
            "authors": [
                "Yang Shi",
                "Jiaheng Liu",
                "Yushuo Guan",
                "Zhenhua Wu",
                "Yuanxing Zhang",
                "Zihao Wang",
                "Weihong Lin",
                "Jingyun Hua",
                "Zekun Wang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Wentao Zhang",
                "Fuzheng Zhang",
                "Wenjing Yang",
                "Di Zhang"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10068.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#architecture",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Mavors: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Mavors Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Mavors Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Mavors Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation",
                    "desc": "This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning."
                },
                "zh": {
                    "title": "Mavorsï¼šé•¿è§†é¢‘ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMavorsçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—æ•ˆç‡ä¸ç»†ç²’åº¦æ—¶ç©ºæ¨¡å¼ä¿ç•™ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚Mavorsé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°å¯¹åŸå§‹è§†é¢‘å†…å®¹çš„ç¼–ç ï¼šä¸€æ˜¯ä½¿ç”¨3Då·ç§¯å’Œè§†è§‰å˜æ¢å™¨çš„å†…éƒ¨å—è§†è§‰ç¼–ç å™¨ï¼ˆIVEï¼‰ï¼Œä»¥ä¿ç•™é«˜åˆ†è¾¨ç‡çš„ç©ºé—´ç‰¹å¾ï¼›äºŒæ˜¯é€šè¿‡åŸºäºå˜æ¢å™¨çš„ä¾èµ–å»ºæ¨¡å’Œå—çº§æ—‹è½¬ä½ç½®ç¼–ç çš„å—é—´ç‰¹å¾èšåˆå™¨ï¼ˆIFAï¼‰ï¼Œå»ºç«‹å—ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶è¿˜é€šè¿‡å­å›¾åƒåˆ†è§£å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘ï¼Œä»è€Œç»Ÿä¸€äº†å›¾åƒå’Œè§†é¢‘çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMavorsåœ¨ä¿æŒç©ºé—´ä¿çœŸåº¦å’Œæ—¶é—´è¿ç»­æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦æ—¶ç©ºæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08942",
            "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
            "url": "https://huggingface.co/papers/2504.08942",
            "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
            "score": 17,
            "issue_id": 3237,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 11",
                "zh": "4æœˆ11æ—¥"
            },
            "hash": "d756012a0eceafb9",
            "authors": [
                "Xing Han LÃ¹",
                "Amirhossein Kazemnejad",
                "Nicholas Meade",
                "Arkil Patel",
                "Dongchan Shin",
                "Alejandra Zambrano",
                "Karolina StaÅ„czak",
                "Peter Shaw",
                "Christopher J. Pal",
                "Siva Reddy"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Google DeepMind",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique MontrÃ©al",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08942.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "AgentRewardBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentRewardBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1302 Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 5 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ 4 LLM, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ° LLM Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Web Agent Evaluation with LLMs",
                    "desc": "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."
                },
                "zh": {
                    "title": "è¯„ä¼°ç½‘ç»œä»£ç†çš„æ–°æ–¹æ³•ï¼šAgentRewardBench",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•è¯„ä¼°ç½‘ç»œä»£ç†çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’æ¥å®Œæˆä»»åŠ¡çš„ä»£ç†ã€‚ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨æ‰©å±•æ–°ä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•å‡†ç¡®è¯†åˆ«æˆåŠŸçš„è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†AgentRewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°ç½‘ç»œä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹1302ä¸ªè½¨è¿¹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰å•ä¸€çš„LLMåœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¿™è¡¨æ˜éœ€è¦å¼€å‘æ›´çµæ´»çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10368",
            "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10368",
            "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.",
            "score": 16,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "c5995fab2f284493",
            "authors": [
                "Wenyuan Zhang",
                "Shuaiyi Nie",
                "Xinghua Zhang",
                "Zefeng Zhang",
                "Tingwen Liu"
            ],
            "affiliations": [
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10368.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸",
                    "desc": "S1-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ 1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LRM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ² 15,5 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ€Ğ°Ğ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LRM."
                },
                "en": {
                    "title": "Evaluating Intuition: S1-Bench for Large Reasoning Models",
                    "desc": "The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„ç›´è§‚æ€ç»´èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†S1-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ›´å€¾å‘äºç›´è§‚çš„ç³»ç»Ÿ1æ€ç»´ï¼Œè€Œéæ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿ2æ¨ç†ã€‚å°½ç®¡LRMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦åˆ†ææ€ç»´çš„ä¾èµ–å¯èƒ½é™åˆ¶äº†å…¶ç³»ç»Ÿ1æ€ç»´èƒ½åŠ›ã€‚ç›®å‰ç¼ºä¹è¯„ä¼°LRMsåœ¨éœ€è¦è¿™ç§èƒ½åŠ›çš„ä»»åŠ¡è¡¨ç°çš„åŸºå‡†ã€‚S1-Benchæä¾›äº†ä¸€ç»„ç®€å•ã€å¤šæ ·ä¸”è‡ªç„¶æ¸…æ™°çš„é—®é¢˜ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LRMsåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 13,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Upper Confidence Bound Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒçº§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçº§å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è®­ç»ƒæ•°æ®è§†ä¸ºç»Ÿä¸€æ•´ä½“ï¼Œå¿½è§†äº†æ•°æ®åˆ†å¸ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸åŒåˆ†å¸ƒçš„é‡‡æ ·æ¦‚ç‡ï¼Œä¼˜å…ˆè€ƒè™‘é«˜å¹³å‡ä¼˜åŠ¿æˆ–ä½æ ·æœ¬æ•°é‡çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå±•ç¤ºäº†åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥çš„ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10127",
            "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
            "url": "https://huggingface.co/papers/2504.10127",
            "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.",
            "score": 12,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "604dff752f16abf2",
            "authors": [
                "Junlei Zhang",
                "Zichen Ding",
                "Chang Ma",
                "Zijie Chen",
                "Qiushi Sun",
                "Zhenzhong Lan",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10127.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° (GUI) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… GUI-ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing GUI Agents through Vision Language Model Training",
                    "desc": "This paper discusses how to improve the performance of Graphical User Interface (GUI) agents by using Vision Language Models (VLMs) trained on various reasoning tasks. The authors found that training on data-rich tasks helps these models generalize better to GUI planning scenarios, leading to significant performance improvements. They conducted experiments showing that tasks like multimodal reasoning and text-based mathematical data can enhance GUI agent performance more than traditional GUI perception data. The study highlights the importance of cross-domain knowledge transfer and provides a method to overcome data scarcity in training GUI agents."
                },
                "zh": {
                    "title": "æå‡GUIä»£ç†æ€§èƒ½çš„å…³é”®åœ¨äºä»»åŠ¡æ³›åŒ–",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å¦‚ä½•æå‡å…¶æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºåœ¨ä¸“é—¨çš„ä¸­æœŸè®­ç»ƒé˜¶æ®µï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ•°æ®å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡æ¥è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä»¥ä¿ƒè¿›å…¶åœ¨GUIè§„åˆ’åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹11ä¸ªä¸­æœŸè®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä»»åŠ¡æ³›åŒ–æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡æ€æ•°å­¦æ¨ç†å¯¹AndroidWorldçš„æ€§èƒ½æå‡è¾¾åˆ°äº†6.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–çš„æ··åˆæ•°æ®é›†ï¼Œä»¥å®ç°æ›´å¤§çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10157",
            "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
            "url": "https://huggingface.co/papers/2504.10157",
            "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
            "score": 11,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "ba0402c49e397963",
            "authors": [
                "Xinnong Zhang",
                "Jiayu Lin",
                "Xinyi Mou",
                "Shiyue Yang",
                "Xiawei Liu",
                "Libo Sun",
                "Hanjia Lyu",
                "Yihang Yang",
                "Weihong Qi",
                "Yue Chen",
                "Guanying Li",
                "Ling Yan",
                "Yao Hu",
                "Siming Chen",
                "Yu Wang",
                "Jingxuan Huang",
                "Jiebo Luo",
                "Shiping Tang",
                "Libo Wu",
                "Baohua Zhou",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "Fudan University",
                "Indiana University",
                "Shanghai Innovation Institute",
                "University of Rochester",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10157.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#social_simulation",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "SocioVerse: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SocioVerse - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ¸Ğ· 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SocioVerse ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "SocioVerse: Enhancing Social Simulations with LLMs",
                    "desc": "This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains."
                },
                "zh": {
                    "title": "SocioVerseï¼šç¤¾ä¼šæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ",
                    "desc": "ç¤¾ä¼šæ¨¡æ‹Ÿæ­£åœ¨é€šè¿‡æ¨¡æ‹Ÿè™šæ‹Ÿä¸ªä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨æ¥æ”¹å˜ä¼ ç»Ÿç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•æ‰ä¸ªä½“å·®å¼‚å’Œé¢„æµ‹ç¾¤ä½“è¡Œä¸ºæ–¹é¢æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç¯å¢ƒã€ç›®æ ‡ç”¨æˆ·ã€äº’åŠ¨æœºåˆ¶å’Œè¡Œä¸ºæ¨¡å¼æ–¹é¢é¢ä¸´å¯¹é½æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SocioVerseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMä»£ç†çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸–ç•Œæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„å¯¹é½ç»„ä»¶å’Œ1000ä¸‡çœŸå®ä¸ªä½“çš„ç”¨æˆ·æ± ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09763",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
            "url": "https://huggingface.co/papers/2504.09763",
            "abstract": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.",
            "score": 11,
            "issue_id": 3240,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "c4ae0eadf040035f",
            "authors": [
                "Zaid Khan",
                "Elias Stengel-Eskin",
                "Archiki Prasad",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09763.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ EFA (Executable Functional Abstraction) - Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ EFAGen, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ EFA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞµĞµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. EFAGen ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ EFA Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ EFA Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Automating Advanced Math Problem Generation with EFAs",
                    "desc": "This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners."
                },
                "zh": {
                    "title": "è‡ªåŠ¨ç”Ÿæˆé«˜çº§æ•°å­¦é—®é¢˜çš„å¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡ï¼ˆEFAï¼‰çš„ç¨‹åºï¼Œè¿™äº›ç¨‹åºèƒ½å¤Ÿä»ç‰¹å®šçš„æ•°å­¦é—®é¢˜å®ä¾‹ä¸­æ¨å¯¼å‡ºæŠ½è±¡è¿‡ç¨‹ï¼Œå¹¶ç”Ÿæˆæ–°çš„ç›¸å…³å®ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ„å»ºé«˜çº§æ•°å­¦é—®é¢˜çš„EFAçš„æ–¹æ³•ï¼Œç§°ä¸ºEFAGenï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®ç§å­æ•°å­¦é—®é¢˜åŠå…¶é€æ­¥è§£å†³æ–¹æ¡ˆç”Ÿæˆå€™é€‰EFAç¨‹åºã€‚é€šè¿‡å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œæˆ‘ä»¬å®šä¹‰äº†æœ‰æ•ˆEFAå¿…é¡»å…·å¤‡çš„å±æ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›æµ‹è¯•ä½œä¸ºå¯éªŒè¯çš„å¥–åŠ±æ¥è®­ç»ƒLLMï¼Œæé«˜å…¶EFAç¼–å†™èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¯æ˜äº†EFAGenç”Ÿæˆçš„EFAèƒ½å¤Ÿä¿æŒä¸ç§å­é—®é¢˜çš„ä¸€è‡´æ€§ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé€‚åˆå­¦ä¹ è€…çš„ä¸åŒéš¾åº¦çš„æ•°å­¦é—®é¢˜å˜ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10471",
            "title": "MIEB: Massive Image Embedding Benchmark",
            "url": "https://huggingface.co/papers/2504.10471",
            "abstract": "Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
            "score": 10,
            "issue_id": 3248,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "5b9a143dfa0081cd",
            "authors": [
                "Chenghao Xiao",
                "Isaac Chung",
                "Imene Kerboua",
                "Jamie Stirling",
                "Xin Zhang",
                "MÃ¡rton Kardos",
                "Roman Solomatin",
                "Noura Al Moubayed",
                "Kenneth Enevoldsen",
                "Niklas Muennighoff"
            ],
            "affiliations": [
                "Aarhus University",
                "Contextual AI",
                "Durham University",
                "Esker",
                "INSA Lyon, LIRIS",
                "ITMO University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "Zendesk"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10471.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MIEB: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Massive Image Embedding Benchmark (MIEB) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. MIEB Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 130 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 38 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ 50 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² MIEB Ğ¸ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unifying Image Embedding Evaluation with MIEB",
                    "desc": "This paper presents the Massive Image Embedding Benchmark (MIEB), a comprehensive evaluation framework for image and image-text embedding models. It addresses the limitations of existing evaluation methods by assessing model performance across 130 tasks in 38 languages, grouped into 8 categories. The study benchmarks 50 different models, revealing that no single model excels in all areas, highlighting both strengths and weaknesses in advanced vision models. Additionally, it finds a strong correlation between the performance of vision encoders on MIEB and their effectiveness in multimodal large language models."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°å›¾åƒåµŒå…¥æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç§°ä¸ºå¤§è§„æ¨¡å›¾åƒåµŒå…¥åŸºå‡†ï¼ˆMIEBï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°å›¾åƒå’Œå›¾åƒ-æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚MIEB æ¶‰åŠ 38 ç§è¯­è¨€å’Œ 130 ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸º 8 ä¸ªé«˜å±‚æ¬¡ç±»åˆ«ï¼Œæ—¨åœ¨æä¾›å¯¹æ¨¡å‹èƒ½åŠ›çš„æ›´å…¨é¢ç†è§£ã€‚ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰å•ä¸€çš„æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶æ­ç¤ºäº†å…ˆè¿›è§†è§‰æ¨¡å‹åœ¨æ–‡æœ¬è§†è§‰è¡¨ç¤ºæ–¹é¢çš„æ½œåŠ›å’Œåœ¨å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä¸­çš„å±€é™æ€§ã€‚æœ€åï¼Œç»“æœè¡¨æ˜ï¼Œè§†è§‰ç¼–ç å™¨åœ¨ MIEB ä¸Šçš„è¡¨ç°ä¸å…¶åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°é«˜åº¦ç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09641",
            "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
            "url": "https://huggingface.co/papers/2504.09641",
            "abstract": "Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
            "score": 8,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "b7c9f390686ff6ef",
            "authors": [
                "Xingjian Zhang",
                "Siwei Wen",
                "Wenjun Wu",
                "Lei Huang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University",
                "Hangzhou International Innovation Institute, Beihang University, Hangzhou, China",
                "SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09641.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#rl",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: TinyLLaVA-Video-R1 Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TinyLLaVA-Video-R1 - Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. TinyLLaVA-Video-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ 'Ğ°Ğ³Ğ°-Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ»ÑÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Empowering Small Models for Big Reasoning in Video Understanding",
                    "desc": "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ä¹Ÿèƒ½æ¨ç†ï¼ŒTinyLLaVA-Video-R1åŠ©åŠ›è§†é¢‘ç†è§£ï¼",
                    "desc": "æœ€è¿‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶åŸºäºé«˜åº¦æ¨ç†å¯†é›†çš„æ•°æ®é›†ï¼Œå¦‚æ•°å­¦å’Œä»£ç ï¼Œä¸”é€šå¸¸é€‰æ‹©å¤§è§„æ¨¡æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ¢ç´¢å°è§„æ¨¡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶è€…ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚æ­¤å¤–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå…¶åœ¨ä¸€èˆ¬é—®ç­”æ•°æ®é›†ä¸Šçš„æ¨ç†è¿‡ç¨‹åŒæ ·å…·æœ‰æ„ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08066",
            "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search",
            "url": "https://huggingface.co/papers/2504.08066",
            "abstract": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.",
            "score": 7,
            "issue_id": 3245,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "a1d1275982a7e4b0",
            "authors": [
                "Yutaro Yamada",
                "Robert Tjarko Lange",
                "Cong Lu",
                "Shengran Hu",
                "Chris Lu",
                "Jakob Foerster",
                "Jeff Clune",
                "David Ha"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "FLAIR, University of Oxford",
                "Sakana AI",
                "University of British Columbia",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08066.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#healthcare",
                    "#ethics",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ The AI Scientist-v2 - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¸ÑˆĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ. ĞĞ´Ğ½Ğ° Ğ¸Ğ· ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ° Ğ½Ğ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€ ICLR, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ˜Ğ˜-ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¿Ñ€Ğ¾ÑˆĞµĞ´ÑˆĞµĞ¹ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Science: The AI Scientist-v2's Autonomous Research Breakthrough",
                    "desc": "The paper presents The AI Scientist-v2, an advanced AI system that autonomously conducts scientific research, from hypothesis generation to manuscript writing. This system improves upon its predecessor by eliminating the need for human-written code and effectively generalizing across various machine learning fields. It utilizes a progressive agentic tree-search method and incorporates a Vision-Language Model for enhancing the quality of figures in its manuscripts. The successful submission of AI-generated papers to a peer-reviewed workshop demonstrates the potential of AI to revolutionize scientific discovery and increase research productivity."
                },
                "zh": {
                    "title": "AIç§‘å­¦å®¶çš„æ–°çºªå…ƒï¼šå®Œå…¨è‡ªä¸»çš„ç§‘å­¦å‘ç°",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†AI Scientist-v2ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®Œå…¨è‡ªä¸»ç”Ÿæˆç§‘å­¦è®ºæ–‡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿­ä»£åœ°æå‡ºç§‘å­¦å‡è®¾ï¼Œè®¾è®¡å’Œæ‰§è¡Œå®éªŒï¼Œåˆ†æå’Œå¯è§†åŒ–æ•°æ®ï¼Œå¹¶æ’°å†™ç§‘å­¦æ‰‹ç¨¿ã€‚ä¸å…¶å‰èº«ç›¸æ¯”ï¼ŒAI Scientist-v2ä¸å†ä¾èµ–äººç±»ç¼–å†™çš„ä»£ç æ¨¡æ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨å­¦ä¹ é¢†åŸŸä¸­æœ‰æ•ˆæ³›åŒ–ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°çš„æ¸è¿›å¼ä»£ç†æ ‘æœç´¢æ–¹æ³•ã€‚é€šè¿‡æäº¤ä¸‰ç¯‡å®Œå…¨è‡ªä¸»æ’°å†™çš„æ‰‹ç¨¿åˆ°åŒè¡Œè¯„å®¡çš„ICLRç ”è®¨ä¼šï¼Œå…¶ä¸­ä¸€ç¯‡æˆåŠŸé€šè¿‡è¯„å®¡ï¼Œæ ‡å¿—ç€AIåœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„èƒ½åŠ›ä¸æ–­å¢å¼ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10415",
            "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.10415",
            "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.",
            "score": 6,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "15b7c5f49cceef01",
            "authors": [
                "Parshin Shojaee",
                "Ngoc-Hieu Nguyen",
                "Kazem Meidani",
                "Amir Barati Farimani",
                "Khoa D Doan",
                "Chandan K Reddy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "VinUniversity",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10415.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "LLM-SRBench: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-SRBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 239 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LLM-SRBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: LSR-Transform Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ LSR-Synth Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 31.5% ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "LLM-SRBench: A New Frontier in Scientific Equation Discovery",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies."
                },
                "zh": {
                    "title": "ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æ–°åŸºå‡†ï¼šLLM-SRBench",
                    "desc": "ç§‘å­¦æ–¹ç¨‹å‘ç°æ˜¯ç§‘å­¦è¿›æ­¥ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ¨å¯¼å‡ºè‡ªç„¶ç°è±¡çš„è§„å¾‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åˆ©ç”¨åµŒå…¥ç§‘å­¦çŸ¥è¯†è¿›è¡Œå‡è®¾ç”Ÿæˆçš„æ½œåŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„çœŸå®å‘ç°èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰åŸºå‡†å¾€å¾€ä¾èµ–äºå®¹æ˜“è¢«LLMsè®°å¿†çš„å¸¸è§æ–¹ç¨‹ï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡è¢«å¤¸å¤§ã€‚æœ¬æ–‡ä»‹ç»äº†LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•çš„è®°å¿†åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09130",
            "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
            "url": "https://huggingface.co/papers/2504.09130",
            "abstract": "Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.",
            "score": 5,
            "issue_id": 3242,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 12",
                "zh": "4æœˆ12æ—¥"
            },
            "hash": "3912c7cbd4c137f9",
            "authors": [
                "Yikun Wang",
                "Siyin Wang",
                "Qinyuan Cheng",
                "Zhaoye Fei",
                "Liang Ding",
                "Qipeng Guo",
                "Dacheng Tao",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09130.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VisuoThink: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "VisuoThink - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ, Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. VisuoThink Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Reasoning with VisuoThink: A Multimodal Approach",
                    "desc": "This paper presents VisuoThink, a new framework designed to improve reasoning in Large Vision-Language Models. It addresses the shortcomings of existing methods that struggle with complex reasoning tasks by integrating visual and textual information in a more human-like manner. VisuoThink allows for progressive reasoning through a combination of visual and linguistic inputs, enhancing the model's ability to perform tasks that require spatial and geometric understanding. The framework shows significant improvements in reasoning capabilities during inference, achieving top performance without the need for additional fine-tuning."
                },
                "zh": {
                    "title": "VisuoThinkï¼šæå‡è§†è§‰-è¯­è¨€æ¨ç†çš„æ–°æ¡†æ¶",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å°è¯•äº†åŸºäºæ–‡æœ¬çš„æ…¢æ€è€ƒæˆ–ç®€å•çš„è§†è§‰è¾…åŠ©ï¼Œä½†æœªèƒ½æœ‰æ•ˆæ•æ‰äººç±»è§†è§‰-è¯­è¨€æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisuoThinkæ¡†æ¶ï¼Œå®ƒå°†è§†è§‰ç©ºé—´å’Œè¯­è¨€é¢†åŸŸæ— ç¼æ•´åˆï¼Œä¿ƒè¿›å¤šæ¨¡æ€çš„æ…¢æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼ŒVisuoThinkåœ¨å‡ ä½•å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09689",
            "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
            "url": "https://huggingface.co/papers/2504.09689",
            "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "11b21970119f2c59",
            "authors": [
                "Jiahao Qiu",
                "Yinghui He",
                "Xinzhe Juan",
                "Yiming Wang",
                "Yuhan Liu",
                "Zixin Yao",
                "Yue Wu",
                "Xun Jiang",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Computer Science, Princeton University",
                "Department of Data Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University",
                "Department of Philosophy, Columbia University",
                "Theta Health Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09689.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#ethics",
                    "#healthcare"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EmoAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. EmoAgent ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: EmoEval, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¸ EmoGuard, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. EmoGuard Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ñ€Ğ¸ÑĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent",
                    "desc": "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."
                },
                "zh": {
                    "title": "EmoAgentï¼šä¿éšœäººæœºäº¤äº’å¿ƒç†å®‰å…¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„äººå·¥æ™ºèƒ½è§’è‰²çš„å…´èµ·ï¼Œç‰¹åˆ«æ˜¯å¯¹å¿ƒç†éšœç¢çš„è„†å¼±ç”¨æˆ·ï¼Œå®‰å…¨é—®é¢˜å¼•èµ·äº†å…³æ³¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†EmoAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå‡è½»äººæœºäº¤äº’ä¸­çš„å¿ƒç†å¥åº·å±å®³ã€‚EmoAgentåŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šEmoEvalæ¨¡æ‹Ÿè™šæ‹Ÿç”¨æˆ·ï¼Œè¯„ä¼°ä¸AIè§’è‰²äº¤äº’å‰åçš„å¿ƒç†å¥åº·å˜åŒ–ï¼Œå¹¶ä½¿ç”¨ç»è¿‡ä¸´åºŠéªŒè¯çš„å¿ƒç†è¯„ä¼°å·¥å…·è¿›è¡Œè¯„ä¼°ã€‚EmoGuardä½œä¸ºä¸­ä»‹ï¼Œç›‘æµ‹ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ï¼Œé¢„æµ‹æ½œåœ¨ä¼¤å®³ï¼Œå¹¶æä¾›çº æ­£åé¦ˆï¼Œä»¥é™ä½é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09522",
            "title": "How new data permeates LLM knowledge and how to dilute it",
            "url": "https://huggingface.co/papers/2504.09522",
            "abstract": "Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/",
            "score": 4,
            "issue_id": 3242,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "d9aa235633073967",
            "authors": [
                "Chen Sun",
                "Renat Aksitov",
                "Andrey Zhmoginov",
                "Nolan Andrew Miller",
                "Max Vladymyrov",
                "Ulrich Rueckert",
                "Been Kim",
                "Mark Sandler"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09522.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#hallucinations",
                    "#long_context",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 'Outlandish' Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ÑĞ»Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ°: Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° 'stepping-stone' Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 'ignore-k'."
                },
                "en": {
                    "title": "Understanding and Controlling Knowledge Priming in LLMs",
                    "desc": "This paper explores how large language models (LLMs) learn new information and how this learning can unintentionally affect their existing knowledge. The authors identify a phenomenon called 'priming,' where new facts can lead to incorrect applications of knowledge in unrelated situations. They introduce a dataset named 'Outlandish' to study this priming effect and find that the likelihood of priming can be predicted by analyzing token probabilities of key words before learning. Additionally, they propose two techniques to mitigate undesirable priming while maintaining the model's learning capabilities, achieving significant reductions in priming effects."
                },
                "zh": {
                    "title": "ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å­¦ä¹ ä¸å¯åŠ¨æ•ˆåº”",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åŸºäºæ¢¯åº¦çš„æ›´æ–°ä¸æ–­å­¦ä¹ ï¼Œä½†æ–°ä¿¡æ¯å¦‚ä½•å½±å“å·²æœ‰çŸ¥è¯†ä»ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å­¦ä¹ æ–°ä¿¡æ¯æ—¶ï¼Œæ¨¡å‹ä¼šå‡ºç°â€œå¯åŠ¨æ•ˆåº”â€ï¼Œå³å­¦ä¹ æ–°äº‹å®å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­é”™è¯¯åº”ç”¨è¿™äº›çŸ¥è¯†ã€‚ä¸ºç³»ç»Ÿç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œOutlandishâ€æ•°æ®é›†ï¼ŒåŒ…å«1320ä¸ªå¤šæ ·åŒ–çš„æ–‡æœ¬æ ·æœ¬ï¼Œæ—¨åœ¨æ¢è®¨æ–°çŸ¥è¯†å¦‚ä½•æ¸—é€åˆ°æ¨¡å‹çš„ç°æœ‰çŸ¥è¯†åº“ä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æµ‹é‡å…³é”®å­—çš„tokenæ¦‚ç‡ï¼Œå¯ä»¥é¢„æµ‹å­¦ä¹ æ–°ä¿¡æ¯åçš„å¯åŠ¨ç¨‹åº¦ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°æŠ€æœ¯æ¥è°ƒèŠ‚æ–°çŸ¥è¯†å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10449",
            "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10449",
            "abstract": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.",
            "score": 3,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "565dae169d16775e",
            "authors": [
                "Junxiong Wang",
                "Wen-Ding Li",
                "Daniele Paliotta",
                "Daniel Ritter",
                "Alexander M. Rush",
                "Tri Dao"
            ],
            "affiliations": [
                "Cornell University",
                "Princeton University",
                "TogetherAI",
                "University of Geneva"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10449.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ RNN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ RNN, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ M1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mamba. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AIME Ğ¸ MATH Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ M1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Deepseek R1 Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğœ1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ñ‚Ñ€ĞµÑ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "M1: A Fast and Efficient Reasoning Model for Complex Math Problems",
                    "desc": "This paper presents a new reasoning model called M1, which combines linear RNNs with the Mamba architecture to improve efficiency in solving complex mathematical problems. Unlike traditional transformer models that struggle with long context due to their computational limits, M1 utilizes a hybrid approach that allows for memory-efficient inference. The model is trained using a distillation process and reinforcement learning, resulting in performance that rivals state-of-the-art models while being faster. Experimental results demonstrate that M1 achieves significant speed improvements and higher accuracy compared to existing reasoning models, making it a promising solution for scalable test-time generation."
                },
                "zh": {
                    "title": "æ··åˆMambaæ¨ç†æ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼ŒåŸºäºMambaæ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤æ‚æ•°å­¦é—®é¢˜çš„æ¨ç†æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒM1åœ¨å†…å­˜ä½¿ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚é€šè¿‡å¯¹ç°æœ‰æ¨ç†æ¨¡å‹çš„è’¸é¦è¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒM1åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„çº¿æ€§RNNæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM1åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šæ¯”åŒè§„æ¨¡çš„å˜æ¢å™¨å¿«è¶…è¿‡3å€ï¼ŒåŒæ—¶åœ¨å›ºå®šç”Ÿæˆæ—¶é—´é¢„ç®—ä¸‹å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10430",
            "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
            "url": "https://huggingface.co/papers/2504.10430",
            "abstract": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.",
            "score": 1,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "01daa1337864118c",
            "authors": [
                "Minqian Liu",
                "Zhiyang Xu",
                "Xinyi Zhang",
                "Heajun An",
                "Sarvech Qadir",
                "Qi Zhang",
                "Pamela J. Wisniewski",
                "Jin-Hee Cho",
                "Sang Won Lee",
                "Ruoxi Jia",
                "Lifu Huang"
            ],
            "affiliations": [
                "UC Davis",
                "Vanderbilt University",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10430.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#rlhf",
                    "#healthcare",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PersuSafety Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 8 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ LLM."
                },
                "en": {
                    "title": "Ensuring Ethical Persuasion in Large Language Models",
                    "desc": "This paper investigates the safety risks associated with Large Language Models (LLMs) when used for persuasion. It focuses on two main areas: the ability of LLMs to reject unethical persuasion tasks and the influence of factors like personality traits on their behavior. The authors introduce a framework called PersuSafety, which evaluates persuasion safety through scene creation, conversation simulation, and safety assessment. Their experiments reveal that many LLMs struggle to identify harmful tasks and often employ unethical strategies, highlighting the need for improved safety measures in persuasive applications."
                },
                "zh": {
                    "title": "æå‡è¯´æœå®‰å…¨æ€§ï¼Œé˜²èŒƒä¸é“å¾·å½±å“",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å…¶åœ¨è¯´æœèƒ½åŠ›ä¸Šæ¥è¿‘äººç±»æ°´å¹³ã€‚ç„¶è€Œï¼Œè¿™ç§æ½œåŠ›ä¹Ÿå¼•å‘äº†å¯¹LLMé©±åŠ¨çš„è¯´æœå®‰å…¨é£é™©çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¯èƒ½é€šè¿‡æ“æ§ã€æ¬ºéª—å’Œåˆ©ç”¨è„†å¼±æ€§ç­‰ä¸é“å¾·æ‰‹æ®µè¿›è¡Œå½±å“ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†LLMè¯´æœçš„å®‰å…¨æ€§ï¼Œé‡ç‚¹å…³æ³¨LLMæ˜¯å¦èƒ½æ‹’ç»ä¸é“å¾·çš„è¯´æœä»»åŠ¡ï¼Œä»¥åŠä¸ªæ€§ç‰¹å¾å’Œå¤–éƒ¨å‹åŠ›å¦‚ä½•å½±å“å…¶è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†PersuSafetyæ¡†æ¶ï¼Œè¯„ä¼°è¯´æœå®‰å…¨æ€§ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°å¤§å¤šæ•°LLMåœ¨è¯†åˆ«æœ‰å®³è¯´æœä»»åŠ¡å’Œä½¿ç”¨ä¸é“å¾·ç­–ç•¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å®‰å…¨éšæ‚£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09858",
            "title": "Reasoning Models Can Be Effective Without Thinking",
            "url": "https://huggingface.co/papers/2504.09858",
            "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.",
            "score": 1,
            "issue_id": 3250,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "6aa94cff6001428d",
            "authors": [
                "Wenjie Ma",
                "Jingxuan He",
                "Charlie Snell",
                "Tyler Griggs",
                "Sewon Min",
                "Matei Zaharia"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09858.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ NoThinking Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NoThinking Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¸ Ğ¸Ñ… Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ."
                },
                "en": {
                    "title": "Rethinking Reasoning: Less Thinking, More Efficiency!",
                    "desc": "This paper investigates the necessity of explicit thinking processes in large language models (LLMs) for reasoning tasks. The authors introduce a method called NoThinking, which simplifies prompting and shows that it can outperform traditional thinking methods in various reasoning challenges. They demonstrate that using NoThinking in a parallel scaling approach, where multiple outputs are generated and aggregated, yields competitive results with lower latency. This research suggests that lengthy thinking processes may not be essential for effective reasoning, especially in resource-constrained environments."
                },
                "zh": {
                    "title": "é‡æ–°æ€è€ƒæ¨ç†è¿‡ç¨‹çš„å¿…è¦æ€§",
                    "desc": "æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œä¸»è¦æ˜¯é€šè¿‡å°†æ˜ç¡®ä¸”å†—é•¿çš„æ€è€ƒè¿‡ç¨‹çº³å…¥ç”Ÿæˆä¸­ã€‚æœ¬æ–‡è´¨ç–‘è¿™ç§æ˜ç¡®æ€è€ƒæ˜¯å¦çœŸçš„å¿…è¦ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„DeepSeek-R1-Distill-Qwenå‘ç°ï¼Œé€šè¿‡ç®€å•æç¤ºè·³è¿‡æ€è€ƒè¿‡ç¨‹ï¼ˆç§°ä¸ºNoThinkingï¼‰åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ä½é¢„ç®—è®¾ç½®ä¸‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒNoThinkingæ–¹æ³•åœ¨ç”Ÿæˆå¤šä¸ªç‹¬ç«‹è¾“å‡ºå¹¶è¿›è¡Œèšåˆæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨ç†æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ€è€ƒè¿‡ç¨‹çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09518",
            "title": "3D CoCa: Contrastive Learners are 3D Captioners",
            "url": "https://huggingface.co/papers/2504.09518",
            "abstract": "3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa.",
            "score": 1,
            "issue_id": 3247,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "053d31a29b03d829",
            "authors": [
                "Ting Huang",
                "Zeyu Zhang",
                "Yemin Wang",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai University of Engineering Science",
                "The Australian National University",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09518.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#3d",
                    "#alignment",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "3D CoCa - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ CLIP-Ğ±ÑĞºĞ±Ğ¾Ğ½, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², 3D CoCa Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ 3D CoCa Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ScanRefer Ğ¸ Nr3D."
                },
                "en": {
                    "title": "Revolutionizing 3D Captioning with Unified Learning",
                    "desc": "This paper introduces 3D CoCa, a new framework for 3D captioning that describes 3D scenes using natural language. It combines contrastive vision-language learning with 3D caption generation, addressing challenges like point cloud sparsity and weak alignment between visual and textual data. The model uses a frozen CLIP backbone for semantic understanding and a 3D scene encoder for geometric context, allowing it to generate captions without relying on external object detectors. Experimental results show that 3D CoCa outperforms existing methods, achieving significant improvements in captioning accuracy on benchmark datasets."
                },
                "zh": {
                    "title": "3D CoCaï¼šæ— ç¼ç»“åˆè§†è§‰ä¸è¯­è¨€çš„3Dæ ‡é¢˜ç”Ÿæˆ",
                    "desc": "3Dæ ‡é¢˜ç”Ÿæˆæ—¨åœ¨ç”¨è‡ªç„¶è¯­è¨€æè¿°3Dåœºæ™¯çš„å†…å®¹ï¼Œä½†ç”±äºç‚¹äº‘ç¨€ç–å’Œè·¨æ¨¡æ€å¯¹é½ä¸è¶³ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†3D CoCaï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹æ¯”è§†è§‰-è¯­è¨€å­¦ä¹ ä¸3Dæ ‡é¢˜ç”Ÿæˆæ— ç¼ç»“åˆçš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„CLIPè§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œæä¾›ä¸°å¯Œçš„è¯­ä¹‰å…ˆéªŒï¼Œä½¿ç”¨ç©ºé—´æ„ŸçŸ¥çš„3Dåœºæ™¯ç¼–ç å™¨æ•æ‰å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è§£ç å™¨ç”Ÿæˆæè¿°æ€§æ ‡é¢˜ã€‚ä¸ä¾èµ–æ˜¾å¼å¯¹è±¡æè®®çš„ä¸¤é˜¶æ®µæ–¹æ³•ä¸åŒï¼Œ3D CoCaåœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­è”åˆä¼˜åŒ–å¯¹æ¯”å’Œæ ‡é¢˜ç”Ÿæˆç›®æ ‡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ£€æµ‹å™¨æˆ–æ‰‹å·¥æè®®çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08120",
            "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
            "url": "https://huggingface.co/papers/2504.08120",
            "abstract": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use.",
            "score": 1,
            "issue_id": 3245,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "2cc9864fc01e2a7b",
            "authors": [
                "Daniil Larionov",
                "Sotaro Takeshita",
                "Ran Zhang",
                "Yanran Chen",
                "Christoph Leiter",
                "Zhipin Wang",
                "Christian Greisinger",
                "Steffen Eger"
            ],
            "affiliations": [
                "University of Mannheim",
                "University of Technology Nuremberg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08120.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑ… Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (reasoning-enabled LLM) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WMT23 Ğ¸ SummEval Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ OpenAI o3-mini Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº DeepSeek-R1 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (32B), Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… (8B)."
                },
                "en": {
                    "title": "Evaluating Reasoning in Language Models for Better NLG Performance",
                    "desc": "This paper investigates how reasoning-enabled large language models (LLMs) perform in evaluating natural language generation tasks like machine translation and text summarization. It compares reasoning-based models, such as DeepSeek-R1 and OpenAI o3, with non-reasoning models across various sizes and architectures. The findings indicate that the effectiveness of reasoning capabilities varies by model and task, with some models benefiting from increased reasoning intensity while others do not. Additionally, the study highlights that distillation of reasoning abilities can preserve performance in medium-sized models but leads to significant degradation in smaller ones."
                },
                "zh": {
                    "title": "æ¨ç†èƒ½åŠ›æå‡è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æ½œåŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†èƒ½åŠ›å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ¨ç†çš„æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒOpenAI o3ï¼‰ä¸éæ¨ç†æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†èƒ½åŠ›çš„ä¼˜åŠ¿ä¾èµ–äºå…·ä½“æ¨¡å‹å’Œä»»åŠ¡ï¼ŒæŸäº›æƒ…å†µä¸‹æ¨ç†æ¨¡å‹çš„è¡¨ç°ä¸å¦‚å…¶éæ¨ç†ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜å‘ç°ï¼Œæ¨ç†èƒ½åŠ›çš„è’¸é¦åœ¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ä¸­ä¿æŒäº†åˆç†çš„æ€§èƒ½ï¼Œä½†åœ¨å°å‹æ¨¡å‹ä¸­æ˜¾è‘—ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05782",
            "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2504.05782",
            "abstract": "Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.",
            "score": 1,
            "issue_id": 3246,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "6764ce8059618273",
            "authors": [
                "Pengfei Zhou",
                "Fanrui Zhang",
                "Xiaopeng Peng",
                "Zhaopan Xu",
                "Jiaxin Ai",
                "Yansheng Qiu",
                "Chuanhao Li",
                "Zhen Li",
                "Ming Li",
                "Yukang Feng",
                "Jianwen Sun",
                "Haoquan Zhang",
                "Zizhen Li",
                "Xiaofeng Mao",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojun Chang",
                "Wenqi Shao",
                "Yang You",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "MBZUAI",
                "NUS",
                "RIT",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "USTC",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05782.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#agi",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MDK12-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MDK12-Bench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ² K-12. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 140 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ´Ğ¾ 12 ĞºĞ»Ğ°ÑÑĞ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° MDK12-Bench Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… MLLM Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Evaluating Multimodal Reasoning with MDK12-Bench",
                    "desc": "This paper introduces MDK12-Bench, a new benchmark designed to evaluate the multimodal reasoning abilities of Multimodal Large Language Models (MLLMs) using real-world K-12 exam questions. It covers six subjects and includes 140,000 reasoning instances with varying difficulty levels, providing a structured way to assess model performance. The benchmark also features detailed annotations and a dynamic evaluation framework to address data contamination issues. Results from testing on MDK12-Bench highlight the current limitations of MLLMs in multimodal reasoning, offering valuable insights for future model development."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨ç†è¯„ä¼°çš„æ–°åŸºå‡†",
                    "desc": "å¤šæ¨¡æ€æ¨ç†æ˜¯å°†è¯­è¨€å’Œè§†è§‰çº¿ç´¢ç»“åˆèµ·æ¥è¿›è¡Œé—®é¢˜è§£å†³å’Œå†³ç­–çš„é‡è¦èƒ½åŠ›ï¼Œæ˜¯äººç±»æ™ºèƒ½çš„åŸºæœ¬ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†MDK12-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå­¦ç§‘åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€åœ°ç†å’Œä¿¡æ¯ç§‘å­¦å…­ä¸ªå­¦ç§‘ï¼ŒåŒ…å«14ä¸‡ä¸ªæ¨ç†å®ä¾‹ï¼Œé€‚ç”¨äºä»å°å­¦åˆ°12å¹´çº§çš„ä¸åŒéš¾åº¦æ°´å¹³ã€‚é€šè¿‡åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬è§£å†³äº†æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†å½“å‰MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09513",
            "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
            "url": "https://huggingface.co/papers/2504.09513",
            "abstract": "Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics.",
            "score": 0,
            "issue_id": 3247,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "ed55bc95454d113a",
            "authors": [
                "Puyu Han",
                "Jiaju Kang",
                "Yuhang Pan",
                "Erting Pan",
                "Zeyu Zhang",
                "Qunchao Jin",
                "Juntao Jiang",
                "Zhichen Liu",
                "Luqi Gong"
            ],
            "affiliations": [
                "AI Geeks",
                "Beijing Normal University",
                "Hebei Guoyan Science and Technology Center",
                "Southern University of Science and Technology",
                "The Australian National University",
                "Wuhan University",
                "Zhejiang Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09513.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#dataset",
                    "#ethics"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "DiffuMural: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ„Ñ€ĞµÑĞ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiffuMural - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ„Ñ€ĞµÑĞ¾Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ControlNet Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 23 ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµÑĞºĞ°Ñ… Ğ”ÑƒĞ½ÑŒÑ…ÑƒĞ°Ğ½Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Reviving Ancient Art: DiffuMural's Innovative Restoration Approach",
                    "desc": "This paper introduces DiffuMural, a novel approach for restoring ancient murals using large-scale pre-trained diffusion models. The method addresses challenges such as large defective areas and limited training samples by employing a Multi-scale convergence and Collaborative Diffusion mechanism. It focuses on aesthetic standards and incorporates a unique evaluation framework that includes metrics for factual accuracy and visual coherence. The results show that DiffuMural significantly outperforms existing methods in restoring intricate details while preserving the cultural significance of the murals."
                },
                "zh": {
                    "title": "DiffuMuralï¼šå¤ä»£å£ç”»ä¿®å¤çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffuMuralçš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤ä»£å£ç”»ä¿®å¤ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¤šå°ºåº¦æ”¶æ•›å’ŒååŒæ‰©æ•£æœºåˆ¶ï¼Œåˆ©ç”¨ControlNetå’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±æ¥ä¼˜åŒ–ç”Ÿæˆå›¾åƒä¸æ¡ä»¶æ§åˆ¶ä¹‹é—´çš„åŒ¹é…ã€‚DiffuMuralåœ¨ä¿®å¤å¤æ‚ç»†èŠ‚å’Œä¿æŒæ•´ä½“ç¾è§‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¼ºå¤±ä¿¡æ¯çš„å£ç”»æ—¶ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå®šé‡æŒ‡æ ‡å’Œäººæ–‡ä»·å€¼è¯„ä¼°ï¼Œç¡®ä¿ä¿®å¤åçš„å£ç”»ä¿ç•™å…¶æ–‡åŒ–å’Œè‰ºæœ¯æ„ä¹‰ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 13,
        "#agents": 5,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 1,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 14,
        "#math": 5,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 3,
        "#training": 11,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 14,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 4,
        "#security": 1,
        "#optimization": 10,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0,
        "#social_simulation": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
        "pinyin": "WÇ’men jiÃ¨shÃ o le InternVL3, zhÃ¨ shÃ¬ InternVL xÃ¬liÃ¨ de zhÃ²ngdÃ  jÃ¬nbÃ¹, cÇiyÃ²ng le bÄ›ndÃ¬ duÅmÃ³shÄ« yÃ¹xÃ¹nliÃ n fÃ nshÃ¬. YÇ” qÃ­tÄ fÄngfÇ bÃ¹tÃ³ng, InternVL3 zÃ i dÄn gÃ¨ yÃ¹xÃ¹nliÃ n jiÄ“duÃ n nÃ¨i, cÃ³ng duÅmÃ³shÄ« shÃ¹jÃ¹ hÃ© chÃºn wÃ©nbÄ›n shÃ¹jÃ¹ zhÅng tÃ³ngshÃ­ huÃ²dÃ© duÅmÃ³shÄ« hÃ© yÇ”yÃ¡n nÃ©nglÃ¬. ZhÃ¨ zhÇ’ng tÇ’ngyÄ« de xÃ¹nliÃ n fÃ nshÃ¬ yÇ’uxiÃ o jiÄ›juÃ© le chuÃ¡ntÇ’ng xÃ¹nliÃ n liÃºchÃ©ng zhÅng de fÃ¹zÃ¡xÃ¬ng hÃ© duÃ¬qÇ tiÇozhÃ n. WÃ¨ile jÃ¬nfÅ« tÄ«gÄo xÃ­ngnÃ©ng hÃ© kÄ›kuÃ²zhÇn xÃ¬ng, InternVL3 shÇyÃ²ng le kÄ›biÃ n shÃ¬juÃ© wÃ¨izhÃ¬ biÄnmÇ hÃ© xiÄnjÃ¬n de hÃ²uxÃ¹nliÃ n jÃ¬shÃ¹. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, InternVL3 zÃ i duÅzhÇ’ng duÅmÃ³shÄ« rÃ¨nwÃ¹ zhÅng biÇoxiÃ n yÅuyÃ¹, tÃ¨biÃ© shÃ¬ InternVL3-78B zÃ i MMMU jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng huÃ²dÃ© le 72.2 de fÄ“nshÃ¹, chuÃ ng xiÃ  xÄ«n jÃ¬lÃ¹. WÇ’men jiÄng gÅngkÄi fÄbÃ¹ xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© mÃ³xÃ­ng quÃ¡nzhÃ²ng, yÇ cÃ¹jÃ¬n xiÃ  yÄ«dÃ i MLLMs de yÃ¡njiÅ« hÃ© kÄifÄ.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'},\n{'word': 'é‡å¤§', 'pinyin': 'zhÃ²ng dÃ ', 'trans': 'major'},\n{'word': 'è¿›æ­¥', 'pinyin': 'jÃ¬n bÃ¹', 'trans': 'progress'},\n{'word': 'é‡‡ç”¨', 'pinyin': 'cÇi yÃ²ng', 'trans': 'adopt'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'},\n{'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'},\n{'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},\n{'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'},\n{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},\n{'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'å¯å˜', 'pinyin': 'kÄ› biÃ n', 'trans': 'variable'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'ä½ç½®', 'pinyin': 'wÃ¨i zhÃ¬', 'trans': 'position'},\n{'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encoding'},\n{'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'},\n{'word': 'åè®­ç»ƒ', 'pinyin': 'hÃ²u xÃ¹n liÃ n', 'trans': 'post-training'},\n{'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},\n{'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨ biÃ©', 'trans': 'especially'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'},\n{'word': 'åˆ†æ•°', 'pinyin': 'fÄ“n shÃ¹', 'trans': 'score'},\n{'word': 'çºªå½•', 'pinyin': 'jÃ¬ lÃ¹', 'trans': 'record'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},\n{'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'},\n{'word': 'æƒé‡', 'pinyin': 'quÃ¡n zhÃ²ng', 'trans': 'weights'},\n{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'},\n{'word': 'ä¸‹ä¸€ä»£', 'pinyin': 'xiÃ  yÄ« dÃ i', 'trans': 'next generation'},\n{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'},\n{'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'development'}]",
        "trans": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.",
        "update_ts": "2025-04-15 09:12"
    }
}