{
    "date": {
        "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 18",
        "zh": "12æœˆ18æ—¥"
    },
    "time_utc": "2024-12-18 21:09",
    "weekday": 2,
    "issue_id": 1199,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.13147",
            "title": "Are Your LLMs Capable of Stable Reasoning?",
            "url": "https://huggingface.co/papers/2412.13147",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK.",
            "score": 56,
            "issue_id": 1181,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "a030a3cb6cc36da2",
            "authors": [
                "Junnan Liu",
                "Hongwei Liu",
                "Linchen Xiao",
                "Ziyi Wang",
                "Kuikun Liu",
                "Songyang Gao",
                "Wenwei Zhang",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13147.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#evaluation",
                    "#reasoning",
                    "#leakage"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ G-Pass@k, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¸ĞºĞ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ĞµÑ‘ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LiveMathBench Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 'Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ…' ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing LLM Evaluation for Real-World Reasoning",
                    "desc": "This paper addresses the gap between the performance of Large Language Models (LLMs) on benchmarks and their effectiveness in real-world scenarios, particularly in complex reasoning tasks. The authors propose a new evaluation metric called G-Pass@k, which assesses model performance over multiple attempts, focusing on both peak performance and stability. Additionally, they introduce LiveMathBench, a benchmark of challenging mathematical problems that reduces data leakage during evaluation. The study reveals that current LLMs have significant room for improvement in their reasoning capabilities, emphasizing the need for better evaluation methods."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸€å·®è·ä¸»è¦æºäºå½“å‰çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡æ— æ³•å…¨é¢åæ˜ LLMsçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å‡†ç¡®æ€§å’Œä¸€è‡´æ€§è‡³å…³é‡è¦çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†G-Pass@kè¿™ä¸€æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¬¡é‡‡æ ·ä¸­æŒç»­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶é‡åŒ–æ¨¡å‹çš„æœ€ä½³è¡¨ç°æ½œåŠ›å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†LiveMathBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€åŸºå‡†ï¼ŒåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°ä»£æ•°å­¦é—®é¢˜ï¼Œæ—¨åœ¨å‡å°‘è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ•°æ®æ³„éœ²é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13018",
            "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
            "url": "https://huggingface.co/papers/2412.13018",
            "abstract": "As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47\\% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in https://github.com/RUC-NLPIR/OmniEval{https://github.com/RUC-NLPIR/OmniEval}.",
            "score": 28,
            "issue_id": 1184,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "293aa1b03b853973",
            "authors": [
                "Shuting Wang",
                "Jiejun Tan",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13018.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#rag",
                    "#science"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "OmniEval: Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniEval - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Retrieval-Augmented Generation (RAG) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² RAG, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ 5 ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 16 Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ°Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. OmniEval Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "OmniEval: Elevating RAG Evaluation in Finance",
                    "desc": "This paper presents OmniEval, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) techniques specifically in the financial domain. It features a multi-dimensional evaluation framework that categorizes queries into various task classes and financial topics, allowing for structured assessments. The benchmark combines automatic data generation using GPT-4 with human annotations to ensure high-quality evaluation instances. The study reveals significant performance variations in RAG systems, highlighting areas for improvement and providing a comprehensive resource for future research in this area."
                },
                "zh": {
                    "title": "OmniEvalï¼šé‡‘èé¢†åŸŸçš„å…¨æ–¹ä½RAGè¯„ä¼°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOmniEvalçš„å…¨æ–¹ä½è‡ªåŠ¨åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åŸºå‡†ï¼Œä¸“æ³¨äºé‡‘èé¢†åŸŸã€‚è¯¥åŸºå‡†å…·æœ‰å¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºäºçŸ©é˜µçš„RAGåœºæ™¯è¯„ä¼°ç³»ç»Ÿï¼Œèƒ½å¤Ÿå°†æŸ¥è¯¢åˆ†ç±»ä¸ºäº”ä¸ªä»»åŠ¡ç±»åˆ«å’Œ16ä¸ªé‡‘èä¸»é¢˜ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ç»“åˆGPT-4è‡ªåŠ¨ç”Ÿæˆå’Œäººå·¥æ ‡æ³¨çš„å¤šç»´è¯„ä¼°æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆå®ä¾‹çš„é«˜æ¥å—ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEvalåœ¨è¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½æ–¹é¢å…·æœ‰å…¨é¢æ€§ï¼Œæ­ç¤ºäº†RAGæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæå‡èƒ½åŠ›çš„æ˜¾è‘—æœºä¼šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12606",
            "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models",
            "url": "https://huggingface.co/papers/2412.12606",
            "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether LMMs align with the diverse needs of humans in real-world scenarios. To bridge this gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which includes over 500 images covering six common scenarios of human life. Notably, the MDI-Benchmark offers two significant advantages over existing evaluations: (1) Each image is accompanied by two types of questions: simple questions to assess the model's understanding of the image, and complex questions to evaluate the model's ability to analyze and reason beyond basic content. (2) Recognizing that people of different age groups have varying needs and perspectives when faced with the same scenario, our benchmark stratifies questions into three age categories: young people, middle-aged people, and older people. This design allows for a detailed assessment of LMMs' capabilities in meeting the preferences and needs of different age groups. With MDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related tasks, indicating that existing LMMs still have considerable room for improvement in addressing real-world applications. Looking ahead, we anticipate that the MDI-Benchmark will open new pathways for aligning real-world personalization in LMMs. The MDI-Benchmark data and evaluation code are available at https://mdi-benchmark.github.io/",
            "score": 28,
            "issue_id": 1182,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "75fba478b54ada06",
            "authors": [
                "YiFan Zhang",
                "Shanglin Lei",
                "Runqi Qiao",
                "Zhuoma GongQue",
                "Xiaoshuai Song",
                "Guanting Dong",
                "Qiuna Tan",
                "Zhe Wei",
                "Peiqing Yang",
                "Ye Tian",
                "Yadong Xue",
                "Xiaofei Wang",
                "Honggang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12606.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multi-Dimensional Insights (MDI) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MDI Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 500 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ 6 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼. Ğ”Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 79% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing LMM Evaluation with Age-Responsive Insights",
                    "desc": "This paper introduces the Multi-Dimensional Insights (MDI) benchmark, designed to evaluate large multimodal models (LMMs) in a more comprehensive way. It includes over 500 images and features two types of questions: simple ones for basic understanding and complex ones for deeper analysis and reasoning. The benchmark also categorizes questions by age groups, recognizing that different ages have unique perspectives and needs. The results show that while models like GPT-4o perform well, there is still significant room for improvement in aligning LMMs with real-world applications."
                },
                "zh": {
                    "title": "å¤šç»´æ´å¯ŸåŸºå‡†ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æ ‡å‡†",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ ‡å‡†æ— æ³•å…¨é¢ã€å®¢è§‚åœ°è¯„ä¼°è¿™äº›æ¨¡å‹æ˜¯å¦æ»¡è¶³äººç±»åœ¨ç°å®åœºæ™¯ä¸­çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç»´æ´å¯Ÿï¼ˆMDIï¼‰åŸºå‡†ï¼ŒåŒ…å«500å¤šå¼ å›¾ç‰‡ï¼Œæ¶µç›–å…­ç§å¸¸è§çš„äººç±»ç”Ÿæ´»åœºæ™¯ã€‚MDIåŸºå‡†çš„ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿æ˜¯ï¼šæ¯å¼ å›¾ç‰‡é…æœ‰ç®€å•å’Œå¤æ‚ä¸¤ç§é—®é¢˜ï¼Œè¯„ä¼°æ¨¡å‹å¯¹å›¾åƒçš„ç†è§£å’Œåˆ†ææ¨ç†èƒ½åŠ›ï¼›åŒæ—¶ï¼ŒåŸºå‡†æ ¹æ®ä¸åŒå¹´é¾„æ®µçš„éœ€æ±‚ï¼Œå°†é—®é¢˜åˆ†ä¸ºå¹´è½»äººã€ä¸­å¹´äººå’Œè€å¹´äººä¸‰ç±»ã€‚é€šè¿‡MDIåŸºå‡†ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨LMMsåœ¨ç°å®åº”ç”¨ä¸­çš„ä¸ªæ€§åŒ–å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13171",
            "title": "Compressed Chain of Thought: Efficient Reasoning Through Dense Representations",
            "url": "https://huggingface.co/papers/2412.13171",
            "abstract": "Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the-shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.",
            "score": 14,
            "issue_id": 1194,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "ae88057a656ae089",
            "authors": [
                "Jeffrey Cheng",
                "Benjamin Van Durme"
            ],
            "affiliations": [
                "Department of Computer Science, Johns Hopkins University, Baltimore, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Compressed Chain-of-Thought (CCoT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CCoT Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ·ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ¾Ğ·ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CCoT Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Reasoning with Compressed Contemplation Tokens",
                    "desc": "This paper introduces Compressed Chain-of-Thought (CCoT), a new framework that enhances reasoning in language models by using variable-length contemplation tokens. These tokens serve as compressed representations of reasoning chains, allowing models to perform additional reasoning without being limited to fixed-length sequences. The method can be applied to existing decoder language models, improving their accuracy by enabling more effective reasoning over dense content. Additionally, the number of contemplation tokens can be adjusted to control the level of reasoning enhancement, providing flexibility in model performance."
                },
                "zh": {
                    "title": "å‹ç¼©é“¾å¼æ€ç»´ï¼šæå‡æ¨ç†æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå‹ç¼©é“¾å¼æ€ç»´ï¼ˆCCoTï¼‰ï¼Œç”¨äºç”Ÿæˆå¯å˜é•¿åº¦çš„æ€ç»´ä»¤ç‰Œï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚æ€ç»´ä»¤ç‰Œæ˜¯æŒ‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ç‰¹æ®Šä»¤ç‰Œï¼Œå…è®¸è¿›è¡Œé¢å¤–çš„è®¡ç®—ã€‚ä¸ä¹‹å‰çš„å›ºå®šé•¿åº¦åºåˆ—ä¸åŒï¼ŒCCoTç”Ÿæˆçš„æ˜¯å‹ç¼©çš„æ¨ç†é“¾è¡¨ç¤ºï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„å†…å®¹ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†CCoTå¦‚ä½•é€šè¿‡å¯†é›†çš„å†…å®¹è¡¨ç¤ºå®ç°æ¨ç†çš„æ”¹è¿›ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´ç”Ÿæˆçš„æ€ç»´ä»¤ç‰Œæ•°é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12276",
            "title": "Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers",
            "url": "https://huggingface.co/papers/2412.12276",
            "abstract": "Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., ``Finding the first noun in a sentence.\") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
            "score": 6,
            "issue_id": 1184,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "9baa157dac26994a",
            "authors": [
                "Seungwook Han",
                "Jinyeop Song",
                "Jeff Gore",
                "Pulkit Agrawal"
            ],
            "affiliations": [
                "Improbable AI",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12276.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#interpretability",
                    "#transfer_learning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ: ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL) Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ICL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹, Ğ¾Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ICL. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unlocking In-Context Learning: The Power of Concept Encoding in Transformers",
                    "desc": "This paper explores how autoregressive transformers, like those used in natural language processing, learn and adapt through a process called in-context learning (ICL). The authors introduce a concept encoding-decoding mechanism that helps explain how these models form and utilize internal abstractions in their representations. By analyzing a small transformer on synthetic ICL tasks, they observe that as the model encodes different concepts, it simultaneously develops decoding strategies that enhance its performance. The study confirms that the quality of concept encoding is crucial for ICL success, providing insights into the workings of large language models."
                },
                "zh": {
                    "title": "æ­ç¤ºè‡ªå›å½’å˜æ¢å™¨çš„å­¦ä¹ æœºåˆ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªå›å½’å˜æ¢å™¨å¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è¿›è¡Œé€‚åº”æ€§å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚å¿µç¼–ç -è§£ç æœºåˆ¶ï¼Œä»¥è§£é‡Šå˜æ¢å™¨å¦‚ä½•åœ¨å…¶è¡¨ç¤ºä¸­å½¢æˆå’Œä½¿ç”¨å†…éƒ¨æŠ½è±¡ã€‚é€šè¿‡å¯¹åˆæˆICLä»»åŠ¡çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨å­¦ä¹ ç¼–ç ä¸åŒæ½œåœ¨æ¦‚å¿µçš„åŒæ—¶ï¼Œæ„å»ºæ¡ä»¶è§£ç ç®—æ³•ï¼Œä»è€Œæé«˜ICLæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†æ¦‚å¿µç¼–ç è´¨é‡ä¸ICLè¡¨ç°ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸä¸å¤±è´¥æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13180",
            "title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration",
            "url": "https://huggingface.co/papers/2412.13180",
            "abstract": "Recent works on accelerating Vision-Language Models show that strong performance can be maintained across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model and find that its strong performance across many tasks is not due to an exceptional ability to compress visual information, but rather the benchmarks' limited ability to assess fine-grained visual capabilities. Namely, we demonstrate a core issue with the acceleration approach where most tokens towards the top of the image are pruned away. Yet, this issue is only reflected in performance for a small subset of tasks such as localization. For the other evaluated tasks, strong performance is maintained with the flawed pruning strategy. Noting the limited visual capabilities of the studied acceleration technique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), a straightforward approach that (1) resolves the identified issue with early-layer pruning, (2) incorporates uniform sampling to ensure coverage across all image regions, and (3) applies pruning in two stages to allow the criteria to become more effective at a later layer while still achieving significant speedup through early-layer pruning. With comparable computational savings, we find that FEATHER has more than 5times performance improvement on the vision-centric localization benchmarks compared to the original acceleration approach.",
            "score": 5,
            "issue_id": 1194,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "ff9e7c2001c6c5b2",
            "authors": [
                "Mark Endo",
                "Xiaohan Wang",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13180.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸª¶",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (Vision-Language Models). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ¼ĞµĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FEATHER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². FEATHER Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with FEATHER: Pruning Smartly for Better Performance",
                    "desc": "This paper investigates the effectiveness of early pruning in Vision-Language Models, revealing that the strong performance of these models is not solely due to their ability to compress visual information. The authors identify a significant flaw in the pruning strategy, where important visual tokens, especially those at the top of images, are often discarded, impacting performance on specific tasks like localization. They introduce a new method called FEATHER, which addresses this issue by ensuring better coverage of image regions through uniform sampling and implementing a two-stage pruning process. The results show that FEATHER achieves over five times improvement in localization tasks while maintaining computational efficiency compared to previous methods."
                },
                "zh": {
                    "title": "æå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å‹ç¼©è§†è§‰ä¿¡æ¯çš„æƒ…å†µä¸‹ä»èƒ½åœ¨å¤šç§ä»»åŠ¡ä¸­ä¿æŒè‰¯å¥½è¡¨ç°ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­æ—©æœŸä¿®å‰ªè§†è§‰æ ‡è®°çš„åŠ é€Ÿæ–¹æ³•ï¼Œå‘ç°å…¶å¼ºæ€§èƒ½å¹¶éæºäºå‹ç¼©è§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œè€Œæ˜¯åŸºå‡†æµ‹è¯•å¯¹ç»†ç²’åº¦è§†è§‰èƒ½åŠ›è¯„ä¼°çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†FEATHERæ–¹æ³•ï¼Œè§£å†³äº†æ—©æœŸä¿®å‰ªçš„æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶é€šè¿‡å‡åŒ€é‡‡æ ·ç¡®ä¿è¦†ç›–æ‰€æœ‰å›¾åƒåŒºåŸŸã€‚ä¸åŸå§‹åŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒFEATHERåœ¨è§†è§‰å®šä½åŸºå‡†ä¸Šå®ç°äº†è¶…è¿‡5å€çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13194",
            "title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
            "url": "https://huggingface.co/papers/2412.13194",
            "abstract": "The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator, an effective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the environment such as user demos or even just the name of the website itself for Internet-browsing agents. Then, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting trajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager and WebArena.To the best of our knowledge, this work represents the first effective learning system to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks with SOTA performances. Our open-source checkpoints and code can be found in https://yanqval.github.io/PAE/",
            "score": 4,
            "issue_id": 1193,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "de947186dd7a199a",
            "authors": [
                "Yifei Zhou",
                "Qianlan Yang",
                "Kaixiang Lin",
                "Min Bai",
                "Xiong Zhou",
                "Yu-Xiong Wang",
                "Sergey Levine",
                "Erran Li"
            ],
            "affiliations": [
                "Amazon",
                "University of California, Berkeley",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13194.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾ Ğ¸Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Proposer-Agent-Evaluator (PAE) Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. PAE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ PAE Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering Agents to Learn Autonomously in the Real World",
                    "desc": "This paper introduces Proposer-Agent-Evaluator (PAE), a novel learning system designed for foundation model agents to autonomously learn and practice diverse skills in real-world environments. PAE features a context-aware task proposer that generates tasks based on environmental cues, allowing agents to engage in practical learning without extensive human-annotated instructions. The agent's performance is evaluated by a vision-language model (VLM) that provides feedback, which is then used as a reward signal for reinforcement learning (RL) to improve the agent's policies. The system demonstrates state-of-the-art performance in vision-based web navigation tasks, showcasing its ability to generalize across various human-annotated benchmarks."
                },
                "zh": {
                    "title": "è‡ªä¸»å­¦ä¹ ä¸ä»»åŠ¡æè®®çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºProposer-Agent-Evaluatorï¼ˆPAEï¼‰çš„å­¦ä¹ ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©åŸºç¡€æ¨¡å‹ä»£ç†è‡ªä¸»å‘ç°å’Œç»ƒä¹ æŠ€èƒ½ã€‚PAEçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä»»åŠ¡æè®®è€…ï¼Œå®ƒæ ¹æ®ç¯å¢ƒä¿¡æ¯è‡ªåŠ¨æå‡ºä»»åŠ¡ä¾›ä»£ç†ç»ƒä¹ ã€‚ä»£ç†é€šè¿‡æ€è€ƒå’Œå®é™…æ“ä½œæ¥å°è¯•è¿™äº›ä»»åŠ¡ï¼Œå¹¶ç”±åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æˆåŠŸè¯„ä¼°å™¨è¿›è¡Œè¯„ä¼°ã€‚è¯¥ç³»ç»Ÿåœ¨è§†è§‰åŸºç¡€çš„ç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨çœŸå®ä¸–ç•Œäººç±»æ ‡æ³¨åŸºå‡†ä¸Šçš„å¹¿æ³›é€‚åº”èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10704",
            "title": "VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2412.10704",
            "abstract": "Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. A key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce a coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, we benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and long-context LLM baselines for end-to-end multimodal document QA by 12-20%.",
            "score": 2,
            "issue_id": 1192,
            "pub_date": "2024-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "5acee7c37cff2e05",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Franck Dernoncourt",
                "Kanika Goswami",
                "Ryan A. Rossi",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10704.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#rag",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VisDoMBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ VisDoMRAG - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ RAG. VisDoMRAG Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… RAG-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ VisDoMRAG - Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ³Ğ´Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Question Answering with VisDoMRAG",
                    "desc": "This paper presents VisDoMBench, a new benchmark for evaluating question answering (QA) systems that work with multiple documents containing rich visual elements like tables and charts. It introduces VisDoMRAG, a novel approach that combines visual and textual retrieval in a Retrieval Augmented Generation (RAG) framework, enhancing the QA process. VisDoMRAG uses a multi-step reasoning method that integrates evidence curation and chain-of-thought reasoning to improve the accuracy of answers derived from both text and visuals. The paper demonstrates that VisDoMRAG significantly outperforms existing unimodal and long-context language models in multimodal document QA tasks, achieving better accuracy and verifiability of answers."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€é—®ç­”çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†VisDoMBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ–‡æ¡£ç¯å¢ƒä¸‹çš„é—®ç­”ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯é‚£äº›åŒ…å«ä¸°å¯Œè§†è§‰å…ƒç´ çš„æ–‡æ¡£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç§°ä¸ºVisDoMRAGï¼Œå®ƒåŒæ—¶åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çš„RAGï¼Œç»“åˆå¼ºå¤§çš„è§†è§‰æ£€ç´¢èƒ½åŠ›å’Œå¤æ‚çš„è¯­è¨€æ¨ç†ã€‚VisDoMRAGé‡‡ç”¨å¤šæ­¥éª¤æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬è¯æ®æ•´ç†å’Œæ€ç»´é“¾æ¨ç†ï¼Œä»¥æ”¯æŒæ–‡æœ¬å’Œè§†è§‰çš„å¹¶è¡ŒRAGç®¡é“ã€‚é€šè¿‡ä¸€è‡´æ€§çº¦æŸçš„æ¨¡æ€èåˆæœºåˆ¶ï¼ŒVisDoMRAGåœ¨æ¨ç†æ—¶å¯¹ä¸åŒæ¨¡æ€çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œå¯¹é½ï¼Œä»è€Œç”Ÿæˆä¸€è‡´çš„æœ€ç»ˆç­”æ¡ˆï¼Œæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€æ–‡æ¡£é—®ç­”çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11713",
            "title": "Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework",
            "url": "https://huggingface.co/papers/2412.11713",
            "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open-source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Block, and Distorted Handling Solution. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi-agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices in real development scenarios, providing valuable insights for future improvements in code reliability.",
            "score": 1,
            "issue_id": 1194,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "7957a02cb5f0752d",
            "authors": [
                "Xuanming Zhang",
                "Yuxuan Chen",
                "Yiming Zheng",
                "Zhexin Zhang",
                "Yuan Yuan",
                "Minlie Huang"
            ],
            "affiliations": [
                "Beihang University",
                "ByteDance",
                "Lingxin AI",
                "The CoAI Group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11713.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#open_source",
                    "#science",
                    "#plp"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°: LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµÑ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ±Ğ»Ğ¾ĞºĞ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Seeker, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². Seeker Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM."
                },
                "en": {
                    "title": "Enhancing Code Reliability with LLMs for Exception Handling",
                    "desc": "This paper addresses the challenges of exception handling in software development, particularly in open-source projects where improper handling can lead to unreliable code. It identifies three main issues: the insensitivity in detecting fragile code, inaccuracies in capturing exception blocks, and distorted solutions for handling exceptions. To tackle these problems, the authors propose a multi-agent framework called Seeker, which utilizes various agents to assist large language models (LLMs) in improving exception handling practices. This research is significant as it is the first systematic study to explore the application of LLMs in enhancing the robustness of exception handling in real-world coding scenarios."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ™ºèƒ½ä½“æå‡å¼‚å¸¸å¤„ç†çš„å¯é æ€§",
                    "desc": "åœ¨è½¯ä»¶å¼€å‘ä¸­ï¼Œå¼‚å¸¸å¤„ç†ä¸å½“ä¼šä¸¥é‡å½±å“ä»£ç çš„å¥å£®æ€§å’Œå¯é æ€§ã€‚è®¸å¤šå¼€å‘è€…åœ¨æ£€æµ‹å’Œç®¡ç†å¼‚å¸¸æ—¶é¢ä¸´å›°éš¾ï¼Œå¯¼è‡´ä»£ç è„†å¼±ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æºé¡¹ç›®ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seekerï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ”¹å–„å¼‚å¸¸å¤„ç†ã€‚Seekeré€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“åä½œï¼Œå¸®åŠ©å¼€å‘è€…æ›´æœ‰æ•ˆåœ°æ£€æµ‹ã€æ•è·å’Œè§£å†³å¼‚å¸¸ï¼Œä»è€Œæé«˜ä»£ç çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12527",
            "title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention",
            "url": "https://huggingface.co/papers/2412.12527",
            "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge). While substantial efforts have been made to leverage both forms of knowledge, scenarios in which the model lacks any relevant knowledge remain underexplored. Such limitations can result in issues like hallucination, causing reduced reliability and potential risks in high-stakes applications. To address such limitations, this paper extends the task scope to encompass cases where the user's request cannot be fulfilled due to the lack of relevant knowledge. To this end, we introduce Contrastive Decoding with Abstention (CDA), a training-free decoding method that empowers LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA evaluates the relevance of each knowledge for a given query, adaptively determining which knowledge to prioritize or which to completely ignore. Extensive experiments with four LLMs on three question-answering datasets demonstrate that CDA can effectively perform accurate generation and abstention simultaneously. These findings highlight CDA's potential to broaden the applicability of LLMs, enhancing reliability and preserving user trust.",
            "score": 1,
            "issue_id": 1192,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "28e931208632a312",
            "authors": [
                "Hyuhng Joon Kim",
                "Youna Kim",
                "Sang-goo Lee",
                "Taeuk Kim"
            ],
            "affiliations": [
                "Hanyang University",
                "IntelliSys, Korea",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12527.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#alignment",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CDA: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Contrastive Decoding with Abstention (CDA). CDA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ, ĞºĞ°ĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CDA Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº LLM."
                },
                "en": {
                    "title": "Empowering LLMs: Generate or Abstain with Contrastive Decoding",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) when they encounter queries without relevant knowledge, which can lead to unreliable outputs or hallucinations. It introduces a novel method called Contrastive Decoding with Abstention (CDA), which allows LLMs to generate responses when they have relevant information and to abstain from answering when they do not. CDA works by evaluating the relevance of available knowledge for each query, enabling the model to prioritize useful information and ignore irrelevant data. The results from experiments on multiple datasets show that CDA improves both the accuracy of responses and the model's reliability, making LLMs more trustworthy in critical applications."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ä¸ä¿¡ä»»",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œåˆ©ç”¨äº†é¢„è®­ç»ƒçŸ¥è¯†å’Œå¤–éƒ¨çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå½“æ¨¡å‹ç¼ºä¹ç›¸å…³çŸ¥è¯†æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°å¹»è§‰ç­‰é—®é¢˜ï¼Œå½±å“å…¶å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£ç æ–¹æ³•â€”â€”å¯¹æ¯”è§£ç ä¸æ”¾å¼ƒï¼ˆCDAï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰ç›¸å…³çŸ¥è¯†æ—¶ç”Ÿæˆå“åº”ï¼Œè€Œåœ¨ç¼ºä¹çŸ¥è¯†æ—¶é€‰æ‹©æ”¾å¼ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDAèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒæ—¶è¿›è¡Œå‡†ç¡®ç”Ÿæˆå’Œæ”¾å¼ƒï¼Œä»è€Œæé«˜LLMsçš„é€‚ç”¨æ€§å’Œç”¨æˆ·ä¿¡ä»»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12877",
            "title": "MIVE: New Design and Benchmark for Multi-Instance Video Editing",
            "url": "https://huggingface.co/papers/2412.12877",
            "abstract": "Recent AI-based video editing has enabled users to edit videos through simple text prompts, significantly simplifying the editing process. However, recent zero-shot video editing techniques primarily focus on global or single-object edits, which can lead to unintended changes in other parts of the video. When multiple objects require localized edits, existing methods face challenges, such as unfaithful editing, editing leakage, and lack of suitable evaluation datasets and metrics. To overcome these limitations, we propose a zero-shot Multi-Instance Video Editing framework, called MIVE. MIVE is a general-purpose mask-based framework, not dedicated to specific objects (e.g., people). MIVE introduces two key modules: (i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and (ii) Instance-centric Probability Redistribution (IPR) to ensure precise localization and faithful editing. Additionally, we present our new MIVE Dataset featuring diverse video scenarios and introduce the Cross-Instance Accuracy (CIA) Score to evaluate editing leakage in multi-instance video editing tasks. Our extensive qualitative, quantitative, and user study evaluations demonstrate that MIVE significantly outperforms recent state-of-the-art methods in terms of editing faithfulness, accuracy, and leakage prevention, setting a new benchmark for multi-instance video editing. The project page is available at https://kaist-viclab.github.io/mive-site/",
            "score": 1,
            "issue_id": 1189,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "5f31fcc15693d0d3",
            "authors": [
                "Samuel Teodoro",
                "Agus Gunawan",
                "Soo Ye Kim",
                "Jihyong Oh",
                "Munchurl Kim"
            ],
            "affiliations": [
                "Adobe Research",
                "Chung-Ang University",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12877.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#video",
                    "#optimization",
                    "#leakage"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MIVE: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ MIVE (Multi-Instance Video Editing). MIVE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚ÑÑ… ĞºĞ°Ğ´Ñ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "MIVE: Revolutionizing Multi-Instance Video Editing with Precision and Faithfulness",
                    "desc": "This paper presents a new framework called MIVE for zero-shot multi-instance video editing, which allows users to edit multiple objects in videos using simple text prompts. MIVE addresses common issues in existing methods, such as editing leakage and unfaithful edits, by introducing two innovative modules: Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR). The framework is designed to work with various objects, not just specific ones, making it versatile for different editing scenarios. Additionally, the authors introduce a new dataset and evaluation metric to assess the performance of multi-instance video editing, demonstrating that MIVE outperforms current techniques in accuracy and editing quality."
                },
                "zh": {
                    "title": "MIVEï¼šå¤šå®ä¾‹è§†é¢‘ç¼–è¾‘çš„æ–°æ ‡å‡†",
                    "desc": "æœ€è¿‘çš„åŸºäºäººå·¥æ™ºèƒ½çš„è§†é¢‘ç¼–è¾‘æŠ€æœ¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬æç¤ºæ¥ç¼–è¾‘è§†é¢‘ï¼Œæå¤§åœ°ç®€åŒ–äº†ç¼–è¾‘è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶-shotè§†é¢‘ç¼–è¾‘æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å…¨å±€æˆ–å•ä¸€å¯¹è±¡çš„ç¼–è¾‘ï¼Œè¿™å¯èƒ½å¯¼è‡´è§†é¢‘å…¶ä»–éƒ¨åˆ†çš„æ„å¤–å˜åŒ–ã€‚å½“å¤šä¸ªå¯¹è±¡éœ€è¦å±€éƒ¨ç¼–è¾‘æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´è¯¸å¦‚ç¼–è¾‘ä¸å‡†ç¡®ã€ç¼–è¾‘æ³„æ¼ä»¥åŠç¼ºä¹åˆé€‚çš„è¯„ä¼°æ•°æ®é›†å’ŒæŒ‡æ ‡ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMIVEçš„é›¶-shotå¤šå®ä¾‹è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-17.html",
    "link_next": "2024-12-19.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 1,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 2,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0,
        "#evaluation": 1
    },
    "zh": {
        "text": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºå‡†æµ‹è¯•æ€§èƒ½ä¸å®é™…åº”ç”¨ä¹‹é—´ä»å­˜åœ¨å·®è·ã€‚ä¸»è¦åŸå› æ˜¯å½“å‰çš„è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡æ— æ³•å……åˆ†æ•æ‰LLMsçš„å…¨éƒ¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†G-Pass@kï¼Œä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¬¡é‡‡æ ·ä¸­è¿ç»­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé‡åŒ–æ¨¡å‹çš„æœ€é«˜æ€§èƒ½æ½œåŠ›å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LiveMathBenchï¼Œä¸€ä¸ªåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§ã€å½“ä»£æ•°å­¦é—®é¢˜çš„åŠ¨æ€åŸºå‡†ï¼Œæ—¨åœ¨å‡å°‘è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ•°æ®æ³„éœ²é£é™©ã€‚è¯¦ç»†ç»“æœå’ŒåŸºå‡†å¯è®¿é—®ï¼šhttps://github.com/open-compass/GPassKã€‚",
        "title": "Are Your LLMs Capable of Stable Reasoning?",
        "pinyin": "DÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i fÃ¹zÃ¡ xÄ«nglÇ rÃ¨nwÃ¹ zhÅng qÇ”dÃ©le xiÇnzhÃ¹ jÃ¬nbÃ¹. RÃ¡n'Ã©r, jÄ«zhÇ”n cÃ¨shÇ xÃ¬ngnÃ©ng yÇ” shÃ­jÃ¬ yÃ¬ngyÃ²ng zhÄ«jiÄn rÃ©ng cÃºnzÃ i chÄjÃ¹. ZhÇ”yÃ o yuÃ¡nyÄ«n shÃ¬ dÄngqiÃ¡n de pÃ­nggÇ” fÄngfÇ hÃ© zhÇbiÄo wÃºfÇ chÅngfÄ“n bÇngqÇ” LLMs de quÃ¡nbÃ¹ nÃ©nglÃ¬, tÃ¨biÃ© shÃ¬ zÃ i xÅ«yÃ o zhÇ”nquÃ¨xÃ¬ng hÃ© yÄ«zhÃ¬xÃ¬ng de fÃ¹zÃ¡ xÄ«nglÇ rÃ¨nwÃ¹ zhÅng. BÄ›nwÃ©n tÃ­chÅ«le G-Pass@k, yÄ«gÃ¨ xÄ«n de pÃ­nggÇ” zhÇbiÄo, nÃ©nggÃ²u zÃ i duÅcÃ¬ cÇiyÃ ng zhÅng liÃ¡nxÃ¹ pÃ­nggÇ” mÃ³xÃ­ng xÃ¬ngnÃ©ng, liÃ ngzhÃ¬ mÃ³xÃ­ng de zuÃ¬gÄo xÃ¬ngnÃ©ng qiÃ¡nlÃ¬ hÃ© wÄ›ndÃ­ngxÃ¬ng. TÃ³ngshÃ­, wÇ’men tuÄ«chÅ«le LiveMathBench, yÄ«gÃ¨ bÄohÃ¡n jÃ¹yÇ’u tiÇozhÃ nxÃ¬ng, dÄngdÃ i shÃ¹xuÃ© wÃ¨ntÃ­ de dÃ²ngtÃ i jÄ«zhÇ”n, zhÇyÃº jiÇnshÇo pÃ­nggÇ” guÃ²chÃ©ng zhÅng de shÃ¹jÃ¹ lÃ²ushÃ¬ fÄ“ngxiÇn. XiÃ¡ngxÃ¬ jiÃ©guÇ’ hÃ© jÄ«zhÇ”n kÄ› fÄngwÃ¨n: https://github.com/open-compass/GPassK.",
        "vocab": "[{'word': 'å¤§å‹', 'pinyin': 'dÃ  xÃ­ng', 'trans': 'large-scale'},\n{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'language model'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'},\n{'word': 'å–å¾—', 'pinyin': 'qÇ” dÃ©', 'trans': 'achieve'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'å®é™…', 'pinyin': 'shÃ­ jÃ¬', 'trans': 'actual'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'application'},\n{'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'},\n{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æŒ‡æ ‡', 'pinyin': 'zhÇ biÄo', 'trans': 'metric'},\n{'word': 'æ•æ‰', 'pinyin': 'bÇ” zhuÅ', 'trans': 'capture'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'capability'},\n{'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'},\n{'word': 'è¿ç»­', 'pinyin': 'liÃ¡n xÃ¹', 'trans': 'continuous'},\n{'word': 'é‡åŒ–', 'pinyin': 'liÃ ng huÃ ', 'trans': 'quantify'},\n{'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'},\n{'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›n dÃ¬ng xÃ¬ng', 'trans': 'stability'},\n{'word': 'æ¨å‡º', 'pinyin': 'tuÄ« chÅ«', 'trans': 'launch'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'æŒ‘æˆ˜æ€§', 'pinyin': 'tiÇo zhÃ n xÃ¬ng', 'trans': 'challenging'},\n{'word': 'å½“ä»£', 'pinyin': 'dÄng dÃ i', 'trans': 'contemporary'},\n{'word': 'æ•°å­¦', 'pinyin': 'shÃ¹ xuÃ©', 'trans': 'mathematics'},\n{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'},\n{'word': 'æ³„éœ²', 'pinyin': 'xiÃ¨ lÃ²u', 'trans': 'leak'},\n{'word': 'é£é™©', 'pinyin': 'fÄ“ng xiÇn', 'trans': 'risk'},\n{'word': 'è¯¦ç»†', 'pinyin': 'xiÃ¡ng xÃ¬', 'trans': 'detailed'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'è®¿é—®', 'pinyin': 'fÇng wÃ¨n', 'trans': 'access'}]",
        "trans": "Large language models (LLMs) have made significant strides in complex reasoning tasks. However, there remains a gap between benchmark test performance and real-world applications. The primary reason is that current evaluation methods and metrics fail to fully capture the capabilities of LLMs, especially in complex reasoning tasks that require accuracy and consistency. This paper introduces G-Pass@k, a new evaluation metric that can continuously assess model performance across multiple samples, quantifying the model's peak performance potential and stability. Additionally, we present LiveMathBench, a dynamic benchmark containing challenging, contemporary mathematical problems aimed at reducing the risk of data leakage during the evaluation process. Detailed results and benchmarks are available at: https://github.com/open-compass/GPassK.",
        "update_ts": "2024-12-18 09:11"
    }
}