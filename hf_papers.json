{
    "date": {
        "ru": "30 июня",
        "en": "June 30",
        "zh": "6月30日"
    },
    "time_utc": "2025-06-30 08:17",
    "weekday": 0,
    "issue_id": 4553,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.21862",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "url": "https://huggingface.co/papers/2506.21862",
            "abstract": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "score": 24,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 июня",
                "en": "June 27",
                "zh": "6月27日"
            },
            "hash": "b9ad171aa3fb5bbf",
            "authors": [
                "Boyuan Sun",
                "Jiaxing Zhao",
                "Xihan Wei",
                "Qibin Hou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#dataset",
                    "#video"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умное сжатие для умных видеомоделей",
                    "desc": "LLaVA-Scissor - это стратегия сжатия токенов для видео мультимодальных больших языковых моделей. Она использует метод Семантически Связанных Компонентов (SCC) для эффективного сжатия токенов, сохраняя при этом семантическое покрытие. LLaVA-Scissor применяет двухэтапный подход к пространственно-временному сжатию токенов, используя SCC как в пространственной, так и во временной областях. Экспериментальные результаты показывают, что LLaVA-Scissor превосходит другие методы сжатия токенов в различных задачах понимания видео."
                },
                "en": {
                    "title": "Efficient Video Understanding with Semantic Token Compression",
                    "desc": "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."
                },
                "zh": {
                    "title": "LLaVA-Scissor：高效的视频令牌压缩策略",
                    "desc": "LLaVA-Scissor是一种针对视频多模态大语言模型的令牌压缩策略。它利用语义连通组件（SCC）方法，有效地将令牌分配到不同的语义区域，从而确保全面的语义覆盖。与以往基于注意力分数的压缩方法不同，LLaVA-Scissor能够减少令牌冗余，并在空间和时间域中进行两步压缩。实验结果表明，该方法在视频理解基准测试中表现优异，尤其是在低令牌保留比率下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17450",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "url": "https://huggingface.co/papers/2506.17450",
            "abstract": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.",
            "score": 24,
            "issue_id": 4550,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "b5bb4470d500be10",
            "authors": [
                "Jiacheng Chen",
                "Ramin Mehran",
                "Xuhui Jia",
                "Saining Xie",
                "Sanghyun Woo"
            ],
            "affiliations": [
                "Google DeepMind",
                "New York University",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17450.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Генеративная композиция сцен с 3D-контролем",
                    "desc": "BlenderFusion - это генеративная система визуальной композиции, использующая диффузионную модель для редактирования и составления сцен. Она работает по принципу разделения на слои, редактирования и композиции, преобразуя визуальные входные данные в редактируемые 3D-объекты. Система использует предобученную диффузионную модель, дообученную на видеокадрах с применением маскирования исходного изображения и симуляции дрожания объектов. BlenderFusion значительно превосходит существующие методы в сложных задачах композиционного редактирования сцен."
                },
                "en": {
                    "title": "Revolutionizing Scene Editing with BlenderFusion",
                    "desc": "BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods."
                },
                "zh": {
                    "title": "生成视觉合成的新方法",
                    "desc": "BlenderFusion是一个生成视觉合成框架，能够通过重新组合对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程，首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D控制的编辑，最后使用生成合成器将它们融合成一个连贯的场景。该生成合成器扩展了预训练的扩散模型，能够并行处理原始场景和编辑后的场景。BlenderFusion在复杂的合成场景编辑任务中显著优于之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21356",
            "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
            "url": "https://huggingface.co/papers/2506.21356",
            "abstract": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.",
            "score": 14,
            "issue_id": 4551,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "5a54508ae68df265",
            "authors": [
                "Hongbo Liu",
                "Jingwen He",
                "Yi Jin",
                "Dian Zheng",
                "Yuhao Dong",
                "Fan Zhang",
                "Ziqi Huang",
                "Yinan He",
                "Yangguang Li",
                "Weichao Chen",
                "Yu Qiao",
                "Wanli Ouyang",
                "Shengjie Zhao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21356.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Новый бенчмарк для оценки понимания языка кино искусственным интеллектом",
                    "desc": "Статья представляет новый набор данных ShotBench для оценки понимания кинематографического языка моделями искусственного интеллекта. Авторы также создали большой датасет ShotQA с 70 тысячами пар вопросов-ответов о кинематографии. На основе ShotQA была разработана модель ShotVL, превзошедшая существующие решения в понимании визуального языка кино. Эти инструменты призваны улучшить способности ИИ в анализе и генерации видеоконтента."
                },
                "en": {
                    "title": "Enhancing AI's Cinematic Language Comprehension with ShotBench and ShotQA",
                    "desc": "This paper introduces ShotBench and ShotQA datasets, along with the ShotVL model, to improve AI's ability to understand and generate cinematic language. Cinematography is a complex visual language that conveys stories and emotions, but current Vision-Language Models (VLMs) struggle with its nuances. The ShotBench benchmark evaluates VLMs on their comprehension of cinematic grammar, revealing significant limitations in their performance. By developing ShotQA and fine-tuning the ShotVL model, the authors achieve state-of-the-art results, providing valuable resources for advancing AI in cinematic understanding and generation."
                },
                "zh": {
                    "title": "提升AI电影语言理解的突破性进展",
                    "desc": "本文介绍了ShotBench和ShotQA数据集以及ShotVL模型，旨在提升人工智能对电影语言的理解和生成能力。电影摄影是传达叙事、情感和美学质量的基本视觉语言，但现有的视觉-语言模型在理解细腻的电影语法方面仍存在不足。我们构建了ShotBench基准，包含3500多个专家注释的问答对，评估了24个领先的视觉-语言模型，结果显示它们在细粒度视觉线索和复杂空间推理方面表现不佳。通过构建ShotQA数据集并开发ShotVL模型，我们在ShotBench上取得了新的最佳性能，推动了AI在电影理解和生成领域的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20279",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2506.20279",
            "abstract": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj",
            "score": 12,
            "issue_id": 4550,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 июня",
                "en": "June 25",
                "zh": "6月25日"
            },
            "hash": "8382f71877fe1997",
            "authors": [
                "Changliang Xia",
                "Chengyou Jia",
                "Zhuohang Dang",
                "Minnan Luo"
            ],
            "affiliations": [
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20279.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективное плотное предсказание с минимумом данных",
                    "desc": "DenseDiT - это подход к генеративным моделям для задач плотного предсказания в компьютерном зрении. Он достигает превосходных результатов на реальных данных, используя минимальное количество обучающих примеров. DenseDiT максимально использует визуальные прайоры генеративных моделей и включает механизм повторного использования параметров. Модель превосходит существующие методы, используя менее 0,01% обучающих данных по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "DenseDiT: Revolutionizing Dense Prediction with Minimal Data",
                    "desc": "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."
                },
                "zh": {
                    "title": "DenseDiT：用最少数据实现密集预测的突破",
                    "desc": "DenseDiT是一种基于生成模型的方法，能够在真实世界的密集预测任务中以最少的训练数据实现优越的性能。密集预测任务在计算机视觉中非常重要，旨在为输入图像学习逐像素的标注标签。现有方法主要集中在理想条件下，缺乏对真实场景的广泛适应性，且面临真实数据稀缺的挑战。DenseDiT通过最大限度地利用生成模型的视觉先验，结合参数重用机制和轻量级分支，能够在多种真实世界的密集预测任务中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21416",
            "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
            "url": "https://huggingface.co/papers/2506.21416",
            "abstract": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.",
            "score": 11,
            "issue_id": 4551,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "4c3c383901d9306f",
            "authors": [
                "Bowen Chen",
                "Mengyi Zhao",
                "Haomiao Sun",
                "Li Chen",
                "Xu Wang",
                "Kang Du",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21416.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точный контроль над множеством объектов в генерации изображений",
                    "desc": "XVerse - это новая модель для контролируемой генерации изображений с несколькими объектами. Она использует модуляцию текстового потока для точного и независимого управления отдельными объектами. XVerse преобразует референсные изображения в смещения для токен-специфичной модуляции, что позволяет контролировать характеристики объектов без нарушения латентного пространства. Это значительно улучшает возможности персонализированной генерации сложных сцен с высокой точностью."
                },
                "en": {
                    "title": "XVerse: Mastering Multi-Subject Control in Image Generation",
                    "desc": "XVerse is a new model that improves text-to-image generation by allowing users to control multiple subjects in an image with high precision. It uses a technique called token-specific text-stream modulation to manage the identity and attributes of each subject, such as pose and lighting, without losing image quality. Traditional methods often create unwanted artifacts or mix up attributes, but XVerse avoids these issues by transforming reference images into specific adjustments. This leads to better coherence and fidelity in generated images, making it easier to create complex scenes with distinct and editable subjects."
                },
                "zh": {
                    "title": "XVerse：精确控制多对象图像生成的创新",
                    "desc": "XVerse是一种增强文本到图像生成的模型，能够对多个对象进行精确和独立的控制。它通过特定的文本流调制，解决了在生成多对象图像时常见的编辑性和一致性问题。XVerse将参考图像转换为偏移量，从而实现对特定对象的控制，而不干扰图像的潜在特征。这一创新显著提高了个性化和复杂场景生成的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22434",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "url": "https://huggingface.co/papers/2506.22434",
            "abstract": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.",
            "score": 7,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 июня",
                "en": "June 27",
                "zh": "6月27日"
            },
            "hash": "d7e89f248d4c331e",
            "authors": [
                "Xi Chen",
                "Mingkang Zhu",
                "Shaoteng Liu",
                "Xiaoyang Wu",
                "Xiaogang Xu",
                "Yu Liu",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HUST",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22434.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самообучение ИИ визуальным рассуждениям без участия человека",
                    "desc": "Статья представляет метод самообучения моделей компьютерного зрения и обработки естественного языка (VLM) для улучшения их способности рассуждать о нескольких изображениях. Авторы используют триплеты изображений и обучение с подкреплением, чтобы научить модель сравнивать тонкие визуальные детали. Этот подход не требует размеченных человеком пар вопрос-ответ и позволяет генерировать цепочки рассуждений. Эксперименты показывают, что полученные навыки обобщаются на широкий спектр задач визуального анализа."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Supervised Image Triplet Learning",
                    "desc": "This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks."
                },
                "zh": {
                    "title": "自监督学习提升视觉语言模型推理能力",
                    "desc": "这篇论文探讨了如何通过使用图像三元组的自监督学习来增强视觉语言模型（VLM）在多图像任务上的推理能力，而无需人工标注的问题-答案对。研究者们构建了由同一图像的两个增强视图和一个相似但不同的图像组成的图像三元组。在训练过程中，模型被要求生成推理过程，以比较这些图像（即判断相同或不同）。实验表明，尽管模型仅在视觉比较任务上训练，但其学习到的推理能力能够有效地推广到各种问题上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21656",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "url": "https://huggingface.co/papers/2506.21656",
            "abstract": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.",
            "score": 5,
            "issue_id": 4548,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "8d063b13fc555964",
            "authors": [
                "Yifan Shen",
                "Yuanzhe Liu",
                "Jingyuan Zhu",
                "Xu Cao",
                "Xiaofeng Zhang",
                "Yixiao He",
                "Wenming Ye",
                "James Matthew Rehg",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "Google",
                "Shanghai Jiao Tong University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21656.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Пространственное мышление ИИ выходит на новый уровень",
                    "desc": "SpatialReasoner-R1 - это новая модель зрительно-языкового рассуждения, которая улучшает пространственное мышление с помощью мультимодельного поиска Монте-Карло по дереву и оптимизации прямых предпочтений. Модель генерирует длинные цепочки рассуждений и использует сегментированную оптимизацию предпочтений для улучшения визуальной и логической согласованности. SpatialReasoner-R1 достигает нового уровня производительности на бенчмарке SPATIALRGPT-Bench, превосходя базовые модели на 9.8% по средней точности. При этом модель сохраняет конкурентоспособность в общих задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Elevating Spatial Reasoning with SpatialReasoner-R1",
                    "desc": "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."
                },
                "zh": {
                    "title": "空间推理的新突破",
                    "desc": "SpatialReasoner-R1是一种视觉-语言推理模型，旨在解决当前视觉-语言模型在细粒度空间推理方面的不足。该模型采用多模型蒙特卡洛树搜索（M3CTS）方法，生成多样且逻辑一致的长链思维推理轨迹，以构建高质量的空间推理监督。除此之外，SpatialReasoner-R1还引入了细粒度直接偏好优化（fDPO），通过空间奖励机制对候选响应进行评估，从而提高描述性基础和逻辑推理的准确性。实验结果表明，SpatialReasoner-R1在SPATIALRGPT-Bench上设定了新的最先进水平，平均准确率比最强基线提高了9.8%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19741",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "url": "https://huggingface.co/papers/2506.19741",
            "abstract": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT",
            "score": 3,
            "issue_id": 4548,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "288a2c7ef1ba6865",
            "authors": [
                "Yihong Luo",
                "Shuchen Xue",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "NUS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Эффективная адаптация генеративных моделей без переобучения",
                    "desc": "Статья представляет новый метод Noise Consistency Training (NCT) для интеграции новых сигналов управления в предобученные одношаговые генераторы без необходимости переобучения. NCT использует адаптерный модуль и функцию потерь согласованности шума в пространстве шума генератора. Этот подход позволяет эффективно адаптировать модели к новым условиям, таким как структурные ограничения или семантические указания. Эксперименты показывают, что NCT превосходит существующие методы по качеству генерации и вычислительной эффективности."
                },
                "en": {
                    "title": "Efficient Control in AI Content Generation with Noise Consistency Training",
                    "desc": "This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content."
                },
                "zh": {
                    "title": "噪声一致性训练：高效可控生成的新方法",
                    "desc": "本文提出了一种新颖的噪声一致性训练（NCT）方法，能够高效地将新的控制信号整合到预训练的一步生成器中，而无需重新训练。传统方法通常需要对基础模型进行昂贵的修改，而NCT通过引入适配模块和噪声一致性损失，在生成器的噪声空间中直接进行调整。该方法在生成质量和计算效率上超越了现有的多步和蒸馏方法，展示了其在可控生成方面的优越性。NCT的模块化设计使其在数据使用上更加高效，易于部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22419",
            "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
            "url": "https://huggingface.co/papers/2506.22419",
            "abstract": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.",
            "score": 1,
            "issue_id": 4553,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 июня",
                "en": "June 27",
                "zh": "6月27日"
            },
            "hash": "179a2fbf84ed5e98",
            "authors": [
                "Bingchen Zhao",
                "Despoina Magka",
                "Minqi Jiang",
                "Xian Li",
                "Roberta Raileanu",
                "Tatiana Shavrina",
                "Jean-Christophe Gagnon-Audet",
                "Kelvin Niu",
                "Shagun Sodhani",
                "Michael Shvartsman",
                "Andrei Lupu",
                "Alisia Lupidi",
                "Edan Toledo",
                "Karen Hambardzumyan",
                "Martin Josifoski",
                "Thomas Foster",
                "Lucia Cipolina-Kun",
                "Abhishek Charnalia",
                "Derek Dunfield",
                "Alexander H. Miller",
                "Oisin Mac Aodha",
                "Jakob Foerster",
                "Yoram Bachrach"
            ],
            "affiliations": [
                "Meta",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22419.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#science",
                    "#agents",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🏃‍♂️",
                "ru": {
                    "title": "ИИ пока не готов к научному спидраннингу",
                    "desc": "Это исследование представляет Автоматизированный бенчмарк спидраннинга ЯБМ для оценки способности ИИ-агентов воспроизводить научные результаты. Бенчмарк основан на задачах спидраннинга NanoGPT, где агенты должны улучшить скорость обучения модели GPT-2. Результаты показывают, что даже современные языковые модели с рассуждениями испытывают трудности с реализацией известных улучшений. Это указывает на то, что способность ИИ автоматизировать воспроизведение научных результатов все еще ограничена."
                },
                "en": {
                    "title": "Benchmarking LLMs: Can They Reproduce Scientific Results?",
                    "desc": "The paper introduces the Automated LLM Speedrunning Benchmark, which assesses the ability of large language models (LLMs) to reproduce scientific results. It utilizes tasks from the NanoGPT speedrun competition, where AI agents attempt to train a GPT-2 model as quickly as possible. Despite advancements in reasoning capabilities, recent LLMs struggle to replicate known improvements in the benchmark, even with detailed hints provided. This benchmark serves as a straightforward measure of an LLM's capacity for automating scientific reproduction, an essential skill for future autonomous research agents."
                },
                "zh": {
                    "title": "评估LLM重现科学成果的自动化基准",
                    "desc": "本文介绍了一种自动化的LLM速度测试基准，旨在评估人工智能代理在科学研究中重现结果的能力。该基准利用NanoGPT速度测试任务，提供19个任务，帮助代理在最短时间内训练GPT-2模型。尽管提供了详细的提示，最新的推理LLM仍然难以重新实现已知的改进。这一基准为评估LLM在科学重现中的自动化能力提供了简单而有效的测量方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21411",
            "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
            "url": "https://huggingface.co/papers/2505.21411",
            "abstract": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.",
            "score": 1,
            "issue_id": 4552,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "e4bcbe7787b328fa",
            "authors": [
                "Yehui Tang",
                "Xiaosong Li",
                "Fangcheng Liu",
                "Wei Guo",
                "Hang Zhou",
                "Yaoyuan Wang",
                "Kai Han",
                "Xianzhi Yu",
                "Jinpeng Li",
                "Hui Zang",
                "Fei Mi",
                "Xiaojun Meng",
                "Zhicheng Liu",
                "Hanting Chen",
                "Binfan Zheng",
                "Can Chen",
                "Youliang Yan",
                "Ruiming Tang",
                "Peifeng Qin",
                "Xinghao Chen",
                "Dacheng Tao",
                "Yunhe Wang"
            ],
            "affiliations": [
                "Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21411.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MoGE: Повышение эффективности больших языковых моделей через групповую экспертизу",
                    "desc": "Статья представляет новый метод Mixture of Grouped Experts (MoGE) для улучшения балансировки нагрузки экспертов в больших языковых моделях. MoGE группирует экспертов при выборе, что обеспечивает более равномерное распределение вычислительной нагрузки между устройствами. На основе MoGE авторы создали модель Pangu Pro MoE с 72 миллиардами параметров, оптимизированную для NPU Ascend. Эксперименты показали, что MoGE повышает эффективность обучения и вывода модели, превосходя сопоставимые плотные модели по производительности."
                },
                "en": {
                    "title": "Balancing Experts for Efficient Language Model Execution",
                    "desc": "The paper introduces Mixture of Grouped Experts (MoGE), a novel approach to improve the efficiency of large language models by enhancing expert load balancing. MoGE ensures that an equal number of experts are activated for each input token, which addresses the issue of uneven expert activation seen in traditional Mixture of Experts (MoE) models. This architectural design allows for better distribution of computational load across multiple devices, significantly increasing throughput during inference. The results demonstrate that MoGE leads to superior performance and cost-effectiveness on Ascend NPUs, particularly with the Pangu Pro MoE model, which achieves impressive inference speeds and outperforms existing dense models."
                },
                "zh": {
                    "title": "混合分组专家：提升大型语言模型的效率与性能",
                    "desc": "混合分组专家（MoGE）是一种改进的模型架构，旨在提高大型语言模型的专家负载平衡和执行效率。通过将专家分组选择，MoGE确保每个输入令牌激活的专家数量相等，从而减少了系统在并行运行时的效率损失。我们在Ascend NPU上构建了Pangu Pro MoE，这是一种基于MoGE的稀疏模型，具有720亿个参数，显著提高了推理性能。实验结果表明，MoGE在模型训练和推理中都能实现更好的专家负载平衡和更高的执行效率。"
                }
            }
        }
    ],
    "link_prev": "2025-06-27.html",
    "link_next": "2025-07-01.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6月27日"
    },
    "short_date_next": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7月1日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}