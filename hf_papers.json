{
    "date": {
        "ru": "1 сентября",
        "en": "September 1",
        "zh": "9月1日"
    },
    "time_utc": "2025-09-01 03:15",
    "weekday": 0,
    "issue_id": 5638,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21113",
            "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
            "url": "https://huggingface.co/papers/2508.21113",
            "abstract": "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.",
            "score": 19,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "e3b0726caba25eb1",
            "authors": [
                "Jie Jiang",
                "Qi Yang",
                "Bolin Ni",
                "Shiming Xiang",
                "Han Hu",
                "Houwen Peng"
            ],
            "affiliations": [
                "Institute of Automation, CAS",
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21113.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "R-4B: Адаптивное мышление для эффективного решения задач",
                    "desc": "R-4B - это мультимодальная большая языковая модель с возможностью автоматического мышления. Она использует двухрежимный отжиг и двухрежимную оптимизацию политики для адаптивного выбора стратегии решения задач. Модель способна определять, когда нужно активировать процесс мышления в зависимости от сложности проблемы. R-4B достигает передовых результатов на 25 сложных бенчмарках при меньших вычислительных затратах по сравнению с более крупными моделями."
                },
                "en": {
                    "title": "R-4B: Smart Thinking for Efficient Problem Solving",
                    "desc": "R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models."
                },
                "zh": {
                    "title": "R-4B：智能思考与高效解决的结合",
                    "desc": "R-4B是一种自动思考的多模态大型语言模型，能够根据问题的复杂性自适应地决定何时进行思考。它采用双模退火和双模策略优化技术，以提高模型在解决问题时的效率和准确性。通过在多样化的数据集上进行训练，R-4B能够在简单问题上避免冗余的思考过程，从而降低计算成本。实验结果表明，R-4B在25个具有挑战性的基准测试中表现优异，超越了许多现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21112",
            "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
            "url": "https://huggingface.co/papers/2508.21112",
            "abstract": "EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
            "score": 17,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "5bbbfa48bbd5fb7c",
            "authors": [
                "Delin Qu",
                "Haoming Song",
                "Qizhi Chen",
                "Zhaoqing Chen",
                "Xianqiang Gao",
                "Xinyi Ye",
                "Qi Lv",
                "Modi Shi",
                "Guanghui Ren",
                "Cheng Ruan",
                "Maoqing Yao",
                "Haoran Yang",
                "Jiacheng Bao",
                "Bin Zhao",
                "Dong Wang"
            ],
            "affiliations": [
                "AgiBot",
                "Fudan University",
                "Northwestern Polytechnical University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21112.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в мультимодальном ИИ для роботов: EO-Robotics объединяет зрение, текст и действие",
                    "desc": "EO-Robotics представляет собой систему, состоящую из модели EO-1 и датасета EO-Data1.5M, которая продвигает мультимодальное воплощенное рассуждение и управление роботами через смешанное предобучение на основе зрения, текста и действий. Модель EO-1 использует унифицированную архитектуру для обработки мультимодальных входных данных и обучается на массивном высококачественном датасете EO-Data1.5M, содержащем более 1,5 миллиона образцов. Обучение модели происходит с использованием авторегрессивного декодирования и денойзинга методом сопоставления потоков. Эксперименты демонстрируют эффективность смешанного обучения на основе зрения, текста и действий для понимания открытого мира и обобщения на различные задачи манипуляции."
                },
                "en": {
                    "title": "Empowering Robots with Multimodal Reasoning and Control",
                    "desc": "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."
                },
                "zh": {
                    "title": "提升机器人控制的多模态推理新突破",
                    "desc": "EO-Robotics是一个新模型，包含EO-1模型和EO-Data1.5M数据集，旨在通过交替的视觉-文本-动作预训练来提升多模态的具身推理和机器人控制能力。EO-1模型能够处理图像、文本、视频和动作等多种输入，展现出在多模态具身推理和机器人控制方面的优越性能。该模型的训练依赖于一个包含超过150万样本的高质量数据集，强调交替的视觉-文本-动作理解。通过大量实验，验证了交替学习在开放世界理解和泛化中的有效性，提供了构建先进具身基础模型的宝贵见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18106",
            "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
            "url": "https://huggingface.co/papers/2508.18106",
            "abstract": "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.",
            "score": 16,
            "issue_id": 5638,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "7b691aaa7b52bcfd",
            "authors": [
                "Keke Lian",
                "Bin Wang",
                "Lei Zhang",
                "Libo Chen",
                "Junjie Wang",
                "Ziming Zhao",
                "Yujiu Yang",
                "Haotong Duan",
                "Haoran Zhao",
                "Shuang Liao",
                "Mingda Guo",
                "Jiazheng Quan",
                "Yilu Zhong",
                "Chenhao He",
                "Zichuan Chen",
                "Jie Wu",
                "Haoling Li",
                "Zhaoxuan Li",
                "Jiongchi Yu",
                "Hui Li",
                "Dong Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "Peking University",
                "Shanghai Jiao Tong University",
                "Singapore Management University",
                "Tencent",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18106.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "A.S.E: Новый стандарт оценки безопасности ИИ-генерируемого кода",
                    "desc": "A.S.E - это новый бенчмарк для оценки безопасности кода, генерируемого большими языковыми моделями (LLM). Он использует реальные репозитории и правила, определенные экспертами, что позволяет получить более точные результаты по сравнению с существующими методами. A.S.E сохраняет полный контекст репозитория, включая системы сборки и зависимости между файлами. Исследование показало, что Claude-3.7-Sonnet демонстрирует лучшую общую производительность, а разрыв в безопасности между проприетарными и открытыми моделями невелик."
                },
                "en": {
                    "title": "A.S.E: Elevating Security Evaluation for AI-Generated Code",
                    "desc": "The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches."
                },
                "zh": {
                    "title": "A.S.E：提升代码生成安全性的基准评估",
                    "desc": "A.S.E是一个用于评估大型语言模型生成代码安全性的基准，利用真实世界的代码库和专家定义的规则。现有的基准测试方法存在不足，无法有效连接输入上下文的质量与输出的安全性。A.S.E通过构建真实代码库中的任务，保留完整的上下文信息，提供可重复的安全评估。我们的评估结果显示，Claude-3.7-Sonnet在整体表现上最佳，而Qwen3-235B-A22B-Instruct在安全性评分上表现突出。"
                }
            }
        }
    ],
    "link_prev": "2025-08-29.html",
    "link_next": "2025-09-02.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8月29日"
    },
    "short_date_next": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9月2日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}