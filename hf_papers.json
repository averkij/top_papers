{
    "date": {
        "ru": "18 декабря",
        "en": "December 18",
        "zh": "12月18日"
    },
    "time_utc": "2024-12-18 03:22",
    "weekday": 2,
    "issue_id": 1181,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.13147",
            "title": "Are Your LLMs Capable of Stable Reasoning?",
            "url": "https://huggingface.co/papers/2412.13147",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK.",
            "score": 8,
            "issue_id": 1181,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "a030a3cb6cc36da2",
            "authors": [
                "Junnan Liu",
                "Hongwei Liu",
                "Linchen Xiao",
                "Ziyi Wang",
                "Kuikun Liu",
                "Songyang Gao",
                "Wenwei Zhang",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13147.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#evaluation",
                    "#reasoning",
                    "#leakage"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Новый подход к оценке способностей языковых моделей в сложных математических задачах",
                    "desc": "Статья представляет новый метод оценки больших языковых моделей (LLM) в задачах сложных рассуждений. Авторы вводят метрику G-Pass@k, которая оценивает как пиковую производительность модели, так и её стабильность при многократных попытках. Также представлен динамический бенчмарк LiveMathBench с современными математическими задачами для минимизации утечки данных при оценке. Эксперименты показывают значительный потенциал для улучшения 'реалистичных' способностей LLM к рассуждениям."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing LLM Evaluation for Real-World Reasoning",
                    "desc": "This paper addresses the gap between the performance of Large Language Models (LLMs) on benchmarks and their effectiveness in real-world scenarios, particularly in complex reasoning tasks. The authors propose a new evaluation metric called G-Pass@k, which assesses model performance over multiple attempts, focusing on both peak performance and stability. Additionally, they introduce LiveMathBench, a benchmark of challenging mathematical problems that reduces data leakage during evaluation. The study reveals that current LLMs have significant room for improvement in their reasoning capabilities, emphasizing the need for better evaluation methods."
                },
                "zh": {
                    "title": "提升大型语言模型推理能力的评估新方法",
                    "desc": "本论文探讨了大型语言模型（LLMs）在复杂推理任务中的表现与实际应用之间的差距。我们认为，这一差距主要源于当前的评估协议和指标无法全面反映LLMs的能力，尤其是在准确性和一致性至关重要的复杂推理任务中。为此，我们提出了G-Pass@k这一新评估指标，能够在多次采样中持续评估模型性能，并量化模型的最佳表现潜力和稳定性。此外，我们还推出了LiveMathBench，这是一个动态基准，包含具有挑战性的现代数学问题，旨在减少评估过程中的数据泄露风险。"
                }
            }
        }
    ],
    "link_prev": "2024-12-17.html",
    "link_next": "2024-12-19.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12月17日"
    },
    "short_date_next": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12月19日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#evaluation": 1
    },
    "zh": {
        "text": "大型语言模型（LLMs）展示了显著的生成能力，但常常出现幻觉。检索增强生成（RAG）通过引入外部知识提供了有效解决方案，但现有方法仍面临一些限制：额外的部署成本、检索文本块中的冗余输入标记以及检索和生成的联合优化缺乏。为解决这些问题，我们提出了RetroLLM，一个统一的框架，将检索和生成整合为单一的过程，使LLMs能够直接从语料库中生成细粒度的证据。此外，我们引入了分层FM-Index约束和前瞻性约束解码策略，以减少不相关的解码空间并提高证据准确性。实验结果表明，RetroLLM在五个开放域问答数据集上表现优异。代码可在https://github.com/sunnynexus/RetroLLM获取。",
        "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
        "pinyin": "大型语言模型（LLMs）展示了显著的生成能力，但常常出现幻觉。检索增强生成（RAG）通过引入外部知识提供了有效解决方案，但现有方法仍面临一些限制：额外的部署成本、检索文本块中的冗余输入标记以及检索和生成的联合优化缺乏。为解决这些问题，我们提出了RetroLLM，一个统一的框架，将检索和生成整合为单一的过程，使LLMs能够直接从语料库中生成细粒度的证据。此外，我们引入了分层FM-Index约束和前瞻性约束解码策略，以减少不相关的解码空间并提高证据准确性。实验结果表明，RetroLLM在五个开放域问答数据集上表现优异。代码可在https://github.com/sunnynexus/RetroLLM获取。\n\ndà xíng yǔ yán mó xíng (LLMs) zhǎn shì le xiǎn zhù de shēng chéng néng lì, dàn cháng cháng chū xiàn huàn jué. Jiǎn suǒ zēng qiáng shēng chéng (RAG) tōng guò yǐn rù wài bù zhī shi tí gōng le yǒu xiào jiě jué fāng ān, dàn xiàn yǒu fāng fǎ réng miàn lìng yī xiē xiàn zhì: é wài de bù shù chéng běn, jiǎn suǒ wén běn kuài zhōng de róng yù shū rù biāo jì yǐ jí jiǎn suǒ hé shēng chéng de lián hé yōu huà quē fǎ. Wèi jiě jué zhè xiē wèn tí, wǒ men tí chū le RetroLLM, yī gè tǒng yī de kuàng jià, jiāng jiǎn suǒ hé shēng chéng zhěng hé wéi dān yī de guò chéng, shǐ LLMs néng gòu zhí jiē kù zhōng shēng chéng xì lì dù de zhèng jù. Cǐ wài, wǒ men yǐn rù le fēn céng FM-Index yuē shuō hé qián zhān xìng yuē shuō jiě mǎ zhuàn lüè, yǐ jiǎn shǎo bù xiāng guān de jiě mǎ kōng jiān yǐng tí gāo zhèng jù zhǔn què xìng. Shí yàn jié guǒ biǎo míng, RetroLLM zài wǔ gè kāi fàng yù qiú shù jù shù jù shàng biǎo xiàn yōu yán. Dài mǎ kě zài https://github.com/sunnynexus/RetroLLM huò qǔ.",
        "vocab": "[{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generation'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'},\n{'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'hallucination'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhanced'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '外部', 'pinyin': 'wàibù', 'trans': 'external'},\n{'word': '知识', 'pinyin': 'zhīshi', 'trans': 'knowledge'},\n{'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solution'},\n{'word': '方案', 'pinyin': 'fāngàn', 'trans': 'scheme'},\n{'word': '面临', 'pinyin': 'miànlín', 'trans': 'face'},\n{'word': '限制', 'pinyin': 'xiànzhì', 'trans': 'limitation'},\n{'word': '额外', 'pinyin': 'éwài', 'trans': 'additional'},\n{'word': '部署', 'pinyin': 'bùshǔ', 'trans': 'deployment'},\n{'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'},\n{'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundant'},\n{'word': '输入', 'pinyin': 'shūrù', 'trans': 'input'},\n{'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'},\n{'word': '联合', 'pinyin': 'liánhé', 'trans': 'joint'},\n{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'},\n{'word': '缺乏', 'pinyin': 'quēfá', 'trans': 'lack'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '统一', 'pinyin': 'tǒngyī', 'trans': 'unified'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '整合', 'pinyin': 'zhěnghé', 'trans': 'integrate'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '使', 'pinyin': 'shǐ', 'trans': 'make'},\n{'word': '直接', 'pinyin': 'zhíjiē', 'trans': 'directly'},\n{'word': '语料库', 'pinyin': 'yǔliào kù', 'trans': 'corpus'},\n{'word': '细粒度', 'pinyin': 'xìlìdù', 'trans': 'fine-grained'},\n{'word': '证据', 'pinyin': 'zhèngjù', 'trans': 'evidence'},\n{'word': '分层', 'pinyin': 'fēncéng', 'trans': 'hierarchical'},\n{'word': 'FM-Index', 'pinyin': 'FM-Index', 'trans': 'FM-Index'},\n{'word': '约束', 'pinyin': 'yuēshù', 'trans': 'constraint'},\n{'word': '前瞻性', 'pinyin': 'qiánzhānxìng', 'trans': 'forward-looking'},\n{'word': '解码', 'pinyin': 'jiěmǎ', 'trans': 'decoding'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'},\n{'word': '不相关', 'pinyin': 'bùxiāngguān', 'trans': 'irrelevant'},\n{'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'},\n{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},\n{'word': '准确性', 'pinyin': 'zhǔnquèxìng', 'trans': 'accuracy'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},\n{'word': '开放域', 'pinyin': 'kāifàng yù', 'trans': 'open-domain'},\n{'word': '问答', 'pinyin': 'wèndá', 'trans': 'question-answering'},\n{'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '优异', 'pinyin': 'yōuyì', 'trans': 'excellent'},\n{'word': '代码', 'pinyin': 'dàimǎ', 'trans': 'code'},\n{'word': '获取', 'pinyin': 'huòqǔ', 'trans': 'obtain'}]",
        "trans": "Large Language Models (LLMs) have demonstrated significant generative capabilities but often suffer from hallucinations. Retrieval-Augmented Generation (RAG) offers an effective solution by introducing external knowledge; however, existing methods still face several limitations: additional deployment costs, redundant input tokens in retrieved text blocks, and a lack of joint optimization for retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single process, enabling LLMs to generate fine-grained evidence directly from a corpus. Additionally, we introduce hierarchical FM-Index constraints and look-ahead constraint decoding strategies to reduce irrelevant decoding space and enhance evidence accuracy. Experimental results show that RetroLLM performs exceptionally well on five open-domain question-answering datasets. The code is available at https://github.com/sunnynexus/RetroLLM.",
        "update_ts": "2024-12-17 09:11"
    }
}