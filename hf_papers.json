{
    "date": {
        "ru": "18 февраля",
        "en": "February 18",
        "zh": "2月18日"
    },
    "time_utc": "2025-02-18 03:13",
    "weekday": 1,
    "issue_id": 2263,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 1,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "🐣",
                "ru": {
                    "title": "Извлечение информации на плечах гигантов: как IE модели могут использовать ресурсы LLM",
                    "desc": "Исследователи представили новый подход к извлечению информации (IE) с использованием ресурсов больших языковых моделей (LLM). Метод под названием 'извлечение следующих токенов' (NTE) позволяет переформулировать задачу предсказания следующего токена в задачу извлечения уже присутствующих в контексте токенов. Модель Cuckoo, обученная на 102,6 млн примеров извлекательных данных, показывает лучшие результаты в условиях малого количества обучающих примеров по сравнению с существующими предобученными IE моделями. Этот подход позволяет IE моделям автоматически развиваться вместе с улучшениями в подготовке данных для LLM без дополнительных ручных усилий."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "利用LLM提升信息提取模型的性能",
                    "desc": "本文探讨了如何利用大型语言模型（LLM）来提升信息提取（IE）模型的性能。我们提出了一种新的提取方法，称为下一标记提取（NTE），通过将下一个标记预测转化为对上下文中已存在标记的提取，从而使IE模型能够利用LLM的资源。我们开发的Cuckoo模型在少量样本的情况下，能够有效适应传统和复杂的指令跟随IE任务，并且表现优于现有的预训练IE模型。Cuckoo作为一个“搭便车者”，能够随着LLM数据准备的进步而自然演变，无需额外的人工努力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 1,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты в доказательном программировании",
                    "desc": "Статья посвящена проблеме обучения языковых моделей программированию, ориентированному на доказательства. Авторы предлагают метод синтетического расширения данных для решения проблемы нехватки корпусов на языках доказательного программирования. Они создают модель PoPilot, которая превосходит GPT-4 на 64% в задачах проектного уровня. Метод также позволяет улучшить результаты GPT-4 на 54% путем исправления его выходных данных."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "合成数据增强，提升证明编程能力！",
                    "desc": "现有的语言模型在面向证明的编程中面临数据稀缺的问题，主要体现在两个方面：缺乏足够的面向证明编程语言（如F*）的语料库，以及缺少大规模的项目级证明实现，无法教会模型复杂的推理过程。我们提出了一种基于合成数据增强的方法，专注于项目级的面向证明编程，既用于生成也用于修复。该方法通过合成基本的面向证明编程问题来解决数据稀缺问题，并结合多样化的编码数据以提高推理能力，同时在现有代码库中创建新的证明和修复数据。我们的14B参数模型PoPilot经过微调后，在项目级面向证明编程中超越了GPT-4o模型64%的性能，并通过修复其输出提高了54%的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2月19日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。\n\nkuò sàn mó xíng (DMs) zài gè zhǒng lǐng yù de shēng chéng rèn wù zhōng chéng wéi shǒu xuǎn. rán ér, tā men yī lài duō cì shùn xù qián xiāng chuán dì, xiǎn zhù xiàn zhì le shí shí xìng néng. yǐ qián de jiā sù fāng fǎ zhǔ yào jī zhōng zài jiǎn shǎo cǎi yàng bù zhòu huò chóng yòng zhōng jiān jié guǒ, wèi néng lì yòng tú xiàng nèi bù kōng jiān qū yù de biàn huà. tōng guò lì yòng kuò sàn biàn shū zhǔ (DiTs) chǔ lǐ kě biàn shù liàng de biāo jì de líng huó xìng, wǒ men yǐn rù le RAS, yī zhǒng xīn de wú xū xùn liàn de cǎi yàng cè lüè, gēn jù DiT mó xíng de guān zhù diǎn dòng tài fēn pèi tú xiàng nèi bù tōng qū yù de cǎi yàng bǐ lǜ. wǒ men de guǎn jiàn guān chá shì, zài měi gè cǎi yàng bù zhòu zhōng, mó xíng jí zhōng zài yǔ yì shàng yǒu yì yì de qū yù, zhè xiē guān zhù qū yù zài lián xù bù zhòu zhōng biǎo xiàn chū qiáng dà de lián xù xìng. lì yòng zhè yī dòng chá, RAS jǐn gēng xīn shǐ dāng qián guān zhù de qū yù, ér qí tā qū yù shǐ yòng shàng yī bù de huǎn cùn zào shēng gēng xīn. mó xíng de guān zhù diǎn gēn jù qián yī bù de shū chū què dìng, lì yòng le wǒ men guān chá dào de shí jiān yī zhì xìng. wǒ men zài Stable Diffusion 3 hé Lumina-Next-T2I shàng píng guǎ RAS, fēn bié shí xiàn le zuì gāo 2.36 bèi hé 2.51 bèi de jiā sù, shēng chéng zhì liàng jǐn qīng wēi xià jiàng. cǐ wài, yòng hù yán jiū biǎo míng, RAS zài rén lèi píng jià xià tí gōng le xiāng sì de zhì liàng, tóng shí shí xiàn le 1.6 bèi de jiā sù. wǒ men de fāng fǎ zài gèng gāo xiào de kuò sàn biàn shū zhǔ fāng miàn zhǔ dé dào le zhòng yào jìn zhǎn, zēng qiáng le tā men zài shí shí yìng yòng zhōng de qián lì.",
        "vocab": "[\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"首选\", \"pinyin\": \"shǒu xuǎn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"顺序\", \"pinyin\": \"shùn xù\", \"trans\": \"sequential\"},\n    {\"word\": \"前向传递\", \"pinyin\": \"qián xiàng chuán dì\", \"trans\": \"forward pass\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"实时性能\", \"pinyin\": \"shí shí xìng néng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus on\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"采样步骤\", \"pinyin\": \"cǎi yàng bù zhòu\", \"trans\": \"sampling steps\"},\n    {\"word\": \"重用\", \"pinyin\": \"chóng yòng\", \"trans\": \"reuse\"},\n    {\"word\": \"中间结果\", \"pinyin\": \"zhōng jiān jié guǒ\", \"trans\": \"intermediate results\"},\n    {\"word\": \"利用\", \"pinyin\": \"lì yòng\", \"trans\": \"utilize\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"内部空间区域\", \"pinyin\": \"nèi bù kōng jiān qū yù\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"变化\", \"pinyin\": \"biàn huà\", \"trans\": \"change\"},\n    {\"word\": \"扩散变压器\", \"pinyin\": \"kuò sàn biàn yā qì\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"采样策略\", \"pinyin\": \"cǎi yàng cè lüè\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"动态分配\", \"pinyin\": \"dòng tài fēn pèi\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"关注点\", \"pinyin\": \"guān zhù diǎn\", \"trans\": \"focus points\"},\n    {\"word\": \"关键观察\", \"pinyin\": \"guǎn jiàn guān chá\", \"trans\": \"key observation\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"有意义\", \"pinyin\": \"yǒu yì yì\", \"trans\": \"meaningful\"},\n    {\"word\": \"连续步骤\", \"pinyin\": \"lián xù bù zhòu\", \"trans\": \"continuous steps\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"连续性\", \"pinyin\": \"lián xù xìng\", \"trans\": \"continuity\"},\n    {\"word\": \"洞察\", \"pinyin\": \"dòng chá\", \"trans\": \"insight\"},\n    {\"word\": \"更新\", \"pinyin\": \"gēng xīn\", \"trans\": \"update\"},\n    {\"word\": \"缓存噪声\", \"pinyin\": \"huǎn cún zào shēng\", \"trans\": \"cached noise\"},\n    {\"word\": \"确定\", \"pinyin\": \"què dìng\", \"trans\": \"determine\"},\n    {\"word\": \"时间一致性\", \"pinyin\": \"shí jiān yī zhì xìng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"acceleration\"},\n    {\"word\": \"生成质量\", \"pinyin\": \"shēng chéng zhì liàng\", \"trans\": \"generation quality\"},\n    {\"word\": \"轻微下降\", \"pinyin\": \"qīng wēi xià jiàng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"用户研究\", \"pinyin\": \"yòng hù yán jiū\", \"trans\": \"user study\"},\n    {\"word\": \"人类评估\", \"pinyin\": \"rén lèi píng gū\", \"trans\": \"human evaluation\"},\n    {\"word\": \"相似\", \"pinyin\": \"xiāng sì\", \"trans\": \"similar\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"potential\"},\n    {\"word\": \"重要进展\", \"pinyin\": \"zhòng yào jìn zhǎn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}