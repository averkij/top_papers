{
    "date": {
        "ru": "9 января",
        "en": "January 9",
        "zh": "1月9日"
    },
    "time_utc": "2025-01-09 06:14",
    "weekday": 3,
    "issue_id": 1575,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.04519",
            "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
            "url": "https://huggingface.co/papers/2501.04519",
            "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising \"deep thinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",
            "score": 28,
            "issue_id": 1572,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "b065003de5fa3bde",
            "authors": [
                "Xinyu Guan",
                "Li Lyna Zhang",
                "Yifei Liu",
                "Ning Shang",
                "Youran Sun",
                "Yi Zhu",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04519.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Малые модели решают большие задачи: rStar-Math превосходит гигантов в математике",
                    "desc": "Статья представляет rStar-Math - подход, позволяющий малым языковым моделям (SLM) достичь или превзойти способности крупных моделей в математических рассуждениях. Метод использует поиск по методу Монте-Карло (MCTS) с двумя специально обученными SLM: политикой и моделью вознаграждения. Авторы вводят новые методы синтеза обучающих данных, обучения модели вознаграждения и итеративного улучшения моделей. В результате rStar-Math значительно повышает эффективность SLM на математических тестах, превосходя более крупные модели."
                },
                "en": {
                    "title": "Empowering Small Models to Excel in Math Reasoning",
                    "desc": "The paper introduces rStar-Math, a framework that enhances the math reasoning abilities of small language models (SLMs) without relying on larger models. It employs Monte Carlo Tree Search (MCTS) to enable deep thinking, allowing the SLM to perform guided search during problem-solving. Key innovations include a code-augmented Chain of Thought (CoT) data synthesis method for generating verified reasoning paths, a refined process preference model (PPM) for better reward training, and a self-evolution strategy for iterative improvement. As a result, rStar-Math significantly boosts the performance of SLMs on math benchmarks, achieving state-of-the-art results in various assessments."
                },
                "zh": {
                    "title": "小型语言模型的数学推理新突破",
                    "desc": "rStar-Math展示了小型语言模型（SLMs）在数学推理能力上可以与OpenAI的o1相媲美，甚至超越它，而无需从更强大的模型中蒸馏。该方法通过蒙特卡洛树搜索（MCTS）实现“深度思考”，在测试时由SLM驱动的过程奖励模型指导数学策略SLM进行搜索。rStar-Math引入了三项创新来解决训练两个SLM的挑战，包括新颖的代码增强的链式推理数据合成方法和更有效的过程偏好模型（PPM）训练方法。经过四轮自我进化，rStar-Math在747,000个数学问题上生成了数百万个合成解，使SLMs的数学推理能力达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04575",
            "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
            "url": "https://huggingface.co/papers/2501.04575",
            "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce InfiGUIAgent, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. InfiGUIAgent achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at https://github.com/Reallm-Labs/InfiGUIAgent.",
            "score": 6,
            "issue_id": 1574,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "501c7ba58ede235b",
            "authors": [
                "Yuhang Liu",
                "Pengxiang Li",
                "Zishu Wei",
                "Congkai Xie",
                "Xueyu Hu",
                "Xinchen Xu",
                "Shengyu Zhang",
                "Xiaotian Han",
                "Hongxia Yang",
                "Fei Wu"
            ],
            "affiliations": [
                "ByteDance Inc",
                "Dalian University of Technology",
                "Reallm Labs",
                "The Hong Kong Polytechnic University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04575.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#training",
                    "#agents",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Умный агент GUI: новый уровень автоматизации интерфейсов",
                    "desc": "InfiGUIAgent - это агент графического пользовательского интерфейса, основанный на мультимодальных больших языковых моделях (MLLM). Он обучается с помощью двухэтапного процесса точной настройки, который улучшает базовые навыки понимания GUI и развивает способности к иерархическому рассуждению. InfiGUIAgent демонстрирует высокую эффективность в автоматизации задач взаимодействия с GUI, превосходя существующие подходы. Разработка направлена на преодоление ограничений, связанных с многошаговыми рассуждениями и зависимостью от текстовых аннотаций."
                },
                "en": {
                    "title": "Empowering GUI Agents with Native Reasoning Skills",
                    "desc": "InfiGUIAgent is a new type of Graphical User Interface (GUI) agent that uses multimodal large language models (MLLMs) to improve task automation on devices like computers and smartphones. This agent addresses the limitations of existing systems by employing a two-stage supervised fine-tuning process. The first stage focuses on developing basic skills such as understanding and interacting with GUIs, while the second stage enhances the agent's ability to perform complex reasoning tasks. As a result, InfiGUIAgent demonstrates strong performance on various GUI benchmarks, showcasing the importance of advanced reasoning capabilities in automating GUI interactions."
                },
                "zh": {
                    "title": "提升GUI交互的原生推理能力",
                    "desc": "本文介绍了一种名为InfiGUIAgent的图形用户界面（GUI）代理，它基于多模态大型语言模型（MLLM）进行任务自动化。InfiGUIAgent通过两阶段的监督微调流程进行训练，第一阶段提升了GUI理解和基础技能，第二阶段则通过合成数据整合了层次推理和期望反思推理能力。该代理在多个GUI基准测试中表现出色，显示了原生推理能力在增强GUI交互中的重要性。此研究为提高计算设备上的自动化任务提供了新的思路和方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04682",
            "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though",
            "url": "https://huggingface.co/papers/2501.04682",
            "abstract": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.",
            "score": 6,
            "issue_id": 1574,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "3479f7793755e586",
            "authors": [
                "Violet Xiang",
                "Charlie Snell",
                "Kanishk Gandhi",
                "Alon Albalak",
                "Anikait Singh",
                "Chase Blagden",
                "Duy Phung",
                "Rafael Rafailov",
                "Nathan Lile",
                "Dakota Mahan",
                "Louis Castricato",
                "Jan-Philipp Franken",
                "Nick Haber",
                "Chelsea Finn"
            ],
            "affiliations": [
                "Stanford University",
                "SynthLabs.ai",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04682.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#rlhf",
                    "#rl",
                    "#multimodal",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Meta-CoT: новый уровень рассуждений для ИИ",
                    "desc": "Исследователи предлагают новую концепцию под названием Meta Chain-of-Thought (Meta-CoT), которая расширяет традиционный подход Chain-of-Thought. Meta-CoT моделирует базовые рассуждения, необходимые для построения цепочки мыслей. Авторы представляют эмпирические доказательства того, что современные языковые модели демонстрируют поведение, согласующееся с контекстным поиском. Они также описывают конкретный процесс обучения модели для генерации Meta-CoT, включающий инструктивную настройку и обучение с подкреплением."
                },
                "en": {
                    "title": "Empowering AI with Enhanced Reasoning through Meta-CoT",
                    "desc": "The paper introduces a new framework called Meta Chain-of-Thought (Meta-CoT), which enhances the traditional Chain-of-Thought (CoT) approach by focusing on the reasoning processes behind generating CoTs. It provides experimental results from advanced models that show behaviors similar to in-context search, and discusses techniques for creating Meta-CoT through process supervision, synthetic data, and search algorithms. The authors propose a detailed training pipeline that combines instruction tuning with search traces and reinforcement learning to improve the generation of Meta-CoTs. Additionally, the paper raises important questions about scaling, the role of verifiers, and the potential for discovering new reasoning methods, aiming to advance the reasoning capabilities of large language models (LLMs)."
                },
                "zh": {
                    "title": "推动人工智能推理能力的元思维链",
                    "desc": "我们提出了一种新颖的框架，称为元思维链（Meta-CoT），它通过明确建模所需的推理过程来扩展传统的思维链（CoT）。我们展示了来自最先进模型的实证证据，这些模型表现出与上下文搜索一致的行为，并探索了通过过程监督、合成数据生成和搜索算法来生成元思维链的方法。最后，我们概述了一个具体的训练流程，结合了指令调优、线性化搜索轨迹和强化学习后训练，以生成元思维链。此项工作为在大型语言模型中实现元思维链提供了理论和实践的路线图，推动了人工智能更强大和更人性化的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04227",
            "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
            "url": "https://huggingface.co/papers/2501.04227",
            "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
            "score": 6,
            "issue_id": 1574,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "ff592ae1a5a88909",
            "authors": [
                "Samuel Schmidgall",
                "Yusheng Su",
                "Ze Wang",
                "Ximeng Sun",
                "Jialian Wu",
                "Xiaodong Yu",
                "Jiang Liu",
                "Zicheng Liu",
                "Emad Barsoum"
            ],
            "affiliations": [
                "AMD",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04227.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#agents",
                    "#rlhf",
                    "#survey"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Автономная лаборатория ИИ: революция в научных исследованиях",
                    "desc": "Статья представляет Agent Laboratory - автономную систему на основе моделей LLM, способную выполнять полный цикл научного исследования. Система проходит через этапы обзора литературы, экспериментов и написания отчета, позволяя пользователям давать обратную связь на каждом этапе. Эксперименты показали, что Agent Laboratory, работающая на модели o1-preview, генерирует лучшие результаты исследований и значительно снижает затраты на исследования. Авторы надеются, что эта система позволит исследователям сосредоточиться на творческом процессе, ускоряя научные открытия."
                },
                "en": {
                    "title": "Accelerating Science with Autonomous Research Frameworks",
                    "desc": "The paper presents Agent Laboratory, an autonomous framework that utilizes large language models (LLMs) to streamline the scientific research process. It operates in three stages: conducting a literature review, performing experiments, and writing reports, all while allowing human researchers to provide feedback. The study shows that Agent Laboratory can produce high-quality research outputs, including code that outperforms existing methods, and significantly reduces research costs by 84%. By automating routine tasks, the framework aims to free researchers to focus more on innovative ideas and less on tedious coding and documentation."
                },
                "zh": {
                    "title": "Agent Laboratory：加速科学发现的智能助手",
                    "desc": "本文介绍了一种名为Agent Laboratory的自主框架，旨在加速科学发现并降低研究成本。该框架基于大型语言模型（LLM），能够完成文献综述、实验和报告撰写等整个研究过程。研究表明，Agent Laboratory在生成研究成果方面表现优异，尤其是在机器学习代码的性能上，达到了最先进的水平。通过人类反馈的参与，研究质量显著提高，同时研究费用减少了84%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02772",
            "title": "GeAR: Generation Augmented Retrieval",
            "url": "https://huggingface.co/papers/2501.02772",
            "abstract": "Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called Generation Augmented Retrieval (GeAR) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to \"focus on\" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.",
            "score": 4,
            "issue_id": 1572,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "dafa87428ce906b5",
            "authors": [
                "Haoyu Liu",
                "Shaohan Huang",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Weiwei Deng",
                "Feng Sun",
                "Furu Wei",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02772.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#data",
                    "#rag",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "GeAR: Новый взгляд на извлечение документов через генерацию",
                    "desc": "Статья предлагает новый метод извлечения документов под названием Generation Augmented Retrieval (GeAR). В отличие от традиционных би-энкодеров, GeAR использует модули слияния и декодирования для генерации релевантного текста на основе запроса и документа. Это позволяет модели фокусироваться на детальной информации, не увеличивая вычислительную нагрузку. Авторы также разработали конвейер для синтеза качественных данных с помощью больших языковых моделей для обучения GeAR."
                },
                "en": {
                    "title": "GeAR: Enhancing Document Retrieval with Fine-Grained Semantic Focus",
                    "desc": "This paper introduces a new method called Generation Augmented Retrieval (GeAR) that enhances document retrieval techniques by focusing on fine-grained semantic relationships. Unlike traditional bi-encoders that primarily assess global semantics, GeAR generates relevant text from documents by fusing the query and document representations. This approach allows for a deeper understanding of retrieval results without increasing computational costs. Additionally, the authors provide a pipeline for synthesizing high-quality training data using large language models, leading to improved performance across various datasets."
                },
                "zh": {
                    "title": "生成增强检索：关注细粒度信息的创新方法",
                    "desc": "本文提出了一种新的文档检索方法，称为生成增强检索（GeAR）。GeAR通过融合查询和文档的表示，生成相关文本，从而关注细粒度信息。与传统的双编码器方法相比，GeAR在检索时不会增加计算负担，同时在多种场景和数据集上表现出竞争力的检索和定位性能。该方法还通过利用大型语言模型合成高质量数据，支持新框架的训练。"
                }
            }
        }
    ],
    "link_prev": "2025-01-08.html",
    "link_next": "2025-01-10.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1月8日"
    },
    "short_date_next": {
        "ru": "10.01",
        "en": "01/10",
        "zh": "1月10日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的强化学习方法，叫做 REINFORCE++。它改进了经典的 REINFORCE 算法，结合了 PPO 的优化技术，但不需要评论网络。REINFORCE++ 有三个主要目标：简单、提高训练稳定性和减少计算开销。实验结果显示，REINFORCE++ 比 GRPO 更稳定，比 PPO 更高效，性能也相当。代码可以在 https://github.com/OpenRLHF/OpenRLHF 找到。",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
        "pinyin": "这篇文章介绍了一种新的强化学习方法，叫做 REINFORCE++。它改进了经典的 REINFORCE 算法，结合了 PPO 的优化技术，但不需要评论网络。REINFORCE++ 有三个主要目标：简单、提高训练稳定性和减少计算开销。实验结果显示，REINFORCE++ 比 GRPO 更稳定，比 PPO 更高效，性能也相当。代码可以在 https://github.com/OpenRLHF/OpenRLHF 找到。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de qiáng huà xuéxí fāngfǎ, jiàozuò REINFORCE++. Tā gǎijìn le jīngdiǎn de REINFORCE suànfǎ, jiéhé le PPO de yōuhuà jìshù, dàn bù xūyào pínglùn wǎngluò. REINFORCE++ yǒu sān gè zhǔyào mùbiāo: jiǎndān, tígāo xùnliàn wěndìngxìng hé jiǎnshǎo jìsuàn kāixiāo. Shíyàn jiéguǒ xiǎnshì, REINFORCE++ bǐ GRPO gèng wěndìng, bǐ PPO gèng gāoxiào, xìngnéng yě xiāngdāng. Dàimǎ kěyǐ zài https://github.com/OpenRLHF/OpenRLHF zhǎo dào.",
        "vocab": "[{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'}, {'word': '经典', 'pinyin': 'jīng diǎn', 'trans': 'classic'}, {'word': '算法', 'pinyin': 'suàn fǎ', 'trans': 'algorithm'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'}, {'word': '评论', 'pinyin': 'píng lùn', 'trans': 'comment'}, {'word': '网络', 'pinyin': 'wǎng luò', 'trans': 'network'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}, {'word': '稳定性', 'pinyin': 'wěn dìng xìng', 'trans': 'stability'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'calculate'}, {'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '找到', 'pinyin': 'zhǎo dào', 'trans': 'find'}]",
        "trans": "This article introduces a new reinforcement learning method called REINFORCE++. It improves upon the classic REINFORCE algorithm by incorporating optimization techniques from PPO, without the need for a critic network. REINFORCE++ has three main objectives: simplicity, enhanced training stability, and reduced computational overhead. Experimental results show that REINFORCE++ is more stable than GRPO and more efficient than PPO, with comparable performance. The code can be found at https://github.com/OpenRLHF/OpenRLHF.",
        "update_ts": "2025-01-08 09:10"
    }
}