{
    "date": {
        "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 14",
        "zh": "2æœˆ14æ—¥"
    },
    "time_utc": "2025-02-14 03:14",
    "weekday": 4,
    "issue_id": 2209,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.09604",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09604",
            "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.",
            "score": 5,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "7aa5ce3731848736",
            "authors": [
                "Yung-Sung Chuang",
                "Benjamin Cohen-Wang",
                "Shannon Zejiang Shen",
                "Zhaofeng Wu",
                "Hu Xu",
                "Xi Victoria Lin",
                "James Glass",
                "Shang-Wen Li",
                "Wen-tau Yih"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09604.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "SelfCite: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ñƒ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "SelfCite - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. SelfCite Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ best-of-N Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ F1-Ğ¼ĞµÑ€Ñ‹ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 5.3 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LongBench-Cite."
                },
                "en": {
                    "title": "Enhancing Citation Quality with SelfCite",
                    "desc": "SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å¼•ç”¨ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "SelfCiteæ˜¯ä¸€ç§æ–°é¢–çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé«˜è´¨é‡ã€ç»†ç²’åº¦çš„å¥å­çº§å¼•ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡æ¶ˆèæä¾›çš„å¥–åŠ±ä¿¡å·ï¼Œå‡å°‘å¯¹æ˜‚è´µå’ŒåŠ³åŠ¨å¯†é›†å‹æ³¨é‡Šçš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœå¼•ç”¨æ˜¯å¿…è¦çš„ï¼Œç§»é™¤è¢«å¼•ç”¨æ–‡æœ¬åº”é˜²æ­¢ç›¸åŒçš„å“åº”ï¼›å¦‚æœè¶³å¤Ÿï¼Œä¿ç•™è¢«å¼•ç”¨æ–‡æœ¬åº”ä¿æŒç›¸åŒçš„å“åº”ã€‚SelfCiteåœ¨LongBench-CiteåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œä½¿å¼•ç”¨çš„F1åˆ†æ•°æé«˜äº†5.3ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08946",
            "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
            "url": "https://huggingface.co/papers/2502.08946",
            "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.",
            "score": 1,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "daecc7f38306f7b8",
            "authors": [
                "Mo Yu",
                "Lemao Liu",
                "Junjie Wu",
                "Tsz Ting Chung",
                "Shunchi Zhang",
                "Jiangnan Li",
                "Dit-Yan Yeung",
                "Jie Zhou"
            ],
            "affiliations": [
                "HKUST",
                "JHU",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08946.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ?",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ PhysiCo Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4 Ğ¸ Gemini 2.0, Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 40% Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ³Ğ°Ñ', Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğµ Ğ¶Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "Unveiling the Limits of LLM Understanding",
                    "desc": "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."
                },
                "zh": {
                    "title": "æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£ç†è§£å®ƒä»¬æ‰€è¯´çš„å†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPhysiCoçš„è¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡ç½‘æ ¼æ ¼å¼çš„è¾“å…¥æ¥å‡è½»è®°å¿†é—®é¢˜ï¼Œè¿™äº›è¾“å…¥æŠ½è±¡åœ°æè¿°äº†ç‰©ç†ç°è±¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨ç†è§£èƒ½åŠ›ä¸Šè½åäºäººç±»çº¦40%ï¼Œå¹¶ä¸”åœ¨ç½‘æ ¼ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºéšæœºé¹¦é¹‰ç°è±¡çš„å­˜åœ¨ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æŒ‘æˆ˜äº†LLMsï¼Œä¸»è¦æ˜¯ç”±äºå†…åœ¨çš„å›°éš¾ï¼Œè€Œéç½‘æ ¼æ ¼å¼çš„ä¸ç†Ÿæ‚‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09100",
            "title": "Logical Reasoning in Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.09100",
            "abstract": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
            "score": 1,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "72b32d40c559c7e4",
            "authors": [
                "Hanmeng Liu",
                "Zhizhang Fu",
                "Mengru Ding",
                "Ruoxi Ning",
                "Chaoli Zhang",
                "Xiaozhang Liu",
                "Yue Zhang"
            ],
            "affiliations": [
                "Hainan University",
                "Westlake University",
                "Zhejiang Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09100.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ’ Ğ½ĞµĞ¼ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing Logical Reasoning in Large Language Models",
                    "desc": "This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems."
                },
                "zh": {
                    "title": "æå‡AIç³»ç»Ÿé€»è¾‘æ¨ç†èƒ½åŠ›çš„æ¢ç´¢",
                    "desc": "éšç€OpenAI o3å’ŒDeepSeek-R1ç­‰å…ˆè¿›æ¨ç†æ¨¡å‹çš„å‡ºç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è¿›è¡Œä¸¥æ ¼é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£ä¹‹è°œã€‚æœ¬æ–‡ç»¼è¿°äº†LLMsä¸­é€»è¾‘æ¨ç†çš„æœ€æ–°è¿›å±•ï¼Œæ¢è®¨äº†é€»è¾‘æ¨ç†çš„èŒƒå›´ã€ç†è®ºåŸºç¡€ä»¥åŠè¯„ä¼°æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ¨ç†èŒƒå¼ï¼ˆå¦‚æ¼”ç»ã€å½’çº³ã€æº¯å› å’Œç±»æ¯”ï¼‰çš„ç°æœ‰èƒ½åŠ›ï¼Œå¹¶è¯„ä¼°äº†å¢å¼ºæ¨ç†æ€§èƒ½çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æ•°æ®ä¸­å¿ƒè°ƒä¼˜ã€å¼ºåŒ–å­¦ä¹ ã€è§£ç ç­–ç•¥å’Œç¥ç»ç¬¦å·æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09056",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
            "url": "https://huggingface.co/papers/2502.09056",
            "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.",
            "score": 0,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "36c3c29072ae279d",
            "authors": [
                "Kunat Pipatanakul",
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09056.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#low_resource",
                    "#multilingual",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿ĞµÑ€ĞµĞ½ĞµÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek R1 Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM. Ğ¦ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ DeepSeek R1."
                },
                "en": {
                    "title": "Empowering Thai LLMs with Enhanced Reasoning Capabilities",
                    "desc": "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."
                },
                "zh": {
                    "title": "æå‡ä½èµ„æºè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æ•°æ®é€‰æ‹©å’Œæ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚DeepSeek R1ï¼‰èå…¥ç‰¹å®šè¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç‰¹åˆ«å…³æ³¨æ³°è¯­LLMã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¢å¼ºç‰¹å®šè¯­è¨€LLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶ç›®æ ‡è¯­è¨€çš„èƒ½åŠ›ã€‚DeepSeek R1åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸»è¦å—ç›Šäºé«˜èµ„æºè¯­è¨€ï¼Œå¦‚è‹±è¯­å’Œä¸­æ–‡ï¼Œè€Œä½èµ„æºè¯­è¨€åˆ™å—åˆ°å¿½è§†ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»…ä½¿ç”¨å…¬å¼€æ•°æ®é›†å’Œ120ç¾å…ƒçš„è®¡ç®—é¢„ç®—ï¼Œå°±å¯ä»¥æå‡ç‰¹å®šè¯­è¨€LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶è¾¾åˆ°DeepSeek R1çš„æ°´å¹³ï¼Œè€Œä¸å½±å“å…¶åœ¨ç›®æ ‡è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-13.html",
    "link_next": "2025-02-17.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æœ€è¿‘çš„å›¾åƒé‡å…‰æ¨¡å‹è¿›å±•ä½¿å¾—ä¸€è‡´çš„å…‰ç…§æ•ˆæœæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡å…‰ä»ç„¶æ»åï¼Œä¸»è¦ç”±äºè®­ç»ƒæˆæœ¬é«˜æ˜‚å’Œç¼ºä¹å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘é‡å…‰æ•°æ®é›†ã€‚é€å¸§åº”ç”¨å›¾åƒé‡å…‰æ¨¡å‹ä¼šå¯¼è‡´å…‰æºå’Œå¤–è§‚ä¸ä¸€è‡´ï¼Œç”Ÿæˆçš„è§†é¢‘ä¼šå‡ºç°é—ªçƒã€‚æˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘é‡å…‰æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸€è‡´å…‰ç…§æ³¨æ„åŠ›æ¨¡å—å’Œæ¸è¿›å…‰èåˆç­–ç•¥æ¥å¢å¼ºå…‰ç…§ä¸€è‡´æ€§ï¼Œç¡®ä¿è§†é¢‘çš„æ—¶é—´è¿è´¯æ€§ã€‚",
        "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
        "pinyin": "æœ€è¿‘çš„å›¾åƒé‡å…‰æ¨¡å‹è¿›å±•ä½¿å¾—ä¸€è‡´çš„å…‰ç…§æ•ˆæœæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡å…‰ä»ç„¶æ»åï¼Œä¸»è¦ç”±äºè®­ç»ƒæˆæœ¬é«˜æ˜‚å’Œç¼ºä¹å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘é‡å…‰æ•°æ®é›†ã€‚é€å¸§åº”ç”¨å›¾åƒé‡å…‰æ¨¡å‹ä¼šå¯¼è‡´å…‰æºå’Œå¤–è§‚ä¸ä¸€è‡´ï¼Œç”Ÿæˆçš„è§†é¢‘ä¼šå‡ºç°é—ªçƒã€‚æˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘é‡å…‰æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸€è‡´å…‰ç…§æ³¨æ„åŠ›æ¨¡å—å’Œæ¸è¿›å…‰èåˆç­–ç•¥æ¥å¢å¼ºå…‰ç…§ä¸€è‡´æ€§ï¼Œç¡®ä¿è§†é¢‘çš„æ—¶é—´è¿è´¯æ€§ã€‚\n\nZuÃ¬jÃ¬n de tÃºxiÃ ng zhÃ²ngguÄng mÃ³xÃ­ng jÃ¬nzhÇn shÇdÃ© yÄ«zhÃ¬ de guÄngzhÃ o xiÃ oguÇ’ chÃ©ngwÃ©i kÄ›nÃ©ng. RÃ¡n'Ã©r, shÃ¬pÃ­n zhÃ²ngguÄng rÃ©ngrÃ¡n zhÃ¬hÃ²u, zhÇ”yÃ o yÃ³uyÃº xÃ¹nliÃ n chÃ©ngbÄ›n gÄotÃ¡ng hÃ© quÄ“fÃ¡ duÅyÃ nghuÃ , gÄo zhÃ¬liÃ ng de shÃ¬pÃ­n zhÃ²ngguÄng shÃ¹jÃ¹jÃ­. ZhÃºzhÃ¨n yÃ¬ngyÃ²ng tÃºxiÃ ng zhÃ²ngguÄng mÃ³xÃ­ng huÃ¬ dÇozhÃ¬ guÄngyuÃ¡n hÃ© wÃ ixiÃ n bÃ¹ yÄ«zhÃ¬, shÄ“ngchÃ©ng de shÃ¬pÃ­n huÃ¬ chÅ«xiÃ n shÇnshuÃ². WÇ’men tÃ­chÅ«le Light-A-Video, yÄ«zhÇ’ng wÃºxÅ« xÃ¹nliÃ n de shÃ¬pÃ­n zhÃ²ngguÄng fÄngfÇ. TÄ tÅngguÃ² yÄ«zhÃ¬ guÄngzhÃ o zhÃ¹yÃ¬lÃ¬ mÃ³kuÃ i hÃ© jiÃ njÃ¬n guÄng rÃ³nghÃ© cÃ¨lÃ¼Ã¨ lÃ¡i zÄ“ngqiÃ¡ng guÄngzhÃ o yÄ«zhÃ¬xÃ¬ng, quÃ¨bÇo shÃ¬pÃ­n de shÃ­jiÄn liÃ¡nhÃ©xÃ¬ng.",
        "vocab": "[{'word': 'é‡å…‰', 'pinyin': 'chÃ³ng guÄng', 'trans': 'relighting'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'ä¸€è‡´', 'pinyin': 'yÄ« zhÃ¬', 'trans': 'consistent'},\n{'word': 'å…‰ç…§', 'pinyin': 'guÄng zhÃ o', 'trans': 'illumination'},\n{'word': 'æ»å', 'pinyin': 'zhÃ¬ hÃ²u', 'trans': 'lag behind'},\n{'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'mainly'},\n{'word': 'ç”±äº', 'pinyin': 'yÃ³u yÃº', 'trans': 'due to'},\n{'word': 'é«˜æ˜‚', 'pinyin': 'gÄo Ã¡ng', 'trans': 'high'},\n{'word': 'ç¼ºä¹', 'pinyin': 'quÄ“ fÃ¡', 'trans': 'lack'},\n{'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅ yÃ ng huÃ ', 'trans': 'diversified'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'é€å¸§', 'pinyin': 'zhÃº zhÄ“n', 'trans': 'frame-by-frame'},\n{'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'},\n{'word': 'å¤–è§‚', 'pinyin': 'wÃ i guÇn', 'trans': 'appearance'},\n{'word': 'é—ªçƒ', 'pinyin': 'shÇn shuÃ²', 'trans': 'flicker'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'æ— éœ€', 'pinyin': 'wÃº xÅ«', 'trans': 'without needing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æ¨¡å—', 'pinyin': 'mÃ³ kuÃ i', 'trans': 'module'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ng qiÃ¡ng', 'trans': 'enhance'},\n{'word': 'ç¡®ä¿', 'pinyin': 'quÃ¨ bÇo', 'trans': 'ensure'},\n{'word': 'è¿è´¯æ€§', 'pinyin': 'liÃ¡n guÃ n xÃ¬ng', 'trans': 'coherence'}]",
        "trans": "Recent advancements in image relighting models have made consistent lighting effects possible. However, video relighting still lags behind, primarily due to the high training costs and the lack of diverse, high-quality video relighting datasets. Applying image relighting models frame-by-frame can result in inconsistent light sources and appearances, causing flickering in the generated videos. We propose Light-A-Video, a training-free video relighting method. It enhances lighting consistency through a consistent lighting attention module and a progressive light blending strategy, ensuring temporal coherence in the video.",
        "update_ts": "2025-02-13 09:11"
    }
}