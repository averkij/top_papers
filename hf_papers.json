{
    "date": {
        "ru": "8 ноября",
        "en": "November 8",
        "zh": "11月8日"
    },
    "time_utc": "2024-11-08 04:14",
    "weekday": 4,
    "issue_id": 465,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.05003",
            "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
            "url": "https://huggingface.co/papers/2411.05003",
            "abstract": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.",
            "score": 17,
            "issue_id": 464,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "f71f2e0f1addbe57",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Переснимаем реальность: новые ракурсы для любого видео",
                    "desc": "ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенерировать исходное видео с другими углами обзора и кинематографическим движением камеры, включая правдоподобное восстановление невидимых частей сцены. Метод работает в два этапа: сначала создается шумное опорное видео с новой траекторией камеры, а затем оно очищается и делается временно согласованным. ReCapture использует мультивидовые диффузионные модели и рендеринг облака точек на основе глубины."
                },
                "en": {
                    "title": "ReCapture: Transforming User Videos with New Camera Perspectives",
                    "desc": "This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience."
                },
                "zh": {
                    "title": "ReCapture：从用户视频生成新视角的魔法",
                    "desc": "最近在视频建模方面取得了突破，使得生成视频中的相机轨迹可控。然而，这些方法无法直接应用于用户提供的非生成视频。本文提出了一种名为ReCapture的方法，可以从单个用户提供的视频生成具有新相机轨迹的新视频。该方法不仅能够从不同角度重新生成参考视频，还能合理地幻觉出参考视频中不可见的场景部分。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04999",
            "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation",
            "url": "https://huggingface.co/papers/2411.04999",
            "abstract": "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/",
            "score": 3,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "47171ef52d95552a",
            "data": {
                "categories": [
                    "#robotics",
                    "#3d",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Динамическая память для роботов в изменяющемся мире",
                    "desc": "DynaMem - новый подход к мобильной манипуляции с открытым словарем в динамических средах. Система использует динамическую пространственно-семантическую память для представления окружения робота, построенную на основе 3D структуры данных облаков точек. DynaMem применяет мультимодальные языковые модели и модели компьютерного зрения для локализации объектов по запросам на естественном языке. Эксперименты показали двукратное улучшение успешности захвата и перемещения нестационарных объектов по сравнению с современными статическими системами."
                },
                "en": {
                    "title": "Empowering Robots with Dynamic Memory for Open-World Manipulation",
                    "desc": "This paper introduces DynaMem, a novel approach for open-vocabulary mobile manipulation that allows robots to adapt to dynamic environments. Unlike traditional systems that rely on static environments, DynaMem utilizes a dynamic spatio-semantic memory to keep track of changes in the robot's surroundings. It employs a 3D data structure to manage point clouds and leverages multimodal large language models (LLMs) for object localization. The results show that DynaMem significantly improves the robot's ability to interact with non-stationary objects, achieving a 70% success rate in pick-and-drop tasks, which is more than double the performance of existing static systems."
                },
                "zh": {
                    "title": "动态记忆，智能操作！",
                    "desc": "本研究提出了一种新的动态空间语义记忆方法DynaMem，用于开放词汇的移动操作。与传统静态环境系统不同，DynaMem能够在不断变化的环境中进行物体定位和操作。该方法利用三维数据结构维护动态记忆，并通过多模态大语言模型进行查询。实验结果表明，DynaMem在非静态物体的抓取和放置任务中成功率达70%，显著优于现有静态系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04709",
            "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2411.04709",
            "abstract": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.",
            "score": 3,
            "issue_id": 464,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 ноября",
                "en": "November 5",
                "zh": "11月5日"
            },
            "hash": "fcc8e4daf79a82b9",
            "data": {
                "categories": [
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TIP-I2V: Революция в изучении промптов для генерации видео из изображений",
                    "desc": "Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и изображений-промптов для генерации видео из изображений. Датасет также включает соответствующие сгенерированные видео от пяти современных моделей преобразования изображений в видео. TIP-I2V позволяет анализировать предпочтения пользователей, оценивать многомерную производительность обученных моделей и решать проблемы безопасности, связанные с дезинформацией. Этот набор данных подчеркивает важность специализированного датасета промптов для генерации видео из изображений и открывает новые возможности для исследований в этой области."
                },
                "en": {
                    "title": "Empowering Image-to-Video Generation with TIP-I2V Dataset",
                    "desc": "This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues."
                },
                "zh": {
                    "title": "TIP-I2V：图像到视频生成的新数据集",
                    "desc": "视频生成模型正在改变内容创作，图像到视频模型因其更好的可控性和视觉一致性而受到关注。尽管这些模型很受欢迎，但目前缺乏专门用于研究用户提供的文本和图像提示的数据集。本文介绍了TIP-I2V，这是第一个大规模的数据集，包含超过170万个独特的用户提供的文本和图像提示，专门用于图像到视频生成。该数据集的推出将推动图像到视频研究的进展，帮助研究人员分析用户偏好并评估模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04996",
            "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
            "url": "https://huggingface.co/papers/2411.04996",
            "abstract": "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\\% of the wall-clock time and text quality in 75.6\\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).",
            "score": 1,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "53d29fd65eda072e",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективные мультимодальные трансформеры: меньше вычислений, та же мощность",
                    "desc": "Эта статья представляет Mixture-of-Transformers (MoT) - новую архитектуру для мультимодальных языковых моделей, которая значительно снижает вычислительные затраты при предобучении. MoT разделяет параметры модели по модальностям, позволяя эффективно обрабатывать текст, изображения и речь в единой системе. Эксперименты показывают, что MoT достигает производительности плотных базовых моделей, используя значительно меньше вычислительных ресурсов. Это позволяет создавать более эффективные мультимодальные системы искусственного интеллекта."
                },
                "en": {
                    "title": "Efficient Multi-Modal Processing with Mixture-of-Transformers",
                    "desc": "This paper presents Mixture-of-Transformers (MoT), a novel sparse multi-modal transformer architecture designed to efficiently handle text, images, and speech. By decoupling non-embedding parameters by modality, MoT allows for specialized processing while maintaining global self-attention across the entire input. The architecture significantly reduces computational costs, achieving comparable performance to dense models with fewer floating point operations (FLOPs). Evaluations show that MoT not only matches but often outperforms dense baselines in various settings, demonstrating its effectiveness in multi-modal tasks."
                },
                "zh": {
                    "title": "混合变换器：高效的多模态学习新方案",
                    "desc": "本论文介绍了一种新的稀疏多模态变换器架构，称为混合变换器（MoT），旨在降低大语言模型的预训练计算成本。MoT通过模态解耦模型的非嵌入参数，使得不同模态（如文本、图像和语音）可以进行特定处理，同时保持全局自注意力机制。实验结果表明，MoT在多个设置下表现出色，能够以更少的计算资源达到与密集基线相当的性能。该模型在图像生成和语音处理任务中均展现了显著的效率优势，证明了其在多模态学习中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04989",
            "title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2411.04989",
            "abstract": "Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guidedx2013offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity.",
            "score": 1,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "a707043470b8dffd",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Управляемая генерация видео без дополнительного обучения",
                    "desc": "SG-I2V - это новый фреймворк для управляемой генерации видео из изображений. Он использует предобученную диффузионную модель без необходимости дополнительного обучения или внешних данных. Метод превосходит неконтролируемые базовые модели и конкурирует с контролируемыми по качеству изображения и точности движения. SG-I2V позволяет легко настраивать конкретные элементы генерируемых видео, такие как движение объектов или камеры."
                },
                "en": {
                    "title": "Zero-Shot Control in Image-to-Video Generation",
                    "desc": "This paper presents SG-I2V, a novel framework for generating videos from images with controllable features. Unlike traditional methods that require fine-tuning on annotated datasets, SG-I2V operates in a zero-shot manner, leveraging a pre-trained image-to-video diffusion model. This approach allows for easier manipulation of elements like object motion and camera movement without the computational costs associated with fine-tuning. The results show that SG-I2V achieves high visual quality and motion fidelity, outperforming unsupervised methods and competing with supervised ones."
                },
                "zh": {
                    "title": "SG-I2V：高效的图像到视频生成方法",
                    "desc": "本文介绍了一种名为SG-I2V的框架，用于可控的图像到视频生成。该方法利用预训练的图像到视频扩散模型，提供零-shot控制，避免了繁琐的微调过程。与无监督基线相比，我们的方法在视觉质量和运动保真度上表现优越，并且与监督模型相竞争。此研究为视频生成提供了一种更高效的解决方案，减少了对标注数据集的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04905",
            "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
            "url": "https://huggingface.co/papers/2411.04905",
            "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.",
            "score": 0,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "799dedd6597ce7ab",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#agents"
                ],
                "emoji": "🧑‍💻",
                "ru": {
                    "title": "OpenCoder: открытая книга рецептов для создания топовых языковых моделей кода",
                    "desc": "OpenCoder - это высококачественная языковая модель для работы с кодом, сопоставимая по производительности с ведущими моделями. Авторы не только предоставляют веса модели и код для инференса, но и полный набор воспроизводимых данных для обучения, конвейер обработки данных и подробные протоколы обучения. Ключевыми ингредиентами для создания модели такого уровня являются оптимизированные эвристические правила очистки данных, методы дедупликации, использование текстового корпуса, связанного с кодом, и высококачественные синтетические данные. Эта открытость призвана ускорить исследования и обеспечить воспроизводимый прогресс в области ИИ для работы с кодом."
                },
                "en": {
                    "title": "OpenCoder: Unlocking Code AI with Transparency and Reproducibility",
                    "desc": "This paper introduces OpenCoder, a high-performance large language model (LLM) specifically designed for code generation and reasoning tasks. It addresses the lack of open-access models that provide reproducible data processing and transparent training protocols, which are essential for scientific research. OpenCoder not only matches the performance of proprietary models but also shares its model weights, inference code, and detailed training methodologies. By emphasizing data cleaning, corpus recall, and synthetic data generation, OpenCoder aims to enhance accessibility and foster reproducible advancements in the field of code AI."
                },
                "zh": {
                    "title": "OpenCoder：开放的顶级代码大语言模型",
                    "desc": "本文介绍了OpenCoder，一个高质量的代码大语言模型（LLM），旨在为科学研究提供开放的资源。与其他模型不同，OpenCoder不仅提供模型权重和推理代码，还包括可重复的训练数据和完整的数据处理流程。我们识别出构建顶级代码LLM的关键要素，包括数据清洗的启发式规则、与代码相关的文本语料库的回忆以及高质量的合成数据。通过这种开放性，我们希望加速代码人工智能的研究和可重复的进展。"
                }
            }
        }
    ],
    "link_prev": "2024-11-07.html",
    "link_next": "2024-11-11.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11月7日"
    },
    "short_date_next": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11月11日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 2,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。",
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "pinyin": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。\n\nzhè piān wén zhāng jiè shào le yī zhǒng gǎi jìn de jiǎn suǒ zēng qiáng shēng chéng (RAG) fāng fǎ, chēng wéi HtmlRAG. chuántǒng de RAG xì tǒng cóng wǎng yè jiǎn suǒ xìn xī, tī qǔ chún wén běn wèi gěi dà yǔ yán mó xìng (LLMs). rán ér, zhè huì diū shī HTML zhōng de jiè gòu hé yǔ yì xìn xī. HtmlRAG zhí jiē shǐ yòng HTML gē shì de zhī shì, bǎo liú gèng duō xìn xī. dàn HTML bāo hán é xiǎo de biǎo qiān hé zào shēng, zuò zhě tí chū le qīng lǐ hé yā suō cè lüè lái jiě jué zhè gè wèn tí. shí yàn zhèng míng, HtmlRAG zài liù gè wèn dá shù jù zhōng biǎo xiàn gèng hǎo.",
        "vocab": "[\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"检索\", \"pinyin\": \"jiǎn suǒ\", \"trans\": \"retrieval\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhancement\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēng wéi\", \"trans\": \"called\"},\n    {\"word\": \"传统\", \"pinyin\": \"chuán tǒng\", \"trans\": \"traditional\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"网页\", \"pinyin\": \"wǎng yè\", \"trans\": \"webpage\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"纯文本\", \"pinyin\": \"chún wén běn\", \"trans\": \"pure text\"},\n    {\"word\": \"喂给\", \"pinyin\": \"wèi gěi\", \"trans\": \"feed\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"丢失\", \"pinyin\": \"diū shī\", \"trans\": \"lose\"},\n    {\"word\": \"结构\", \"pinyin\": \"jié gòu\", \"trans\": \"structure\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantics\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìn xī\", \"trans\": \"information\"},\n    {\"word\": \"直接\", \"pinyin\": \"zhí jiē\", \"trans\": \"directly\"},\n    {\"word\": \"格式\", \"pinyin\": \"gé shì\", \"trans\": \"format\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shì\", \"trans\": \"knowledge\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎo liú\", \"trans\": \"retain\"},\n    {\"word\": \"额外\", \"pinyin\": \"é wài\", \"trans\": \"extra\"},\n    {\"word\": \"标签\", \"pinyin\": \"biāo qiān\", \"trans\": \"tag\"},\n    {\"word\": \"噪声\", \"pinyin\": \"zào shēng\", \"trans\": \"noise\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"清理\", \"pinyin\": \"qīng lǐ\", \"trans\": \"clean\"},\n    {\"word\": \"压缩\", \"pinyin\": \"yā suō\", \"trans\": \"compress\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces an improved Retrieval-Augmented Generation (RAG) method called HtmlRAG. Traditional RAG systems retrieve information from web pages and extract plain text to feed into large language models (LLMs). However, this approach loses the structural and semantic information present in HTML. HtmlRAG directly uses knowledge in HTML format, preserving more information. But since HTML contains additional tags and noise, the authors propose cleaning and compression strategies to address this issue. Experiments show that HtmlRAG performs better on six question-answering datasets.",
        "update_ts": "2024-11-07 10:12"
    }
}