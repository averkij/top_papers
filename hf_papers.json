{
    "date": {
        "ru": "28 октября",
        "en": "October 28",
        "zh": "10月28日"
    },
    "time_utc": "2024-10-28 05:14",
    "weekday": 0,
    "issue_id": 303,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17856",
            "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
            "url": "https://huggingface.co/papers/2410.17856",
            "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
            "score": 10,
            "issue_id": 302,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "0a477d46d035f1d4",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Визуально-временной контекст открывает новые горизонты для VLM в воплощенном ИИ",
                    "desc": "Статья представляет новый подход к использованию визуально-языковых моделей (VLM) в задачах принятия решений в открытых средах. Авторы предлагают протокол визуально-временного контекстного промптинга для улучшения взаимодействия между VLM и моделями политик. Метод использует сегментацию объектов из прошлых и текущих наблюдений для управления взаимодействием политики и среды. Эксперименты в Minecraft показывают, что подход позволяет агентам решать ранее недостижимые задачи, особенно требующие пространственного понимания."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19008",
            "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
            "url": "https://huggingface.co/papers/2410.19008",
            "abstract": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.",
            "score": 7,
            "issue_id": 300,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "85936de603f8cc7a",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#medicine"
                ],
                "emoji": "❤️",
                "ru": {
                    "title": "PULSE: Революция в автоматическом анализе ЭКГ с помощью мультимодальных языковых моделей",
                    "desc": "Статья представляет новый подход к автоматической интерпретации ЭКГ с использованием мультимодальных больших языковых моделей (MLLM). Авторы создали ECGInstruct - набор данных из более чем миллиона образцов ЭКГ для обучения моделей, и разработали PULSE - специализированную MLLM для понимания изображений ЭКГ. Также был создан бенчмарк ECGBench для оценки моделей по четырем ключевым задачам интерпретации ЭКГ. Эксперименты показали, что PULSE превосходит общие MLLM на 15-30% по точности, демонстрируя потенциал для улучшения интерпретации ЭКГ в клинической практике."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19168",
            "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2410.19168",
            "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
            "score": 3,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "7a747d9a9b0717cc",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "MMAU: новый рубеж в понимании аудио для ИИ",
                    "desc": "MMAU - это новый эталонный тест для оценки моделей мультимодального понимания аудио. Он включает 10 тысяч аудиоклипов с вопросами и ответами, охватывающими речь, звуки окружающей среды и музыку. MMAU требует от моделей демонстрации 27 различных навыков и экспертных знаний в области аудио. Даже самые продвинутые модели, такие как Gemini Pro v1.5 и Qwen2-Audio, достигают точности лишь около 53%, что указывает на значительное пространство для улучшения в этой области."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19355",
            "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
            "url": "https://huggingface.co/papers/2410.19355",
            "abstract": "In this paper, we present \\textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67times speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.",
            "score": 2,
            "issue_id": 301,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "2de0b0700140bc7e",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "FasterCache: Ускорение видео-диффузии без потери качества",
                    "desc": "FasterCache - это новая стратегия для ускорения вывода видео-моделей диффузии без дополнительного обучения. Метод анализирует существующие кэш-подходы и оптимизирует повторное использование условных и безусловных признаков. FasterCache включает динамическую стратегию переиспользования признаков и CFG-кэш для сохранения качества видео. Эксперименты показали значительное ускорение генерации видео (например, в 1,67 раза для Vchitect-2.0) при сохранении качества на уровне базовой модели."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19290",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
            "url": "https://huggingface.co/papers/2410.19290",
            "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
            "score": 1,
            "issue_id": 302,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "a11e3c6587db25fc",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Prereq-Tune: Уменьшение галлюцинаций в языковых моделях путем разделения обучения навыкам и знаниям",
                    "desc": "В статье представлена новая стратегия дообучения языковых моделей под названием Prereq-Tune, направленная на уменьшение галлюцинаций. Метод разделяет обучение навыкам и знаниям, вводя дополнительный этап предварительного обучения необходимым знаниям перед основным дообучением. Prereq-Tune также может использоваться с синтетическими данными для улучшения привязки выходных данных модели к ее внутренним знаниям. Эксперименты показывают, что Prereq-Tune превосходит существующие базовые методы в повышении фактической точности языковых моделей при решении задач вопросно-ответных систем и генерации длинных текстов."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18076",
            "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
            "url": "https://huggingface.co/papers/2410.18076",
            "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
            "score": 1,
            "issue_id": 300,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "4c364a800e98d62f",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Эффективное исследование в RL с помощью непомеченных данных",
                    "desc": "Эта статья представляет метод SUPE для эффективного исследования в обучении с подкреплением, используя непомеченные предварительные данные траекторий. SUPE сначала извлекает низкоуровневые навыки с помощью вариационного автоэнкодера (VAE), затем псевдо-размечает непомеченные траектории с использованием оптимистичной модели вознаграждения. Затем метод использует эти преобразованные примеры как дополнительные офф-полиси данные для онлайн-обучения с подкреплением. SUPE превосходит предыдущие стратегии в решении задач с долгосрочным горизонтом и редкими вознаграждениями."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18558",
            "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
            "url": "https://huggingface.co/papers/2410.18558",
            "abstract": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.",
            "score": 1,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "18e760a965f56e6d",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Большие данные - ключ к улучшению открытых визуально-языковых моделей",
                    "desc": "Исследователи представили Infinity-MM - крупномасштабный мультимодальный набор данных с инструкциями, содержащий 40 миллионов образцов. Они также предложили метод генерации синтетических инструкций на основе открытых визуально-языковых моделей. Используя эти данные, они обучили 2-миллиардную модель Aquila-VL-2B, достигшую наилучших результатов среди моделей аналогичного масштаба. Это демонстрирует, что расширение обучающих данных и генерация синтетических данных могут значительно улучшить производительность открытых моделей."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19133",
            "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
            "url": "https://huggingface.co/papers/2410.19133",
            "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.",
            "score": 0,
            "issue_id": 301,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "f6e6f7e5b9e467fe",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Оптимальное сочетание человеческого и машинного интеллекта для обучения ИИ",
                    "desc": "Статья представляет новый подход к обучению языковых моделей на основе предпочтений. Авторы предлагают гибридный метод, сочетающий аннотации от людей и языковых моделей для повышения качества обучения. Ключевая идея заключается в использовании модели маршрутизации, которая определяет, какие примеры требуют человеческой оценки. Эксперименты показывают, что такой подход позволяет добиться лучших результатов по сравнению с использованием только человеческих или только машинных аннотаций."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        }
    ],
    "link_prev": "2024-10-25.html",
    "link_next": "2024-10-29.html",
    "short_date_prev": {
        "ru": "25.10",
        "en": "10/25",
        "zh": "10月25日"
    },
    "short_date_next": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种对比损失的计算策略。传统方法受限于GPU内存消耗的增加。作者提出了一种基于块的计算策略，避免了相似矩阵的完全实例化。此外，他们还引入了多级块策略，利用分布式系统的层次结构。实验结果表明，该方法可以显著扩大批量大小，并保持准确性。",
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "pinyin": "Zhè piān wénzhāng jièshào le yī zhǒng duìbǐ sǔnshī de jìsuàn cèlüè. Chuántǒng fāngfǎ shòuxiàn yú GPU nèicùn xiāohào de zēngjiā. Zuòzhě tíchū le yī zhǒng jīyú kuài de jìsuàn cèlüè, bìmiǎn le xiāngsì jǔzhèn de wánquán shílìhuà. Cǐwài, tāmen hái yǐnrù le duō jí kuài cèlüè, lìyòng fēnbùshì xìtǒng de céngcì jiégòu. Shíyàn jiéguǒ biǎomíng, gāi fāngfǎ kěyǐ xiǎnzhù kuòdà pīliàng dàxiao, bìng bǎochí zhǔnquèxìng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"对比损失\", \"pinyin\": \"duìbǐ sǔnshī\", \"trans\": \"contrastive loss\"},\n    {\"word\": \"计算策略\", \"pinyin\": \"jìsuàn cèlüè\", \"trans\": \"computational strategy\"},\n    {\"word\": \"传统方法\", \"pinyin\": \"chuántǒng fāngfǎ\", \"trans\": \"traditional method\"},\n    {\"word\": \"受限于\", \"pinyin\": \"shòu xiàn yú\", \"trans\": \"limited by\"},\n    {\"word\": \"GPU内存消耗\", \"pinyin\": \"GPU nèicún xiāohào\", \"trans\": \"GPU memory consumption\"},\n    {\"word\": \"提出\", \"pinyin\": \"tíchū\", \"trans\": \"propose\"},\n    {\"word\": \"基于块的计算策略\", \"pinyin\": \"jīyú kuài de jìsuàn cèlüè\", \"trans\": \"block-based computational strategy\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"相似矩阵\", \"pinyin\": \"xiāngsì jǔzhèn\", \"trans\": \"similarity matrix\"},\n    {\"word\": \"完全实例化\", \"pinyin\": \"wánquán shílìhuà\", \"trans\": \"fully instantiate\"},\n    {\"word\": \"多级块策略\", \"pinyin\": \"duōjí kuài cèlüè\", \"trans\": \"multi-level block strategy\"},\n    {\"word\": \"利用\", \"pinyin\": \"lìyòng\", \"trans\": \"utilize\"},\n    {\"word\": \"分布式系统\", \"pinyin\": \"fēnbùshì xìtǒng\", \"trans\": \"distributed system\"},\n    {\"word\": \"层次结构\", \"pinyin\": \"céngcì jiégòu\", \"trans\": \"hierarchical structure\"},\n    {\"word\": \"实验结果\", \"pinyin\": \"shíyàn jiéguǒ\", \"trans\": \"experimental results\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"扩大\", \"pinyin\": \"kuòdà\", \"trans\": \"expand\"},\n    {\"word\": \"批量大小\", \"pinyin\": \"pīliàng dàxiaǒ\", \"trans\": \"batch size\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔnquèxìng\", \"trans\": \"accuracy\"}\n]",
        "trans": "This article introduces a strategy for computing contrastive loss. Traditional methods are limited by the increase in GPU memory consumption. The authors propose a block-based computing strategy that avoids the complete instantiation of similarity matrices. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results demonstrate that this method can significantly increase batch size while maintaining accuracy.",
        "update_ts": "2024-10-27 10:11"
    }
}