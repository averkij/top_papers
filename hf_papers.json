{
    "date": {
        "ru": "2 апреля",
        "en": "April 2",
        "zh": "4月2日"
    },
    "time_utc": "2025-04-02 04:13",
    "weekday": 2,
    "issue_id": 3019,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.24379",
            "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.24379",
            "abstract": "To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/",
            "score": 25,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "dce65db5da1b8c34",
            "authors": [
                "Shengqiong Wu",
                "Weicai Ye",
                "Jiahao Wang",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Shuicheng Yan",
                "Hao Fei",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24379.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Универсальный контроль генерации видео через интерпретацию любых входных данных",
                    "desc": "Any2Caption - это новая система для контролируемой генерации видео с использованием различных входных данных. Ключевая идея заключается в разделении этапов интерпретации условий и синтеза видео. Система использует мультимодальные большие языковые модели для создания подробных структурированных описаний на основе разнообразных входных данных. Авторы также представили большой набор данных Any2CapIns для обучения модели генерации описаний по различным условиям."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Any2Caption",
                    "desc": "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."
                },
                "zh": {
                    "title": "可控视频生成的新突破：Any2Caption",
                    "desc": "为了克服当前视频生成领域中准确理解用户意图的瓶颈，我们提出了Any2Caption，这是一个新颖的可控视频生成框架。其核心思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频及特定提示（如区域、运动和相机姿态）转化为密集的结构化字幕，从而为视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K实例和407K条件的大规模数据集，用于任何条件到字幕的指令调优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24376",
            "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
            "url": "https://huggingface.co/papers/2503.24376",
            "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.",
            "score": 14,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "d22966d0969ded43",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Rui Wang",
                "Yixiao Ge",
                "Lu Qiu",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24376.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Обучение с подкреплением улучшает понимание видео мультимодальными ИИ-моделями",
                    "desc": "Статья представляет новый бенчмарк SEED-Bench-R1 для оценки методов пост-обучения мультимодальных больших языковых моделей (MLLM) в задачах понимания видео. Исследователи сравнивают обучение с подкреплением (RL) и обычное обучение с учителем (SFT) на модели Qwen2-VL-Instruct-7B. Результаты показывают, что RL более эффективно использует данные и лучше работает как на распределении обучающей выборки, так и вне его. Однако анализ выявляет, что RL улучшает визуальное восприятие, но иногда производит менее логически связные цепочки рассуждений."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with SEED-Bench-R1",
                    "desc": "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."
                },
                "zh": {
                    "title": "强化学习提升多模态模型推理能力",
                    "desc": "最近，链式思维（COT）生成的进展显著提升了大型语言模型（LLMs）的推理能力，而强化学习（RL）成为一种有效的后训练方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在需要感知和逻辑推理的任务中仍然未被充分探索。为了解决这个问题，我们引入了SEED-Bench-R1，这是一个旨在系统评估MLLMs在视频理解中后训练方法的基准，包含复杂的真实视频和多项选择题的日常规划任务。我们的研究表明，RL在数据效率和性能上优于监督微调（SFT），但在逻辑连贯性方面存在一定的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00906",
            "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
            "url": "https://huggingface.co/papers/2504.00906",
            "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
            "score": 8,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "e51174b579a417e9",
            "authors": [
                "Saaket Agashe",
                "Kyle Wong",
                "Vincent Tu",
                "Jiachen Yang",
                "Ang Li",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Simular Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00906.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Agent S2: Новый уровень автоматизации компьютерных задач с помощью ИИ",
                    "desc": "Статья представляет Agent S2 - новую композиционную систему для автоматизации компьютерных задач через графический интерфейс. Система использует множество специализированных и обобщенных моделей, а также новые методы точной локализации элементов интерфейса и иерархического планирования действий. Agent S2 достигает значительных улучшений производительности по сравнению с существующими решениями на нескольких бенчмарках для разных операционных систем. Авторы утверждают, что их подход открывает новые возможности для повышения продуктивности человека при работе с компьютером."
                },
                "en": {
                    "title": "Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding",
                    "desc": "This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems."
                },
                "zh": {
                    "title": "Agent S2：智能代理的新纪元",
                    "desc": "本文介绍了一种名为Agent S2的新型智能代理框架，旨在通过将认知任务分配给不同的通用模型和专业模型来提高数字任务的自动化效率。我们提出了一种新的混合定位技术，以实现精确的图形用户界面（GUI）元素定位，并引入了主动层次规划，能够根据不断变化的观察动态调整行动计划。评估结果表明，Agent S2在三个主要的计算机使用基准测试中达到了新的最先进性能，显著超越了现有的领先代理。特别是在OSWorld评估中，Agent S2相较于其他代理实现了18.9%和32.7%的相对提升，展现了其在不同操作系统和应用中的良好泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24377",
            "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
            "url": "https://huggingface.co/papers/2503.24377",
            "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.",
            "score": 8,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "d18ba97a459aea36",
            "authors": [
                "Rui Wang",
                "Hongru Wang",
                "Boyang Xue",
                "Jianhui Pang",
                "Shudong Liu",
                "Yi Chen",
                "Jiahao Qiu",
                "Derek Fai Wong",
                "Heng Ji",
                "Kam-Fai Wong"
            ],
            "affiliations": [
                "Princeton University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "University of Illinois Urbana-Champaign",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24377.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#survey",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Экономия рассуждений в больших языковых моделях: балансируя производительность и затраты",
                    "desc": "Статья рассматривает проблему баланса между производительностью и вычислительными затратами в больших языковых моделях (LLM) при выполнении задач рассуждения. Авторы анализируют причины неэффективности рассуждений, различные паттерны рассуждений и потенциальные решения для достижения экономии рассуждений. Исследование охватывает как этап после обучения, так и этап вывода во время тестирования LLM. Авторы стремятся предоставить ценные insights и выделить открытые проблемы для продвижения исследований в этой области."
                },
                "en": {
                    "title": "Balancing Performance and Cost in Language Model Reasoning",
                    "desc": "This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research."
                },
                "zh": {
                    "title": "推理经济：平衡性能与计算成本的关键",
                    "desc": "近年来，大型语言模型（LLMs）的进步显著提升了其执行复杂推理任务的能力，尤其是在快速直觉思维（系统1）与缓慢深度推理（系统2）之间的转变。虽然系统2推理提高了任务的准确性，但由于其思维缓慢和推理行为低效，往往会带来较高的计算成本。相对而言，系统1推理计算效率高，但可能导致性能不佳。因此，平衡性能与计算成本之间的权衡，形成了推理经济的概念，这是本研究的核心。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01016",
            "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.01016",
            "abstract": "Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.",
            "score": 5,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "9430b45c3324fb61",
            "authors": [
                "Tian-Xing Xu",
                "Xiangjun Gao",
                "Wenbo Hu",
                "Xiaoyu Li",
                "Song-Hai Zhang",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "HKUST",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01016.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "GeometryCrafter: Высокоточная оценка глубины видео для 3D реконструкции",
                    "desc": "GeometryCrafter - это новая система для высокоточной оценки глубины видео. Она использует вариационный автоэнкодер для кодирования карт точек и диффузионную модель для моделирования последовательностей карт точек. Система позволяет выполнять точную 3D/4D реконструкцию и оценку параметров камеры. Эксперименты показали, что GeometryCrafter превосходит существующие методы по точности 3D, временной согласованности и способности к обобщению."
                },
                "en": {
                    "title": "GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps",
                    "desc": "This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets."
                },
                "zh": {
                    "title": "GeometryCrafter：高保真视频深度估计的新框架",
                    "desc": "尽管视频深度估计取得了显著进展，但现有方法在几何保真度方面存在固有局限，限制了其在重建和其他度量基础下游任务中的应用。我们提出了GeometryCrafter，这是一种新颖的框架，可以从开放世界视频中恢复具有时间一致性的高保真点图序列，从而实现准确的3D/4D重建和相机参数估计。我们的方法核心是一个点图变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以有效地进行点图编码和解码。通过利用VAE，我们训练了一个视频扩散模型，以建模基于输入视频的点图序列的分布。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00595",
            "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
            "url": "https://huggingface.co/papers/2504.00595",
            "abstract": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.",
            "score": 4,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "3e8c667bd93d754e",
            "authors": [
                "Weizhi Wang",
                "Yu Tian",
                "Linjie Yang",
                "Heng Wang",
                "Xifeng Yan"
            ],
            "affiliations": [
                "Nvidia Research",
                "Seed Vision Team, ByteDance",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00595.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытая и эффективная мультимодальная ИИ-модель",
                    "desc": "Статья представляет Open-Qwen2VL - полностью открытую мультимодальную языковую модель с 2 миллиардами параметров. Модель была эффективно обучена на 29 миллионах пар изображение-текст, используя всего 442 часа GPU A100-40G. Авторы применили динамическое разрешение изображений и мультимодальную упаковку последовательностей для повышения эффективности предобучения. Open-Qwen2VL превосходит частично открытую модель Qwen2-VL-2B по различным мультимодальным бенчмаркам, демонстрируя высокую эффективность обучения."
                },
                "en": {
                    "title": "Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL",
                    "desc": "The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning."
                },
                "zh": {
                    "title": "高效开源的多模态大语言模型",
                    "desc": "本文介绍了Open-Qwen2VL，这是一个完全开源的多模态大语言模型，具有20亿参数，使用2900万对图像-文本数据进行高效预训练。我们采用了动态图像分辨率和多模态序列打包技术，显著提高了预训练的效率。通过使用MLLM和CLIP的过滤技术，提升了数据质量和训练效率。最终，Open-Qwen2VL在多个多模态基准测试中超越了部分开源的最先进模型，展示了其卓越的训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01005",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.01005",
            "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "score": 3,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "ee8e4951bf6c7a18",
            "authors": [
                "Nishad Singhi",
                "Hritik Bansal",
                "Arian Hosseini",
                "Aditya Grover",
                "Kai-Wei Chang",
                "Marcus Rohrbach",
                "Anna Rohrbach"
            ],
            "affiliations": [
                "Google DeepMind",
                "Mila",
                "TU Darmstadt & hessian.AI",
                "University of California Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01005.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс между генерацией решений и их верификацией в языковых моделях",
                    "desc": "Статья исследует стратегии улучшения способностей больших языковых моделей (LLM) к рассуждениям, особенно в задачах математического характера. Авторы сравнивают метод Self-Consistency (SC), генерирующий множество решений и выбирающий наиболее частое, с подходом Generative Reward Models (GenRM), который оценивает решения путем генерации цепочек рассуждений. Исследование показывает, что SC более эффективен по вычислительным ресурсам для большинства практических задач. Авторы также выводят законы масштабирования для парадигмы GenRM, предоставляя практические рекомендации по оптимизации вычислений во время тестирования."
                },
                "en": {
                    "title": "Balancing Solution Generation and Verification for Efficient Reasoning in LLMs",
                    "desc": "This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance."
                },
                "zh": {
                    "title": "优化推理能力：解生成与验证的平衡",
                    "desc": "本文探讨了在大语言模型（LLMs）中，如何通过扩展测试时计算来提升推理能力，尤其是在数学问题解决任务中。传统的自一致性（Self-Consistency, SC）方法通过生成多个解并采用多数投票选择最常见的答案。最近的生成奖励模型（Generative Reward Models, GenRM）则将验证重构为下一个标记预测任务，从而在推理时引入新的扩展方式。研究表明，在固定的推理预算下，SC在大多数实际情况下比GenRM更具计算效率，提供了在测试时扩展中优化解生成与验证的实用指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00557",
            "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
            "url": "https://huggingface.co/papers/2504.00557",
            "abstract": "Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.",
            "score": 3,
            "issue_id": 3018,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "c5628fbf1a189a06",
            "authors": [
                "Jewon Lee",
                "Ki-Ung Song",
                "Seungmin Yang",
                "Donguk Lim",
                "Jaeyeon Kim",
                "Wooksu Shin",
                "Bo-Kyeong Kim",
                "Yong Jae Lee",
                "Tae-Ho Kim"
            ],
            "affiliations": [
                "Nota Inc.",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00557.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Эффективное сжатие визуальных данных в мультимодальных ИИ-моделях",
                    "desc": "Статья представляет метод снижения вычислительных затрат в крупных визуально-языковых моделях путем сокращения визуальных токенов. Авторы фокусируются на моделях с кросс-вниманием, выявляя проблему большого размера кэша ключ-значение для визуальных токенов. Они предлагают алгоритм избирательного удаления избыточных визуальных признаков, основанный на разреженности карт кросс-внимания. Метод позволяет снизить задержку и использование памяти при сохранении производительности модели на уровне базовых показателей."
                },
                "en": {
                    "title": "Trimmed Llama: Efficient Visual Token Reduction for Faster Inference",
                    "desc": "This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks."
                },
                "zh": {
                    "title": "视觉特征修剪，提升推理效率",
                    "desc": "本论文提出了一种视觉标记减少的方法，以降低大型视觉语言模型（LVLMs）在推理时的计算成本。与以往只针对自注意力模型的研究不同，我们的工作专注于基于交叉注意力的模型，这些模型在性能上更为优越。我们发现交叉注意力层中图像标记的键值（KV）缓存大小远大于自注意力层中的文本标记，成为计算瓶颈。通过利用交叉注意力图的稀疏特性，我们选择性地修剪冗余的视觉特征，从而有效减少KV缓存需求，降低推理延迟和内存使用，同时保持基准性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00294",
            "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
            "url": "https://huggingface.co/papers/2504.00294",
            "abstract": "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.",
            "score": 3,
            "issue_id": 3018,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "b455a4adb4eae588",
            "authors": [
                "Vidhisha Balachandran",
                "Jingya Chen",
                "Lingjiao Chen",
                "Shivam Garg",
                "Neel Joshi",
                "Yash Lara",
                "John Langford",
                "Besmira Nushi",
                "Vibhav Vineet",
                "Yue Wu",
                "Safoora Yousefi"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00294.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабирование LLM: потенциал и ограничения в сложных задачах",
                    "desc": "Статья исследует влияние масштабирования во время вывода на способности крупных языковых моделей (LLM) решать сложные задачи. Авторы сравнивают обычные модели и модели, настроенные на масштабирование, на восьми различных типах задач. Результаты показывают, что преимущества масштабирования варьируются в зависимости от задачи и уменьшаются с ростом сложности проблемы. Исследование также выявляет потенциал для улучшения производительности моделей при использовании совершенных верификаторов или сильной обратной связи."
                },
                "en": {
                    "title": "Scaling Inference for Smarter Problem Solving in LLMs",
                    "desc": "This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements."
                },
                "zh": {
                    "title": "推理时间扩展：提升模型推理能力的关键",
                    "desc": "本研究探讨了推理时间扩展对大型语言模型（LLMs）在复杂问题上的推理能力的影响。我们分析了九种最先进模型在八个具有挑战性的任务上的表现，包括数学推理和空间推理等。结果表明，推理时间扩展的优势因任务而异，且在问题复杂性增加时会减弱。尽管使用更多的标记并不总能提高准确性，但在有强反馈的情况下，所有模型都显示出显著的性能提升潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23361",
            "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
            "url": "https://huggingface.co/papers/2503.23361",
            "abstract": "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.",
            "score": 3,
            "issue_id": 3017,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 марта",
                "en": "March 30",
                "zh": "3月30日"
            },
            "hash": "9b957c49c958aea3",
            "authors": [
                "Linxin Song",
                "Xuwei Ding",
                "Jieyu Zhang",
                "Taiwei Shi",
                "Ryotaro Shimizu",
                "Rahul Gupta",
                "Yang Liu",
                "Jian Kang",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "AGI",
                "Amazon",
                "University of Rochester",
                "University of Southern California",
                "University of Washington",
                "University of Wisconsin-Madison",
                "ZOZO Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23361.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#optimization",
                    "#graphs",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "SEA: Эффективный поиск пробелов в знаниях языковых моделей",
                    "desc": "Эта статья представляет новый метод под названием SEA (стохастический подъем ошибок) для эффективного обнаружения пробелов в знаниях крупных языковых моделей (LLM). SEA использует итеративный процесс оптимизации, чтобы находить новые кандидаты на ошибки, основываясь на семантическом сходстве с ранее обнаруженными ошибками. Метод применяет иерархический поиск и построение графа отношений для выявления систематических ошибок. Эмпирические результаты показывают, что SEA значительно превосходит существующие методы по эффективности обнаружения ошибок в LLM."
                },
                "en": {
                    "title": "Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA",
                    "desc": "This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery."
                },
                "zh": {
                    "title": "发现LLM知识缺陷的新方法",
                    "desc": "大型语言模型（LLMs）在语言能力上表现出色，但常常无法准确保留事实知识，导致幻觉和不可靠的输出。我们提出了一种名为随机错误上升（SEA）的框架，用于在严格的查询预算下发现闭合权重LLMs中的知识缺陷。SEA通过利用与先前观察到的失败的语义相似性，迭代检索新的高错误候选项，从而将错误发现过程形式化为随机优化过程。实验证明，SEA发现的知识错误数量显著高于现有方法，同时大幅降低了每个错误的成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00810",
            "title": "Z1: Efficient Test-time Scaling with Code",
            "url": "https://huggingface.co/papers/2504.00810",
            "abstract": "Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.",
            "score": 2,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "d982593a14ba7da9",
            "authors": [
                "Zhaojian Yu",
                "Yinghao Wu",
                "Yilun Zhao",
                "Arman Cohan",
                "Xiao-Ping Zhang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00810.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#dataset",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное масштабирование рассуждений в языковых моделях",
                    "desc": "Статья представляет эффективный метод масштабирования языковых моделей во время тестирования, основанный на обучении траекториям рассуждений, связанным с кодом. Авторы создали датасет Z1-Code-Reasoning-107K, содержащий задачи по программированию с короткими и длинными решениями. Они также предложили технику Shifted Thinking Window для уменьшения избыточных токенов мышления. Модель Z1-7B, обученная на этих данных, демонстрирует способность адаптировать уровень рассуждений к сложности задач и обобщать на более широкие задачи рассуждения."
                },
                "en": {
                    "title": "Efficient Reasoning in Large Language Models",
                    "desc": "This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models."
                },
                "zh": {
                    "title": "高效推理，简化思考！",
                    "desc": "本文提出了一种高效的测试时间扩展方法，旨在通过训练大型语言模型（LLMs）在代码相关的推理轨迹上，减少多余的思考标记，同时保持性能。我们创建了一个名为Z1-Code-Reasoning-107K的数据集，包含简单和复杂的编码问题及其解决轨迹。我们还提出了一种新颖的移位思维窗口，通过去除上下文分隔标签和限制推理标记，来减轻过度思考的负担。经过训练的模型Z1-7B能够根据问题的复杂性调整推理水平，并在不同的推理任务中实现高效的测试时间扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23733",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
            "url": "https://huggingface.co/papers/2503.23733",
            "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.",
            "score": 2,
            "issue_id": 3017,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "ed45063868071c13",
            "authors": [
                "Yiyang Du",
                "Xiaochen Wang",
                "Chi Chen",
                "Jiabo Ye",
                "Yiru Wang",
                "Peng Li",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Zhifang Sui",
                "Maosong Sun",
                "Yang Liu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                "Institute of Intelligent Computing, Alibaba Group",
                "Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China",
                "ModelTC Open Source Organization, Beijing, China",
                "School of Software Microelectronics, Peking University, Beijing, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23733.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "AdaMMS: Эффективное слияние разнородных мультимодальных языковых моделей",
                    "desc": "Статья представляет AdaMMS - новый метод объединения мультимодальных языковых моделей (MLLM) с разнородной архитектурой. Метод включает три этапа: отображение параметров между моделями, их слияние с помощью линейной интерполяции и поиск оптимальных гиперпараметров. AdaMMS решает проблемы, связанные с различиями в архитектуре и асимметрией в пространстве параметров разнородных MLLM. Эксперименты показали превосходство AdaMMS над существующими методами объединения моделей на различных мультимодальных задачах."
                },
                "en": {
                    "title": "Merging Diverse Models with AdaMMS",
                    "desc": "This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods."
                },
                "zh": {
                    "title": "异质模型合并的新突破",
                    "desc": "最近，模型合并方法在结合多个大型语言模型（LLMs）在不同任务上的能力方面表现出强大的优势。以往的模型合并方法主要集中在合并具有相同架构的同质模型，但在处理具有固有异质性的多模态大型语言模型（MLLMs）时面临挑战。我们提出了AdaMMS，这是一种专为异质MLLMs设计的新型模型合并方法，采用映射、合并和搜索三个步骤来解决这些挑战。通过设计模型之间的映射函数、对模型权重进行线性插值以及提出无监督的超参数选择方法，AdaMMS在各种视觉-语言基准测试中超越了以往的模型合并方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21860",
            "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
            "url": "https://huggingface.co/papers/2503.21860",
            "abstract": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.",
            "score": 2,
            "issue_id": 3017,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 марта",
                "en": "March 27",
                "zh": "3月27日"
            },
            "hash": "0d9ea55946287027",
            "authors": [
                "Kailin Li",
                "Puhao Li",
                "Tengyu Liu",
                "Yuyang Li",
                "Siyuan Huang"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21860.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#robotics",
                    "#transfer_learning",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ManipTrans: эффективная передача человеческих навыков манипуляции роботам",
                    "desc": "ManipTrans - это новый метод передачи навыков бимануальной манипуляции от человека к роботизированным рукам в симуляции. Он включает предварительное обучение имитатора траектории движения рук и дообучение специфического остаточного модуля с учетом ограничений взаимодействия. Метод превосходит существующие подходы по точности и эффективности выполнения сложных задач манипуляции. На его основе создан крупномасштабный датасет DexManipNet с 3.3 тыс. эпизодов роботизированной манипуляции для обучения политик управления ловкими руками."
                },
                "en": {
                    "title": "Efficiently Teaching Robots to Manipulate Like Humans",
                    "desc": "This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands."
                },
                "zh": {
                    "title": "高效转移人类双手技能的机器人手",
                    "desc": "本论文介绍了一种名为ManipTrans的新方法，用于将人类双手的技能高效地转移到灵巧的机器人手上。该方法分为两个阶段：首先训练一个通用的轨迹模仿器来模拟手部动作，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，ManipTrans在成功率、保真度和效率上超越了现有的最先进方法。此外，利用ManipTrans，我们创建了一个名为DexManipNet的大规模数据集，包含了3.3K个机器人操作的实例，支持进一步的策略训练和实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00869",
            "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
            "url": "https://huggingface.co/papers/2504.00869",
            "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.",
            "score": 1,
            "issue_id": 3019,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "bd9586b08ce02a05",
            "authors": [
                "Xiaoke Huang",
                "Juncheng Wu",
                "Hui Liu",
                "Xianfeng Tang",
                "Yuyin Zhou"
            ],
            "affiliations": [
                "Amazon Research",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00869.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#healthcare",
                    "#science",
                    "#training",
                    "#inference"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Медицинские знания важнее глубины рассуждений",
                    "desc": "В статье исследуется применение техники масштабирования во время тестирования для улучшения медицинских рассуждений в больших языковых моделях. Авторы представляют метод m1, который повышает способность модели к медицинским рассуждениям на этапе вывода. Исследование показывает, что масштабирование во время тестирования улучшает результаты на медицинских задачах, но выявляет, что недостаток медицинских знаний ограничивает дальнейший прогресс. Для достижения лучших результатов необходимо увеличивать объем данных, улучшать их качество и расширять возможности модели."
                },
                "en": {
                    "title": "Enhancing Medical Reasoning with Test-Time Scaling",
                    "desc": "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."
                },
                "zh": {
                    "title": "医学推理的新突破：测试时缩放的力量",
                    "desc": "本文探讨了测试时缩放技术在医学推理中的应用，提出了一种名为m1的方法，能够有效提升模型在推理时的医学能力。研究表明，测试时缩放在多种医学任务中均能显著提高推理效果，尤其是对于参数少于10B的轻量级微调模型，能够达到新的最佳性能。我们发现，最佳的推理令牌预算约为4K，超出此范围可能导致性能下降。此外，增加数据规模、提高数据质量和扩展模型容量是提升医学知识基础的关键，尤其是在小模型性能饱和的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00509",
            "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
            "url": "https://huggingface.co/papers/2504.00509",
            "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.",
            "score": 0,
            "issue_id": 3018,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "c9697e67f23cfa4e",
            "authors": [
                "Kai Yan",
                "Yufei Xu",
                "Zhengyin Du",
                "Xuesong Yao",
                "Zheyu Wang",
                "Xiaowen Guo",
                "Jiecao Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00509.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Языковые модели: умные рассуждения или простое воспроизведение?",
                    "desc": "Авторы статьи предлагают новый мультимодальный бенчмарк RoR-Bench для выявления склонности языковых моделей (LLM) к простому воспроизведению заученных решений. Исследование показало, что даже передовые LLM демонстрируют серьезные проблемы с рассуждениями при небольших изменениях в условиях задач. Результаты указывают на значительное снижение производительности ведущих моделей (до 60%) на элементарных арифметических и логических задачах при изменении всего одной фразы. Авторы призывают сообщество переосмыслить реальный уровень интеллекта современных LLM."
                },
                "en": {
                    "title": "Reassessing LLM Intelligence: Are They Truly Reasoning?",
                    "desc": "This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance drops—up to 60%—when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence."
                },
                "zh": {
                    "title": "重新审视LLM的智能水平",
                    "desc": "近年来，LLM基准测试的难度从小学水平迅速上升到前沿问题，这让研究人员感到我们离超越人类智能只有一步之遥。然而，LLM的推理能力是否真的是人类标准下的真正智能，还是仅仅在训练中见过的解决方案的复述？为了解决这个问题，我们提出了RoR-Bench，一个新颖的多模态基准，用于检测LLM在简单推理问题中是否存在复述行为。我们的实证分析发现，现有的顶尖LLM在条件稍微改变时，表现出极其严重的复述行为，这促使我们重新评估这些模型的真实智能水平。"
                }
            }
        }
    ],
    "link_prev": "2025-04-01.html",
    "link_next": "2025-04-03.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "01.04",
        "en": "04/01",
        "zh": "4月1日"
    },
    "short_date_next": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的视频生成任务，称为 Talking Characters。它能通过语音和文本直接生成说话的动画角色。与之前的 talking head 不同，Talking Characters 生成的是一个或多个角色的全身像，而不仅仅是面部。研究提出了 MoCha 模型，并使用语音-视频窗口注意力机制来精确同步语音和视频。为了解决大规模语音标注视频数据集稀缺的问题，研究还提出了一种联合训练策略，提升了模型的泛化能力。",
        "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
        "pinyin": "这篇文章介绍了一种新的视频生成任务，称为 Talking Characters。它能通过语音和文本直接生成说话的动画角色。与之前的 talking head 不同，Talking Characters 生成的是一个或多个角色的全身像，而不仅仅是面部。研究提出了 MoCha 模型，并使用语音-视频窗口注意力机制来精确同步语音和视频。为了解决大规模语音标注视频数据集稀缺的问题，研究还提出了一种联合训练策略，提升了模型的泛化能力。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de shì pǐn shēng chéng rèn wù, chēng wéi Talking Characters. tā néng tōng guò yǔ yīn hé wén běn zhí jiē shēng chéng shuō huà de dòng huà jué sè. yǔ zhī qián de talking head bù tóng, Talking Characters shēng chéng de shì yī gè huò duō gè jué sè de quán shēn xiàng, ér bù jǐn jǐn shì miàn bù. yán jiū tí chū le MoCha mó xíng, bìng shǐ yòng yǔ yīn-shì pǐn chuāng kǒu zhù yì lì jī zhī lái jīng xiào tóng bù yǔ yīn hé shì pǐn. wèi le jiě jué dà guī mó yǔ yīn biāo zhù shì pǐn shù jù jí xī quē de wèn tí, yán jiū hái tí chū le yī zhǒng lián hé xùn liàn cè lüè, tí shēng le mó xíng de fàn huà néng lì.",
        "vocab": "[\n    {\"word\": \"视频生成\", \"pinyin\": \"shìpín shēngchéng\", \"trans\": \"video generation\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēngwéi\", \"trans\": \"called\"},\n    {\"word\": \"Talking Characters\", \"pinyin\": \"Talking Characters\", \"trans\": \"Talking Characters\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔyīn\", \"trans\": \"audio\"},\n    {\"word\": \"文本\", \"pinyin\": \"wénběn\", \"trans\": \"text\"},\n    {\"word\": \"动画角色\", \"pinyin\": \"dònghuà juésè\", \"trans\": \"animated character\"},\n    {\"word\": \"talking head\", \"pinyin\": \"talking head\", \"trans\": \"talking head\"},\n    {\"word\": \"全身像\", \"pinyin\": \"quánshēn xiàng\", \"trans\": \"full-body image\"},\n    {\"word\": \"面部\", \"pinyin\": \"miànbù\", \"trans\": \"face\"},\n    {\"word\": \"MoCha\", \"pinyin\": \"MoCha\", \"trans\": \"MoCha\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"语音-视频窗口注意力机制\", \"pinyin\": \"yǔyīn shìpín chuāngkǒu zhùyìlì jīzhì\", \"trans\": \"audio-video window attention mechanism\"},\n    {\"word\": \"精确同步\", \"pinyin\": \"jīngquè tóngbù\", \"trans\": \"precise synchronization\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dàguīmó\", \"trans\": \"large-scale\"},\n    {\"word\": \"语音标注\", \"pinyin\": \"yǔyīn biāozhù\", \"trans\": \"audio annotation\"},\n    {\"word\": \"视频数据集\", \"pinyin\": \"shìpín shùjùjí\", \"trans\": \"video dataset\"},\n    {\"word\": \"稀缺\", \"pinyin\": \"xīquē\", \"trans\": \"scarce\"},\n    {\"word\": \"联合训练策略\", \"pinyin\": \"liánhé xùnliàn cèlüè\", \"trans\": \"joint training strategy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improve\"},\n    {\"word\": \"泛化能力\", \"pinyin\": \"fànhuà nénglì\", \"trans\": \"generalization capability\"}\n]",
        "trans": "This article introduces a new video generation task called Talking Characters, which can directly generate speaking animated characters from speech and text. Unlike previous talking head approaches, Talking Characters generates full-body images of one or more characters, not just the face. The research proposes the MoCha model and employs a speech-video window attention mechanism to precisely synchronize speech and video. To address the scarcity of large-scale speech-annotated video datasets, the research also proposes a joint training strategy that enhances the model's generalization capability.",
        "update_ts": "2025-04-01 09:12"
    }
}