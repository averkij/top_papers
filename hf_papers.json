{
    "date": {
        "ru": "16 –æ–∫—Ç—è–±—Ä—è",
        "en": "October 16",
        "zh": "10Êúà16Êó•"
    },
    "time_utc": "2025-10-16 02:21",
    "weekday": 3,
    "issue_id": 6444,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.13621",
            "title": "The Role of Computing Resources in Publishing Foundation Model Research",
            "url": "https://huggingface.co/papers/2510.13621",
            "abstract": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/",
            "score": 5,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "f9f73f3064dbf2e4",
            "authors": [
                "Yuexing Hao",
                "Yue Huang",
                "Haoran Zhang",
                "Chenyang Zhao",
                "Zhenwen Liang",
                "Paul Pu Liang",
                "Yue Zhao",
                "Lichao Sun",
                "Saleh Kalantari",
                "Xiangliang Zhang",
                "Marzyeh Ghassemi"
            ],
            "affiliations": [
                "CSE, University of Notre Dame, South Bend, 46556, USA",
                "Computer Science Department, Lehigh University, Bethlehem, 18015, USA",
                "Computer Science Department, University of California, Los Angeles, 90095, USA",
                "Cornell University, Ithaca, 14850, USA",
                "EECS, MIT, Cambridge, 02135, USA",
                "School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13621.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#ethics"
                ],
                "emoji": "üí∞",
                "ru": {
                    "title": "–î–µ–Ω—å–≥–∏ —Ä–µ—à–∞—é—Ç: –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è foundation models",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–∞—É—á–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ foundation models (–±–æ–ª—å—à–∏—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π). –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∏–ª–∏ 6517 —Å—Ç–∞—Ç–µ–π –∏ –æ–ø—Ä–æ—Å–∏–ª–∏ 229 –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –æ–±–Ω–∞—Ä—É–∂–∏–≤ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –¥–æ—Å—Ç—É–ø–æ–º –∫ GPU –∏ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –∞ —Ç–∞–∫–∂–µ —Ü–∏—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é —Ä–∞–±–æ—Ç. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Å–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ —Ç–∏–ø–æ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è –∏–ª–∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–∞—è), –¥–æ–º–µ–Ω–æ–º –∏–ª–∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–µ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ —Ü–µ–Ω–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –±–∞—Ä—å–µ—Ä –≤—Ö–æ–¥–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏."
                },
                "en": {
                    "title": "Empowering AI Research Through Shared Computing Resources",
                    "desc": "This paper investigates how computing resources, such as GPUs and funding, influence the progress of foundation model research in AI. By analyzing 6517 papers and surveying 229 authors, the study finds a strong correlation between increased computing resources and national funding and citations. However, it reveals that these resources do not significantly impact the research environment, domain, or methodology. The authors recommend creating shared computing resources to support under-resourced researchers, promoting diversity and innovation in the field."
                },
                "zh": {
                    "title": "ËÆ°ÁÆóËµÑÊ∫ê‰∏éÂü∫Á°ÄÊ®°ÂûãÁ†îÁ©∂ÁöÑÂÖ≥Á≥ª",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËÆ°ÁÆóËµÑÊ∫ê‰∏éÂü∫Á°ÄÊ®°ÂûãÁ†îÁ©∂ÁöÑÁßëÂ≠¶ËøõÂ±ï‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫Ü6517ÁØá2022Ëá≥2024Âπ¥Èó¥ÂèëË°®ÁöÑÂü∫Á°ÄÊ®°ÂûãËÆ∫ÊñáÔºåÂπ∂Ë∞ÉÊü•‰∫Ü229‰ΩçÁ¨¨‰∏Ä‰ΩúËÄÖÂØπËÆ°ÁÆóËµÑÊ∫êÂΩ±ÂìçÁöÑÁúãÊ≥ï„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËÆ°ÁÆóËµÑÊ∫êÁöÑÂ¢ûÂä†‰∏éÂõΩÂÆ∂ËµÑÈáëÂàÜÈÖçÂíåÂºïÁî®Ê¨°Êï∞Áõ∏ÂÖ≥Ôºå‰ΩÜ‰∏éÁ†îÁ©∂ÁéØÂ¢É„ÄÅÈ¢ÜÂüüÊàñÁ†îÁ©∂ÊñπÊ≥ïÊ≤°ÊúâÊòæËëóÁõ∏ÂÖ≥ÊÄß„ÄÇÊàë‰ª¨Âª∫ËÆÆ‰∏™‰∫∫ÂíåÊú∫ÊûÑÂ∫î‰∏ìÊ≥®‰∫éÂàõÂª∫ÂÖ±‰∫´ÂíåÂèØË¥üÊãÖÁöÑËÆ°ÁÆóÊú∫‰ºöÔºå‰ª•Èôç‰ΩéËµÑÊ∫ê‰∏çË∂≥Á†îÁ©∂ËÄÖÁöÑËøõÂÖ•Èó®Êßõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13802",
            "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
            "url": "https://huggingface.co/papers/2510.13802",
            "abstract": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
            "score": 4,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "2a519ed83bf3da2a",
            "authors": [
                "Xinhang Liu",
                "Yuxi Xiao",
                "Donny Y. Chen",
                "Jiashi Feng",
                "Yu-Wing Tai",
                "Chi-Keung Tang",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Dartmouth College",
                "HKUST",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13802.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–û—Ç—Å–ª–µ–¥–∏—Ç—å –≤—Å—ë: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Trace Anything ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤ –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–∏–¥–µ–æ –∫–∞–∫ –ø–æ–ª–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –≥–¥–µ –∫–∞–∂–¥–æ–º—É –ø–∏–∫—Å–µ–ª—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è 3D-—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è, –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è B-—Å–ø–ª–∞–π–Ω–∞–º–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º–∏ —Ç–æ—á–∫–∞–º–∏. –û–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö 4D –¥–∞–Ω–Ω—ã—Ö, —Å–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Ü–µ–Ω–∫–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç emergent abilities, –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –±—É–¥—É—â–µ–º –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Predicting Video Trajectories with Efficiency and Precision",
                    "desc": "The paper introduces Trace Anything, a neural network designed to predict video trajectories efficiently in a single pass. It utilizes a novel representation called Trajectory Field, which maps each pixel in a video to a continuous 3D trajectory function over time. This approach allows the model to generate control points for B-splines, enabling accurate trajectory predictions at any moment. The results show that Trace Anything not only achieves state-of-the-art performance but also demonstrates significant efficiency and advanced capabilities like motion forecasting and goal-conditioned manipulation."
                },
                "zh": {
                    "title": "ÂçïÊ¨°È¢ÑÊµãÔºåËΩ®ËøπËøΩË∏™ÁöÑÊú™Êù•",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Trace AnythingÁöÑÁ•ûÁªèÁΩëÁªúÔºåÁî®‰∫éÂú®ÂçïÊ¨°ÂâçÂêë‰º†Êí≠‰∏≠È¢ÑÊµãËßÜÈ¢ëÁöÑËΩ®ËøπÂú∫„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜËßÜÈ¢ëË°®Á§∫‰∏∫ÊØè‰∏™ÂÉèÁ¥†ÁöÑËøûÁª≠‰∏âÁª¥ËΩ®ËøπÂáΩÊï∞ÔºåÊù•ÊúâÊïàÂª∫Ê®°ÂíåÈ¢ÑÊµãËßÜÈ¢ë‰∏≠ÁöÑÂä®ÊÄÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTrace AnythingÂú®ËΩ®ËøπÂú∫‰º∞ËÆ°ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®ÁÇπË∑üË∏™Âü∫ÂáÜ‰∏ä‰πüÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂûãÂú®ÊïàÁéá‰∏äÊúâÊòæËëóÊèêÂçáÔºåËÉΩÂ§üÂÆûÁé∞ÁõÆÊ†áÊù°‰ª∂ÁöÑÊìç‰Ωú„ÄÅËøêÂä®È¢ÑÊµãÂíåÊó∂Á©∫ËûçÂêàÁ≠âÊñ∞ÂÖ¥ËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13515",
            "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
            "url": "https://huggingface.co/papers/2510.13515",
            "abstract": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
            "score": 4,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "4e7810d5695a73ea",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Kaichen Zhang",
                "Xiang An",
                "Ziyong Feng",
                "Yueyi Zhang",
                "Weidong Cai",
                "Jiankang Deng",
                "Lidong Bing"
            ],
            "affiliations": [
                "Imperial College London",
                "LMMs-Lab Team",
                "M.R.L. Team",
                "MiroMind AI",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "MLLM –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è —É–º–Ω–æ–≥–æ –º–∞–π–Ω–∏–Ω–≥–∞ hard negatives –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniME-V2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLMs) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ MLLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–∞—Ä –∑–∞–ø—Ä–æ—Å-–∫–∞–Ω–¥–∏–¥–∞—Ç –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—è–≥–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è. –≠—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –ø–æ–º–æ–≥–∞—é—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ hard negatives, –∞ —Ç–∞–∫–∂–µ —Å–ª—É–∂–∞—Ç –º—è–≥–∫–∏–º–∏ –º–µ—Ç–∫–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MMEB –∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞, —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å-—Ä–∞–Ω–∫–µ—Ä UniME-V2-Reranker –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with Smart Negative Mining",
                    "desc": "The paper introduces a new model called Universal Multimodal Embedding version 2 (UniME-V2) that improves how machines understand and represent different types of data. It uses advanced machine learning language models (MLLMs) to find and evaluate hard negative examples, which are crucial for training. By generating soft semantic matching scores, the model can better distinguish between similar candidates and improve its ability to identify relevant information. The results show that UniME-V2 outperforms existing methods in various tasks, making it a significant advancement in multimodal representation learning."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅË°®Á§∫Â≠¶‰π†ÁöÑÂàõÊñ∞Ê®°Âûã",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈÄöÁî®Â§öÊ®°ÊÄÅÂµåÂÖ•Ê®°ÂûãÔºàUniME-V2ÔºâÔºåÊó®Âú®ÈÄöËøáËØÜÂà´Â§öÊ†∑ÂåñÁöÑÈ´òË¥®ÈáèÂõ∞ÈöæË¥üÊ†∑Êú¨Êù•Â¢ûÂº∫Ë°®Á§∫Â≠¶‰π†„ÄÇËØ•Ê®°ÂûãÂà©Áî®Â§öËØ≠Ë®ÄÂ§ßÊ®°ÂûãÔºàMLLMsÔºâÊù•ËØÑ‰º∞Êü•ËØ¢-ÂÄôÈÄâÂØπÁöÑËØ≠‰πâÂØπÈΩêÔºåÂπ∂ÁîüÊàêËΩØËØ≠‰πâÂåπÈÖçÂàÜÊï∞Ôºå‰ªéËÄåÊîπÂñÑÂå∫ÂàÜËÉΩÂäõ„ÄÇÈÄöËøáÊûÑÂª∫ÊΩúÂú®ÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ÈõÜÔºåUniME-V2ËÉΩÂ§üÊúâÊïàÂáèËΩªÂÅáË¥üÊ†∑Êú¨ÁöÑÂΩ±ÂìçÔºåÂπ∂ËØÜÂà´Âá∫Â§öÊ†∑ÂåñÁöÑÈ´òË¥®ÈáèÂõ∞ÈöæË¥üÊ†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Ê£ÄÁ¥¢‰ªªÂä°‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13809",
            "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.13809",
            "abstract": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "74cd3b0ff8137389",
            "authors": [
                "Sihui Ji",
                "Xi Chen",
                "Xin Tao",
                "Pengfei Wan",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13809.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#rl",
                    "#video",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–∞–º —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è",
                    "desc": "PhysMaster —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, –¥–æ–±–∞–≤–ª—è—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π PhysEncoder. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –∏ Direct Preference Optimization, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ –≤–∏–¥–µ–æ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. PhysEncoder –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ–∑–∏—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è) –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ—ë –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ plug-in —Ä–µ—à–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º, –ø—Ä–∏–±–ª–∏–∂–∞—è video generation –º–æ–¥–µ–ª–∏ –∫ —Ä–æ–ª–∏ –Ω–∞—Å—Ç–æ—è—â–∏—Ö world models."
                },
                "en": {
                    "title": "Enhancing Video Realism with Physics-Aware Generation",
                    "desc": "PhysMaster is a novel approach to video generation that incorporates physical knowledge to enhance the realism of generated videos. It utilizes a component called PhysEncoder, which extracts physical information from input images to guide the video generation process. By employing reinforcement learning and Direct Preference Optimization, PhysMaster optimizes the model's understanding of physical dynamics, ensuring that the generated videos are not only visually appealing but also adhere to the laws of physics. This method demonstrates the potential for creating more accurate world models that can be applied to various physical scenarios."
                },
                "zh": {
                    "title": "PhysMasterÔºöÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÁâ©ÁêÜÊÑèËØÜ",
                    "desc": "PhysMaster ÊòØ‰∏ÄÁßçÈÄöËøáÊï¥ÂêàÁâ©ÁêÜÁü•ËØÜÊù•Â¢ûÂº∫ËßÜÈ¢ëÁîüÊàêÁöÑÊ®°Âûã„ÄÇÂÆÉ‰ΩøÁî® PhysEncoder ÁºñÁ†ÅÁâ©ÁêÜ‰ø°ÊÅØÔºå‰ª•ÊèêÈ´òËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÁâ©ÁêÜÊÑèËØÜ„ÄÇËØ•Ê®°ÂûãÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÂíåÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÊù•‰ºòÂåñÁâ©ÁêÜË°®Á§∫ÔºåÁ°Æ‰øùÁîüÊàêÁöÑËßÜÈ¢ëÁ¨¶ÂêàÁâ©ÁêÜËßÑÂæã„ÄÇPhysMaster Êèê‰æõ‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈÄÇÁî®‰∫éÂêÑÁßçÁâ©ÁêÜËøáÁ®ãÁöÑË°®Á§∫Â≠¶‰π†ÔºåËÉΩÂ§üÂπøÊ≥õÂ∫îÁî®‰∫éÁâ©ÁêÜÊÑèËØÜËßÜÈ¢ëÁîüÊàê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13804",
            "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
            "url": "https://huggingface.co/papers/2510.13804",
            "abstract": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "85e2081e6a4e6525",
            "authors": [
                "Xinchen Zhang",
                "Xiaoying Zhang",
                "Youbin Wu",
                "Yanbin Cao",
                "Renrui Zhang",
                "Ruihang Chu",
                "Ling Yang",
                "Yujiu Yang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13804.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Generative Universal Verifier ‚Äî –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ vision-language –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ ViVerBench –∏–∑ 16 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–¥–∞—á –∏ –æ–±—É—á–∏–ª–∏ OmniVerifier-7B ‚Äî –ø–µ—Ä–≤—É—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø–æ–∫–∞–∑–∞–≤—à—É—é –ø—Ä–∏—Ä–æ—Å—Ç +8.3 –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ OmniVerifier-TTS –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ test-time scaling, –∫–æ—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–µ—Ç–∞–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã —Ç–∏–ø–∞ Best-of-N –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö T2I-ReasonBench (+3.7) –∏ GenEval++ (+4.3), –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–µ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reliable Visual Verification",
                    "desc": "The Generative Universal Verifier is a new tool that improves how machines understand and generate visual information alongside text. It introduces ViVerBench, a benchmark for testing how well models can verify visual outcomes, revealing that current models struggle compared to human performance. The paper also presents OmniVerifier-7B, a generative verifier that enhances visual verification capabilities and shows significant improvements in benchmark scores. Additionally, OmniVerifier-TTS offers a method for refining image generation and editing, leading to better overall performance in multimodal reasoning tasks."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÂèØÈù†ÊÄß‰∏éÁîüÊàêËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞Ê¶ÇÂøµ‚Äî‚ÄîÁîüÊàêÈÄöÁî®È™åËØÅÂô®ÔºàGenerative Universal VerifierÔºâÔºåÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫ÜViVerBenchÔºåËøôÊòØ‰∏Ä‰∏™Ê∂µÁõñ16Á±ªÂÖ≥ÈîÆ‰ªªÂä°ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑËßÜËßâÁªìÊûú„ÄÇÈÄöËøáËÆ≠ÁªÉOmniVerifier-7BÔºåÊàë‰ª¨ËØÜÂà´Âá∫ËßÜËßâÈ™åËØÅ‰∏≠ÁöÑ‰∏âÁßçÂü∫Êú¨ËÉΩÂäõÔºåÂπ∂Â±ïÁ§∫ÂÆÉ‰ª¨ÁöÑÂçèÂêå‰ΩúÁî®„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜOmniVerifier-TTSÔºåÈÄöËøáËø≠‰ª£‰ºòÂåñÊèêÂçáÁîüÊàêËÉΩÂäõÔºåÊé®Âä®‰∫ÜÊõ¥ÂèØÈù†ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13678",
            "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
            "url": "https://huggingface.co/papers/2510.13678",
            "abstract": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100times faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "ef944b6ef4d97bd1",
            "authors": [
                "Xinyang Li",
                "Tengfei Wang",
                "Zixiao Gu",
                "Shengchuan Zhang",
                "Chunchao Guo",
                "Liujuan Cao"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University",
                "Tencent",
                "Yes Lab, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13678.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–º–∏—Ä–æ–≤: —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤–º–µ—Å—Ç–µ",
                    "desc": "FlashWorld ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç 3D-—Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∑–∞ —Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–µ–∫—É–Ω–¥—ã, —Ä–∞–±–æ—Ç–∞—è –≤ 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π multi-view –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –ø—Ä—è–º–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é 3D Gaussian –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ dual-mode pre-training –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –æ–±–æ–∏—Ö —Ä–µ–∂–∏–º–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∑–∞—Ç–µ–º cross-mode post-training –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—É—Ç—ë–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–ª–∏—á–Ω–æ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é."
                },
                "en": {
                    "title": "FlashWorld: Fast and High-Quality 3D Scene Generation",
                    "desc": "FlashWorld is a generative model that creates high-quality 3D scenes from single images or text prompts in a fraction of the time compared to previous methods. It innovatively combines multi-view-oriented and 3D-oriented generation techniques, allowing for faster rendering while maintaining 3D consistency. The model employs a dual-mode pre-training phase and a cross-mode post-training phase to enhance visual quality and reduce denoising steps during inference. By utilizing a large dataset of single-view images and text prompts, FlashWorld improves its ability to generalize to new inputs effectively."
                },
                "zh": {
                    "title": "FlashWorldÔºöÂø´ÈÄüÁîüÊàêÈ´òË¥®Èáè3DÂú∫ÊôØÁöÑÂàõÊñ∞Ê®°Âûã",
                    "desc": "FlashWorldÊòØ‰∏ÄÁßçÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•Âø´ÈÄü‰ªéÂçïÂº†ÂõæÂÉèÊàñÊñáÊú¨ÊèêÁ§∫ÁîüÊàêÈ´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ„ÄÇÂÆÉÁªìÂêà‰∫ÜÂ§öËßÜËßíÂØºÂêëÂíå3DÂØºÂêëÁöÑÁîüÊàêÊñπÊ≥ïÔºå‰ΩøÂæóÁîüÊàêÈÄüÂ∫¶ÊØî‰ª•ÂæÄÂø´10Âà∞100ÂÄçÔºåÂêåÊó∂‰øùÊåÅ‰ºòË∂äÁöÑÊ∏≤ÊüìË¥®Èáè„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂèåÊ®°ÂºèÈ¢ÑËÆ≠ÁªÉÂíå‰∫§ÂèâÊ®°ÂºèÂêéËÆ≠ÁªÉÔºåÊúâÊïàÊï¥Âêà‰∫Ü‰∏§ÁßçÊñπÊ≥ïÁöÑ‰ºòÁÇπÔºåÁ°Æ‰øù‰∫Ü3D‰∏ÄËá¥ÊÄßÂπ∂ÊèêÂçá‰∫ÜËßÜËßâË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlashWorldÂú®ÁîüÊàêÊïàÁéáÂíåÊïàÊûú‰∏äÈÉΩ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10977",
            "title": "Revisiting Model Interpolation for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2510.10977",
            "abstract": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at https://github.com/wutaiqiang/MI{Github}.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 13",
                "zh": "10Êúà13Êó•"
            },
            "hash": "bdc53b166ac44504",
            "authors": [
                "Taiqiang Wu",
                "Runming Yang",
                "Tao Liu",
                "Jiahao Wang",
                "Ngai Wong"
            ],
            "affiliations": [
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10977.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üîÄ",
                "ru": {
                    "title": "–¢—Ä–∏ —Å—Ç–∞–¥–∏–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏: –ø—Ä–æ—Å—Ç–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–±–µ–∂–¥–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π - –ø—Ä—è–º—É—é –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –≤–µ—Å–æ–≤ –º–µ–∂–¥—É Instruct –∏ Thinking –º–æ–¥–µ–ª—è–º–∏. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Ç—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç–∞–¥–∏–∏ —ç–≤–æ–ª—é—Ü–∏–∏ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ª–æ–∂–Ω—ã–µ baseline –º–µ—Ç–æ–¥—ã —Å–ª–∏—è–Ω–∏—è –∫–∞–∫ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø—Ä–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –±–∞–ª–∞–Ω—Å–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∑–∞—Ç—Ä–∞—Ç."
                },
                "en": {
                    "title": "Unlocking Efficient Reasoning through Model Interpolation",
                    "desc": "This paper explores the concept of model merging, particularly focusing on Instruct and Thinking models, to enhance reasoning efficiency. The authors analyze a basic method of directly interpolating weights from two models, revealing a three-stage evolution in the reasoning process. Their findings indicate that a well-interpolated model can outperform more complex merging techniques in terms of both efficiency and effectiveness. The research provides a structured approach to model interpolation, enabling the development of models with specific reasoning strengths, supported by comprehensive experiments and ablation studies."
                },
                "zh": {
                    "title": "Ê®°ÂûãÊèíÂÄºÔºöÈ´òÊïàÊé®ÁêÜÁöÑÊñ∞Ë∑ØÂæÑ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊ®°ÂûãÂêàÂπ∂ÔºåÁâπÂà´ÊòØÂú®Êåá‰ª§ÂíåÊÄùÁª¥Ê®°Âûã‰∏äÁöÑÂ∫îÁî®ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®È´òÊïàÊé®ÁêÜÊñπÈù¢ÁöÑÂçìË∂äË°®Áé∞„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÂõûÈ°æ‰∫ÜÊúÄÁÆÄÂçïÁöÑÂêàÂπ∂ÊñπÊ≥ïÔºåÂç≥Áõ¥Êé•ÊèíÂÄº‰∏§‰∏™ÊùÉÈáçÔºåÂπ∂ËßÇÂØüÂà∞Ê®°ÂûãÊèíÂÄºÈÅµÂæ™‰∏âÈò∂ÊÆµÁöÑÊºîÂèòËåÉÂºèÔºåÂÖ∑Êúâ‰∏çÂêåÁöÑÊé®ÁêÜËΩ®ËøπÁâπÂæÅ„ÄÇËøô‰∫õÂä®ÊÄÅ‰∏∫Âú®ÊÄßËÉΩ‰∏éÊàêÊú¨‰πãÈó¥ÁöÑÊùÉË°°Êèê‰æõ‰∫ÜÂéüÂàôÊÄßÊåáÂØº„ÄÇÂÆûËØÅÁªìÊûúË°®ÊòéÔºåÁªèËøáÊàòÁï•ÊÄßÊèíÂÄºÁöÑÊ®°ÂûãÂú®ÊïàÁéáÂíåÊúâÊïàÊÄß‰∏äË∂ÖË∂ä‰∫ÜÂ§çÊùÇÁöÑÊ®°ÂûãÂêàÂπ∂Âü∫Á∫øÔºåËøõ‰∏ÄÊ≠•ÈÄöËøáÂπøÊ≥õÁöÑÊ∂àËûçÁ†îÁ©∂È™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÂèëÁé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13778",
            "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
            "url": "https://huggingface.co/papers/2510.13778",
            "abstract": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "9f1a9178757021cf",
            "authors": [
                "Xinyi Chen",
                "Yilun Chen",
                "Yanwei Fu",
                "Ning Gao",
                "Jiaya Jia",
                "Weiyang Jin",
                "Hao Li",
                "Yao Mu",
                "Jiangmiao Pang",
                "Yu Qiao",
                "Yang Tian",
                "Bin Wang",
                "Bolun Wang",
                "Fangjing Wang",
                "Hanqing Wang",
                "Tai Wang",
                "Ziqin Wang",
                "Xueyuan Wei",
                "Chao Wu",
                "Shuai Yang",
                "Jinhui Ye",
                "Junqiu Yu",
                "Jia Zeng",
                "Jingjing Zhang",
                "Jinyu Zhang",
                "Shi Zhang",
                "Feng Zheng",
                "Bowen Zhou",
                "Yangkun Zhu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13778.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞",
                    "desc": "InternVLA-M1 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ —É—á–∏—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å ¬´–≥–¥–µ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å¬ª –Ω–∞ 2.3 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∑–∞—Ç–µ–º —É—á–∏—Ç—Å—è ¬´–∫–∞–∫ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å¬ª –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç 4% –¥–æ 20% –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏—è—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –Ω–æ–≤—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spatial grounding –∫–∞–∫ –º–æ—Å—Ç–∞ –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ —Ä–æ–±–æ—Ç–∞."
                },
                "en": {
                    "title": "Empowering Robots with Spatially Guided Intelligence",
                    "desc": "The paper presents InternVLA-M1, a framework that enhances robots' ability to follow instructions by integrating spatial grounding with vision-language-action training. This approach involves a two-stage process: first, pre-training the model on a large dataset to understand where to act based on spatial reasoning, and second, fine-tuning it to determine how to act using spatial prompts. The results show significant performance improvements in various robotic tasks, demonstrating the effectiveness of spatial guidance in robot control. Overall, this work emphasizes the importance of spatially informed training for developing versatile and intelligent robots."
                },
                "zh": {
                    "title": "Á©∫Èó¥ÂºïÂØºËÆ≠ÁªÉÔºöÊèêÂçáÊú∫Âô®‰∫∫Êô∫ËÉΩÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫InternVLA-M1ÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÈÅµÂæ™Êåá‰ª§ÁöÑÊú∫Âô®‰∫∫Êô∫ËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁ©∫Èó¥ÂºïÂØºÁöÑËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ËÆ≠ÁªÉÔºåÂª∫Á´ã‰∫ÜÊåá‰ª§‰∏éÊú∫Âô®‰∫∫Âä®‰Ωú‰πãÈó¥ÁöÑÂÖ≥ÈîÆËÅîÁ≥ª„ÄÇInternVLA-M1ÈááÁî®‰∏§Èò∂ÊÆµÊµÅÁ®ãÔºöÈ¶ñÂÖàËøõË°åÁ©∫Èó¥ÂºïÂØºÁöÑÈ¢ÑËÆ≠ÁªÉÔºå‰ª•Á°ÆÂÆö‚ÄúÂú®Âì™ÈáåË°åÂä®‚ÄùÔºõÁÑ∂ÂêéËøõË°åÁ©∫Èó¥ÂºïÂØºÁöÑÂêéËÆ≠ÁªÉÔºå‰ª•ÁîüÊàê‚ÄúÂ¶Ç‰ΩïË°åÂä®‚ÄùÁöÑÂÖ∑‰ΩìÂä®‰Ωú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°ÂíåÊ®°Êãü‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫ÁöÑË°®Áé∞ÔºåÂ±ïÁ§∫‰∫ÜÁ©∫Èó¥ÂºïÂØºËÆ≠ÁªÉÂú®ÂèØÊâ©Â±ïÂíåÈÄöÁî®Êú∫Âô®‰∫∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13744",
            "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
            "url": "https://huggingface.co/papers/2510.13744",
            "abstract": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "ac775fe89e3a3d8b",
            "authors": [
                "Shrey Pandit",
                "Austin Xu",
                "Xuan-Phi Nguyen",
                "Yifei Ming",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13744.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ Hard2Verify ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é 500 —á–∞—Å–æ–≤ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—å –æ—à–∏–±–∫–∏ –≤ —Ä–µ—à–µ–Ω–∏—è—Ö —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 29 –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ open-source –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –æ—Ç closed-source —Å–∏—Å—Ç–µ–º. –†–∞–±–æ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏—á–∏–Ω—ã –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤–ª–∏—è–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ AI-–º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Hard2Verify: Bridging the Gap in LLM Verification",
                    "desc": "The paper introduces Hard2Verify, a benchmark designed to evaluate step-level verifiers for large language model (LLM)-based mathematical reasoning systems. It highlights the importance of strong verifiers that can accurately identify mistakes in mathematical proofs, which are crucial for achieving high performance in competitions like IMO 2025. The benchmark was created through extensive human annotation, involving over 500 hours of labor, to rigorously assess the capabilities of various verification models. The study reveals significant performance gaps between open-source and closed-source verifiers, while also exploring factors that contribute to these discrepancies and the dynamics of verification processes."
                },
                "zh": {
                    "title": "Hard2VerifyÔºöËØÑ‰º∞Êï∞Â≠¶Êé®ÁêÜÁöÑÈÄêÊ≠•È™åËØÅÂô®",
                    "desc": "Hard2VerifyÊòØ‰∏Ä‰∏™‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊï∞Â≠¶Êé®ÁêÜÁ≥ªÁªüÁöÑÈÄêÊ≠•È™åËØÅÂô®„ÄÇËØ•Âü∫ÂáÜÂº∫Ë∞É‰∫ÜÂºÄÊ∫êÊ®°ÂûãÂíåÈó≠Ê∫êÊ®°Âûã‰πãÈó¥ÁöÑÊåëÊàòÂíåÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ‰∏∫‰∫ÜÂú®Â§çÊùÇÁöÑÂºÄÊîæÂºèÁéØÂ¢É‰∏≠ËÆ≠ÁªÉLLMÊé®ÁêÜÂô®ÔºåÂº∫Â§ßÁöÑÈ™åËØÅÂô®ÊòØÂøÖ‰∏çÂèØÂ∞ëÁöÑÔºåÂÆÉ‰ª¨ËÉΩÂ§üÊçïÊçâÈÄêÊ≠•ÈîôËØØ„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫Ü29ÁßçÁîüÊàêÊÄßÊâπËØÑËÄÖÂíåËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºåÁªìÊûúÊòæÁ§∫ÔºåÈô§‰∫ÜÂ∞ëÊï∞‰ºòÁßÄÁöÑÊ®°ÂûãÂ§ñÔºåÂºÄÊ∫êÈ™åËØÅÂô®ÁöÑË°®Áé∞ÊôÆÈÅçËêΩÂêé‰∫éÈó≠Ê∫êÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13586",
            "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
            "url": "https://huggingface.co/papers/2510.13586",
            "abstract": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 15",
                "zh": "10Êúà15Êó•"
            },
            "hash": "b4af6fcdd0d708b9",
            "authors": [
                "Pasin Buakhaw",
                "Kun Kerdthaisong",
                "Phuree Phenhiran",
                "Pitikorn Khlaisamniang",
                "Supasate Vorathammathorn",
                "Piyalitt Ittichaiwong",
                "Nutchanon Yongsatianchot"
            ],
            "affiliations": [
                "Artificial Intelligence Association of Thailand",
                "Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University",
                "School of Biomedical Engineering & Imaging Sciences, Kings College London",
                "Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University",
                "Thammasat School of Engineering, Thammasat University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13586.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#games"
                ],
                "emoji": "üéÆ",
                "ru": {
                    "title": "–£–º–Ω—ã–µ –∏–≥—Ä–æ–≤—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥",
                    "desc": "–ö–æ–º–∞–Ω–¥–∞ Tu_Character_lab —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∞ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö NPC –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –∏–≥—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞: –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ —Å —Ç–µ—Ö–Ω–∏–∫–æ–π Deflanderization –¥–ª—è API —Ç—Ä–µ–∫–∞ –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–∏ Qwen3-14B —Å –ø–æ–º–æ—â—å—é SFT –∏ LoRA –¥–ª—è GPU —Ç—Ä–µ–∫–∞. –¢–µ—Ö–Ω–∏–∫–∞ Deflanderization –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–∞–≤–∏—Ç—å –∏–∑–ª–∏—à–Ω—é—é —Ä–æ–ª–µ–≤—É—é –∏–≥—Ä—É –∏ —É–ª—É—á—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–º. –†–µ—à–µ–Ω–∏—è –∑–∞–Ω—è–ª–∏ –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –≤ –∑–∞–¥–∞—á–∞—Ö 1 –∏ 3 (API) –∏ —á–µ—Ç–≤–µ—Ä—Ç–æ–µ –º–µ—Å—Ç–æ –≤ –∑–∞–¥–∞—á–µ 3 (GPU) –Ω–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏ CPDC 2025."
                },
                "en": {
                    "title": "Elevating NPC Dialogue with Fine-Tuned Models and Smart Prompting",
                    "desc": "This paper discusses the participation of the Tu_Character_lab team in the CPDC 2025, focusing on creating advanced non-player characters (NPCs) for dialogue challenges. The team utilized lightweight prompting techniques and fine-tuned large language models to enhance dialogue generation and task execution. They implemented a unique Deflanderization method to maintain task fidelity while minimizing excessive role-play. Their strategies led to impressive rankings, achieving 2nd place in both Task 1 and Task 3 of the API track, and 4th place in Task 3 of the GPU track."
                },
                "zh": {
                    "title": "ËΩªÈáèÁ∫ßÊèêÁ§∫‰∏éÂæÆË∞ÉÊ®°ÂûãÁöÑÂÆåÁæéÁªìÂêà",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜÊàë‰ª¨Âú®2025Âπ¥Â∏∏ËØÜ‰∫∫Ê†ºÂØπËØùÊåëÊàòËµõÔºàCPDCÔºâ‰∏≠ÁöÑÂèÇ‰∏éÊÉÖÂÜµ„ÄÇÊàë‰ª¨‰ΩøÁî®ËΩªÈáèÁ∫ßÊèêÁ§∫ÊäÄÊúØÂíåÂæÆË∞ÉÁöÑÂ§ßÂûãÊ®°ÂûãÔºåÊàêÂäüÂú∞Âú®‰ªªÂä°ÂØºÂêëÂíå‰∏ä‰∏ãÊñáÊÑüÁü•ÂØπËØùÊåëÊàò‰∏≠ÂèñÂæó‰∫ÜÈ´òÊéíÂêç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÈááÁî®‰∫ÜDeflanderizationÊèêÁ§∫ÊñπÊ≥ïÊù•ÊäëÂà∂ËøáÂ∫¶ËßíËâ≤ÊâÆÊºîÔºåÂπ∂ÊèêÈ´ò‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨Âà©Áî®Qwen3-14BÊ®°ÂûãËøõË°åÁõëÁù£ÂæÆË∞ÉÂíå‰ΩéÁß©ÈÄÇÂ∫îÔºåÊèêÂçá‰∫ÜÂØπËØùÁîüÊàêÁöÑË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11958",
            "title": "Direct Multi-Token Decoding",
            "url": "https://huggingface.co/papers/2510.11958",
            "abstract": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 13",
                "zh": "10Êúà13Êó•"
            },
            "hash": "a9246dd44d38f230",
            "authors": [
                "Xuan Luo",
                "Weizhi Wang",
                "Xifeng Yan"
            ],
            "affiliations": [
                "Department of Computer Science, UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11958.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Direct Multi-Token Decoding (DMTD), –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç inference –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥–∏–ø–æ—Ç–µ–∑–µ, —á—Ç–æ —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –ø–æ–Ω–∏–º–∞—é—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å—Ä–µ–¥–Ω–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –∑–∞–¥–∞—á—É, –∞ –ø–æ–∑–¥–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ —Ç–æ–∫–µ–Ω—ã. –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–Ω–Ω–∏–º–∏ –∏ —Å—Ä–µ–¥–Ω–∏–º–∏ —Å–ª–æ—è–º–∏ –º–æ–∂–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏, –∏–∑–±–µ–≥–∞—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ —á–µ—Ä–µ–∑ –≤—Å—é —Å–µ—Ç—å. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen3-4B –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞."
                },
                "en": {
                    "title": "Speed Up Language Generation with DMTD!",
                    "desc": "Direct Multi-Token Decoding (DMTD) is a new method that speeds up the process of generating text with large language models by only using the late layers of the model for token generation. This approach takes advantage of the fact that early and middle layers have already processed the input, allowing the late layers to efficiently produce multiple tokens without reprocessing. DMTD does not require any extra parameters or complex routines, making it a straightforward enhancement to existing models. Initial results show that a fine-tuned DMTD model can achieve up to a 2x increase in speed with minimal impact on performance, and its effectiveness is expected to grow with larger datasets."
                },
                "zh": {
                    "title": "Áõ¥Êé•Â§öÊ†áËÆ∞Ëß£Á†ÅÔºöÂä†ÈÄüÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Áõ¥Êé•Â§öÊ†áËÆ∞Ëß£Á†ÅÔºàDMTDÔºâÈÄöËøá‰ªÖ‰ΩøÁî®ÂêéÂ±ÇËøõË°åÊ†áËÆ∞ÁîüÊàêÔºåÂä†ÈÄü‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈÄüÂ∫¶‰∏îÊÄßËÉΩÊçüÂ§±ÊûÅÂ∞è„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊó©Êúü„ÄÅ‰∏≠ÊúüÂíåÂêéÊúüÂ±ÇÂêÑËá™ÊâøÊãÖ‰∏çÂêåÁöÑËßíËâ≤„ÄÇÊàë‰ª¨ÁöÑÂÅáËÆæÊòØÔºå‰∏ÄÊó¶Êó©ÊúüÂíå‰∏≠ÊúüÂ±ÇÂ§ÑÁêÜÂÆåËæìÂÖ•ÔºåÁîüÊàêÁöÑÈöêËóèÁä∂ÊÄÅÂ∞±Ë∂≥‰ª•ÊîØÊåÅ‰ªÖ‰ΩøÁî®ÂêéÊúüÂ±ÇÁîüÊàêÂ§ö‰∏™Ê†áËÆ∞Ôºå‰ªéËÄåÈÅøÂÖçÈáçÂ§çÈÅçÂéÜÊó©ÊúüÂíå‰∏≠ÊúüÂ±Ç„ÄÇDMTDÊñπÊ≥ïÂú®‰∏çÂ¢ûÂä†È¢ùÂ§ñÂèÇÊï∞ÊàñËæÖÂä©Á®ãÂ∫èÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∑≤Âú®ÊúâÈôêÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫Âá∫ËâØÂ•ΩÁöÑÊïàÊûúÔºåÈÄüÂ∫¶ÊèêÂçáÂèØËææ2ÂÄçÔºå‰∏îÊÄßËÉΩÊçüÂ§±ÂæàÂ∞è„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10611",
            "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
            "url": "https://huggingface.co/papers/2510.10611",
            "abstract": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) Ineffective group collaboration modeling, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) Limited task-adaptiveness in communication topology design, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose HyperAgent, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication.",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 12",
                "zh": "10Êúà12Êó•"
            },
            "hash": "38135ea1ec6a6548",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Zijian Zhang",
                "Haochen You",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University, USA",
                "Shanghai Jiao Tong University, China",
                "South China Normal University, China",
                "University of Michigan, USA",
                "University of Pennsylvania, USA",
                "University of Science and Technology of China, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10611.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#graphs",
                    "#games"
                ],
                "emoji": "üï∏Ô∏è",
                "ru": {
                    "title": "–ì–∏–ø–µ—Ä–≥—Ä–∞—Ñ—ã –¥–ª—è —É–º–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HyperAgent ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö —Å LLM. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –ø–∞—Ä–Ω—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏, HyperAgent –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≥–∏–ø–µ—Ä—Ä—ë–±—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥—Ä—É–ø–ø–æ–≤–æ–π –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –°–∏—Å—Ç–µ–º–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Ç–æ–ø–æ–ª–æ–≥–∏—é –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ GSM8K —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 95.07% –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 25.33%, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "HyperAgent: Enhancing Multi-Agent Collaboration with Hypergraphs",
                    "desc": "HyperAgent is a new framework that uses hypergraphs to improve how multiple agents communicate and work together. Traditional methods struggle with modeling group collaboration and adapting communication for different tasks, which can lead to inefficiencies. By using hyperedges, HyperAgent can connect several agents at once, allowing for better information sharing and coordination. The framework also adjusts its communication structure based on the complexity of the task, resulting in better performance and reduced communication costs."
                },
                "zh": {
                    "title": "Ë∂ÖÂõæ‰ºòÂåñÔºöÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÁöÑÂà©Âô®",
                    "desc": "HyperAgentÊòØ‰∏Ä‰∏™Âü∫‰∫éË∂ÖÂõæÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ºòÂåñÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠ÁöÑÈÄö‰ø°ÊãìÊâëÔºåÂπ∂ÊçïÊçâÁæ§‰ΩìÂçè‰ΩúÊ®°ÂºèÔºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩÂíåÊïàÁéá„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®Âª∫Ê®°Áæ§‰ΩìÂçè‰ΩúÊó∂ÁöÑ‰∏çË∂≥ÔºåËÉΩÂ§üÊúâÊïàÂú∞Ë°®Á§∫Â§ö‰∏™Êô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇHyperAgentÈÄöËøáË∂ÖËæπËøûÊé•Âêå‰∏ÄÂ≠ê‰ªªÂä°‰∏≠ÁöÑÂ§ö‰∏™Êô∫ËÉΩ‰ΩìÔºåÂπ∂Âà©Áî®Ë∂ÖÂõæÂç∑ÁßØÂ±ÇÂÆûÁé∞Âçè‰ΩúÁªÑÂÜÖÁöÑ‰∏ÄÊ≠•‰ø°ÊÅØËÅöÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHyperAgentÂú®ÊÄßËÉΩÂíåÊïàÁéá‰∏äÂùá‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜË∂ÖÂõæ‰ºòÂåñÂú®Â§öÊô∫ËÉΩ‰ΩìÈÄö‰ø°‰∏≠ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10581",
            "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
            "url": "https://huggingface.co/papers/2510.10581",
            "abstract": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: (i) distinguishing symptoms from root causes in multi-agent error propagation, and (ii) tracing information dependencies beyond temporal order. To address these issues, we introduce GraphTracer, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 12",
                "zh": "10Êúà12Êó•"
            },
            "hash": "4456157e4d3ee0bf",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Haochen You",
                "Zijian Zhang",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University",
                "Shanghai Jiao Tong University",
                "South China Normal University",
                "University of Michigan",
                "University of Pennsylvania",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10581.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#dataset",
                    "#graphs",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üï∏Ô∏è",
                "ru": {
                    "title": "–ì—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GraphTracer ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –±–∞–∑–µ LLM. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–µ –º–æ–≥—É—Ç –æ—Ç–ª–∏—á–∏—Ç—å —Å–∏–º–ø—Ç–æ–º—ã –æ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫, –∫–æ–≥–¥–∞ –æ–Ω–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç—Å—è –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏. GraphTracer —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (IDG), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç, –∫–∞–∫ –∞–≥–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –¥—Ä—É–≥ –¥—Ä—É–≥–∞, –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –Ω–∞ 18% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –æ—à–∏–±–æ–∫ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ 5-14%."
                },
                "en": {
                    "title": "Revolutionizing Multi-Agent Debugging with GraphTracer",
                    "desc": "GraphTracer is a novel framework designed to improve failure attribution in multi-agent systems by utilizing Information Dependency Graphs (IDGs). It addresses the challenges of identifying root causes of errors that propagate across multiple agents and distinguishing them from mere symptoms. By analyzing information flow rather than just temporal sequences, GraphTracer enhances debugging accuracy and provides a clearer understanding of how agents interact. The framework has shown significant improvements in attribution accuracy and performance in real-world applications, making it a valuable tool for developers working with complex multi-agent systems."
                },
                "zh": {
                    "title": "GraphTracerÔºöÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊïÖÈöúÂΩíÂõ†ÁöÑÂáÜÁ°ÆÊÄß",
                    "desc": "GraphTracer ÊòØ‰∏ÄÁßçÈíàÂØπÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊïÖÈöúÂΩíÂõ†ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊûÑÂª∫‰ø°ÊÅØ‰æùËµñÂõæÊù•ËøΩË∏™‰ø°ÊÅØÊµÅÔºå‰ªéËÄåÊèêÈ´òË∞ÉËØïÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂú®Â§öÊô∫ËÉΩ‰ΩìÈîôËØØ‰º†Êí≠‰∏≠Âå∫ÂàÜÁóáÁä∂‰∏éÊ†πÊú¨ÂéüÂõ†ÁöÑÊåëÊàòÔºåÂπ∂ËÉΩÂ§üË∂ÖË∂äÊó∂Èó¥È°∫Â∫èËøΩË∏™‰ø°ÊÅØ‰æùËµñ„ÄÇGraphTracer ÈÄöËøáÂàÜÊûê‰ø°ÊÅØÊµÅÔºåÈáçÊñ∞ÂÆö‰πâ‰∫ÜÊïÖÈöúÂΩíÂõ†ÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÂÆö‰ΩçÊ†πÊú¨ÂéüÂõ†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGraphTracer Âú®ÊïÖÈöúÂΩíÂõ†ÂáÜÁ°ÆÊÄß‰∏äÊØîÁé∞ÊúâÊ®°ÂûãÊèêÈ´ò‰∫Ü 18.18%ÔºåÂπ∂Âú®Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂‰∏≠ÂÆûÁé∞‰∫Ü 4.8% Âà∞ 14.2% ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-10-15.html",
    "link_next": "2025-10-17.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10Êúà15Êó•"
    },
    "short_date_next": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10Êúà17Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}