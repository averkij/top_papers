{
    "date": {
        "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 16",
        "zh": "12æœˆ16æ—¥"
    },
    "time_utc": "2025-12-16 03:26",
    "weekday": 1,
    "issue_id": 73,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.10071",
            "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
            "url": "https://huggingface.co/papers/2512.10071",
            "abstract": "A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.  \t\t\t\t\tAI-generated summary \t\t\t\t The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on Ï€_{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
            "score": 10,
            "issue_id": 73,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "a2ad5a2832241e05",
            "authors": [
                "Junjie Bai",
                "Yu-Wei Chao",
                "Qizhi Chen",
                "Jinwei Gu",
                "Moo Jin Kim",
                "Zhaoshuo Li",
                "Xuan Li",
                "Tsung-Yi Lin",
                "Ming-Yu Liu",
                "Nic Ma",
                "Kaichun Mo",
                "Delin Qu",
                "Shangkun Sun",
                "Hongchi Xia",
                "Fangyin Wei",
                "Xiaohui Zeng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2512.10071.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ BEHAVIOR Challenge 2025, Ğ³Ğ´Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€Ğ¾ÑÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ï€â‚€.â‚… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ñ‰ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑƒÑ€Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ foundation models Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Mastering Household Tasks with Advanced Training Techniques",
                    "desc": "This paper presents a solution for the 2025 BEHAVIOR Challenge, which focuses on teaching robots to perform everyday household tasks. The authors utilized pre-training and post-training techniques to enhance the performance of their model, achieving significant improvements over other submissions. They conducted detailed experiments to understand how different training methods and data impact the effectiveness of their approach. The findings offer valuable insights and recommendations for the embodied AI community, particularly in adapting advanced models for real-world applications."
                },
                "zh": {
                    "title": "æå‡å®¶åŠ¡ä»»åŠ¡çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨2025 BEHAVIORæŒ‘æˆ˜èµ›ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æŒ‘æˆ˜èµ›æ—¨åœ¨è¯„ä¼°ç‰©ç†ä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®Œæˆé•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¥å¸¸å®¶åŠ¡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§»åŠ¨æ“æ§æ–¹é¢ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–æäº¤çš„æ–¹æ¡ˆã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒå’Œåè®­ç»ƒæŠ€æœ¯çš„ç³»ç»Ÿç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¿™äº›è®­ç»ƒé˜¶æ®µå¯¹ç«äº‰æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬æ€»ç»“äº†å®è·µç»éªŒå’Œè®¾è®¡å»ºè®®ï¼Œå¸Œæœ›ä¸ºæ›´å¹¿æ³›çš„å…·èº«äººå·¥æ™ºèƒ½ç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09636",
            "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
            "url": "https://huggingface.co/papers/2512.09636",
            "abstract": "MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.",
            "score": 7,
            "issue_id": 73,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "5437ffcf2149ac8e",
            "authors": [
                "Mengxi Xiao",
                "Kailai Yang",
                "Pengde Zhao",
                "Enze Zhang",
                "Ziyan Kuang",
                "Zhiwei Liu",
                "Weiguang Han",
                "Shu Liao",
                "Lianting Huang",
                "Jinpeng Hu",
                "Min Peng",
                "Qianqian Xie",
                "Sophia Ananiadou"
            ],
            "affiliations": [
                "Center for Language and Information Research, Wuhan University",
                "Mount Holyoke College",
                "School of Artificial Intelligence, Wuhan University",
                "School of Computer Science, Wuhan University",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09636.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf",
                    "#hallucinations",
                    "#healthcare",
                    "#science",
                    "#training",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MentraSuite â€” ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MentraBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 13 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Mindora â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° SFT-RL Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Mindora Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ LLM Ğ¿Ğ¾ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ."
                },
                "en": {
                    "title": "Advancing Reliable Mental Health Reasoning with MentraSuite",
                    "desc": "MentraSuite is a comprehensive framework designed to enhance reliable reasoning in mental health applications using a model called Mindora, which is fine-tuned with a combination of supervised fine-tuning and reinforcement learning. The framework includes MentraBench, a benchmark that evaluates various aspects of reasoning quality and task performance across multiple datasets and tasks. Mindora is specifically optimized to detect inconsistencies in reasoning, ensuring that the outputs are coherent and grounded in clinical relevance. The results show that Mindora outperforms other large language models in mental health reasoning, making it a valuable tool for addressing complex mental health scenarios."
                },
                "zh": {
                    "title": "æå‡å¿ƒç†å¥åº·æ¨ç†çš„å¯é æ€§",
                    "desc": "MentraSuiteæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¿ƒç†å¥åº·æ¨ç†çš„å¯é æ€§ã€‚å®ƒä½¿ç”¨Mindoraæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ··åˆçš„SFT-RLæ–¹æ³•è¿›è¡Œåè®­ç»ƒï¼Œå¹¶é€šè¿‡MentraBenchåŸºå‡†è¿›è¡Œè¯„ä¼°ã€‚MentraBenchæ¶µç›–äº”ä¸ªæ ¸å¿ƒæ¨ç†æ–¹é¢ï¼Œè¯„ä¼°ä»»åŠ¡è¡¨ç°å’Œæ¨ç†è´¨é‡ã€‚Mindoraåœ¨20ä¸ªè¯„ä¼°çš„è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤æ‚å¿ƒç†å¥åº·åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12602",
            "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
            "url": "https://huggingface.co/papers/2512.12602",
            "abstract": "Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
            "score": 6,
            "issue_id": 73,
            "pub_date": "2025-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "94c0dbad2d65ed6c",
            "authors": [
                "Jingdi Lei",
                "Di Zhang",
                "Soujanya Poria"
            ],
            "affiliations": [
                "Fudan University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12602.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (EFLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞµÑ‘ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ·Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ½Ğ³Ğ° 1, Ğ¾Ğ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ğ ÑƒĞ½Ğ³Ğµ-ĞšÑƒÑ‚Ñ‚Ñ‹ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ EFLA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… downstream-Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Revolutionizing Attention: Fast, Stable, and Error-Free!",
                    "desc": "Error-Free Linear Attention (EFLA) is a new attention mechanism designed to improve language modeling by addressing the computational challenges of traditional softmax attention. It operates in linear time, allowing for efficient processing of long-context data while maintaining numerical stability. EFLA formulates the learning process as a continuous-time dynamical system, enabling it to achieve accurate results without error accumulation. Experimental results demonstrate that EFLA outperforms existing models like DeltaNet in various tasks, all while keeping the model size constant."
                },
                "zh": {
                    "title": "æ— è¯¯å·®çº¿æ€§æ³¨æ„åŠ›ï¼šé«˜æ•ˆä¸”ç¨³å®šçš„è¯­è¨€æ¨¡å‹æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„çº¿æ€§æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ— è¯¯å·®çº¿æ€§æ³¨æ„åŠ›ï¼ˆEFLAï¼‰ï¼Œå®ƒåœ¨è¯­è¨€å»ºæ¨¡å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºDeltaNetã€‚EFLAå…·æœ‰æ•°å€¼ç¨³å®šæ€§å’Œå®Œå…¨å¹¶è¡Œæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ä¸­çš„äºŒæ¬¡æˆæœ¬ç“¶é¢ˆã€‚æˆ‘ä»¬å°†åœ¨çº¿å­¦ä¹ æ›´æ–°å½¢å¼åŒ–ä¸ºè¿ç»­æ—¶é—´åŠ¨æ€ç³»ç»Ÿï¼Œå¹¶è¯æ˜å…¶ç²¾ç¡®è§£å¯ä»¥åœ¨ä¿æŒçº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶è®¡ç®—ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†EFLAåœ¨å™ªå£°ç¯å¢ƒä¸­çš„å¼ºå¤§æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†è¯­è¨€å»ºæ¨¡çš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12730",
            "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
            "url": "https://huggingface.co/papers/2512.12730",
            "abstract": "NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
            "score": 2,
            "issue_id": 73,
            "pub_date": "2025-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "00565ddc9131f583",
            "authors": [
                "Jingzhe Ding",
                "Shengda Long",
                "Changxin Pu",
                "Huan Zhou",
                "Hongwan Gao",
                "Xiang Gao",
                "Chao He",
                "Yue Hou",
                "Fei Hu",
                "Zhaojian Li",
                "Weiran Shi",
                "Zaiyuan Wang",
                "Daoguang Zan",
                "Chenchen Zhang",
                "Xiaoxu Zhang",
                "Qizhi Chen",
                "Xianfu Cheng",
                "Bo Deng",
                "Qingshui Gu",
                "Kai Hua",
                "Juntao Lin",
                "Pai Liu",
                "Mingchen Li",
                "Xuanguang Pan",
                "Zifan Peng",
                "Yujia Qin",
                "Yong Shan",
                "Zhewen Tan",
                "Weihao Xie",
                "Zihan Wang",
                "Yishuo Yuan",
                "Jiayu Zhang",
                "Enduo Zhao",
                "Yunfei Zhao",
                "He Zhu",
                "Chenyang Zou",
                "Ming Ding",
                "Jianpeng Jiao",
                "Jiaheng Liu",
                "Minghao Liu",
                "Qian Liu",
                "Chongyao Tao",
                "Jian Yang",
                "Tong Yang",
                "Zhaoxiang Zhang",
                "Xinjie Chen",
                "Wenhao Huang",
                "Ge Zhang"
            ],
            "affiliations": [
                "2077AI",
                "Beihang University",
                "Beijing University of Posts and Telecommunications",
                "ByteDance",
                "Humanlaya Data",
                "M-A-P",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12730.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#plp"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²",
                    "desc": "NL2Repo Bench â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Python Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 40% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ÑÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ½ĞµĞ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¾Ñ‚Ğ½Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Long-Horizon Software Development in Coding Agents",
                    "desc": "The paper introduces NL2Repo Bench, a benchmark designed to evaluate the long-horizon software development capabilities of coding agents. It focuses on the ability of these agents to generate complete Python libraries from natural-language requirements, which involves complex tasks like architecture design and dependency management. The study reveals that current coding agents struggle with long-horizon tasks, achieving less than 40% success in generating complete repositories. Key challenges identified include issues with coherence, dependency management, and planning over extended interactions, highlighting the need for improved long-horizon reasoning in future coding agents."
                },
                "zh": {
                    "title": "è¯„ä¼°ç¼–ç ä»£ç†çš„é•¿æœŸè½¯ä»¶å¼€å‘èƒ½åŠ›",
                    "desc": "NL2Repo Bench æ˜¯ä¸€ä¸ªè¯„ä¼°ç¼–ç ä»£ç†åœ¨è½¯ä»¶å¼€å‘ä¸­é•¿æœŸèƒ½åŠ›çš„åŸºå‡†ï¼Œä¸“æ³¨äºä»è‡ªç„¶è¯­è¨€éœ€æ±‚ç”Ÿæˆå®Œæ•´çš„ Python åº“ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å±€éƒ¨ä»£ç ç”Ÿæˆå’ŒçŸ­æœŸä¿®å¤ä»»åŠ¡ï¼Œæœªèƒ½æœ‰æ•ˆæµ‹è¯•ä»£ç†åœ¨æ„å»ºå®Œæ•´è½¯ä»¶ç³»ç»Ÿæ—¶çš„æŒç»­æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å¼ºçš„ç¼–ç ä»£ç†åœ¨é•¿æœŸåº“ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä»ç„¶ä¸ç†æƒ³ï¼Œå¹³å‡æµ‹è¯•é€šè¿‡ç‡ä½äº 40%ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†é•¿æœŸæ¨ç†çš„å…³é”®ç“¶é¢ˆï¼Œå¹¶ä¸ºæœªæ¥çš„è‡ªä¸»ç¼–ç ä»£ç†æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„æµ‹è¯•å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11883",
            "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
            "url": "https://huggingface.co/papers/2512.11883",
            "abstract": "State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
            "score": 2,
            "issue_id": 73,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "caba7798259afbac",
            "authors": [
                "Wenqi Marshall Guo",
                "Qingyun Qian",
                "Khalad Hasan",
                "Shan Du"
            ],
            "affiliations": [
                "Department of CMPS, University of British Columbia, Canada",
                "Department of MEOW, Weathon Software, Canada",
                "Department of WOOF, Weathon Software, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11883.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° ĞºÑ€Ğ°ÑĞ¾Ñ‚Ğ° ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ñ†ĞµĞ½Ğ·ÑƒÑ€Ğ¾Ğ¹: Ğ±Ğ¾Ñ€ÑŒĞ±Ğ° Ğ·Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ reward models Ğ¸Ğ¼ĞµÑÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ĞºĞ¾Ğ½Ğ²ĞµĞ½Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ñ‚Ğ¸-ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ¼ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (reward models) Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒÑÑ‚ Ğ°Ğ½Ñ‚Ğ¸-ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ ÑĞ²Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Challenging Aesthetic Bias in AI Image Generation",
                    "desc": "This paper discusses the bias in image generation and reward models that favor traditional aesthetics, which limits the ability to create anti-aesthetic images. The authors argue that this bias undermines user intent, especially when users seek unconventional or critical artistic expressions. They evaluate this issue by creating a diverse dataset that tests the performance of current models, revealing that they often produce aesthetically pleasing images even when instructed otherwise. The findings highlight a systemic issue where reward models penalize non-traditional outputs, thus restricting artistic diversity and user autonomy."
                },
                "zh": {
                    "title": "æ‰“ç ´ç¾å­¦åè§ï¼Œå°Šé‡ç”¨æˆ·åˆ›æ„",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å½“å‰å›¾åƒç”Ÿæˆå’Œå¥–åŠ±æ¨¡å‹åœ¨ç¾å­¦æ–¹é¢çš„åè§ï¼Œå°¤å…¶æ˜¯å®ƒä»¬å¯¹ä¼ ç»Ÿç¾å­¦çš„è¿‡åº¦ä¾èµ–ã€‚è¿™ç§åè§å¯¼è‡´æ¨¡å‹åœ¨ç”¨æˆ·è¯·æ±‚ç”Ÿæˆåç¾å­¦å›¾åƒæ—¶ï¼Œæ— æ³•æ»¡è¶³ç”¨æˆ·çš„æ„å›¾ï¼Œå½±å“äº†ç”¨æˆ·çš„è‡ªä¸»æ€§å’Œç¾å­¦å¤šæ ·æ€§ã€‚é€šè¿‡æ„å»ºå¹¿æ³›çš„ç¾å­¦æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬å¸¸å¸¸é»˜è®¤ç”Ÿæˆä¼ ç»Ÿç¾ä¸½çš„å›¾åƒï¼Œè€Œå¿½è§†äº†ä½è´¨é‡æˆ–è´Ÿé¢å›¾åƒçš„è¯·æ±‚ã€‚æ­¤å¤–ï¼Œå¥–åŠ±æ¨¡å‹å¯¹åç¾å­¦å›¾åƒè¿›è¡Œæƒ©ç½šï¼Œå³ä½¿è¿™äº›å›¾åƒå®Œå…¨ç¬¦åˆç”¨æˆ·çš„æ˜ç¡®è¦æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13592",
            "title": "Image Diffusion Preview with Consistency Solver",
            "url": "https://huggingface.co/papers/2512.13592",
            "abstract": "Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.  \t\t\t\t\tAI-generated summary \t\t\t\t The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
            "score": 1,
            "issue_id": 73,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "9431a63073aeaad2",
            "authors": [
                "Fu-Yun Wang",
                "Hao Zhou",
                "Liangzhe Yuan",
                "Sanghyun Woo",
                "Boqing Gong",
                "Bohyung Han",
                "Ming-Hsuan Yang",
                "Han Zhang",
                "Yukun Zhu",
                "Ting Liu",
                "Long Zhao"
            ],
            "affiliations": [
                "Google DeepMind",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13592.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Diffusion Preview Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ConsistencySolver â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ 47% Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° 50%."
                },
                "en": {
                    "title": "Enhancing Image Generation with Fast, Consistent Previews",
                    "desc": "The paper introduces Diffusion Preview, a new method that enhances the quality and consistency of images generated with fewer steps in diffusion models. It utilizes a trainable solver called ConsistencySolver, which is designed to provide quick, preliminary image outputs for user feedback before finalizing the images. This approach addresses the slow inference times of traditional diffusion models, allowing for a more interactive user experience. Experimental results show that Diffusion Preview not only improves the quality of low-step image generation but also significantly reduces user interaction time while maintaining high standards of output."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç”Ÿæˆè´¨é‡ä¸ä¸€è‡´æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "Diffusion Preview æ˜¯ä¸€ç§æ–°é¢–çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä½æ­¥æ•°ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§åä¸º ConsistencySolver çš„é«˜é˜¶å¯è®­ç»ƒæ±‚è§£å™¨ï¼Œé€šè¿‡å¿«é€Ÿçš„ä½æ­¥é‡‡æ ·ç”Ÿæˆåˆæ­¥è¾“å‡ºï¼Œç”¨æˆ·å¯ä»¥åœ¨æ»¡æ„ä¹‹å‰è¿›è¡Œè¯„ä¼°ã€‚ä¸ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒConsistencySolver èƒ½å¤Ÿåœ¨ä¿æŒé«˜è´¨é‡é¢„è§ˆçš„åŒæ—¶ï¼Œç¡®ä¿é¢„è§ˆä¸æœ€ç»ˆè¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½æ­¥æ•°åœºæ™¯ä¸‹æ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œä¸€è‡´æ€§ï¼Œå‡å°‘äº†ç”¨æˆ·äº¤äº’æ—¶é—´è¿‘50%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-15.html",
    "link_next": "2025-12-17.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "15.12",
        "en": "12/15",
        "zh": "12æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}