{
    "date": {
        "ru": "5 декабря",
        "en": "December 5",
        "zh": "12月5日"
    },
    "time_utc": "2024-12-05 15:11",
    "weekday": 3,
    "issue_id": 969,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.02687",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "url": "https://huggingface.co/papers/2412.02687",
            "abstract": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.",
            "score": 33,
            "issue_id": 961,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "d766bad745d5f322",
            "authors": [
                "Viet Nguyen",
                "Anh Nguyen",
                "Trung Dao",
                "Khoi Nguyen",
                "Cuong Pham",
                "Toan Tran",
                "Anh Tran"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech.",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02687.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Повышение стабильности и гибкости одношаговых диффузионных моделей",
                    "desc": "Статья представляет SNOOPI - новый фреймворк для улучшения одношаговых диффузионных моделей генерации изображений. Авторы предлагают метод PG-SB для повышения стабильности обучения путем использования случайного масштаба бесклассификаторного руководства. Также вводится метод NASA для интеграции негативных промптов через кросс-внимание. Эксперименты показывают значительное улучшение базовых моделей по различным метрикам, достигая нового рекорда HPSv2 в 31.08 для одношаговых диффузионных моделей."
                },
                "en": {
                    "title": "SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance",
                    "desc": "This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08."
                },
                "zh": {
                    "title": "SNOOPI：提升一步扩散模型的稳定性与生成质量",
                    "desc": "本论文提出了一种新框架SNOOPI，旨在解决现有一步扩散模型的局限性。我们通过Proper Guidance-SwiftBrush (PG-SB)方法增强了训练的稳定性，采用随机尺度的无分类器引导策略。我们还提出了一种无训练的方法Negative-Away Steer Attention (NASA)，通过交叉注意力将负提示集成到一步扩散模型中，以抑制生成图像中的不必要元素。实验结果表明，我们的方法在多个指标上显著提高了基线模型的性能，创造了一步扩散模型的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03555",
            "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
            "url": "https://huggingface.co/papers/2412.03555",
            "abstract": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
            "score": 27,
            "issue_id": 964,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "12d0d9bcc8060099",
            "authors": [
                "Andreas Steiner",
                "André Susano Pinto",
                "Michael Tschannen",
                "Daniel Keysers",
                "Xiao Wang",
                "Yonatan Bitton",
                "Alexey Gritsenko",
                "Matthias Minderer",
                "Anthony Sherbondy",
                "Shangbang Long",
                "Siyang Qin",
                "Reeve Ingle",
                "Emanuele Bugliarello",
                "Sahar Kazemzadeh",
                "Thomas Mesnard",
                "Ibrahim Alabdulmohsin",
                "Lucas Beyer",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03555.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PaliGemma 2: Новый уровень мультимодального ИИ",
                    "desc": "PaliGemma 2 - это улучшенная версия открытой мультимодальной модели PaliGemma, основанная на семействе языковых моделей Gemma 2. Модель сочетает визуальный энкодер SigLIP-So400m с рядом моделей Gemma 2 разных размеров, от 2B до 27B параметров. Обучение проводилось на изображениях разного разрешения (224px, 448px и 896px) в несколько этапов для приобретения широких знаний. PaliGemma 2 демонстрирует отличные результаты на различных задачах, включая распознавание структуры таблиц, молекулярных структур, нотных записей, а также генерацию подробных описаний изображений и радиологических отчетов."
                },
                "en": {
                    "title": "PaliGemma 2: Advancing Vision-Language Understanding",
                    "desc": "PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning."
                },
                "zh": {
                    "title": "PaliGemma 2：视觉与语言的完美结合",
                    "desc": "PaliGemma 2 是基于 Gemma 2 语言模型家族的 PaliGemma 开放视觉语言模型的升级版。我们结合了 SigLIP-So400m 视觉编码器和不同规模的 Gemma 2 模型，进行多阶段训练，以提高模型的知识迁移能力。通过在三种分辨率下训练，我们能够研究影响迁移性能的因素，如学习率，并分析任务类型、模型大小和分辨率之间的关系。PaliGemma 2 扩展了迁移任务的数量和范围，涵盖了多种光学字符识别相关任务，并在这些任务上取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03552",
            "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
            "url": "https://huggingface.co/papers/2412.03552",
            "abstract": "360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.",
            "score": 20,
            "issue_id": 958,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "90dc986cabb575af",
            "authors": [
                "Jing Tan",
                "Shuai Yang",
                "Tong Wu",
                "Jingwen He",
                "Yuwei Guo",
                "Ziwei Liu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03552.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Погружение в 360°: от обычного видео к панорамному опыту",
                    "desc": "Статья представляет Imagine360 - первую систему для генерации 360-градусных видео из обычных перспективных видео. Система использует двухветвевую архитектуру с модулями шумоподавления для перспективного и панорамного видео, а также антиподальную маску для захвата дальних зависимостей движения. Предложены решения для адаптации к изменениям угла обзора во входных видео. Эксперименты показывают превосходное качество графики и согласованность движения по сравнению с существующими методами."
                },
                "en": {
                    "title": "Transforming Perspective Videos into Immersive 360° Experiences",
                    "desc": "The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos."
                },
                "zh": {
                    "title": "Imagine360：个性化沉浸式360度视频创作的未来",
                    "desc": "360度视频提供了一种超沉浸式体验，让观众可以从全方位探索动态场景。为实现更友好和个性化的360度视频内容创作，我们提出了Imagine360，这是首个将标准视角视频转换为360度视频的框架。Imagine360通过有限的360度视频数据学习细致的球面视觉和运动模式，采用双分支设计来提供局部和全局约束。实验表明，Imagine360在图形质量和运动一致性方面优于现有的360度视频生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03515",
            "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2412.03515",
            "abstract": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.",
            "score": 20,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6e733cf9c0a1b851",
            "authors": [
                "Shengyuan Zhang",
                "An Zhao",
                "Ling Yang",
                "Zejian Li",
                "Chenye Meng",
                "Haoran Xu",
                "Tianrun Chen",
                "AnYang Wei",
                "Perry Pengyun GU",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Быстрое и качественное завершение 3D LiDAR-сцен для автономных транспортных средств",
                    "desc": "Статья представляет новый метод дистилляции для моделей завершения 3D LiDAR-сцен под названием ScoreLiDAR. Этот метод позволяет значительно ускорить процесс семплирования при сохранении высокого качества завершения сцены. Авторы также вводят новую Структурную Потерю, которая помогает дистиллированной модели лучше улавливать геометрическую структуру 3D LiDAR-сцены. Эксперименты показывают, что ScoreLiDAR ускоряет время завершения более чем в 5 раз и превосходит современные модели завершения 3D LiDAR-сцен."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with ScoreLiDAR",
                    "desc": "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."
                },
                "zh": {
                    "title": "高效3D LiDAR场景补全的新方法",
                    "desc": "扩散模型因其强大的训练稳定性和高质量的场景补全而被应用于3D LiDAR场景补全。然而，慢速采样速度限制了基于扩散的场景补全模型的实际应用，因为自动驾驶车辆需要高效感知周围环境。本文提出了一种新颖的蒸馏方法，称为ScoreLiDAR，旨在实现高效且高质量的场景补全。通过引入结构损失，ScoreLiDAR能够更好地捕捉3D LiDAR场景的几何结构，同时显著加快了每帧的补全时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03069",
            "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2412.03069",
            "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.",
            "score": 15,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "820e62e1bd498d55",
            "authors": [
                "Liao Qu",
                "Huichao Zhang",
                "Yiheng Liu",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Daniel K. Du",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "TokenFlow: единый токенизатор для понимания и генерации изображений",
                    "desc": "TokenFlow - это новый универсальный токенизатор изображений, объединяющий задачи мультимодального понимания и генерации. Он использует инновационную архитектуру с двойным кодбуком, которая разделяет обучение семантических и пиксельных признаков, сохраняя их выравнивание через общий механизм отображения. TokenFlow превосходит существующие модели в задачах понимания, достигая улучшения на 7.2% по сравнению с LLaVA-1.5 13B. Модель также показывает высокие результаты в реконструкции изображений и авторегрессивной генерации, достигая показателей на уровне современных моделей."
                },
                "en": {
                    "title": "TokenFlow: Bridging Understanding and Generation in Image Processing",
                    "desc": "TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics."
                },
                "zh": {
                    "title": "TokenFlow：多模态理解与生成的桥梁",
                    "desc": "本文介绍了一种新颖的图像标记器TokenFlow，它弥合了多模态理解与生成之间的差距。研究表明，理解和生成任务需要不同粒度的视觉信息，传统的单一重建目标向量量化编码器无法有效处理这一问题。TokenFlow通过创新的双代码本架构，解耦了语义和像素级特征学习，同时通过共享映射机制保持它们的对齐。实验结果表明，TokenFlow在多项任务中表现优越，首次证明离散视觉输入在理解性能上超越了LLaVA-1.5 13B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03517",
            "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
            "url": "https://huggingface.co/papers/2412.03517",
            "abstract": "Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.",
            "score": 13,
            "issue_id": 960,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "9d51bf0b60be344b",
            "authors": [
                "Lingen Li",
                "Zhaoyang Zhang",
                "Yaowei Li",
                "Jiale Xu",
                "Xiaoyu Li",
                "Wenbo Hu",
                "Weihao Cheng",
                "Jinwei Gu",
                "Tianfan Xue",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтез новых ракурсов без явного выравнивания видов",
                    "desc": "NVComposer - это новый подход к синтезу новых ракурсов, который устраняет необходимость во внешнем выравнивании видов. Он использует двухпоточную модель диффузии для одновременной генерации целевых ракурсов и позиций камер. Метод включает модуль выравнивания признаков с учетом геометрии, который извлекает геометрические закономерности из плотных стерео моделей во время обучения. Эксперименты показывают, что NVComposer достигает наилучших результатов в задачах генеративного многоракурсного синтеза новых видов."
                },
                "en": {
                    "title": "NVComposer: Generating Novel Views Without External Alignment",
                    "desc": "This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis."
                },
                "zh": {
                    "title": "NVComposer：无须外部对齐的生成新视图合成",
                    "desc": "最近生成模型的进展显著提升了多视图数据的新的视图合成（NVS）能力。然而，现有方法依赖于外部的多视图对齐过程，如显式的姿态估计或预重建，这限制了它们的灵活性和可访问性，尤其是在视图之间重叠不足或遮挡时对齐不稳定的情况下。本文提出了NVComposer，这是一种新颖的方法，消除了对显式外部对齐的需求。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系，从而在生成多视图NVS任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03205",
            "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs",
            "url": "https://huggingface.co/papers/2412.03205",
            "abstract": "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.   To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release mu-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.   The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on mu-MATH.",
            "score": 12,
            "issue_id": 968,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "8df63a02d444d462",
            "authors": [
                "Konstantin Chernyshev",
                "Vitaliy Polshkov",
                "Ekaterina Artemova",
                "Alex Myasnikov",
                "Vlad Stepanov",
                "Alexei Miasnikov",
                "Sergei Tilga"
            ],
            "affiliations": [
                "Gradarius",
                "Stevens Institute of Technology",
                "Toloka AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03205.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#math",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "U-MATH: новый рубеж в оценке математических способностей ИИ",
                    "desc": "Авторы представляют новый бенчмарк U-MATH для оценки математических способностей языковых моделей (LLM). Он содержит 1100 задач университетского уровня по шести основным предметам, включая 20% мультимодальных задач. Для оценки решений используется специально обученная языковая модель, для чего был создан датасет mu-MATH. Эксперименты показали, что современные LLM достигают точности лишь 63% на текстовых и 45% на визуальных задачах U-MATH, а лучшая модель-оценщик имеет F1-меру 80% на mu-MATH."
                },
                "en": {
                    "title": "U-MATH: Elevating Math Evaluation for LLMs",
                    "desc": "This paper presents U-MATH, a new benchmark designed to evaluate the mathematical skills of large language models (LLMs) using 1,100 open-ended university-level problems. The benchmark covers six core subjects and includes multimodal tasks, which incorporate visual elements, addressing the limitations of existing evaluations. To assess the performance of LLMs on these problems, the authors introduce mu-MATH, a dataset specifically for evaluating the correctness of solutions generated by LLMs. The results indicate that LLMs struggle with these tasks, achieving only 63% accuracy on text-based problems and 45% on visual ones, highlighting the need for further advancements in LLM capabilities."
                },
                "zh": {
                    "title": "U-MATH：提升LLMs数学能力评估的新基准",
                    "desc": "目前对大型语言模型（LLMs）数学技能的评估存在局限性，现有基准测试相对较小，主要集中在基础和高中问题上，且缺乏主题多样性。此外，任务中视觉元素的包含仍然未得到充分探索。为了解决这些问题，我们引入了U-MATH，这是一个包含1100个未发表的开放式大学级问题的新基准，涵盖六个核心学科，其中20%的问题为多模态问题。我们的研究表明，LLMs在文本任务上的最高准确率仅为63%，而在视觉问题上的准确率更低，仅为45%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00493",
            "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2412.00493",
            "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.",
            "score": 12,
            "issue_id": 964,
            "pub_date": "2024-11-30",
            "pub_date_card": {
                "ru": "30 ноября",
                "en": "November 30",
                "zh": "11月30日"
            },
            "hash": "10c214b548697656",
            "authors": [
                "Duo Zheng",
                "Shijia Huang",
                "Liwei Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00493.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Video-3D LLM: Прорыв в понимании трехмерных сцен",
                    "desc": "Статья представляет новую модель Video-3D LLM для понимания трехмерных сцен. Модель рассматривает 3D-сцены как динамические видео и использует 3D-позиционное кодирование для лучшего соответствия видеопредставлений реальным пространственным контекстам. Авторы применили технику выборки с максимальным покрытием для оптимизации баланса между вычислительными затратами и эффективностью. Эксперименты показывают, что модель достигает наилучших результатов на нескольких эталонных тестах по пониманию 3D-сцен."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Understanding with Video-3D LLM",
                    "desc": "This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding."
                },
                "zh": {
                    "title": "提升3D场景理解的创新模型",
                    "desc": "这篇论文介绍了一种新型的多模态大语言模型（MLLM），称为Video-3D LLM，旨在提高3D场景理解能力。传统的MLLM主要基于2D数据训练，导致它们在处理3D环境时存在局限性。通过将3D场景视为动态视频，并引入3D位置编码，Video-3D LLM能够更准确地对齐视频表示与现实世界的空间上下文。此外，论文还提出了一种最大覆盖采样技术，以优化计算成本和性能效率之间的平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19103",
            "title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.19103",
            "abstract": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.",
            "score": 11,
            "issue_id": 964,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "4507a3a2ac0bc8b5",
            "authors": [
                "Jeongho Ju",
                "Daeyoung Kim",
                "SunYoung Park",
                "Youngjune Kim"
            ],
            "affiliations": [
                "NC Research, NCSOFT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19103.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#multimodal",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "VARCO-VISION: Прорыв в двуязычном компьютерном зрении",
                    "desc": "В этой статье представлена новая мультиязычная модель компьютерного зрения VARCO-VISION для корейского и английского языков. Авторы применили пошаговую стратегию обучения, позволяющую модели усваивать как лингвистическую, так и визуальную информацию. VARCO-VISION демонстрирует высокую производительность в различных задачах, требующих двуязычного понимания и генерации текста и изображений. Модель также способна выполнять задачи локализации объектов, референции и оптического распознавания символов, что расширяет ее потенциальное применение в реальных сценариях."
                },
                "en": {
                    "title": "VARCO-VISION: Bridging Korean and English through Vision-Language Learning",
                    "desc": "This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area."
                },
                "zh": {
                    "title": "VARCO-VISION：双语视觉语言模型的新里程碑",
                    "desc": "本文介绍了一种开源的韩英视觉语言模型VARCO-VISION。我们采用逐步训练策略，使模型能够同时学习语言和视觉信息，同时保留基础模型的知识。与同类模型相比，VARCO-VISION在双语图像文本理解和生成能力方面表现出色。该模型还具备定位、引用和光学字符识别（OCR）功能，扩展了其在现实场景中的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03439",
            "title": "CleanDIFT: Diffusion Features without Noise",
            "url": "https://huggingface.co/papers/2412.03439",
            "abstract": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.",
            "score": 9,
            "issue_id": 963,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "cd474064bf17503a",
            "authors": [
                "Nick Stracke",
                "Stefan Andreas Baumann",
                "Kolja Bauer",
                "Frank Fundel",
                "Björn Ommer"
            ],
            "affiliations": [
                "CompVis @ LMU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03439.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение семантических признаков диффузионных моделей без шума",
                    "desc": "Исследователи обнаружили, что внутренние признаки, извлекаемые из предобученных диффузионных моделей, являются мощными семантическими дескрипторами для различных задач. Однако добавление шума к изображениям перед их обработкой моделью критически влияет на полезность этих признаков. Авторы предлагают метод легковесной неконтролируемой донастройки, позволяющий получать качественные семантические признаки без шума. Эти признаки значительно превосходят предыдущие подходы по эффективности в различных задачах при меньших вычислительных затратах."
                },
                "en": {
                    "title": "Unlocking Noise-Free Semantic Features from Diffusion Models",
                    "desc": "This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient."
                },
                "zh": {
                    "title": "无噪声的高质量语义特征提取",
                    "desc": "最近，大规模预训练扩散模型的内部特征被确立为强大的语义描述符，适用于多种下游任务。通常，这些特征需要在图像中添加噪声后才能提取，因为模型在处理几乎没有噪声的图像时，提供的特征效果不佳。我们发现噪声对特征的有效性有重要影响，且通过不同随机噪声的集成无法解决这个问题。为此，我们提出了一种轻量级的无监督微调方法，使扩散模型能够提供高质量、无噪声的语义特征，显著超越了之前的扩散特征。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02030",
            "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training",
            "url": "https://huggingface.co/papers/2412.02030",
            "abstract": "We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.",
            "score": 8,
            "issue_id": 966,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "4c749ff913210111",
            "authors": [
                "Dar-Yen Chen",
                "Hmrishav Bandyopadhyay",
                "Kai Zou",
                "Yi-Zhe Song"
            ],
            "affiliations": [
                "NetMind.AI",
                "SketchX, CVSSP, University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02030.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "NitroFusion: Революция в одношаговой генерации изображений",
                    "desc": "NitroFusion - это новый подход к одношаговой диффузии, использующий динамическую состязательную структуру для генерации высококачественных изображений. Метод применяет большой пул специализированных дискриминаторов, каждый из которых фокусируется на определенном аспекте качества изображения на разных уровнях шума. NitroFusion включает механизмы обновления дискриминаторов для предотвращения переобучения и глобально-локальные головки дискриминаторов для многомасштабной оценки качества. Подход позволяет гибко выбирать от 1 до 4 шагов денойзинга, обеспечивая компромисс между качеством и скоростью."
                },
                "en": {
                    "title": "NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators",
                    "desc": "NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used."
                },
                "zh": {
                    "title": "NitroFusion：高效与高质量生成的完美结合",
                    "desc": "NitroFusion是一种全新的单步扩散生成方法，通过动态对抗框架实现高质量生成。与传统的单步方法相比，NitroFusion在生成质量上有显著提升，尽管单步方法在速度上具有优势。该方法利用多个专业的判别器组，针对不同的噪声水平提供多样化的反馈，从而提高生成的保真度。通过灵活的部署机制，用户可以根据需要在1到4个去噪步骤之间动态选择，实现质量与速度的平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03085",
            "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
            "url": "https://huggingface.co/papers/2412.03085",
            "abstract": "Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/",
            "score": 7,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "a065164e5fdadf2c",
            "authors": [
                "Shuai Tan",
                "Biao Gong",
                "Yutong Feng",
                "Kecheng Zheng",
                "Dandan Zheng",
                "Shuwei Shi",
                "Yujun Shen",
                "Jingdong Chen",
                "Ming Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03085.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Mimir: Улучшение генерации видео с помощью больших языковых моделей",
                    "desc": "Эта статья представляет Mimir - новый подход к генерации видео на основе текста. Авторы предлагают использовать большие языковые модели (LLM) для улучшения понимания текста и воображения в процессе генерации. Ключевой элемент Mimir - специальный 'token fuser', который объединяет выходы текстовых энкодеров и LLM. Результаты показывают, что Mimir эффективен в создании качественных видео с хорошим пониманием текста, особенно для коротких описаний и динамичных сцен."
                },
                "en": {
                    "title": "Mimir: Bridging Text Understanding and Video Generation",
                    "desc": "This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements."
                },
                "zh": {
                    "title": "Mimir：提升文本到视频生成的智能",
                    "desc": "本文提出了一种名为Mimir的端到端训练框架，用于文本到视频生成（T2V）。该框架通过精心设计的令牌融合器，解决了文本编码器与大型语言模型（LLMs）之间的特征分布差距。Mimir能够充分利用学习到的视频先验，同时增强LLMs在文本理解方面的能力。实验结果表明，Mimir在生成高质量视频时，尤其在处理短文本和动态变化时，表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03558",
            "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
            "url": "https://huggingface.co/papers/2412.03558",
            "abstract": "This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.",
            "score": 7,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "5e1a4c1e1017e7af",
            "authors": [
                "Zehuan Huang",
                "Yuan-Chen Guo",
                "Xingqiao An",
                "Yunhan Yang",
                "Yangguang Li",
                "Zi-Xin Zou",
                "Ding Liang",
                "Xihui Liu",
                "Yan-Pei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "The University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03558.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#diffusion",
                    "#training",
                    "#3d"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "MIDI: Революционный подход к генерации 3D-сцен из одного изображения",
                    "desc": "Статья представляет MIDI - новый подход к генерации трехмерных сцен из одного изображения. MIDI использует предобученные модели генерации 3D-объектов и расширяет их до многоэкземплярных диффузионных моделей, позволяя одновременно генерировать несколько 3D-объектов с точными пространственными отношениями. Ключевой особенностью является новый механизм многоэкземплярного внимания, который эффективно учитывает взаимодействия между объектами и пространственную согласованность непосредственно в процессе генерации. MIDI демонстрирует передовые результаты в генерации сцен из изображений, что подтверждается оценками на синтетических данных, реальных сценах и стилизованных изображениях."
                },
                "en": {
                    "title": "MIDI: Revolutionizing 3D Scene Generation from Single Images",
                    "desc": "This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes."
                },
                "zh": {
                    "title": "MIDI：从单图像生成3D场景的新方法",
                    "desc": "本文介绍了一种名为MIDI的新方法，用于从单张图像生成组合3D场景。与现有依赖重建或检索技术的方法不同，MIDI扩展了预训练的图像到3D对象生成模型，采用多实例扩散模型，实现了多个3D实例的同时生成。MIDI的核心是一个新颖的多实例注意机制，能够有效捕捉对象间的交互和空间一致性，简化了生成过程。该方法在图像到场景生成方面表现出色，经过合成数据、真实场景数据和文本到图像扩散模型生成的风格化场景图像的评估验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01106",
            "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
            "url": "https://huggingface.co/papers/2412.01106",
            "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.",
            "score": 7,
            "issue_id": 957,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13d96f9bb346e344",
            "authors": [
                "Jun Xiang",
                "Yudong Guo",
                "Leipeng Hu",
                "Boyang Guo",
                "Yancheng Yuan",
                "Juyong Zhang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01106.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Реалистичный говорящий аватар из одного фото",
                    "desc": "Статья описывает новый метод создания реалистичных аватаров, способных говорить и двигаться, на основе всего одного изображения. Авторы используют диффузионные модели для генерации псевдо-видео кадров, которые служат обучающими данными. Они предлагают гибридное представление аватара, сочетающее 3D гауссовы сплаты и полигональную сетку. Метод позволяет точно контролировать жесты и мимику аватара, преодолевая ограничения существующих подходов."
                },
                "en": {
                    "title": "From One Image to a Lifelike Talking Avatar!",
                    "desc": "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."
                },
                "zh": {
                    "title": "从单张图像生成全身会说话的虚拟头像",
                    "desc": "本文提出了一种从单张图像构建全身会说话的虚拟头像的方法。我们解决了复杂动态建模和对新手势与表情的泛化这两个关键问题。通过使用姿态引导的图像到视频扩散模型，我们生成了不完美的视频帧作为伪标签，以实现无缝泛化。实验结果表明，我们的方法能够从单张图像创建出逼真、可精确动画和富有表现力的全身虚拟头像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03187",
            "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
            "url": "https://huggingface.co/papers/2412.03187",
            "abstract": "While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.",
            "score": 4,
            "issue_id": 961,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6da11fbf4e1ea7d9",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Tianyuan Shi",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "WRPO: Эффективное слияние языковых моделей без прямого объединения параметров",
                    "desc": "В этой статье предлагается новый метод слияния разнородных языковых моделей с открытым исходным кодом - Weighted-Reward Preference Optimization (WRPO). WRPO использует оптимизацию предпочтений между исходными и целевой моделями для эффективного переноса их возможностей, устраняя необходимость в выравнивании словарей и слиянии матриц распределения. Метод вводит стратегию прогрессивной адаптации для решения проблемы различий в распределениях между моделями. Эксперименты показывают, что WRPO превосходит существующие методы слияния знаний и базовые подходы к дообучению на нескольких бенчмарках."
                },
                "en": {
                    "title": "Effortless Fusion of LLMs with WRPO!",
                    "desc": "This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "加权奖励偏好优化：高效融合多种大语言模型",
                    "desc": "本论文提出了一种隐式融合方法，称为加权奖励偏好优化（WRPO），旨在有效整合不同架构和规模的开源大语言模型（LLMs）。WRPO通过优化源模型与目标模型之间的偏好，避免了词汇对齐和矩阵融合的复杂性。该方法引入了渐进适应策略，逐步调整对目标模型和源模型的依赖，从而解决了分布偏差问题。实验结果表明，WRPO在多个基准测试中表现优于现有的知识融合方法和微调基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02980",
            "title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models",
            "url": "https://huggingface.co/papers/2412.02980",
            "abstract": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.",
            "score": 3,
            "issue_id": 968,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "8055d4be8211be80",
            "authors": [
                "Alex Havrilla",
                "Andrew Dai",
                "Laura O'Mahony",
                "Koen Oostermeijer",
                "Vera Zisler",
                "Alon Albalak",
                "Fabrizio Milo",
                "Sharath Chandra Raparthy",
                "Kanishk Gandhi",
                "Baber Abbasi",
                "Duy Phung",
                "Maia Iyer",
                "Dakota Mahan",
                "Chase Blagden",
                "Srishti Gureja",
                "Mohammed Hamdy",
                "Wen-Ding Li",
                "Giovanni Paolini",
                "Pawan Sasanka Ammanamanchi",
                "Elliot Meyerson"
            ],
            "affiliations": [
                "Aleph Alpha @ IPAI",
                "Cognizant AI Labs",
                "Cohere for AI Community",
                "Cornell University",
                "Eleuther AI",
                "Georgia Tech",
                "IBM",
                "Independent",
                "Reka AI",
                "Sakana AI",
                "Stanford University",
                "SynthLabs",
                "University of Bologna",
                "University of Limerick"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02980.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#optimization",
                    "#training",
                    "#data",
                    "#dataset",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс качества и разнообразия в синтетических данных для ИИ",
                    "desc": "Статья рассматривает генерацию синтетических данных с помощью больших языковых моделей для расширения естественных данных в различных задачах. Авторы предлагают оценивать алгоритмы по качеству, разнообразию и сложности генерируемых данных. Исследование показывает важность баланса этих характеристик для эффективного обучения с подкреплением и алгоритмов самоулучшения. Авторы отмечают, что многие модели оптимизируются только по качеству выходных данных, ограничивая их разнообразие и потенциал самоулучшения."
                },
                "en": {
                    "title": "Balancing Quality, Diversity, and Complexity in Synthetic Data Generation",
                    "desc": "This paper discusses the generation of synthetic data using Large Language Models (LLMs) and its importance in enhancing natural data for various tasks. It evaluates synthetic data generation algorithms based on three key characteristics: quality, diversity, and complexity, which are crucial for the performance of downstream models. The authors highlight the trade-offs between these characteristics, noting that while quality is vital for in-distribution generalization, diversity is necessary for out-of-distribution generalization. The paper emphasizes the need for a balanced approach to these trade-offs to improve the effectiveness of reinforcement learning and self-improvement algorithms."
                },
                "zh": {
                    "title": "合成数据生成：平衡质量与多样性",
                    "desc": "本论文探讨了使用大型语言模型生成合成数据的潜力，强调了合成数据在自然数据增强中的重要性。我们提出通过数据质量、多样性和复杂性来评估合成数据生成算法，这三者对下游模型的能力有显著影响。研究发现，数据质量对模型的分布内泛化至关重要，而多样性则对分布外泛化至关重要，复杂性对两者都有益。我们还强调了训练数据中的质量-多样性权衡及其对模型性能的影响，认为在未来的自我改进算法中平衡这些权衡是至关重要的。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03565",
            "title": "Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning",
            "url": "https://huggingface.co/papers/2412.03565",
            "abstract": "Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.",
            "score": 3,
            "issue_id": 967,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "72af31b504d0aac1",
            "authors": [
                "Wujian Peng",
                "Lingchen Meng",
                "Yitong Chen",
                "Yiweng Xie",
                "Yang Liu",
                "Tao Gui",
                "Hang Xu",
                "Xipeng Qiu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "School of Computer Science, Fudan University",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03565.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#alignment",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение понимания экземпляров в LMM с помощью явных визуальных подсказок",
                    "desc": "Статья представляет Inst-IT - решение для улучшения понимания экземпляров в больших мультимодальных моделях (LMM) с помощью явных визуальных подсказок. Авторы разработали автоматизированный конвейер аннотаций с использованием GPT-4 для извлечения информации на уровне экземпляров из изображений и видео. Inst-IT включает в себя эталонный тест для диагностики мультимодального понимания на уровне экземпляров, большой набор данных для обучения с инструкциями и парадигму непрерывного обучения. Экспериментальные результаты показывают значительное улучшение как в понимании экземпляров, так и в общем понимании изображений и видео."
                },
                "en": {
                    "title": "Enhancing Instance Understanding in Multimodal Models with Inst-IT",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."
                },
                "zh": {
                    "title": "提升实例理解能力的创新方法",
                    "desc": "大型多模态模型（LMMs）在指令调优方面取得了显著突破，但在实例级理解上仍然存在挑战。实例级理解关注特定元素，这对于深入理解图像和视频至关重要。我们提出了一种自动注释管道，利用GPT-4o通过明确的视觉提示提取实例级信息。基于此，我们开发了Inst-IT，通过明确的视觉提示指令调优来增强LMMs的实例理解能力，实验结果表明，Inst-IT显著提升了模型在多种图像和视频理解基准上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00177",
            "title": "LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting",
            "url": "https://huggingface.co/papers/2412.00177",
            "abstract": "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.",
            "score": 1,
            "issue_id": 969,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "210b042d1a430116",
            "authors": [
                "Xiaoyan Xing",
                "Konrad Groh",
                "Sezer Karaoglu",
                "Theo Gevers",
                "Anand Bhattad"
            ],
            "affiliations": [
                "BCAI-Bosch",
                "Toyota Technological Institute at Chicago",
                "UvA-Bosch Delta Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00177.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#cv",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "LumiNet: Реалистичный перенос освещения с помощью латентных представлений",
                    "desc": "LumiNet - это новая архитектура для переноса освещения между изображениями, использующая генеративные модели и латентные представления. Она включает стратегию подготовки данных на основе StyleGAN и модифицированную версию ControlNet, обрабатывающую латентные свойства исходного и целевого изображений. LumiNet дополнительно улучшает перенос освещения с помощью обученного адаптера, внедряющего латентные внешние свойства целевого изображения. Эксперименты показывают, что метод успешно переносит сложные световые эффекты между сценами с различной геометрией и материалами."
                },
                "en": {
                    "title": "LumiNet: Mastering Lighting Transfer with Generative Models",
                    "desc": "LumiNet is a new machine learning architecture designed for transferring lighting effects from one image to another. It uses generative models to create a relit version of a source image by applying the lighting characteristics of a target image. The model incorporates a unique data curation strategy and a modified diffusion-based ControlNet that processes both intrinsic and extrinsic properties of the images. By utilizing cross-attention and fine-tuning, LumiNet effectively captures complex lighting phenomena, outperforming traditional methods in challenging indoor environments."
                },
                "zh": {
                    "title": "LumiNet：高效光照转移的新方法",
                    "desc": "LumiNet是一种新颖的架构，利用生成模型和潜在内在表示来实现有效的光照转移。该方法通过输入源图像和目标光照图像，合成出一个捕捉目标光照的重新照明版本。LumiNet的两个关键贡献包括基于StyleGAN的重新照明模型的数据整理策略，以及处理源图像的潜在内在属性和目标图像的潜在外在属性的改进扩散控制网络。通过交叉注意力和微调，LumiNet进一步通过学习适配器（MLP）注入目标的潜在外在属性，从而改善光照转移效果。"
                }
            }
        }
    ],
    "link_prev": "2024-12-04.html",
    "link_next": "2024-12-06.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "short_date_next": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 0,
        "#cv": 9,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 5,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 13,
        "#survey": 0,
        "#diffusion": 9,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。",
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "pinyin": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de kuàngjià SNOOPI, zhǐ zài gǎijìn dān bù kuòsàn móxíng de zhǐdǎo jīzhì. Xiànyǒu fāngfǎ zài chǔlǐ bùtóng kuòsàn móxíng gǔjià shí biǎoxiàn bùículai, qiě bù zhīchí fùmiàn tíshì zhǐdǎo. SNOOPI tōngguò PG-SB hé NASA liǎng zhǒng fāngfǎ jiějué le zhèxiē wèntí. Shíyàn jiéguǒ xiǎnshì, SNOOPI xiǎnzhù tíshēng le jīzhǔn móxíng de xíngnéng, dá dào le xīn de zuìjiā shuǐpíng.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '改进', 'pinyin': 'gǎijìn', 'trans': 'improve'},\n{'word': '单步', 'pinyin': 'dānbù', 'trans': 'single-step'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '指导', 'pinyin': 'zhǐdǎo', 'trans': 'guidance'},\n{'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': '处理', 'pinyin': 'chǔlǐ', 'trans': 'handle'},\n{'word': '不同', 'pinyin': 'bùtóng', 'trans': 'different'},\n{'word': '骨架', 'pinyin': 'gǔjià', 'trans': 'skeleton'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '不稳定', 'pinyin': 'bùwěndìng', 'trans': 'unstable'},\n{'word': '且', 'pinyin': 'qiě', 'trans': 'and'},\n{'word': '不支持', 'pinyin': 'bù zhīchí', 'trans': 'not support'},\n{'word': '负面', 'pinyin': 'fùmiàn', 'trans': 'negative'},\n{'word': '提示', 'pinyin': 'tíshì', 'trans': 'prompt'},\n{'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through'},\n{'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'},\n{'word': 'NASA', 'pinyin': '', 'trans': 'NASA'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'},\n{'word': '这些', 'pinyin': 'zhèxiē', 'trans': 'these'},\n{'word': '问题', 'pinyin': 'wèntí', 'trans': 'problems'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'},\n{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'},\n{'word': '达到', 'pinyin': 'dádào', 'trans': 'reach'},\n{'word': '新的', 'pinyin': 'xīn de', 'trans': 'new'},\n{'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'},\n{'word': '水平', 'pinyin': 'shuǐpíng', 'trans': 'level'}]",
        "trans": "This article introduces a new framework called SNOOPI, aimed at improving the guidance mechanism of single-step diffusion models. Existing methods perform unstably when handling different diffusion model backbones and do not support negative prompt guidance. SNOOPI addresses these issues through the PG-SB and NASA methods. Experimental results demonstrate that SNOOPI significantly enhances the performance of benchmark models, achieving new best-in-class levels.",
        "update_ts": "2024-12-05 09:11"
    }
}