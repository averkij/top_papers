{
    "date": {
        "ru": "7 августа",
        "en": "August 7",
        "zh": "8月7日"
    },
    "time_utc": "2025-08-07 08:18",
    "weekday": 3,
    "issue_id": 5225,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.04026",
            "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
            "url": "https://huggingface.co/papers/2508.04026",
            "abstract": "VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.",
            "score": 49,
            "issue_id": 5221,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "2692487ee60e017e",
            "authors": [
                "Shunyu Liu",
                "Minghao Liu",
                "Huichi Zhou",
                "Zhenyu Cui",
                "Yang Zhou",
                "Yuhao Zhou",
                "Wendong Fan",
                "Ge Zhang",
                "Jiajun Shi",
                "Weihao Xuan",
                "Jiaxing Huang",
                "Shuang Luo",
                "Fang Wu",
                "Heli Qi",
                "Qingcheng Zeng",
                "Ziqi Ren",
                "Jialiang Gao",
                "Jindi Lv",
                "Junjie Wang",
                "Aosong Feng",
                "Heng Zhou",
                "Wangchunshu Zhou",
                "Zhenfei Yin",
                "Wenlong Zhang",
                "Guohao Li",
                "Wenhao Yu",
                "Irene Li",
                "Lei Ma",
                "Lei Bai",
                "Qunshu Lin",
                "Mingli Song",
                "Dacheng Tao"
            ],
            "affiliations": [
                "VeriGUI Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04026.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "VeriGUI: Новый стандарт для оценки долгосрочных GUI-агентов",
                    "desc": "VeriGUI - это новый набор данных для оценки GUI-агентов в долгосрочных задачах, акцентирующий внимание на сложности длинных цепочек действий и верифицируемости на уровне подзадач. Датасет состоит из траекторий GUI-задач для десктопных и веб-приложений, аннотированных экспертами. Он позволяет разрабатывать и оценивать универсальные GUI-агенты, работающие в реалистичных компьютерных средах. Эксперименты на VeriGUI с использованием различных агентов и языковых моделей выявили значительные пробелы в производительности при работе с долгосрочными задачами."
                },
                "en": {
                    "title": "Empowering GUI Agents for Complex Tasks with VeriGUI",
                    "desc": "VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making."
                },
                "zh": {
                    "title": "VeriGUI：长时间任务中的智能体评估新标准",
                    "desc": "VeriGUI是一个新颖的数据集，用于评估在长时间任务中执行图形用户界面（GUI）操作的智能体。该数据集强调了长链复杂性和子任务级可验证性，允许任务被分解为数百个相互依赖的子任务。通过这种方式，任何子任务都可以作为有效的起点，促进了多样化的探索策略。实验结果显示，现有智能体在处理长时间任务时存在显著性能差距，表明需要更强大的规划和决策能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04700",
            "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
            "url": "https://huggingface.co/papers/2508.04700",
            "abstract": "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.",
            "score": 28,
            "issue_id": 5221,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "6c7450255f7c28bc",
            "authors": [
                "Zeyi Sun",
                "Ziyu Liu",
                "Yuhang Zang",
                "Yuhang Cao",
                "Xiaoyi Dong",
                "Tong Wu",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04700.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#open_source",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самообучающиеся агенты для освоения нового ПО",
                    "desc": "SEAgent - это фреймворк для автономного обучения агентов использованию компьютерного программного обеспечения. Он позволяет агентам исследовать новое ПО через экспериментальное обучение и выполнение автоматически генерируемых заданий возрастающей сложности. SEAgent использует модель состояния мира для оценки траектории действий и генератор учебного плана для создания разнообразных задач. Обновление политики агента происходит через имитационное обучение на неудачных действиях и оптимизацию на успешных, что позволяет достичь значительного улучшения производительности по сравнению с существующими методами."
                },
                "en": {
                    "title": "Empowering Agents to Learn and Evolve Autonomously",
                    "desc": "SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments."
                },
                "zh": {
                    "title": "SEAgent：自主进化的智能体框架",
                    "desc": "SEAgent是一种自我进化的智能体框架，能够使计算机使用代理通过体验学习自主掌握新软件。该框架通过逐步的任务课程，帮助代理在没有人类标注的情况下，探索和学习陌生的软件环境。SEAgent设计了世界状态模型和课程生成器，以便代理能够通过试错学习不断提高其能力。最终，SEAgent的表现超越了多个专门化代理的组合，显示出其在新软件环境中的优越性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01191",
            "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
            "url": "https://huggingface.co/papers/2508.01191",
            "abstract": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",
            "score": 24,
            "issue_id": 5220,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 августа",
                "en": "August 2",
                "zh": "8月2日"
            },
            "hash": "427ac75c7123b50a",
            "authors": [
                "Chengshuai Zhao",
                "Zhen Tan",
                "Pingchuan Ma",
                "Dawei Li",
                "Bohan Jiang",
                "Yancheng Wang",
                "Yingzhen Yang",
                "Huan Liu"
            ],
            "affiliations": [
                "Arizona State University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01191.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Ограниченность CoT-рассуждений в LLM: мираж вне распределения обучающих данных",
                    "desc": "Исследование показывает, что рассуждения по цепочке (CoT) в больших языковых моделях (LLM) ограничены расхождением между распределениями обучающих и тестовых данных. Авторы разработали среду DataAlchemy для изучения CoT-рассуждений по трем измерениям: задача, длина и формат. Результаты демонстрируют, что CoT-рассуждения неустойчивы и исчезают при выходе за пределы распределения обучающих данных. Это исследование подчеркивает сложность достижения подлинных и обобщаемых рассуждений в LLM."
                },
                "en": {
                    "title": "Unmasking the Fragility of CoT Reasoning in LLMs",
                    "desc": "This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs."
                },
                "zh": {
                    "title": "链式思维推理的局限性与挑战",
                    "desc": "本文探讨了链式思维（CoT）推理在大型语言模型（LLM）中的局限性，特别是训练数据与测试数据之间的分布差异对其影响。研究表明，CoT推理并不是一种稳健的推理形式，因为它的有效性受到训练数据和测试查询之间分布差异的限制。通过设计一个名为DataAlchemy的控制环境，作者系统性地分析了CoT推理在不同任务、长度和格式下的表现。结果显示，当CoT推理超出训练分布时，其效果会显著下降，揭示了实现真正可推广推理的持续挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02694",
            "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
            "url": "https://huggingface.co/papers/2508.02694",
            "abstract": "A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.",
            "score": 19,
            "issue_id": 5222,
            "pub_date": "2025-07-24",
            "pub_date_card": {
                "ru": "24 июля",
                "en": "July 24",
                "zh": "7月24日"
            },
            "hash": "0c518e4f5949dae3",
            "authors": [
                "Ningning Wang",
                "Xavier Hu",
                "Pai Liu",
                "He Zhu",
                "Yue Hou",
                "Heyuan Huang",
                "Shengyu Zhang",
                "Jian Yang",
                "Jiaheng Liu",
                "Ge Zhang",
                "Changwang Zhang",
                "Jun Wang",
                "Yuchen Eleanor Jiang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "OPPO-PersonalAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02694.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Оптимизация агентских систем: баланс эффективности и затрат",
                    "desc": "Исследование посвящено анализу компромисса между эффективностью и результативностью в системах агентов, управляемых большими языковыми моделями (LLM). Авторы изучают оптимальную структуру агентских фреймворков для снижения затрат при сохранении производительности. Проводится эмпирический анализ на бенчмарке GAIA, оценивающий влияние выбора LLM, дизайна фреймворков и стратегий масштабирования. На основе результатов разработан новый фреймворк Efficient Agents, сохраняющий 96.7% производительности ведущего open-source решения при снижении операционных затрат на 28.4%."
                },
                "en": {
                    "title": "Balancing Cost and Performance in AI Agents",
                    "desc": "This paper explores how to balance efficiency and effectiveness in systems powered by Large Language Models (LLMs). It identifies the optimal design for agent frameworks that can lower costs while still performing well. The study answers key questions about task complexity, the diminishing returns of adding modules, and how to enhance efficiency through better design. The results show that the proposed Efficient Agents framework maintains high performance while significantly reducing operational costs, making AI solutions more accessible and sustainable."
                },
                "zh": {
                    "title": "高效代理系统：降低成本与保持性能的平衡",
                    "desc": "本研究探讨了大型语言模型（LLM）驱动的代理系统中的效率与效果之间的权衡，旨在设计出既能降低成本又能保持性能的最佳代理框架。我们分析了代理任务的复杂性、额外模块的边际效益以及通过高效代理框架设计所能获得的效率提升。通过对GAIA基准的实证分析，我们评估了LLM骨干选择、代理框架设计和测试时扩展策略的影响。研究结果表明，开发高效代理系统可以在保持高性能的同时显著降低运营成本，推动AI解决方案的可及性和可持续性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03159",
            "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
            "url": "https://huggingface.co/papers/2508.03159",
            "abstract": "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  \t\t\t\t\tAI-generated summary \t\t\t\t Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.",
            "score": 17,
            "issue_id": 5222,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "229c53307b839ab7",
            "authors": [
                "Jueon Park",
                "Yein Park",
                "Minju Song",
                "Soyon Park",
                "Donghyeon Lee",
                "Seungheun Baek",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences, Seoul 04778, Republic of Korea",
                "Department of Computer Science and Engineering, Korea University, Seoul 17035, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03159.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#interpretability",
                    "#healthcare",
                    "#reasoning",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "CoTox: Интеллектуальное прогнозирование токсичности лекарств с помощью LLM",
                    "desc": "CoTox - это новая система, которая объединяет большие языковые модели (LLM) с рассуждениями по цепочке мыслей для прогнозирования множественной токсичности лекарств. Она использует данные о химической структуре, биологических путях и термины генной онтологии для генерации интерпретируемых предсказаний токсичности. CoTox превосходит традиционные методы машинного обучения и глубокого обучения, улучшая интерпретируемость и предсказательную способность. Система демонстрирует потенциал для улучшения оценки безопасности лекарств на ранних стадиях разработки."
                },
                "en": {
                    "title": "CoTox: Enhancing Drug Toxicity Prediction with LLMs and Chain-of-Thought Reasoning",
                    "desc": "CoTox is a new framework that combines large language models (LLMs) with chain-of-thought reasoning to predict multi-toxicity in drugs. It enhances the prediction process by integrating chemical structure data, biological pathways, and gene ontology terms, which helps in making the predictions more interpretable. By using step-by-step reasoning, CoTox improves the model's ability to understand complex biological mechanisms and organ-specific toxicities. The framework has shown superior performance compared to traditional machine learning and deep learning models, making it a valuable tool for early-stage drug safety assessment."
                },
                "zh": {
                    "title": "CoTox：提升药物毒性预测的智能框架",
                    "desc": "CoTox是一个将大型语言模型（LLMs）与链式推理相结合的框架，旨在提高多种毒性预测的准确性。它通过整合化学结构数据、生物通路和基因本体术语，生成可解释的毒性预测。与传统的机器学习和深度学习模型相比，CoTox在药物开发中表现出更好的预测性能。该框架的设计使得毒性预测与生理反应相一致，展示了LLM框架在药物安全性评估中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03680",
            "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.03680",
            "abstract": "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.",
            "score": 16,
            "issue_id": 5221,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "78a8398db0f71f63",
            "authors": [
                "Xufang Luo",
                "Yuge Zhang",
                "Zhiyuan He",
                "Zilong Wang",
                "Siyun Zhao",
                "Dongsheng Li",
                "Luna K. Qiu",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03680.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rag",
                    "#games",
                    "#math",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Agent Lightning: универсальный фреймворк для обучения ИИ-агентов",
                    "desc": "Agent Lightning - это гибкий фреймворк для обучения с подкреплением больших языковых моделей в различных агентах. Он использует иерархический алгоритм обучения с подкреплением и отделяет выполнение от обучения для обработки сложных взаимодействий. Фреймворк позволяет интегрироваться с существующими агентами, разработанными различными способами, практически без изменения кода. Эксперименты показали стабильные улучшения в задачах text-to-SQL, генерации с использованием извлечения информации и использования математических инструментов."
                },
                "en": {
                    "title": "Decoupling Training and Execution for Enhanced AI Agent Performance",
                    "desc": "Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications."
                },
                "zh": {
                    "title": "Agent Lightning：灵活的智能体训练框架",
                    "desc": "Agent Lightning是一个灵活的强化学习框架，旨在为各种智能体训练大型语言模型（LLMs）。它通过将执行与训练解耦，使用层次化的强化学习算法，能够处理复杂的交互逻辑。该框架允许与现有智能体的无缝集成，几乎不需要代码修改。实验结果表明，Agent Lightning在文本到SQL、增强生成和数学工具使用等任务中表现出稳定的持续改进，展示了其在实际智能体训练和部署中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03905",
            "title": "Sotopia-RL: Reward Design for Social Intelligence",
            "url": "https://huggingface.co/papers/2508.03905",
            "abstract": "Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.",
            "score": 11,
            "issue_id": 5220,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "815208c1d75b8f05",
            "authors": [
                "Haofei Yu",
                "Zhengyang Qi",
                "Yining Zhao",
                "Kolby Nottingham",
                "Keyang Xuan",
                "Bodhisattwa Prasad Majumder",
                "Hao Zhu",
                "Paul Pu Liang",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Carnegie Mellon University",
                "Massachusetts Institute of Technology",
                "Stanford University",
                "University of California Irvine",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03905.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#alignment",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Sotopia-RL: прорыв в обучении социальному интеллекту для ИИ",
                    "desc": "Sotopia-RL - это новая система обучения с подкреплением для улучшения социального интеллекта больших языковых моделей. Она преобразует обратную связь в многомерные награды на уровне отдельных высказываний, что позволяет эффективнее обучать модели социальным задачам. Система решает проблемы частичной наблюдаемости и многомерности социальных взаимодействий, которые затрудняют применение классических методов обучения с подкреплением. Эксперименты показали, что Sotopia-RL значительно превосходит существующие подходы в решении социальных задач."
                },
                "en": {
                    "title": "Enhancing Social Intelligence in LLMs with Sotopia-RL",
                    "desc": "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."
                },
                "zh": {
                    "title": "提升社交智能的强化学习新框架",
                    "desc": "Sotopia-RL是一种新颖的强化学习框架，旨在提升大型语言模型的社交智能。它通过将反馈细化为发言级别的多维奖励，来改善模型在社交任务中的表现。该框架解决了社交互动中的部分可观察性和多维性问题，使得模型能够更有效地学习复杂的社交策略。实验结果表明，Sotopia-RL在社交目标完成评分上达到了最先进的水平，显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03560",
            "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
            "url": "https://huggingface.co/papers/2508.03560",
            "abstract": "LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.",
            "score": 9,
            "issue_id": 5221,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "14b52fe9021b5b26",
            "authors": [
                "Yi Gui",
                "Zhen Li",
                "Zhongyi Zhang",
                "Guohao Wang",
                "Tianpeng Lv",
                "Gaoyang Jiang",
                "Yi Liu",
                "Dongping Chen",
                "Yao Wan",
                "Hongyu Zhang",
                "Wenbin Jiang",
                "Xuanhua Shi",
                "Hai Jin"
            ],
            "affiliations": [
                "Chongqing University, Chongqing, China",
                "Huazhong University of Science and Technology, Wuhan, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03560.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#architecture",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "LaTCoder: улучшение сохранения макета при генерации кода из дизайна веб-страниц",
                    "desc": "LaTCoder - это новый подход к задаче преобразования дизайна веб-страниц в код, который улучшает сохранение макета. Метод использует разделение дизайна на блоки и применяет рассуждения по цепочке мыслей (Chain-of-Thought) с помощью мультимодальных больших языковых моделей (MLLM). LaTCoder показывает значительные улучшения по автоматическим метрикам, таким как TreeBLEU и MAE. В ходе оценки предпочтений пользователей, веб-страницы, сгенерированные LaTCoder, были выбраны в более чем 60% случаев."
                },
                "en": {
                    "title": "Enhancing Layout Preservation in Design-to-Code with LaTCoder",
                    "desc": "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."
                },
                "zh": {
                    "title": "提升网页设计布局保留的LaTCoder",
                    "desc": "LaTCoder是一种新方法，旨在提高网页设计到代码生成过程中的布局保留能力。它通过将网页设计分割成多个图像块，并使用基于思维链的推理方法来生成每个块的代码。该方法结合了绝对定位和基于多模态大语言模型的组装策略，以选择最佳输出。实验结果表明，LaTCoder在多个基准测试中显著提高了自动评估指标和人类偏好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01858",
            "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
            "url": "https://huggingface.co/papers/2508.01858",
            "abstract": "A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner",
            "score": 9,
            "issue_id": 5220,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 августа",
                "en": "August 3",
                "zh": "8月3日"
            },
            "hash": "ad30239b3abef884",
            "authors": [
                "Yuhan Guo",
                "Cong Guo",
                "Aiwen Sun",
                "Hongliang He",
                "Xinyu Yang",
                "Yue Lu",
                "Yingji Zhang",
                "Xuntao Guo",
                "Dong Zhang",
                "Jianzhuang Liu",
                "Jiang Duan",
                "Yijia Xiao",
                "Liangjian Wen",
                "Hai-Ming Xu",
                "Yong Dai"
            ],
            "affiliations": [
                "Central South University",
                "Fudan University",
                "Harbin Institute of Technology",
                "Hithink Research",
                "Shanghai Jiao Tong University",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Southwestern University of Finance and Economics",
                "University of Adelaide",
                "University of California, Los Angeles",
                "University of Manchester",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01858.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Когнитивная структура для веб-агентов нового поколения",
                    "desc": "Статья представляет новую структуру для веб-агентов, разделяющую их возможности на изучение контента знаний и когнитивные процессы. Авторы предлагают Web-CogKnowledge Framework, категоризирующий знания как фактические, концептуальные и процедурные. Для облегчения приобретения знаний создан структурированный набор данных Web-CogDataset из 14 реальных веб-сайтов. Разработан новый агент Web-CogReasoner, использующий знание-ориентированную цепочку рассуждений (Chain-of-Thought), показывающий превосходство над существующими моделями."
                },
                "en": {
                    "title": "Empowering Web Agents through Structured Knowledge and Reasoning",
                    "desc": "This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models."
                },
                "zh": {
                    "title": "智能体能力的双重分解：知识与认知",
                    "desc": "本文提出了一种网络智能体的框架，将其能力分解为知识内容学习和认知过程。我们定义了Web-CogKnowledge框架，将知识分为事实性、概念性和程序性三类，以支持智能体的学习和推理。通过构建Web-CogDataset，我们为智能体提供了系统化的知识基础，帮助其掌握必要的核心知识。最后，我们开发了Web-CogReasoner，并通过实验验证了其在处理新任务时的优越性，尤其是在结构化知识至关重要的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03789",
            "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
            "url": "https://huggingface.co/papers/2508.03789",
            "abstract": "HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.",
            "score": 8,
            "issue_id": 5220,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "a2a0678cfc88e0ef",
            "authors": [
                "Yuhang Ma",
                "Xiaoshi Wu",
                "Keqiang Sun",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CPII, InnoHK",
                "CUHK MMLab",
                "Kings College London",
                "Mizzen AI",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03789.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "HPSv3: Новый стандарт оценки качества генерации изображений по тексту",
                    "desc": "HPSv3 - это новый метод оценки качества генерации изображений по тексту, основанный на широком спектре данных и учитывающий неопределенность при ранжировании. Авторы представили обширный датасет HPDv3, содержащий 1.08 миллиона пар текст-изображение и 1.17 миллиона аннотированных попарных сравнений. Метод использует модель на основе VLM, обученную с помощью функции потерь, учитывающей неопределенность при ранжировании. Также предложен итеративный подход Chain-of-Human-Preference для улучшения качества сгенерированных изображений."
                },
                "en": {
                    "title": "Enhancing Image Generation with Human Preference Score v3",
                    "desc": "The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment."
                },
                "zh": {
                    "title": "提升图像生成质量的HPSv3与CoHP方法",
                    "desc": "HPSv3是一种人类偏好评分，利用广泛的数据集和考虑不确定性的排名损失，提升文本到图像生成的质量。我们发布了HPDv3，这是第一个包含108万对文本-图像和117万对标注比较的广泛人类偏好数据集。我们还引入了一种基于视觉语言模型的偏好模型，使用不确定性感知的排名损失进行细致排名。此外，我们提出了人类偏好链（CoHP），通过迭代图像优化，在每一步选择最佳图像，从而提高生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02215",
            "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
            "url": "https://huggingface.co/papers/2508.02215",
            "abstract": "LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.",
            "score": 8,
            "issue_id": 5220,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "caa8de613517d011",
            "authors": [
                "Yike Zhang",
                "Zhiyuan He",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Yuqing Yang",
                "Jianyong Wang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02215.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🔪",
                "ru": {
                    "title": "Эффективное обрезание кэша для ускорения языковых моделей",
                    "desc": "LeanK - это метод машинного обучения для оптимизации больших языковых моделей. Он уменьшает использование памяти и ускоряет декодирование, удаляя неважные каналы в кэше ключей. LeanK использует двухэтапный процесс обучения для создания маски каналов, удовлетворяющей требованиям разреженности и аппаратного выравнивания. Эксперименты показывают сокращение памяти кэша ключей до 70% и ускорение вычисления внимания в 1,3 раза."
                },
                "en": {
                    "title": "LeanK: Pruning for Efficient Language Model Performance",
                    "desc": "LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation."
                },
                "zh": {
                    "title": "LeanK：高效解码的大型语言模型优化方案",
                    "desc": "LeanK是一种基于学习的方法，旨在减少大型语言模型中的不重要的关键缓存通道，从而降低内存使用并加速解码，同时不影响准确性。该方法利用静态通道稀疏性，通过一种新颖的两阶段训练过程，学习满足特定稀疏比和硬件对齐要求的通道静态掩码。实验结果表明，LeanK可以减少高达70%的K缓存和16%-18%的V缓存内存，并且自定义解码内核使注意力计算速度提高了1.3倍。通过分析学习到的重要性分布，我们还提供了对长上下文推理过程中模型通道和注意力头的深入见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23785",
            "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
            "url": "https://huggingface.co/papers/2507.23785",
            "abstract": "A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
            "score": 7,
            "issue_id": 5220,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "47c7686978b9c4dc",
            "authors": [
                "Bowen Zhang",
                "Sicheng Xu",
                "Chuxin Wang",
                "Jiaolong Yang",
                "Feng Zhao",
                "Dong Chen",
                "Baining Guo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23785.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#synthetic",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Генерация динамического 3D-контента из видео с помощью диффузионных моделей",
                    "desc": "Представлена новая система для создания динамического 3D-контента из одиночных видеовходов. Используется VAE для кодирования канонических гауссовых сплатов и их временных вариаций, а также диффузионная модель с учетом времени для генерации. Система обучена на тщательно отобранных анимируемых 3D-объектах из набора данных Objaverse. Демонстрирует превосходное качество генерации и обобщение на реальные видеовходы, несмотря на обучение только на синтетических данных."
                },
                "en": {
                    "title": "Transforming Videos into Dynamic 3D Worlds",
                    "desc": "This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation."
                },
                "zh": {
                    "title": "从视频生成高质量动态3D内容的创新框架",
                    "desc": "本文提出了一种新颖的框架，用于从单个视频输入生成高质量的动态3D内容。我们引入了直接的4DMesh到高斯样条（GS）变分场变分自编码器（VAE），能够直接编码3D动画数据中的高斯样条及其时间变化。通过这种高效的表示方式，我们训练了一个基于时间感知的高斯变分场扩散模型，能够根据输入视频和高斯样条生成动态内容。实验结果表明，该模型在生成质量上优于现有方法，并且在处理真实视频输入时表现出良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04664",
            "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
            "url": "https://huggingface.co/papers/2508.04664",
            "abstract": "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.",
            "score": 6,
            "issue_id": 5220,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "1575b65bda95c5f9",
            "authors": [
                "Mo Li",
                "L. H. Xu",
                "Qitai Tan",
                "Ting Cao",
                "Yunxin Liu"
            ],
            "affiliations": [
                "Independent Researcher",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04664.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Sculptor: Умное управление контекстом для улучшения работы языковых моделей",
                    "desc": "Статья представляет фреймворк Sculptor для активного управления контекстом в больших языковых моделях (LLM). Sculptor позволяет LLM проактивно управлять вниманием и рабочей памятью, что снижает проактивную интерференцию и улучшает надежность рассуждений. Фреймворк включает инструменты для фрагментации контекста, суммирования и интеллектуального поиска. Эксперименты показывают, что Sculptor значительно улучшает производительность LLM на длинных контекстах без специального обучения."
                },
                "en": {
                    "title": "Sculptor: Mastering Memory for Better Long-Context Reasoning",
                    "desc": "The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies."
                },
                "zh": {
                    "title": "主动上下文管理，提升LLM性能！",
                    "desc": "Sculptor是一个用于主动上下文管理的框架，旨在提高大型语言模型（LLM）在处理长上下文时的表现。该框架通过主动管理注意力和工作记忆，减少了前期信息的干扰，从而改善推理的可靠性。Sculptor提供了三种工具：上下文碎片化、摘要、隐藏与恢复，以及智能搜索，帮助LLM更有效地处理信息。实验结果表明，Sculptor在没有特定训练的情况下，显著提升了模型的性能，强调了主动上下文管理的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04586",
            "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
            "url": "https://huggingface.co/papers/2508.04586",
            "abstract": "The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.",
            "score": 5,
            "issue_id": 5220,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "0e9d3ff69536a24d",
            "authors": [
                "Nuo Chen",
                "Moming Duan",
                "Andre Huikai Lin",
                "Qian Wang",
                "Jiaying Wu",
                "Bingsheng He"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04586.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#survey"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Революция в формате конференций ИИ: от централизации к федерации",
                    "desc": "Данная статья анализирует структурные проблемы конференций по искусственному интеллекту, включая рост числа публикаций, углеродный след, негативное настроение сообщества и логистические трудности. Авторы выделяют четыре ключевые области напряженности: научную, экологическую, психологическую и логистическую. Для решения этих проблем предлагается модель Community-Federated Conference (CFC), которая разделяет рецензирование, презентации и нетворкинг на глобально координируемые, но локально организованные компоненты. Эта модель предлагает более устойчивый, инклюзивный и гибкий подход к проведению исследований в области ИИ."
                },
                "en": {
                    "title": "Towards Sustainable AI Conferences: A Community-Federated Approach",
                    "desc": "This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research."
                },
                "zh": {
                    "title": "构建可持续的人工智能会议新模式",
                    "desc": "这篇论文诊断了人工智能会议的结构性问题，包括发表率、碳足迹、负面社区情绪和后勤挑战。随着会议数量的快速增长，传统的集中式会议模式变得越来越不可持续。论文提出了一种基于社区的联合会议模型，旨在解决这些问题，促进科学传播的公平性和社区的福祉。该模型将同行评审、展示和网络交流分开，提供了一种更可持续、包容和有韧性的人工智能研究发展路径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04295",
            "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
            "url": "https://huggingface.co/papers/2508.04295",
            "abstract": "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.",
            "score": 3,
            "issue_id": 5221,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 августа",
                "en": "August 6",
                "zh": "8月6日"
            },
            "hash": "0dc09e7a8e2bad95",
            "authors": [
                "Chaofan Wang",
                "Tingrui Yu",
                "Jie Wang",
                "Dong Chen",
                "Wenrui Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Beijun Shen"
            ],
            "affiliations": [
                "Huawei Technologies Co., Ltd",
                "Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04295.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#plp",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "EvoC2Rust: Эволюционный подход к автоматическому переводу C в Rust",
                    "desc": "EvoC2Rust - это автоматизированный фреймворк для перевода проектов на C в Rust, использующий подход на основе скелета кода. Он сочетает методы на основе правил и языковых моделей для улучшения синтаксиса, семантики и безопасности кода. Фреймворк работает в три этапа: декомпозиция проекта, инкрементальный перевод функций и исправление ошибок компиляции. Оценка на открытых и промышленных проектах показала превосходство EvoC2Rust над существующими подходами в точности перевода и безопасности кода."
                },
                "en": {
                    "title": "EvoC2Rust: Bridging C to Rust with Safety and Precision",
                    "desc": "EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust."
                },
                "zh": {
                    "title": "EvoC2Rust：高效的C到Rust自动转换框架",
                    "desc": "EvoC2Rust是一个自动化框架，旨在将整个C项目转换为Rust代码。它采用了骨架引导的方法，结合了基于规则和基于大语言模型（LLM）的方法，以提高代码的语法、语义和安全性。该框架通过三个进化阶段进行项目级翻译，首先将C项目分解为功能模块，然后逐步翻译函数，最后通过集成LLM和静态分析修复编译错误。评估结果显示，EvoC2Rust在C到Rust的翻译中表现优越，语法和语义准确性分别提高了17.24%和14.32%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01197",
            "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
            "url": "https://huggingface.co/papers/2508.01197",
            "abstract": "A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.",
            "score": 3,
            "issue_id": 5220,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 августа",
                "en": "August 2",
                "zh": "8月2日"
            },
            "hash": "d7be41190836a7cc",
            "authors": [
                "Zhan Shi",
                "Song Wang",
                "Junbo Chen",
                "Jianke Zhu"
            ],
            "affiliations": [
                "College of Computer Science, Zhejiang University, Hangzhou 310027, China",
                "College of Software Technology, Zhejiang University",
                "Udeer.ai, Hangzhou 310000, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01197.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Точное восприятие объектов для беспилотных автомобилей с помощью 3D грунтовки",
                    "desc": "Статья представляет новый бенчмарк для задачи 3D грунтовки объектов с использованием естественного языка и воксельных аннотаций в контексте автономного вождения. Авторы предлагают модель GroundingOcc, которая объединяет визуальные, текстовые и облачные признаки для предсказания местоположения и занятости объектов. Модель включает мультимодальный энкодер, модули для предсказания воксельной занятости и уточнения локализации, а также дополнительные компоненты для 2D грунтовки и оценки глубины. Эксперименты показывают превосходство предложенного метода над существующими базовыми моделями в задаче 3D грунтовки занятости."
                },
                "en": {
                    "title": "Enhancing Object Perception with 3D Occupancy Grounding",
                    "desc": "This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "提升自动驾驶物体感知的3D占用基础视觉定位",
                    "desc": "本论文提出了一种新的基准和模型，用于通过自然语言和体素级注释进行3D占用基础的视觉定位，旨在提高自动驾驶中的物体感知能力。现有的视觉定位任务通常依赖于边界框，这种方法无法捕捉到细粒度的细节，导致物体表示不准确。我们引入的GroundingOcc模型通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体位置和占用信息。实验结果表明，我们的方法在3D占用基础的视觉定位任务中优于现有的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01630",
            "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
            "url": "https://huggingface.co/papers/2508.01630",
            "abstract": "OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.",
            "score": 2,
            "issue_id": 5222,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 августа",
                "en": "August 3",
                "zh": "8月3日"
            },
            "hash": "aca28561e07e250a",
            "authors": [
                "Maziyar Panahi"
            ],
            "affiliations": [
                "CNRS Paris, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01630.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#ethics",
                    "#dataset",
                    "#training",
                    "#transfer_learning",
                    "#open_source",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "OpenMed NER: Открытый прорыв в распознавании биомедицинских сущностей",
                    "desc": "OpenMed NER - это набор моделей на основе трансформеров с открытым исходным кодом, использующих методы DAPT и LoRA для распознавания именованных сущностей в биомедицинских текстах. Модели достигают наилучших результатов на 10 из 12 эталонных наборов данных, охватывающих химические вещества, заболевания, гены и виды. Обучение проводится эффективно - менее 12 часов на одном GPU с низким углеродным следом. Проект демонстрирует, что стратегически адаптированные модели с открытым исходным кодом могут превзойти закрытые решения в области биомедицинского NER."
                },
                "en": {
                    "title": "OpenMed NER: Efficiently Transforming Biomedical NER with Open-Source Innovation",
                    "desc": "OpenMed NER is a collection of open-source transformer models designed for named-entity recognition (NER) in the biomedical field. It utilizes domain-adaptive pre-training (DAPT) and Low-Rank Adaptation (LoRA) to achieve high performance while being computationally efficient. The models were trained on a large dataset of clinical notes and research papers, and they excelled in identifying various biomedical entities, outperforming existing models on multiple benchmarks. This work highlights the potential of open-source solutions to achieve superior results compared to proprietary models, all while maintaining a low environmental impact."
                },
                "zh": {
                    "title": "OpenMed NER：高效的生物医学命名实体识别解决方案",
                    "desc": "OpenMed NER 是一套开源的变换器模型，结合了轻量级的领域自适应预训练（DAPT）和参数高效的低秩适应（LoRA），在生物医学命名实体识别（NER）基准测试中表现出色。该模型在350,000段来自伦理来源的临床笔记和研究文献的语料库上进行训练，能够高效提取医疗数据中的结构化信息。OpenMed NER 在12个生物医学NER基准测试中取得了10个数据集的新最优微F1分数，尤其在疾病和化学基准上有显著提升。该模型的训练效率高，单个GPU下训练时间少于12小时，且碳足迹低于1.2公斤CO2e，适合帮助从业者遵守数据保护和人工智能法规。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00222",
            "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
            "url": "https://huggingface.co/papers/2508.00222",
            "abstract": "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
            "score": 2,
            "issue_id": 5224,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "ec45fc053d8a3704",
            "authors": [
                "Yihong Dong",
                "Xue Jiang",
                "Yongding Tao",
                "Huanyu Liu",
                "Kechi Zhang",
                "Lili Mou",
                "Rongyu Cao",
                "Yingwei Ma",
                "Jue Chen",
                "Binhua Li",
                "Zhi Jin",
                "Fei Huang",
                "Yongbin Li",
                "Ge Li"
            ],
            "affiliations": [
                "Department of Computing Science, University of Alberta",
                "School of Computer Science, Peking University",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RL-PLUS: Прорыв в обучении с подкреплением для улучшения рассуждений языковых моделей",
                    "desc": "Статья представляет RL-PLUS - новый гибридный подход к оптимизации политики для улучшения способностей рассуждения больших языковых моделей (LLM). RL-PLUS объединяет метод множественной выборки по важности и функцию преимущества на основе исследования для преодоления ограничений базовой модели. Этот метод превосходит существующий подход RLVR на различных тестовых наборах данных по математическим рассуждениям и задачам вне распределения. RL-PLUS также эффективно решает проблему коллапса границ возможностей, характерную для RLVR."
                },
                "en": {
                    "title": "Breaking Boundaries in LLM Reasoning with RL-PLUS",
                    "desc": "The paper introduces RL-PLUS, a new hybrid-policy optimization method designed to improve the reasoning abilities of Large Language Models (LLMs). It combines Multiple Importance Sampling and Exploration-Based Advantage Function to enhance the model's performance beyond the limitations of traditional Reinforcement Learning with Verifiable Reward (RLVR). By addressing the issues of distributional mismatch and guiding exploration towards valuable reasoning paths, RL-PLUS effectively prevents capability boundary collapse. Extensive experiments show that RL-PLUS outperforms existing methods on various benchmarks, achieving significant improvements in reasoning tasks."
                },
                "zh": {
                    "title": "RL-PLUS：突破推理能力边界的创新方法",
                    "desc": "RL-PLUS是一种混合策略优化方法，旨在提升大型语言模型（LLM）的推理能力。它通过结合多重重要性采样和基于探索的优势函数，克服了传统强化学习方法（如RLVR）在能力边界崩溃方面的局限。RL-PLUS能够有效利用外部数据，指导模型探索高价值的推理路径，从而实现更强的推理能力。实验结果表明，RL-PLUS在多个基准测试中表现优异，显著超越了现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03448",
            "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
            "url": "https://huggingface.co/papers/2508.03448",
            "abstract": "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.",
            "score": 1,
            "issue_id": 5223,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "534674700c0af141",
            "authors": [
                "Jan Melechovsky",
                "Ambuj Mehrish",
                "Dorien Herremans"
            ],
            "affiliations": [
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03448.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Универсальное улучшение качества музыки с помощью ИИ",
                    "desc": "SonicMaster - это унифицированная генеративная модель для улучшения качества аудио в музыке. Она использует управление на основе текста и парадигму обучения с сопоставлением потоков для устранения различных артефактов. Модель может работать как в автоматическом режиме, так и с целевыми улучшениями на основе текстовых инструкций. SonicMaster обучается на специально созданном наборе данных, включающем пары деградированных и высококачественных треков."
                },
                "en": {
                    "title": "SonicMaster: Revolutionizing Music Restoration with AI",
                    "desc": "SonicMaster is a novel generative model designed to enhance music audio quality by correcting various audio artifacts. It utilizes text-based control to allow users to specify desired improvements, making it versatile for both targeted and automatic restoration. The model is trained on a large dataset that pairs degraded audio tracks with their high-quality counterparts, using a flow-matching generative training approach. Results show that SonicMaster significantly enhances sound quality, as confirmed by both objective metrics and subjective listener preferences."
                },
                "zh": {
                    "title": "SonicMaster：音乐音频质量的统一生成模型",
                    "desc": "SonicMaster是一种统一的生成模型，旨在通过文本控制和流匹配生成训练范式来改善音乐音频质量。该模型能够处理多种音频伪影，如混响过度、失真和音调不平衡等，尤其适用于非专业环境下录制的音乐。SonicMaster通过自然语言指令进行有针对性的增强，或在自动模式下进行一般修复。实验结果表明，SonicMaster在所有伪影类别中显著提高了音质，且听众更喜欢其增强的输出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02807",
            "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
            "url": "https://huggingface.co/papers/2508.02807",
            "abstract": "DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/",
            "score": 1,
            "issue_id": 5224,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "f3f693bca2e57a94",
            "authors": [
                "Tongchun Zuo",
                "Zaiyu Huang",
                "Shuliang Ning",
                "Ente Lin",
                "Chao Liang",
                "Zerong Zheng",
                "Jianwen Jiang",
                "Yuan Zhang",
                "Mingyuan Gao",
                "Xin Dong"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02807.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#synthetic",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "👚",
                "ru": {
                    "title": "DreamVVT: Реалистичная виртуальная примерка одежды на видео с помощью ИИ",
                    "desc": "DreamVVT - это двухэтапная система для виртуальной примерки одежды на видео, использующая диффузионные трансформеры и LoRA-адаптеры. Она улучшает сохранение деталей одежды и временную согласованность, используя непарные данные с изображениями людей и предобученные модели. На первом этапе система создает высококачественные ключевые кадры с примеркой, используя мультикадровую модель и визуально-языковую модель. На втором этапе применяется предобученная модель генерации видео с LoRA-адаптерами для обеспечения временной согласованности и правдоподобных движений."
                },
                "en": {
                    "title": "Enhancing Video Try-On with DreamVVT: Consistency Meets Detail",
                    "desc": "DreamVVT is a two-stage framework designed to improve video virtual try-on (VVT) by using Diffusion Transformers and LoRA adapters. It effectively utilizes unpaired human-centric data and pretrained models to maintain garment details and ensure temporal consistency in videos. The first stage generates high-quality keyframe images using a multi-frame try-on model and a vision-language model, while the second stage focuses on video generation by incorporating motion and appearance data. This innovative approach allows DreamVVT to outperform existing methods in preserving garment fidelity and achieving smooth motion in dynamic scenarios."
                },
                "zh": {
                    "title": "DreamVVT：提升视频虚拟试穿的创新框架",
                    "desc": "DreamVVT是一种两阶段框架，利用扩散变换器和LoRA适配器，提升视频虚拟试穿技术。该方法通过使用未配对的人体中心数据和预训练模型，能够更好地保留服装细节和时间一致性。第一阶段通过多帧试穿模型生成高保真关键帧图像，第二阶段则利用预训练的视频生成模型确保动态运动的连贯性。实验结果表明，DreamVVT在真实场景中优于现有方法，能够更好地保持服装内容的细节和时间稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23313",
            "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models",
            "url": "https://huggingface.co/papers/2507.23313",
            "abstract": "Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.",
            "score": 1,
            "issue_id": 5221,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "c584e9c932383ec6",
            "authors": [
                "Alfio Ferrara",
                "Sergio Picascia",
                "Elisabetta Rocchetti"
            ],
            "affiliations": [
                "Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23313.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Скрытое понимание искусства: как ИИ разделяет содержание и стиль",
                    "desc": "Исследование показало, что трансформерные модели диффузии для генерации изображений по тексту демонстрируют различную степень разделения содержания и стиля в создаваемых произведениях искусства. Анализ проводился с использованием тепловых карт кросс-внимания, которые позволяют соотнести пиксели сгенерированных изображений с конкретными токенами промпта. Результаты выявили, что во многих случаях токены содержания влияют преимущественно на области, связанные с объектами, в то время как токены стиля воздействуют на фон и текстуры. Это указывает на то, что модели диффузии формируют некоторое внутреннее представление о различии между содержанием и стилем без явного обучения этому."
                },
                "en": {
                    "title": "Decoding Art: Understanding Content and Style in AI-Generated Images",
                    "desc": "This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision."
                },
                "zh": {
                    "title": "探索内容与风格的分离：扩散模型的艺术生成",
                    "desc": "本研究探讨了基于变换器的文本到图像扩散模型在生成艺术作品时如何编码内容和风格的概念。通过交叉注意力热图，我们能够将生成图像中的像素归因于特定的提示令牌，从而区分受内容描述和风格描述影响的图像区域。研究发现，扩散模型在不同艺术提示和风格请求下表现出不同程度的内容与风格分离。结果表明，内容令牌主要影响与物体相关的区域，而风格令牌则影响背景和纹理区域，显示出模型对内容与风格区分的理解。"
                }
            }
        }
    ],
    "link_prev": "2025-08-06.html",
    "link_next": "2025-08-08.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "06.08",
        "en": "08/06",
        "zh": "8月6日"
    },
    "short_date_next": {
        "ru": "08.08",
        "en": "08/08",
        "zh": "8月8日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 7,
        "#agents": 5,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 2,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}