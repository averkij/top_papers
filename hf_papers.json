{
    "date": {
        "ru": "28 января",
        "en": "January 28",
        "zh": "1月28日"
    },
    "time_utc": "2025-01-28 20:10",
    "weekday": 1,
    "issue_id": 1912,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.15368",
            "title": "Baichuan-Omni-1.5 Technical Report",
            "url": "https://huggingface.co/papers/2501.15368",
            "abstract": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
            "score": 34,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "f40b7f7b108c1500",
            "authors": [
                "Yadong Li",
                "Jun Liu",
                "Tao Zhang",
                "Tao Zhang",
                "Song Chen",
                "Tianpeng Li",
                "Zehuan Li",
                "Lijun Liu",
                "Lingfeng Ming",
                "Guosheng Dong",
                "Da Pan",
                "Chong Li",
                "Yuanbo Fang",
                "Dongdong Kuang",
                "Mingrui Wang",
                "Chenglin Zhu",
                "Youwei Zhang",
                "Hongyu Guo",
                "Fengyu Zhang",
                "Yuran Wang",
                "Bowen Ding",
                "Wei Song",
                "Xu Li",
                "Yuqi Huo",
                "Zheng Liang",
                "Shusen Zhang",
                "Xin Wu",
                "Shuai Zhao",
                "Linchu Xiong",
                "Yozhen Wu",
                "Jiahui Ye",
                "Wenhao Lu",
                "Bowen Li",
                "Yan Zhang",
                "Yaqi Zhou",
                "Xin Chen",
                "Lei Su",
                "Hongda Zhang",
                "Fuzhong Chen",
                "Xuezhen Dong",
                "Na Nie",
                "Zhiying Wu",
                "Bin Xiao",
                "Ting Li",
                "Shunya Dang",
                "Ping Zhang",
                "Yijia Sun",
                "Jincheng Wu",
                "Jinjie Yang",
                "Xionghai Lin",
                "Zhi Ma",
                "Kegeng Wu",
                "Jia li",
                "Aiyuan Yang",
                "Hui Liu",
                "Jianqiang Zhang",
                "Xiaoxi Chen",
                "Guangwei Ai",
                "Wentao Zhang",
                "Yicong Chen",
                "Xiaoqin Huang",
                "Kun Li",
                "Wenjing Luo",
                "Yifei Duan",
                "Lingling Zhu",
                "Ran Xiao",
                "Zhe Su",
                "Jiani Pu",
                "Dian Wang",
                "Xu Jia",
                "Tianyu Zhang",
                "Mengyu Ai",
                "Mang Wang",
                "Yujing Qiao",
                "Lei Zhang",
                "Yanjun Shen",
                "Fan Yang",
                "Miao Zhen",
                "Yijie Zhou",
                "Mingyang Chen",
                "Fei Li",
                "Chenzheng Zhu",
                "Keer Lu",
                "Yaqi Zhao",
                "Hao Liang",
                "Youquan Li",
                "Yanzhao Qin",
                "Linzhuang Sun",
                "Jianhua Xu",
                "Haoze Sun",
                "Mingan Lin",
                "Zenan Zhou",
                "Weipeng Chen"
            ],
            "affiliations": [
                "Baichuan Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15368.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Baichuan-Omni-1.5: Прорыв в омнимодальном ИИ",
                    "desc": "Baichuan-Omni-1.5 - это омнимодальная модель, обладающая способностями понимания и генерации аудио. Для достижения качественного взаимодействия между модальностями, авторы оптимизировали три ключевых аспекта: создали комплексный пайплайн для обработки мультимодальных данных, разработали аудио-токенизатор для захвата семантической и акустической информации, и применили многоэтапную стратегию обучения. Модель демонстрирует ведущие результаты в омнимодальных возможностях и сравнима с передовыми моделями в различных мультимодальных медицинских бенчмарках."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Interaction with Baichuan-Omni-1.5",
                    "desc": "Baichuan-Omni-1.5 is a cutting-edge omni-modal model designed for seamless interaction across text, audio, and visual data. It utilizes a robust data cleaning and synthesis pipeline to process approximately 500 billion high-quality multimodal data points. The model features a specialized audio-tokenizer that captures both semantic and acoustic elements, enhancing its compatibility with multi-layered language models (MLLM). Through a multi-stage training approach, it effectively aligns and fine-tunes across modalities, outperforming existing models in various multimodal tasks, particularly in medical benchmarks."
                },
                "zh": {
                    "title": "全模态交互的新纪元",
                    "desc": "我们介绍了Baichuan-Omni-1.5，这是一种全模态模型，具备全模态理解和端到端音频生成能力。为了实现不同模态之间流畅且高质量的交互，我们优化了三个关键方面。首先，我们建立了一个全面的数据清洗和合成管道，获得了约5000亿条高质量的多模态数据（文本、音频和视觉）。其次，我们设计了一个音频标记器（Baichuan-Audio-Tokenizer），能够捕捉音频的语义和声学信息，从而增强与多模态大语言模型的兼容性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15383",
            "title": "Qwen2.5-1M Technical Report",
            "url": "https://huggingface.co/papers/2501.15383",
            "abstract": "We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data synthesis, progressive pre-training, and multi-stage supervised fine-tuning are employed to effectively enhance long-context performance while reducing training costs.   To promote the use of long-context models among a broader user base, we present and open-source our inference framework. This framework includes a length extrapolation method that can expand the model context lengths by at least four times, or even more, without additional training. To reduce inference costs, we implement a sparse attention method along with chunked prefill optimization for deployment scenarios and a sparsity refinement method to improve precision. Additionally, we detail our optimizations in the inference engine, including kernel optimization, pipeline parallelism, and scheduling optimization, which significantly enhance overall inference performance. By leveraging our inference framework, the Qwen2.5-1M models achieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million tokens of context. This framework provides an efficient and powerful solution for developing applications that require long-context processing using open-source models.   The Qwen2.5-1M series currently includes the open-source models Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed model Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly improved in long-context tasks without compromising performance in short-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model significantly outperforms GPT-4o-mini in long-context tasks and supports contexts eight times longer.",
            "score": 22,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "203817e55fc3eb45",
            "authors": [
                "An Yang",
                "Bowen Yu",
                "Chengyuan Li",
                "Dayiheng Liu",
                "Fei Huang",
                "Haoyan Huang",
                "Jiandong Jiang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Kexin Yang",
                "Le Yu",
                "Mei Li",
                "Minmin Sun",
                "Qin Zhu",
                "Rui Men",
                "Tao He",
                "Weijia Xu",
                "Wenbiao Yin",
                "Wenyuan Yu",
                "Xiafei Qiu",
                "Xingzhang Ren",
                "Xinlong Yang",
                "Yong Li",
                "Zhiying Xu",
                "Zipeng Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15383.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Миллион токенов: новый рубеж для языковых моделей",
                    "desc": "Статья представляет серию моделей Qwen2.5-1M с контекстным окном в 1 миллион токенов. Авторы применили техники синтеза длинных данных, прогрессивного предобучения и многоэтапной супервизированной донастройки для улучшения работы с длинным контекстом. Разработан фреймворк для инференса, включающий метод экстраполяции длины и оптимизации для ускорения обработки. Модели Qwen2.5-1M демонстрируют значительное улучшение на задачах с длинным контекстом без ухудшения производительности на коротких текстах."
                },
                "en": {
                    "title": "Unlocking the Power of 1 Million Tokens with Qwen2.5-1M",
                    "desc": "The Qwen2.5-1M models introduce a significant advancement in handling long-context inputs, extending the context length to 1 million tokens. This is achieved through innovative techniques like long data synthesis and multi-stage supervised fine-tuning, which enhance performance while minimizing training costs. The open-source inference framework allows users to expand context lengths without additional training and includes optimizations for efficient deployment. Overall, these models demonstrate superior performance in long-context tasks compared to existing models, making them a valuable resource for applications requiring extensive context processing."
                },
                "zh": {
                    "title": "Qwen2.5-1M：长上下文处理的新突破",
                    "desc": "我们介绍了Qwen2.5-1M系列模型，能够处理长达100万标记的上下文。与之前的128K版本相比，Qwen2.5-1M在长上下文能力上有显著提升，采用了长数据合成、渐进式预训练和多阶段监督微调等关键技术。为了降低推理成本，我们实现了稀疏注意力机制和分块预填充优化，同时优化了推理引擎的性能。Qwen2.5-1M模型在处理长上下文任务时表现优异，且在短上下文场景中性能没有下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16142",
            "title": "Towards General-Purpose Model-Free Reinforcement Learning",
            "url": "https://huggingface.co/papers/2501.16142",
            "abstract": "Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.",
            "score": 13,
            "issue_id": 1898,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "0cf7cd0c9c1f5964",
            "authors": [
                "Scott Fujimoto",
                "Pierluca D'Oro",
                "Amy Zhang",
                "Yuandong Tian",
                "Michael Rabbat"
            ],
            "affiliations": [
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16142.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "MR.Q: На пути к универсальному обучению с подкреплением",
                    "desc": "Статья представляет новый алгоритм обучения с подкреплением под названием MR.Q. Этот алгоритм объединяет преимущества модельного и безмодельного подходов, используя представления, линеаризующие функцию ценности. MR.Q показывает конкурентоспособные результаты на различных бенчмарках с единым набором гиперпараметров. Исследование направлено на создание универсального безмодельного алгоритма глубокого обучения с подкреплением."
                },
                "en": {
                    "title": "Towards Universal Problem-Solving with MR.Q in Reinforcement Learning",
                    "desc": "This paper presents a new model-free deep reinforcement learning algorithm called MR.Q, which aims to solve a wide range of problems without needing extensive tuning of hyperparameters. The authors utilize model-based representations to simplify the value function, allowing the algorithm to benefit from the advantages of model-based RL while avoiding the complexities of planning. MR.Q is evaluated across various standard RL benchmarks using a single set of hyperparameters, demonstrating competitive performance against both specialized and general algorithms. This work represents a significant advancement towards creating versatile and efficient model-free deep RL solutions."
                },
                "zh": {
                    "title": "构建通用的无模型深度强化学习算法",
                    "desc": "强化学习（RL）提供了一种通用问题解决框架，但在实际应用中，RL算法通常针对特定基准进行调整，依赖于精心调节的超参数和算法选择。最近，强大的基于模型的RL方法在多个基准上表现出色，但其复杂性和较慢的运行时间限制了其更广泛的应用。本文提出了一种统一的无模型深度RL算法MR.Q，旨在解决多样化的领域和问题设置。我们利用基于模型的表示方法，近似线性化价值函数，从而在避免规划或模拟轨迹相关成本的同时，利用基于模型的RL所使用的更密集的任务目标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15907",
            "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
            "url": "https://huggingface.co/papers/2501.15907",
            "abstract": "Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles. To bridge this gap, we introduce Emilia-Pipe, an open-source preprocessing pipeline to extract high-quality training data from valuable yet underexplored in-the-wild data that capture spontaneous human speech in real-world contexts. By leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech generation dataset derived from in-the-wild speech data. This dataset comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it the largest open-source speech generation dataset available. Extensive experiments demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, showcasing superior performance in capturing diverse speaker timbre and speaking styles of real-world human speech. Furthermore, this work underscores the importance of scaling dataset size to advance speech generation research and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation.",
            "score": 10,
            "issue_id": 1903,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "bd221795c86585eb",
            "authors": [
                "Haorui He",
                "Zengqiang Shang",
                "Chaoren Wang",
                "Xuyuan Li",
                "Yicheng Gu",
                "Hua Hua",
                "Liwei Liu",
                "Chen Yang",
                "Jiaqi Li",
                "Peiyang Shi",
                "Yuancheng Wang",
                "Kai Chen",
                "Pengyuan Zhang",
                "Zhizheng Wu"
            ],
            "affiliations": [
                "Chinese University of Hong Kong, Shenzhen, China",
                "Laboratory of Speech and Intelligent Information Processing, Institute of Acoustics, CAS, Beijing, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15907.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#audio",
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#low_resource"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Emilia: новый этап в генерации естественной речи",
                    "desc": "Исследователи представили Emilia-Pipe - открытый конвейер предобработки для извлечения высококачественных данных из спонтанной речи в реальных условиях. На его основе создан многоязычный датасет Emilia, содержащий более 101 тысячи часов речи на 6 языках. Расширенная версия Emilia-Large включает более 216 тысяч часов и является крупнейшим открытым датасетом для генерации речи. Эксперименты показали превосходство Emilia над традиционными аудиокнижными датасетами в генерации естественной и спонтанной речи."
                },
                "en": {
                    "title": "Unlocking Spontaneous Speech with Emilia-Pipe",
                    "desc": "This paper presents Emilia-Pipe, a preprocessing tool designed to extract high-quality training data from spontaneous human speech in real-world settings. The authors introduce Emilia, a multilingual speech generation dataset that includes over 101k hours of diverse speech data across six languages. They further expand this dataset to Emilia-Large, which contains more than 216k hours, making it the largest open-source resource for speech generation. The results show that models trained on Emilia outperform those trained on traditional audiobook datasets, effectively capturing the variability and naturalness of human speech."
                },
                "zh": {
                    "title": "打破传统，捕捉真实语音的多样性",
                    "desc": "近年来，语音生成的进展主要依赖于大规模的训练数据集。然而，目前的模型在捕捉真实人类语音的自发性和多样性方面存在不足，因为它们依赖于仅限于正式朗读风格的有声书数据集。为了解决这个问题，我们提出了Emilia-Pipe，这是一个开源的预处理管道，用于从有价值但未被充分探索的真实环境数据中提取高质量的训练数据。通过利用Emilia-Pipe，我们构建了Emilia，这是第一个基于真实环境语音数据的多语言语音生成数据集，包含超过101k小时的语音，涵盖六种语言。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15570",
            "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
            "url": "https://huggingface.co/papers/2501.15570",
            "abstract": "As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at https://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside}, https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
            "score": 10,
            "issue_id": 1900,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "063647dfe2bd7b63",
            "authors": [
                "Lin Yueyu",
                "Li Zhiyuan",
                "Peter Yue",
                "Liu Xiao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.15570.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#architecture",
                    "#small_models",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Повышение эффективности и выразительности RNN через дистилляцию знаний",
                    "desc": "Статья представляет новые модели, основанные на чистом нативном внимании RWKV-7, дистиллированные из Qwen 2.5. Цель исследования - повысить выразительность RNN и продемонстрировать способность отслеживания состояния, превосходящую трансформеры. Авторы работают с QRWK 32B на архитектуре RWKV-6, что позволяет сократить время обработки знаний до 8 часов на 16 GPU AMD MI300X. Процесс дистилляции может использовать любую большую языковую модель для передачи знаний меньшим моделям с меньшим количеством токенов."
                },
                "en": {
                    "title": "Enhancing RNN Expressiveness with RWKV Attention",
                    "desc": "This paper presents a new series of models derived from Qwen 2.5, focusing on enhancing the expressiveness of RNNs through a native RWKV-7 attention mechanism. The authors demonstrate that their hybrid quadratic and subquadratic attention models outperform traditional Transformer and Linear RNN architectures by significantly reducing key-value (KV) complexity. They introduce the QRWK 32B model, which achieves impressive efficiency by processing knowledge in just 8 hours using 16 AMD MI300X GPUs while retaining the performance of Qwen 2.5. Additionally, the distillation process allows for knowledge transfer from larger language models (LLMs) to smaller ones, making it a versatile approach for building more powerful foundation models."
                },
                "zh": {
                    "title": "提升RNN表达能力的新模型",
                    "desc": "本文介绍了一种新型的混合二次和亚二次注意力模型，旨在提高RNN的表达能力。我们基于RWKV-7注意力架构，提出了一系列从Qwen 2.5中提炼的模型，展示了超越Transformer的状态跟踪能力。通过使用16个AMD MI300X GPU，我们的QRWK 32B模型将知识处理时间缩短至仅8小时，同时保持了Qwen 2.5的性能。该提炼过程可以利用任何大型语言模型（LLM），实现从更大模型到更小模型的知识转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15369",
            "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
            "url": "https://huggingface.co/papers/2501.15369",
            "abstract": "We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, i.e., ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios.",
            "score": 7,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "50e030854cdc071f",
            "authors": [
                "Chuanyang Zheng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.15369.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "iFormer: Эффективные нейросети для мобильного компьютерного зрения",
                    "desc": "iFormer - это новое семейство мобильных гибридных сетей компьютерного зрения, оптимизированных для мобильных приложений. Оно сочетает быструю локальную репрезентативную способность свёрточных сетей с эффективным глобальным моделированием механизма внимания. iFormer использует облегченную версию ConvNeXt и новый механизм модуляционного внимания для мобильных устройств. Эксперименты показывают, что iFormer превосходит существующие легковесные сети по точности и скорости работы на различных задачах, включая классификацию изображений, обнаружение объектов и сегментацию."
                },
                "en": {
                    "title": "iFormer: Optimizing Mobile Vision with Speed and Accuracy",
                    "desc": "The paper introduces iFormer, a new type of mobile hybrid vision network designed to enhance both speed and accuracy for mobile applications. It combines the quick local processing of convolutional networks with the effective global understanding of self-attention mechanisms. By modifying a standard convolutional architecture, ConvNeXt, iFormer creates a lightweight model that reduces memory usage while improving performance. Experimental results show that iFormer achieves high accuracy on ImageNet-1k and excels in various downstream tasks, all while maintaining low latency on mobile devices."
                },
                "zh": {
                    "title": "iFormer：移动应用中的高效视觉网络",
                    "desc": "我们提出了一种新的移动混合视觉网络家族，称为iFormer，旨在优化移动应用的延迟和准确性。iFormer有效地结合了卷积的快速局部表示能力和自注意力的高效全局建模能力。通过将标准卷积网络ConvNeXt转化为更轻量级的移动网络，iFormer实现了局部交互的优化。我们的移动调制注意力机制去除了多头自注意力中的内存密集型操作，并采用高效的调制机制来增强动态全局表示能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16295",
            "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
            "url": "https://huggingface.co/papers/2501.16295",
            "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining. Our code can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba",
            "score": 4,
            "issue_id": 1898,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "011d06607305f0f8",
            "authors": [
                "Weixin Liang",
                "Junhong Shen",
                "Genghan Zhang",
                "Ning Dong",
                "Luke Zettlemoyer",
                "Lili Yu"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "FAIR at Meta",
                "Machine Learning Department, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16295.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Mixture-of-Mamba: Эффективное мультимодальное обучение с модальность-специфической разреженностью",
                    "desc": "В этой статье представлена новая архитектура модели состояний (SSM) под названием Mixture-of-Mamba, которая вводит модальность-специфическую разреженность для мультимодального предобучения. Mixture-of-Mamba расширяет преимущества модальность-осведомленной разреженности на SSM, сохраняя при этом их вычислительную эффективность. Модель была оценена в трех настройках мультимодального предобучения: Transfusion, Chameleon и расширенной трехмодальной системе, включающей речь. Результаты показывают, что Mixture-of-Mamba достигает тех же значений потерь на более ранних этапах обучения со значительно меньшими вычислительными затратами по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "Revolutionizing Multi-Modal Learning with Efficient SSMs",
                    "desc": "This paper introduces Mixture-of-Mamba, a new State Space Model (SSM) that enhances multi-modal pretraining by incorporating modality-aware sparsity. By parameterizing the Mamba block specifically for different modalities, the model efficiently utilizes features from various data types like text, images, and speech. The results show that Mixture-of-Mamba achieves comparable performance to existing models while significantly reducing computational costs, using fewer floating point operations (FLOPs). This work demonstrates the effectiveness of modality-aware sparsity in improving SSMs, setting new benchmarks in the field of multi-modal learning."
                },
                "zh": {
                    "title": "模态感知稀疏性：提升SSM的多模态预训练效率",
                    "desc": "状态空间模型（SSMs）作为序列建模的有效替代方案，面临无法利用特定模态特征的问题。我们提出了一种新颖的SSM架构——Mixture-of-Mamba，通过对Mamba模块进行模态特定参数化，引入了模态感知稀疏性。该模型在多模态预训练中表现出色，能够在较早的训练步骤中达到相同的损失值，同时显著降低计算成本。我们的研究表明，模态感知稀疏性是一个有效的设计原则，能够将其影响从变换器扩展到SSMs，并在多模态预训练中设定新的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2403.09193",
            "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
            "url": "https://huggingface.co/papers/2403.09193",
            "abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.",
            "score": 3,
            "issue_id": 1911,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 марта",
                "en": "March 14",
                "zh": "3月14日"
            },
            "hash": "e5fc94d983fca41c",
            "authors": [
                "Paul Gavrikov",
                "Jovita Lukasik",
                "Steffen Jung",
                "Robert Geirhos",
                "Bianca Lamm",
                "Muhammad Jehanzeb Mirza",
                "Margret Keuper",
                "Janis Keuper"
            ],
            "affiliations": [
                "Google DeepMind",
                "ICG, Graz University of Technology",
                "IMLA, Offenburg University",
                "Max Planck Institute for Informatics, Saarland Informatics Campus",
                "University of Mannheim",
                "University of Siegen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2403.09193.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#ethics",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Текст направляет взгляд: как языковые подсказки влияют на визуальные предубеждения ИИ",
                    "desc": "Статья исследует визуальные предубеждения в мультимодальных моделях, объединяющих зрение и язык (VLM). Авторы обнаружили, что VLM часто более ориентированы на форму объектов, чем чисто визуальные модели. Эксперименты показали, что текстовые подсказки могут значительно влиять на баланс между ориентацией на форму и текстуру в VLM. Однако даже после оптимизации, VLM все еще уступают человеческому зрению в ориентации на форму объектов."
                },
                "en": {
                    "title": "Steering Visual Biases with Language in Vision Language Models",
                    "desc": "This paper investigates how vision language models (VLMs) incorporate human visual biases, particularly the texture vs. shape bias, which refers to the preference for local versus global information in images. The authors find that VLMs tend to be more shape-biased than traditional vision models, suggesting that language prompts can influence visual processing. Through experiments, they demonstrate that the shape bias can be adjusted significantly by changing the text prompts used with the models. However, despite these adjustments, the VLMs still do not fully match the strong human bias towards shape recognition."
                },
                "zh": {
                    "title": "通过语言引导视觉偏差的可能性",
                    "desc": "视觉语言模型（VLMs）在计算机视觉领域带来了显著变化，支持从零样本图像分类到图像描述和视觉问答等多种应用。这些模型通过语言提示提供了一种直观的方式来访问视觉内容。我们研究了VLMs中存在的视觉偏差，特别是纹理与形状偏差，发现VLMs在形状偏差上往往比纯视觉模型更强。这表明，通过文本的多模态融合，视觉偏差可以在一定程度上被调节，且我们可以通过语言来引导视觉偏差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15427",
            "title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas",
            "url": "https://huggingface.co/papers/2501.15427",
            "abstract": "Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release our synthetic characters and instruction-tuning dialogues to support public research.",
            "score": 3,
            "issue_id": 1910,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "fa7a70d2c9f398b9",
            "authors": [
                "Xiaoyang Wang",
                "Hongming Zhang",
                "Tao Ge",
                "Wenhao Yu",
                "Dian Yu",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15427.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#agents",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Обучение ИИ искусству перевоплощения",
                    "desc": "Исследование посвящено обучению больших языковых моделей (LLM) способности к обобщению характеров персонажей. Авторы синтезируют большой набор профилей персонажей и диалогов для инструктивной настройки модели. Используя эти данные, они проводят supervised fine-tuning модели LLaMA-3 8B. Полученная модель показывает результаты, сравнимые с GPT-4 в задачах ролевого диалога."
                },
                "en": {
                    "title": "Empowering LLMs with Character Generalization for Role-Playing",
                    "desc": "This paper discusses how to improve large language models (LLMs) for role-playing tasks by enabling them to adopt different character personas. The authors create a large dataset of character profiles and use two methods—response rewriting and response generation—to produce responses that match these characters. They then fine-tune the LLaMA-3 8B model with this synthetic data to enhance its ability to generate character-aligned dialogues. The results show that their improved model performs similarly to advanced models like GPT-4o in role-playing scenarios, and they provide their resources for further research."
                },
                "zh": {
                    "title": "增强大型语言模型的角色扮演能力",
                    "desc": "本文研究了如何通过大规模数据合成来增强大型语言模型（LLMs）的角色扮演能力。我们首先利用Persona Hub合成大量角色档案，然后探索了两种策略：响应重写和响应生成，以创建与角色对齐的指令响应。通过对LLaMA-3 8B模型进行监督微调（SFT），我们验证了合成指令调优数据在角色泛化方面的有效性。最终，我们的最佳模型在角色扮演对话中表现出色，达到了与GPT-4o模型相当的性能，并公开发布了合成角色和指令调优对话以支持公共研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12370",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "url": "https://huggingface.co/papers/2501.12370",
            "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.",
            "score": 3,
            "issue_id": 1905,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "bffcdc51c572d8f2",
            "authors": [
                "Samira Abnar",
                "Harshay Shah",
                "Dan Busbridge",
                "Alaaeldin Mohamed Elnouby Ali",
                "Josh Susskind",
                "Vimal Thilak"
            ],
            "affiliations": [
                "Apple",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12370.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимальная разреженность - ключ к эффективному масштабированию языковых моделей",
                    "desc": "Статья исследует взаимосвязь между количеством параметров и вычислительной мощностью в контексте разреженных моделей Mixture-of-Experts (MoE). Авторы изучают, как изменение уровня разреженности влияет на производительность модели во время предварительного обучения и последующей оценки few-shot. Результаты показывают, что существует оптимальный уровень разреженности, который улучшает как эффективность обучения, так и производительность модели. Это исследование дополняет существующие работы в области масштабирования языковых моделей и предлагает insights для разработки более эффективных архитектур."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of Sparsity in Language Models",
                    "desc": "This paper investigates how to improve language models by scaling their capacity, focusing on two main factors: the number of parameters and the compute required for each example. It specifically looks at sparse Mixture-of-Experts (MoEs), which allow for a larger number of parameters without a corresponding increase in computational load. The authors explore how different levels of sparsity, or the proportion of inactive parameters, affect the model's performance during training and evaluation. Their findings suggest that there is an optimal level of sparsity that enhances both efficiency and performance, providing valuable insights for developing more effective machine learning architectures."
                },
                "zh": {
                    "title": "优化稀疏性，提升模型性能",
                    "desc": "本文探讨了语言模型容量的扩展，特别是在稀疏混合专家（MoEs）框架下。容量主要由模型参数数量和每个样本的计算量决定。研究发现，在不同的约束条件下，存在一个最佳的稀疏水平，可以提高训练效率和模型性能。此研究为理解稀疏性在MoEs扩展法则中的影响提供了新的视角，并为设计更高效的架构提供了见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16273",
            "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
            "url": "https://huggingface.co/papers/2501.16273",
            "abstract": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.",
            "score": 2,
            "issue_id": 1910,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "bd97733bda9e3557",
            "authors": [
                "Mohamed Elfeki",
                "Rui Liu",
                "Chad Voegele"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16273.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Энкодер-декодер: эффективное решение для малых языковых моделей",
                    "desc": "Исследование показывает преимущества архитектуры энкодер-декодер для малых языковых моделей (до 1 млрд параметров) по сравнению с декодер-онли моделями. На периферийных устройствах энкодер-декодер модели демонстрируют на 47% меньшую задержку первого токена и в 4,7 раза большую пропускную способность. Предложен новый фреймворк дистилляции знаний, позволяющий энкодер-декодер моделям использовать возможности больших декодер-онли учителей. Результаты исследования ставят под сомнение тренд на масштабирование декодер-онли архитектур, особенно для ресурсно-ограниченных сред."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of Encoder-Decoder Models in Small Language Tasks",
                    "desc": "This paper highlights the advantages of encoder-decoder architectures over large decoder-only language models, especially for small language models (SLMs) with 1 billion parameters or fewer. The authors demonstrate that encoder-decoder models can achieve significantly lower latency and higher throughput on edge devices due to their efficient processing of input and separation of understanding and generation phases. They introduce a new knowledge distillation framework that allows these models to benefit from the capabilities of larger decoder-only models while maintaining their efficiency. The study concludes that as parameter budgets decrease, the choice of architecture becomes critical for effective deployment in resource-constrained environments."
                },
                "zh": {
                    "title": "编码-解码架构的优势与应用",
                    "desc": "本论文分析了编码-解码架构在小型语言模型（SLMs）中的优势，尤其是在边缘设备上的表现。研究表明，编码-解码模型在首次令牌延迟上比仅解码模型低47%，并且吞吐量提高了4.7倍。这种优势源于编码-解码架构的一次性输入处理和理解与生成阶段的高效分离。我们还提出了一种新的知识蒸馏框架，使编码-解码模型能够利用大型解码教师的能力，同时保持其架构优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14912",
            "title": "Feasible Learning",
            "url": "https://huggingface.co/papers/2501.14912",
            "abstract": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL demands satisfactory performance on every individual data point. Since any model that meets the prescribed performance threshold is a valid FL solution, the choice of optimization algorithm and its dynamics play a crucial role in shaping the properties of the resulting solutions. In particular, we study a primal-dual approach which dynamically re-weights the importance of each sample during training. To address the challenge of setting a meaningful threshold in practice, we introduce a relaxation of FL that incorporates slack variables of minimal norm. Our empirical analysis, spanning image classification, age regression, and preference optimization in large language models, demonstrates that models trained via FL can learn from data while displaying improved tail behavior compared to ERM, with only a marginal impact on average performance.",
            "score": 2,
            "issue_id": 1898,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "7ded44debecf7694",
            "authors": [
                "Juan Ramirez",
                "Ignacio Hounie",
                "Juan Elenter",
                "Jose Gallego-Posada",
                "Meraj Hashemizadeh",
                "Alejandro Ribeiro",
                "Simon Lacoste-Julien"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Mila & Université de Montréal",
                "Spotify",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14912.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Индивидуальный подход к каждому образцу данных",
                    "desc": "В статье представлена новая парадигма обучения моделей машинного обучения - Feasible Learning (FL). В отличие от традиционного подхода минимизации эмпирического риска (ERM), FL стремится обеспечить удовлетворительную производительность для каждого отдельного образца данных. Авторы предлагают примально-двойственный подход, который динамически переопределяет важность каждого образца во время обучения. Эмпирический анализ на задачах классификации изображений, регрессии возраста и оптимизации предпочтений в больших языковых моделях показывает, что модели, обученные с помощью FL, демонстрируют улучшенное поведение на редких случаях по сравнению с ERM."
                },
                "en": {
                    "title": "Ensuring Individual Sample Success with Feasible Learning",
                    "desc": "Feasible Learning (FL) is a new approach in machine learning that focuses on ensuring each training sample meets a specific performance standard, rather than just optimizing for overall average performance like traditional methods. This paradigm treats the training process as a feasibility problem, where any model that satisfies the performance criteria for all samples is considered valid. The paper explores a primal-dual optimization technique that adjusts the importance of each sample during training, enhancing the model's ability to learn effectively. Through various applications, including image classification and language model optimization, FL shows improved performance on challenging cases while maintaining similar average results compared to conventional methods."
                },
                "zh": {
                    "title": "可行学习：每个样本都要优秀！",
                    "desc": "我们介绍了一种新的学习范式，称为可行学习（Feasible Learning，FL），它通过解决一个可行性问题来训练模型，从而限制每个训练样本的损失。与传统的经验风险最小化（Empirical Risk Minimization，ERM）框架不同，FL要求每个数据点都能达到满意的性能。FL的有效性依赖于优化算法的选择及其动态调整样本重要性的能力。我们的实证分析表明，使用FL训练的模型在图像分类、年龄回归和大语言模型的偏好优化中，能够在保持平均性能的同时，改善模型在极端情况下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14723",
            "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
            "url": "https://huggingface.co/papers/2501.14723",
            "abstract": "Scaling test-time compute is a promising axis for improving LLM capabilities. However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research. Here, we explore this problem in the context of solving real-world GitHub issues from the SWE-bench dataset. Our system, named CodeMonkeys, allows models to iteratively edit a codebase by jointly generating and running a testing script alongside their draft edit. We sample many of these multi-turn trajectories for every issue to generate a collection of candidate edits. This approach lets us scale \"serial\" test-time compute by increasing the number of iterations per trajectory and \"parallel\" test-time compute by increasing the number of trajectories per problem. With parallel scaling, we can amortize up-front costs across multiple downstream samples, allowing us to identify relevant codebase context using the simple method of letting an LLM read every file. In order to select between candidate edits, we combine voting using model-generated tests with a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys resolves 57.4% of issues from SWE-bench Verified using a budget of approximately 2300 USD. Our selection method can also be used to combine candidates from different sources. Selecting over an ensemble of edits from existing top SWE-bench Verified submissions obtains a score of 66.2% and outperforms the best member of the ensemble on its own. We fully release our code and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
            "score": 1,
            "issue_id": 1912,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "0aee5401febd2bf6",
            "authors": [
                "Ryan Ehrlich",
                "Bradley Brown",
                "Jordan Juravsky",
                "Ronald Clark",
                "Christopher Ré",
                "Azalia Mirhoseini"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14723.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#plp",
                    "#open_source"
                ],
                "emoji": "🐒",
                "ru": {
                    "title": "CodeMonkeys: Масштабирование вычислений LLM для решения реальных задач программирования",
                    "desc": "Статья представляет систему CodeMonkeys для решения реальных проблем GitHub с помощью больших языковых моделей (LLM). Система позволяет моделям итеративно редактировать кодовую базу, генерируя и запуская тестовые скрипты вместе с черновыми правками. CodeMonkeys использует как последовательное, так и параллельное масштабирование вычислений во время тестирования, что позволяет эффективно идентифицировать релевантный контекст кодовой базы. Метод выбора кандидатов на основе голосования и финальной многоходовой траектории позволил системе решить 57.4% проблем из набора данных SWE-bench Verified."
                },
                "en": {
                    "title": "Enhancing LLMs with Scalable Test-Time Compute for Code Editing",
                    "desc": "This paper presents CodeMonkeys, a system designed to enhance the capabilities of large language models (LLMs) by scaling test-time compute during code editing tasks. It combines iterative code generation with testing script execution, allowing models to refine their edits through multiple iterations and trajectories. By leveraging both serial and parallel scaling, CodeMonkeys efficiently identifies relevant code context and selects the best candidate edits through a voting mechanism. The system demonstrates effectiveness by resolving over 57% of real-world GitHub issues while optimizing resource usage, and it shows improved performance when combining edits from various sources."
                },
                "zh": {
                    "title": "通过CodeMonkeys提升代码编辑能力",
                    "desc": "本文探讨了如何通过扩展测试时计算来提升大型语言模型（LLM）的能力。我们提出了一个名为CodeMonkeys的系统，它可以通过生成和运行测试脚本来迭代编辑代码库，从而解决实际的GitHub问题。该方法通过增加每个问题的迭代次数和轨迹数量，实现了串行和并行的测试时计算扩展。最终，CodeMonkeys成功解决了57.4%的问题，并且我们的选择方法也能有效结合来自不同来源的候选编辑。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15420",
            "title": "Visual Generation Without Guidance",
            "url": "https://huggingface.co/papers/2501.15420",
            "abstract": "Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT.",
            "score": 1,
            "issue_id": 1912,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 января",
                "en": "January 26",
                "zh": "1月26日"
            },
            "hash": "d7e67912a685cbf9",
            "authors": [
                "Huayu Chen",
                "Kai Jiang",
                "Kaiwen Zheng",
                "Jianfei Chen",
                "Hang Su",
                "Jun Zhu"
            ],
            "affiliations": [
                "Department of Computer Science & Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15420.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GFT: Эффективная генерация изображений без направляющей выборки",
                    "desc": "Статья представляет новый метод обучения визуальных генеративных моделей - Guidance-Free Training (GFT). GFT позволяет достичь производительности Classifier-Free Guidance (CFG), но требует вдвое меньше вычислений при генерации изображений. Метод прост в реализации и может применяться для обучения моделей с нуля. Эксперименты показали эффективность GFT для различных типов моделей, включая диффузионные, авторегрессионные и модели с маскированием."
                },
                "en": {
                    "title": "Guidance-Free Training: Simplifying Visual Generative Models",
                    "desc": "This paper introduces Guidance-Free Training (GFT), a new approach for visual generative models that eliminates the need for classifier-free guidance during sampling. GFT achieves similar performance to traditional Classifier-Free Guidance (CFG) while only requiring a single model for inference, thus reducing computational costs by half. The method allows for training from scratch, avoiding reliance on pre-trained CFG networks, and retains the same maximum likelihood objective as CFG with minimal changes to existing implementations. Extensive experiments show that GFT performs comparably or better than CFG across various visual modeling domains, maintaining a good balance between diversity and fidelity."
                },
                "zh": {
                    "title": "无引导训练：降低计算成本的视觉生成新方法",
                    "desc": "无引导采样的视觉模型是本研究的核心。我们提出的无引导训练（GFT）算法在性能上与传统的分类器引导（CFG）相当，但只需使用单一模型进行采样，从而减少了计算成本。GFT可以直接从头开始训练，而不依赖于预训练的CFG网络，且实现简单。通过在五种不同的视觉模型上进行广泛实验，我们证明了GFT的有效性和多样性。"
                }
            }
        }
    ],
    "link_prev": "2025-01-27.html",
    "link_next": "2025-01-29.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1月27日"
    },
    "short_date_next": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1月29日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 4,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "我们介绍了 Baichuan-Omni-1.5，这是一个具有全模态理解和端到端音频生成能力的模型。为了实现流畅的高质量跨模态交互，我们优化了三个关键方面。首先，我们建立了一个全面的数据清洗和合成管道，获得了大约 500B 的高质量数据（文本、音频和视觉）。其次，我们设计了一个音频分词器，捕捉音频的语义和声学信息。最后，我们设计了一个多阶段训练策略，确保所有模态的有效协同。Baichuan-Omni-1.5 在全模态能力方面领先于当前模型，并在多个多模态医疗基准上取得了可比的结果。",
        "title": "Baichuan-Omni-1.5 Technical Report",
        "pinyin": "Wǒmen jièshào le Baichuan-Omni-1.5, zhè shì yīgè jùyǒu quán móshì lǐjiě hé duān dào duān yīnpiàn shēngchéng nénglì de móxíng. Wèile shíxiàn liúchàng de gāo zhìliàng kuà móshì jiāohù, wǒmen yōuhuà le sān gè guǎnjiàn fāngcè. Shǒuxiān, wǒmen jiànlì le yīgè quánmiàn de shùjù qīngxī hé héchéng guǎndǎo, huòdé le dàyuē 500B de gāo zhìliàng shùjù (wénběn, yīnpiàn hé shìjué). Qícì, wǒmen shèjì le yīgè yīnpiàn fēncíqì, bīngzhuō yīnpiàn de yǔyì hé shēngxué xìnxī. Zuìhòu, wǒmen shèjì le yīgè duō jiēduàn xùnliàn cèlüè, quèbǎo suǒyǒu móshì de yǒuxiào xiétóng. Baichuan-Omni-1.5 zài quán móshì nénglì fāngmiàn lǐngxiān yú dāngqián móxíng, bìng zài duō gè duō móshì yīliáo jīzhǔn shàng qudé le kěbǐ de jiéguǒ.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},\n{'word': 'Baichuan-Omni-1.5', 'pinyin': 'Bài chuān-Ōu mí-1.5', 'trans': 'Baichuan-Omni-1.5'},\n{'word': '具有', 'pinyin': 'jù yǒu', 'trans': 'have'},\n{'word': '全模态', 'pinyin': 'quán mó shì', 'trans': 'full modality'},\n{'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understanding'},\n{'word': '端到端', 'pinyin': 'duān dào duān', 'trans': 'end-to-end'},\n{'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '流畅', 'pinyin': 'liú chàng', 'trans': 'smooth'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '跨模态', 'pinyin': 'kuà mó shì', 'trans': 'cross-modality'},\n{'word': '交互', 'pinyin': 'jiāo hù', 'trans': 'interaction'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'},\n{'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'},\n{'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'},\n{'word': '建立', 'pinyin': 'jiàn lì', 'trans': 'establish'},\n{'word': '全面', 'pinyin': 'quán miàn', 'trans': 'comprehensive'},\n{'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'},\n{'word': '清洗', 'pinyin': 'qīng xǐ', 'trans': 'clean'},\n{'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesize'},\n{'word': '管道', 'pinyin': 'guǎn dào', 'trans': 'pipeline'},\n{'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'},\n{'word': '大约', 'pinyin': 'dà yuē', 'trans': 'about'},\n{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'},\n{'word': '分词器', 'pinyin': 'fēn cí qì', 'trans': 'tokenizer'},\n{'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'},\n{'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantics'},\n{'word': '声学', 'pinyin': 'shēng xué', 'trans': 'acoustics'},\n{'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'},\n{'word': '多阶段', 'pinyin': 'duō jiē duàn', 'trans': 'multi-stage'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '协同', 'pinyin': 'xié tóng', 'trans': 'coordination'},\n{'word': '领先', 'pinyin': 'lǐng xiān', 'trans': 'lead'},\n{'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '可比', 'pinyin': 'kě bǐ', 'trans': 'comparable'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}]",
        "trans": "We introduce Baichuan-Omni-1.5, a model with full-modal understanding and end-to-end audio generation capabilities. To achieve smooth, high-quality cross-modal interaction, we optimized three key aspects. First, we established a comprehensive data cleaning and synthesis pipeline, obtaining approximately 500B high-quality data (text, audio, and visual). Second, we designed an audio tokenizer to capture the semantic and acoustic information of audio. Lastly, we designed a multi-stage training strategy to ensure effective collaboration across all modalities. Baichuan-Omni-1.5 leads in full-modal capabilities and achieves comparable results on multiple multimodal medical benchmarks.",
        "update_ts": "2025-01-28 09:10"
    }
}