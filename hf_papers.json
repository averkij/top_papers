{
    "date": {
        "ru": "4 декабря",
        "en": "December 4",
        "zh": "12月4日"
    },
    "time_utc": "2024-12-04 08:29",
    "weekday": 2,
    "issue_id": 938,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19943",
            "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability",
            "url": "https://huggingface.co/papers/2411.19943",
            "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.",
            "score": 20,
            "issue_id": 933,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "aaf523f6bd9412e3",
            "authors": [
                "Zicheng Lin",
                "Tian Liang",
                "Jiahao Xu",
                "Xing Wang",
                "Ruilin Luo",
                "Chufan Shi",
                "Siheng Li",
                "Yujiu Yang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Tsinghua University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19943.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности рассуждений LLM путем выявления критических токенов",
                    "desc": "Исследование посвящено влиянию отдельных токенов на результаты рассуждений в больших языковых моделях (LLM). Авторы обнаружили существование 'критических токенов', которые приводят к неправильным траекториям рассуждений. Они предложили новый метод cDPO для автоматического распознавания и обучения с учетом критических токенов в процессе выравнивания модели. Экспериментальные результаты на бенчмарках GSM8K и MATH500 с использованием моделей Llama-3 и deepseek-math продемонстрировали эффективность предложенного подхода."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs by Identifying Critical Tokens",
                    "desc": "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."
                },
                "zh": {
                    "title": "识别关键token，提升推理准确性",
                    "desc": "大型语言模型（LLMs）在推理任务中表现出色，能够通过自回归的方式生成推理过程。本文探讨了单个token对推理任务最终结果的影响，发现存在“关键token”，这些token会导致错误的推理轨迹。我们提出了一种新方法cDPO，旨在自动识别关键token并在对齐过程中进行token级奖励。通过对比正负模型的生成可能性，我们能够识别出在错误轨迹中导致错误结果的关键token，从而提高模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02259",
            "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
            "url": "https://huggingface.co/papers/2412.02259",
            "abstract": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.",
            "score": 19,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "5a007f38be3e3ba7",
            "authors": [
                "Mingzhe Zheng",
                "Yongqi Xu",
                "Haojian Huang",
                "Xuran Ma",
                "Yexin Liu",
                "Wenjie Shu",
                "Yatian Pang",
                "Feilong Tang",
                "Qifeng Chen",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "Peking University",
                "University of Central Florida",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02259.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#story_generation",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VGoT: Новый уровень в генерации многокадровых видео с сохранением логики повествования",
                    "desc": "Статья представляет новый подход к генерации многокадровых видео под названием VideoGen-of-Thought (VGoT). Эта архитектура разделяет процесс на несколько этапов: генерация сценария, создание ключевых кадров, генерация отдельных сцен и механизм сглаживания. VGoT использует встраивания для сохранения идентичности персонажей между сценами и применяет межкадровое сглаживание для обеспечения визуальной согласованности. Эксперименты показывают, что VGoT превосходит существующие методы в создании качественных и связных многокадровых видео."
                },
                "en": {
                    "title": "Revolutionizing Multi-Shot Video Generation with VGoT",
                    "desc": "The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots."
                },
                "zh": {
                    "title": "多镜头视频生成的新突破",
                    "desc": "当前的视频生成模型在生成短片方面表现出色，但在创建多镜头、电影般的视频时仍然存在困难。现有模型通常只针对单镜头目标进行训练，因此在保持逻辑故事线和视觉一致性方面显得不足。为此，我们提出了VideoGen-of-Thought（VGoT），这是一种专门为多镜头视频生成设计的协作和无训练架构。VGoT通过脚本生成、关键帧生成和镜头级视频生成等模块化步骤，确保了合理的叙事设计和跨镜头的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01981",
            "title": "Free Process Rewards without Process Labels",
            "url": "https://huggingface.co/papers/2412.01981",
            "abstract": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.",
            "score": 14,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13434e4f301a0d88",
            "authors": [
                "Lifan Yuan",
                "Wendi Li",
                "Huayu Chen",
                "Ganqu Cui",
                "Ning Ding",
                "Kaiyan Zhang",
                "Bowen Zhou",
                "Zhiyuan Liu",
                "Hao Peng"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01981.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение PRM без пошаговой разметки",
                    "desc": "Статья представляет новый подход к обучению процессуальных моделей вознаграждения (PRM) в машинном обучении. Авторы показывают, что неявную PRM можно получить без дополнительных затрат, просто обучая модель вознаграждения за результат (ORM) на более дешевых метках уровня ответов. Эксперименты на датасете MATH демонстрируют, что предложенный метод превосходит сильный бейзлайн на основе MCTS, используя менее 1/38 обучающих данных. Исследование также выявляет, что масштабирование инструкций и ответов улучшает производительность неявной PRM."
                },
                "en": {
                    "title": "Unlocking Efficient Training for Process Reward Models",
                    "desc": "This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples."
                },
                "zh": {
                    "title": "隐式过程奖励模型：高效训练的新思路",
                    "desc": "本文提出了一种新的过程奖励模型（PRM），与传统的结果奖励模型（ORM）不同，PRM能够逐步评估推理过程，提供更细致的奖励。然而，训练PRM需要在每个中间步骤都有标注，这在数据收集上面临挑战。我们展示了通过训练ORM并使用响应级别的标签，可以在没有额外成本的情况下获得隐式PRM。实验结果表明，隐式PRM在数据效率和性能上优于传统方法，尤其是在数据稀缺的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02611",
            "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?",
            "url": "https://huggingface.co/papers/2412.02611",
            "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.",
            "score": 11,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "f63565048b4948b4",
            "authors": [
                "Kaixiong Gong",
                "Kaituo Feng",
                "Bohao Li",
                "Yibing Wang",
                "Mofan Cheng",
                "Shijia Yang",
                "Jiaming Han",
                "Benyou Wang",
                "Yutong Bai",
                "Zhuoran Yang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "CUHK MMLab",
                "Stanford University",
                "UC Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02611.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Слышат ли ИИ-модели то, что видят?",
                    "desc": "Статья представляет новый тест DeafTest и бенчмарк AV-Odyssey Bench для оценки мультимодальных больших языковых моделей (MLLM) в задачах аудио-визуального понимания. Исследователи обнаружили, что современные MLLM часто не справляются с простыми задачами определения громкости и высоты звуков. Бенчмарк включает 4,555 задач с текстом, изображениями и аудио, требующих эффективного использования обоих модальностей. Авторы провели тестирование ряда закрытых и открытых моделей, выявив ограничения текущих систем для дальнейшего улучшения сбора данных и разработки моделей."
                },
                "en": {
                    "title": "Unveiling the Limits of Multimodal Models with AV-Odyssey Bench",
                    "desc": "This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation."
                },
                "zh": {
                    "title": "揭示多模态模型的局限性",
                    "desc": "最近，多模态大型语言模型（MLLMs）如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其在视觉和音频方面的能力。尽管这些模型在多种音频-视觉应用中表现出色，但我们的DeafTest显示，MLLMs在一些人类认为简单的任务上常常表现不佳，例如判断两个声音哪个更响和哪个音调更高。为此，我们提出了AV-Odyssey Bench，这是一个全面的音频-视觉基准，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准包含4555个精心设计的问题，要求模型有效利用视觉和音频输入中的线索，以准确推断答案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02632",
            "title": "Scaling Image Tokenizers with Grouped Spherical Quantization",
            "url": "https://huggingface.co/papers/2412.02632",
            "abstract": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.",
            "score": 5,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "60eda94a31cded90",
            "authors": [
                "Jiangtao Wang",
                "Zhen Qin",
                "Yifan Zhang",
                "Vincent Tao Hu",
                "Björn Ommer",
                "Rania Briq",
                "Stefan Kesselheim"
            ],
            "affiliations": [
                "CompVis @ LMU Munich",
                "Jülich Supercomputing Centre",
                "TapTap",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02632.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#cv",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "GSQ: Эффективная токенизация изображений на сферической поверхности",
                    "desc": "В статье представлен новый метод токенизации изображений - Групповая Сферическая Квантизация (GSQ). GSQ использует сферическую инициализацию кодовой книги и регуляризацию поиска для ограничения латентного пространства кодовой книги сферической поверхностью. Авторы провели эмпирический анализ стратегий обучения токенизаторов изображений и показали, что GSQ-GAN превосходит современные методы по качеству реконструкции при меньшем количестве итераций обучения. Исследование масштабируемости GSQ выявило различное поведение при высоких и низких уровнях пространственного сжатия, подчеркивая сложности представления высокоразмерных латентных пространств."
                },
                "en": {
                    "title": "Efficient Image Processing with Grouped Spherical Quantization",
                    "desc": "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."
                },
                "zh": {
                    "title": "分组球面量化：高效的视觉标记器新方法",
                    "desc": "本文介绍了一种新的视觉标记器方法，称为分组球面量化（GSQ），旨在解决传统方法中的一些问题。GSQ通过球面代码本初始化和查找正则化，限制了代码本潜在空间在球面上的分布。我们的实证分析表明，GSQ-GAN在重建质量上优于现有的最先进方法，并且训练迭代次数更少。研究还系统地考察了GSQ在潜在维度、代码本大小和压缩比等方面的扩展行为，揭示了高维潜在空间表示的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01292",
            "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
            "url": "https://huggingface.co/papers/2412.01292",
            "abstract": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
            "score": 4,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "e8f8ddd05e13e9ef",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Junyan Li",
                "Shuailei Ma",
                "Xinyu Sun",
                "Tianhang Xiang",
                "Yinjie Lei",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "Northeastern University",
                "Pazhou Laboratory",
                "Sichuan University",
                "South China University of Technology",
                "Tencent Robotics X",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01292.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Умное понимание 3D-сцен с LSceneLLM",
                    "desc": "Исследование 3D Vision-Language Models (3D-VLMs) важно для развития воплощенного AI в 3D-сценах, таких как визуальная навигация и ответы на вопросы. Из-за высокой плотности визуальных признаков в больших 3D-сценах сложно точно определить важную для задачи информацию. Предлагается адаптивная структура LSceneLLM, которая автоматически определяет важные области, используя визуальные предпочтения LLM для разных задач. Эксперименты показывают, что наш метод превосходит существующие методы в понимании больших сцен и улучшает результаты при интеграции в существующие 3D-VLMs."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with LSceneLLM",
                    "desc": "This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods."
                },
                "zh": {
                    "title": "自适应3D视觉语言模型，提升场景理解能力",
                    "desc": "3D视觉语言模型（3D-VLMs）的研究越来越受到关注，这对在3D场景中发展具身人工智能至关重要，如视觉导航和具身问答。由于3D场景中视觉特征的高密度，准确定位与任务相关的视觉信息变得具有挑战性。现有方法尝试对所有对象进行分割，并将其特征视为场景表示，但这些与任务无关的对象特征包含大量冗余信息和缺失的细节。为了解决这些问题，我们提出了LSceneLLM，一个自适应框架，通过利用大语言模型（LLM）对不同任务的视觉偏好，自动识别与任务相关的区域，并通过可插拔的场景放大模块捕捉聚焦区域的细粒度细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19067",
            "title": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation",
            "url": "https://huggingface.co/papers/2411.19067",
            "abstract": "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.",
            "score": 3,
            "issue_id": 934,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "74d4a17af3574a5d",
            "authors": [
                "Minhyun Lee",
                "Seungho Lee",
                "Song Park",
                "Dongyoon Han",
                "Byeongho Heo",
                "Hyunjung Shim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19067.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#survey",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Маскирование для улучшения сегментации изображений по описанию",
                    "desc": "Статья посвящена задаче сегментации изображений по текстовому описанию (RIS). Авторы предлагают новый метод обучения под названием MaskRIS, который использует маскирование изображений и текста. Этот подход улучшает устойчивость модели к окклюзиям, неполной информации и языковым сложностям. Эксперименты показывают, что MaskRIS превосходит существующие методы как при полностью контролируемом, так и при слабо контролируемом обучении."
                },
                "en": {
                    "title": "Enhancing Referring Image Segmentation with Masking Techniques",
                    "desc": "Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets."
                },
                "zh": {
                    "title": "Masked Referring Image Segmentation：提升图像分割性能的新方法",
                    "desc": "引用图像分割（RIS）是一种先进的视觉-语言任务，旨在根据自由形式的文本描述识别和分割图像中的对象。本文探讨了有效的数据增强技术，并提出了一种新的训练框架，称为Masked Referring Image Segmentation（MaskRIS）。研究表明，传统的图像增强方法在RIS中效果不佳，而简单的随机遮罩显著提升了RIS的性能。MaskRIS结合了图像和文本遮罩，并采用了失真感知上下文学习（DCL），从而提高了模型对遮挡、不完整信息和语言复杂性的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02592",
            "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2412.02592",
            "abstract": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench",
            "score": 2,
            "issue_id": 937,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "91dbac114744b1e9",
            "authors": [
                "Junyuan Zhang",
                "Qintong Zhang",
                "Bin Wang",
                "Linke Ouyang",
                "Zichen Wen",
                "Ying Li",
                "Ka-Ho Chow",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The University of HongKong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02592.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#rag",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "OHRBench: раскрывая влияние OCR на системы RAG",
                    "desc": "OHRBench - это первый бенчмарк для оценки влияния оптического распознавания символов (OCR) на системы генерации с извлечением информации (RAG). Он включает 350 неструктурированных PDF-документов из шести реальных областей применения RAG, а также вопросы и ответы, созданные на основе мультимодальных элементов документов. Исследователи выделяют два основных типа шума OCR: семантический и форматный, и применяют возмущения для создания структурированных данных с различной степенью каждого типа шума. Результаты показывают уязвимость систем RAG к ошибкам OCR и потенциал использования мультимодальных языковых моделей (VLM) без OCR в системах RAG."
                },
                "en": {
                    "title": "Enhancing RAG: Understanding OCR's Impact with OHRBench",
                    "desc": "This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems."
                },
                "zh": {
                    "title": "揭示OCR对RAG系统的影响",
                    "desc": "本论文介绍了OHRBench，这是第一个用于理解光学字符识别（OCR）对检索增强生成（RAG）系统影响的基准。研究发现，OCR在处理非结构化PDF文档时会引入语义噪声和格式噪声，导致知识库质量下降。通过对350个真实世界应用领域的文档进行评估，结果显示现有的OCR解决方案无法有效构建高质量的知识库。最后，论文探讨了在RAG系统中使用视觉语言模型（VLMs）而不依赖OCR的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19542",
            "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
            "url": "https://huggingface.co/papers/2411.19542",
            "abstract": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.",
            "score": 2,
            "issue_id": 936,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "27226211eddf71d4",
            "authors": [
                "Luo Yu",
                "Liu Yucheng",
                "Shen Haihao"
            ],
            "affiliations": [
                "Intel Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19542.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение инференса ИИ на гибридных CPU: балансировка для максимальной производительности",
                    "desc": "Статья представляет динамический метод параллельных вычислений для гибридных процессоров, оптимизирующий инференс больших языковых моделей (LLM). Авторы обнаружили, что существующие фреймворки для инференса ИИ не учитывают неравномерные возможности ядер в гибридных CPU, что приводит к низкой производительности. Предложенный метод балансирует нагрузку между ядрами перед началом параллельной работы, что значительно повышает эффективность инференса LLM. В результате, инструмент Neural Speed достиг более 90% использования пропускной способности памяти на двух гибридных процессорах Intel."
                },
                "en": {
                    "title": "Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs",
                    "desc": "The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs."
                },
                "zh": {
                    "title": "动态平衡，提升AI推理性能！",
                    "desc": "AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，目前的AI推理框架忽视了混合CPU的不平衡硬件能力，导致推理性能低下。为了解决这个问题，我们提出了一种动态并行方法，显著提高了混合CPU的LLM推理性能，通过在并行工作开始之前平衡每个核心的工作负载。该方法使Neural Speed在两款混合Intel CPU上实现了超过90%的内存带宽。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01558",
            "title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval",
            "url": "https://huggingface.co/papers/2412.01558",
            "abstract": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .",
            "score": 1,
            "issue_id": 935,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "12235c4ebf26fe4a",
            "authors": [
                "Dhiman Paul",
                "Md Rizwan Parvez",
                "Nabeel Mohammed",
                "Shafin Rahman"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh",
                "Qatar Computing Research Institute (QCRI), Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01558.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#synthetic",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VideoLights: Новый подход к анализу ключевых моментов видео с помощью продвинутых нейросетевых архитектур",
                    "desc": "Статья представляет VideoLights - новую систему для обнаружения ключевых моментов видео и поиска по ним. Авторы предлагают улучшенные методы выравнивания видео и текста, двунаправленное слияние модальностей и механизм обратной связи между задачами. Они также вводят адаптивные функции потерь и используют большие мультимодальные модели для улучшения представления данных. Эксперименты показывают, что VideoLights превосходит существующие методы на нескольких наборах данных."
                },
                "en": {
                    "title": "Enhancing Video-Text Integration with VideoLights",
                    "desc": "This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis."
                },
                "zh": {
                    "title": "VideoLights：提升视频与文本分析的全新框架",
                    "desc": "本论文提出了一种名为VideoLights的视频高亮检测和时刻检索框架，旨在解决现有模型在视频与文本对齐和跨任务动态方面的不足。我们引入了卷积投影和特征精炼模块，以提高视频和文本特征的对齐效果，并采用双向跨模态融合网络来增强查询感知的片段表示。通过单向联合任务反馈机制，我们能够提升两个任务之间的相关性，同时引入硬正负损失以改善学习效果。实验结果表明，VideoLights在多个基准数据集上表现出色，达到了最先进的性能。"
                }
            }
        }
    ],
    "link_prev": "2024-12-03.html",
    "link_next": "2024-12-05.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12月3日"
    },
    "short_date_next": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12月5日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。",
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "pinyin": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。\n\nzhè piān wén zhāng jiè shào le duō mó tài dà yǔ yán mó xíng (MLLMs) zài shì jué lǐ jiě hé shēng chéng rèn wù zhōng de jìn zhàn. rán ér, shēng chéng jiāo cuò de tú xiàng-wén běn nèi róng réng shì yī gè tiǎo zhàn. xiàn yǒu de jī zhǔn cè shì yóu yú shù jù liàng hé duō yàng xìng de xiàn zhì, wú fǎ chōng fēn píng gū zhè xiē fāng fǎ. wèi le jiě jué zhè gè wèn tí, zuò zhě tí chū le GATE OpenING (OpenING), yī gè bāo hán 5,400 gè gāo zhì liàng rén gōng biāo zhù shí lì de quán miàn jī zhǔn, hán gǎi le 56 gè zhēn shí shì jiè rèn wù. OpenING hán gǎi le gè zhǒng rì cháng chǎng jīng, rú lǚ xíng zhǐ nán, shè jì hé tóu nǎo fēng bào, wèi tiǎo zhàn xìng de jiāo cuò shēng chéng fāng fǎ tí gōng le yī gè qiáng dà de píng tái. cǐ wài, zuò zhě hái jiè shào le IntJudge, yī gè yòng yú píng gū kāi fàng shì duō mó tài shēng chéng fāng fǎ de píng gū mó xíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"视觉理解\", \"pinyin\": \"shì jué lǐ jiě\", \"trans\": \"visual understanding\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"交错\", \"pinyin\": \"jiāo cuò\", \"trans\": \"interleaved\"},\n    {\"word\": \"图像-文本\", \"pinyin\": \"tú xiàng wén běn\", \"trans\": \"image-text\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"数据量\", \"pinyin\": \"shù jù liàng\", \"trans\": \"data volume\"},\n    {\"word\": \"多样性\", \"pinyin\": \"duō yàng xìng\", \"trans\": \"diversity\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"人工标注\", \"pinyin\": \"réngōng biāo zhù\", \"trans\": \"manual annotation\"},\n    {\"word\": \"实例\", \"pinyin\": \"shí lì\", \"trans\": \"instance\"},\n    {\"word\": \"真实世界\", \"pinyin\": \"zhēn shí shì jiè\", \"trans\": \"real world\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"日常场景\", \"pinyin\": \"rì cháng chǎng jǐng\", \"trans\": \"daily scenarios\"},\n    {\"word\": \"旅行指南\", \"pinyin\": \"lǚ xíng zhǐ nán\", \"trans\": \"travel guide\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"头脑风暴\", \"pinyin\": \"tóu nǎo fēng bào\", \"trans\": \"brainstorming\"},\n    {\"word\": \"平台\", \"pinyin\": \"píng tài\", \"trans\": \"platform\"},\n    {\"word\": \"评估模型\", \"pinyin\": \"píng gū mó xíng\", \"trans\": \"evaluation model\"},\n    {\"word\": \"开放式\", \"pinyin\": \"kāi fàng shì\", \"trans\": \"open-ended\"}\n]",
        "trans": "This article discusses the advancements of Multimodal Large Language Models (MLLMs) in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge. Existing benchmarks, due to limitations in data volume and diversity, are insufficient for evaluating these methods. To address this issue, the authors propose GATE OpenING (OpenING), a comprehensive benchmark containing 5,400 high-quality, manually annotated instances covering 56 real-world tasks. OpenING encompasses a variety of everyday scenarios, such as travel guides, design, and brainstorming, providing a robust platform for challenging interleaved generation methods. Additionally, the authors introduce IntJudge, a model for evaluating open-ended multimodal generation methods.",
        "update_ts": "2024-12-03 09:11"
    }
}