{
    "date": {
        "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 7",
        "zh": "8æœˆ7æ—¥"
    },
    "time_utc": "2025-08-07 03:29",
    "weekday": 3,
    "issue_id": 5220,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.03905",
            "title": "Sotopia-RL: Reward Design for Social Intelligence",
            "url": "https://huggingface.co/papers/2508.03905",
            "abstract": "Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.",
            "score": 6,
            "issue_id": 5220,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "815208c1d75b8f05",
            "authors": [
                "Haofei Yu",
                "Zhengyang Qi",
                "Yining Zhao",
                "Kolby Nottingham",
                "Keyang Xuan",
                "Bodhisattwa Prasad Majumder",
                "Hao Zhu",
                "Paul Pu Liang",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Carnegie Mellon University",
                "Massachusetts Institute of Technology",
                "Stanford University",
                "University of California Irvine",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03905.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#alignment",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Sotopia-RL: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Sotopia-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Sotopia-RL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Social Intelligence in LLMs with Sotopia-RL",
                    "desc": "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."
                },
                "zh": {
                    "title": "æå‡ç¤¾äº¤æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "Sotopia-RLæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚å®ƒé€šè¿‡å°†åé¦ˆç»†åŒ–ä¸ºå‘è¨€çº§åˆ«çš„å¤šç»´å¥–åŠ±ï¼Œæ¥æ”¹å–„æ¨¡å‹åœ¨ç¤¾äº¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç¤¾äº¤äº’åŠ¨ä¸­çš„éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œå¤šç»´æ€§é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¤æ‚çš„ç¤¾äº¤ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSotopia-RLåœ¨ç¤¾äº¤ç›®æ ‡å®Œæˆè¯„åˆ†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01191",
            "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
            "url": "https://huggingface.co/papers/2508.01191",
            "abstract": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",
            "score": 6,
            "issue_id": 5220,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 2",
                "zh": "8æœˆ2æ—¥"
            },
            "hash": "427ac75c7123b50a",
            "authors": [
                "Chengshuai Zhao",
                "Zhen Tan",
                "Pingchuan Ma",
                "Dawei Li",
                "Bohan Jiang",
                "Yancheng Wang",
                "Yingzhen Yang",
                "Huan Liu"
            ],
            "affiliations": [
                "Arizona State University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01191.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM: Ğ¼Ğ¸Ñ€Ğ°Ğ¶ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ DataAlchemy Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°, Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM."
                },
                "en": {
                    "title": "Unmasking the Fragility of CoT Reasoning in LLMs",
                    "desc": "This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs."
                },
                "zh": {
                    "title": "é“¾å¼æ€ç»´æ¨ç†çš„å±€é™æ€§ä¸æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚å¯¹å…¶å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCoTæ¨ç†å¹¶ä¸æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†å½¢å¼ï¼Œå› ä¸ºå®ƒçš„æœ‰æ•ˆæ€§å—åˆ°è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æŸ¥è¯¢ä¹‹é—´åˆ†å¸ƒå·®å¼‚çš„é™åˆ¶ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªåä¸ºDataAlchemyçš„æ§åˆ¶ç¯å¢ƒï¼Œä½œè€…ç³»ç»Ÿæ€§åœ°åˆ†æäº†CoTæ¨ç†åœ¨ä¸åŒä»»åŠ¡ã€é•¿åº¦å’Œæ ¼å¼ä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“CoTæ¨ç†è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶ï¼Œå…¶æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å®ç°çœŸæ­£å¯æ¨å¹¿æ¨ç†çš„æŒç»­æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04586",
            "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
            "url": "https://huggingface.co/papers/2508.04586",
            "abstract": "The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.",
            "score": 3,
            "issue_id": 5220,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "0e9d3ff69536a24d",
            "authors": [
                "Nuo Chen",
                "Moming Duan",
                "Andre Huikai Lin",
                "Qian Wang",
                "Jiaying Wu",
                "Bingsheng He"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04586.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#survey"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ¾ÑÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹, ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´, Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ, ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Community-Federated Conference (CFC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµÑ‚Ğ²Ğ¾Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ, Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¹, Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Towards Sustainable AI Conferences: A Community-Federated Approach",
                    "desc": "This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research."
                },
                "zh": {
                    "title": "æ„å»ºå¯æŒç»­çš„äººå·¥æ™ºèƒ½ä¼šè®®æ–°æ¨¡å¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡è¯Šæ–­äº†äººå·¥æ™ºèƒ½ä¼šè®®çš„ç»“æ„æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å‘è¡¨ç‡ã€ç¢³è¶³è¿¹ã€è´Ÿé¢ç¤¾åŒºæƒ…ç»ªå’Œåå‹¤æŒ‘æˆ˜ã€‚éšç€ä¼šè®®æ•°é‡çš„å¿«é€Ÿå¢é•¿ï¼Œä¼ ç»Ÿçš„é›†ä¸­å¼ä¼šè®®æ¨¡å¼å˜å¾—è¶Šæ¥è¶Šä¸å¯æŒç»­ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¤¾åŒºçš„è”åˆä¼šè®®æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä¿ƒè¿›ç§‘å­¦ä¼ æ’­çš„å…¬å¹³æ€§å’Œç¤¾åŒºçš„ç¦ç¥‰ã€‚è¯¥æ¨¡å‹å°†åŒè¡Œè¯„å®¡ã€å±•ç¤ºå’Œç½‘ç»œäº¤æµåˆ†å¼€ï¼Œæä¾›äº†ä¸€ç§æ›´å¯æŒç»­ã€åŒ…å®¹å’Œæœ‰éŸ§æ€§çš„äººå·¥æ™ºèƒ½ç ”ç©¶å‘å±•è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03789",
            "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
            "url": "https://huggingface.co/papers/2508.03789",
            "abstract": "HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.",
            "score": 3,
            "issue_id": 5220,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "a2a0678cfc88e0ef",
            "authors": [
                "Yuhang Ma",
                "Xiaoshi Wu",
                "Keqiang Sun",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CPII, InnoHK",
                "CUHK MMLab",
                "Kings College London",
                "Mizzen AI",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03789.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "HPSv3: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "HPSv3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HPDv3, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1.08 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ 1.17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Chain-of-Human-Preference Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Image Generation with Human Preference Score v3",
                    "desc": "The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç”Ÿæˆè´¨é‡çš„HPSv3ä¸CoHPæ–¹æ³•",
                    "desc": "HPSv3æ˜¯ä¸€ç§äººç±»åå¥½è¯„åˆ†ï¼Œåˆ©ç”¨å¹¿æ³›çš„æ•°æ®é›†å’Œè€ƒè™‘ä¸ç¡®å®šæ€§çš„æ’åæŸå¤±ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬å‘å¸ƒäº†HPDv3ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«108ä¸‡å¯¹æ–‡æœ¬-å›¾åƒå’Œ117ä¸‡å¯¹æ ‡æ³¨æ¯”è¾ƒçš„å¹¿æ³›äººç±»åå¥½æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åå¥½æ¨¡å‹ï¼Œä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ’åæŸå¤±è¿›è¡Œç»†è‡´æ’åã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†äººç±»åå¥½é“¾ï¼ˆCoHPï¼‰ï¼Œé€šè¿‡è¿­ä»£å›¾åƒä¼˜åŒ–ï¼Œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€ä½³å›¾åƒï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04664",
            "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
            "url": "https://huggingface.co/papers/2508.04664",
            "abstract": "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.",
            "score": 2,
            "issue_id": 5220,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "1575b65bda95c5f9",
            "authors": [
                "Mo Li",
                "L. H. Xu",
                "Qitai Tan",
                "Ting Cao",
                "Yunxin Liu"
            ],
            "affiliations": [
                "Independent Researcher",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.04664.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Sculptor: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Sculptor Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Sculptor Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Sculptor Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Sculptor: Mastering Memory for Better Long-Context Reasoning",
                    "desc": "The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies."
                },
                "zh": {
                    "title": "ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæå‡LLMæ€§èƒ½ï¼",
                    "desc": "Sculptoræ˜¯ä¸€ä¸ªç”¨äºä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸»åŠ¨ç®¡ç†æ³¨æ„åŠ›å’Œå·¥ä½œè®°å¿†ï¼Œå‡å°‘äº†å‰æœŸä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œæ”¹å–„æ¨ç†çš„å¯é æ€§ã€‚Sculptoræä¾›äº†ä¸‰ç§å·¥å…·ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ‘˜è¦ã€éšè—ä¸æ¢å¤ï¼Œä»¥åŠæ™ºèƒ½æœç´¢ï¼Œå¸®åŠ©LLMæ›´æœ‰æ•ˆåœ°å¤„ç†ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSculptoråœ¨æ²¡æœ‰ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23785",
            "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
            "url": "https://huggingface.co/papers/2507.23785",
            "abstract": "A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
            "score": 2,
            "issue_id": 5220,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "47c7686978b9c4dc",
            "authors": [
                "Bowen Zhang",
                "Sicheng Xu",
                "Chuxin Wang",
                "Jiaolong Yang",
                "Feng Zhao",
                "Dong Chen",
                "Baining Guo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23785.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#synthetic",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ VAE Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Objaverse. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Transforming Videos into Dynamic 3D Worlds",
                    "desc": "This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation."
                },
                "zh": {
                    "title": "ä»è§†é¢‘ç”Ÿæˆé«˜è´¨é‡åŠ¨æ€3Då†…å®¹çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸ªè§†é¢‘è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€3Då†…å®¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ç›´æ¥çš„4DMeshåˆ°é«˜æ–¯æ ·æ¡ï¼ˆGSï¼‰å˜åˆ†åœºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ç¼–ç 3DåŠ¨ç”»æ•°æ®ä¸­çš„é«˜æ–¯æ ·æ¡åŠå…¶æ—¶é—´å˜åŒ–ã€‚é€šè¿‡è¿™ç§é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ—¶é—´æ„ŸçŸ¥çš„é«˜æ–¯å˜åˆ†åœºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥è§†é¢‘å’Œé«˜æ–¯æ ·æ¡ç”ŸæˆåŠ¨æ€å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤„ç†çœŸå®è§†é¢‘è¾“å…¥æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02215",
            "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
            "url": "https://huggingface.co/papers/2508.02215",
            "abstract": "LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.",
            "score": 1,
            "issue_id": 5220,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "caa8de613517d011",
            "authors": [
                "Yike Zhang",
                "Zhiyuan He",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Yuqing Yang",
                "Jianyong Wang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02215.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ”ª",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LeanK - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ÑÑ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ Ğ² ĞºÑÑˆĞµ ĞºĞ»ÑÑ‡ĞµĞ¹. LeanK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑÑ‰ĞµĞ¹ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ´Ğ¾ 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² 1,3 Ñ€Ğ°Ğ·Ğ°."
                },
                "en": {
                    "title": "LeanK: Pruning for Efficient Language Model Performance",
                    "desc": "LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation."
                },
                "zh": {
                    "title": "LeanKï¼šé«˜æ•ˆè§£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ",
                    "desc": "LeanKæ˜¯ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸é‡è¦çš„å…³é”®ç¼“å­˜é€šé“ï¼Œä»è€Œé™ä½å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿè§£ç ï¼ŒåŒæ—¶ä¸å½±å“å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œå­¦ä¹ æ»¡è¶³ç‰¹å®šç¨€ç–æ¯”å’Œç¡¬ä»¶å¯¹é½è¦æ±‚çš„é€šé“é™æ€æ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanKå¯ä»¥å‡å°‘é«˜è¾¾70%çš„Kç¼“å­˜å’Œ16%-18%çš„Vç¼“å­˜å†…å­˜ï¼Œå¹¶ä¸”è‡ªå®šä¹‰è§£ç å†…æ ¸ä½¿æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æé«˜äº†1.3å€ã€‚é€šè¿‡åˆ†æå­¦ä¹ åˆ°çš„é‡è¦æ€§åˆ†å¸ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†è¿‡ç¨‹ä¸­æ¨¡å‹é€šé“å’Œæ³¨æ„åŠ›å¤´çš„æ·±å…¥è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01858",
            "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
            "url": "https://huggingface.co/papers/2508.01858",
            "abstract": "A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner",
            "score": 1,
            "issue_id": 5220,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 3",
                "zh": "8æœˆ3æ—¥"
            },
            "hash": "ad30239b3abef884",
            "authors": [
                "Yuhan Guo",
                "Cong Guo",
                "Aiwen Sun",
                "Hongliang He",
                "Xinyu Yang",
                "Yue Lu",
                "Yingji Zhang",
                "Xuntao Guo",
                "Dong Zhang",
                "Jianzhuang Liu",
                "Jiang Duan",
                "Yijia Xiao",
                "Liangjian Wen",
                "Hai-Ming Xu",
                "Yong Dai"
            ],
            "affiliations": [
                "Central South University",
                "Fudan University",
                "Harbin Institute of Technology",
                "Hithink Research",
                "Shanghai Jiao Tong University",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Southwestern University of Finance and Economics",
                "University of Adelaide",
                "University of California, Los Angeles",
                "University of Manchester",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01858.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Web-CogKnowledge Framework, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Web-CogDataset Ğ¸Ğ· 14 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Web-CogReasoner, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Web Agents through Structured Knowledge and Reasoning",
                    "desc": "This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“èƒ½åŠ›çš„åŒé‡åˆ†è§£ï¼šçŸ¥è¯†ä¸è®¤çŸ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç½‘ç»œæ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œå°†å…¶èƒ½åŠ›åˆ†è§£ä¸ºçŸ¥è¯†å†…å®¹å­¦ä¹ å’Œè®¤çŸ¥è¿‡ç¨‹ã€‚æˆ‘ä»¬å®šä¹‰äº†Web-CogKnowledgeæ¡†æ¶ï¼Œå°†çŸ¥è¯†åˆ†ä¸ºäº‹å®æ€§ã€æ¦‚å¿µæ€§å’Œç¨‹åºæ€§ä¸‰ç±»ï¼Œä»¥æ”¯æŒæ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ¨ç†ã€‚é€šè¿‡æ„å»ºWeb-CogDatasetï¼Œæˆ‘ä»¬ä¸ºæ™ºèƒ½ä½“æä¾›äº†ç³»ç»ŸåŒ–çš„çŸ¥è¯†åŸºç¡€ï¼Œå¸®åŠ©å…¶æŒæ¡å¿…è¦çš„æ ¸å¿ƒçŸ¥è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†Web-CogReasonerï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„åŒ–çŸ¥è¯†è‡³å…³é‡è¦çš„æƒ…å†µä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01197",
            "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
            "url": "https://huggingface.co/papers/2508.01197",
            "abstract": "A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.",
            "score": 1,
            "issue_id": 5220,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 2",
                "zh": "8æœˆ2æ—¥"
            },
            "hash": "d7be41190836a7cc",
            "authors": [
                "Zhan Shi",
                "Song Wang",
                "Junbo Chen",
                "Jianke Zhu"
            ],
            "affiliations": [
                "College of Computer Science, Zhejiang University, Hangzhou 310027, China",
                "College of Software Technology, Zhejiang University",
                "Udeer.ai, Hangzhou 310000, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01197.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GroundingOcc, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ 2D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Object Perception with 3D Occupancy Grounding",
                    "desc": "This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨é©¾é©¶ç‰©ä½“æ„ŸçŸ¥çš„3Då ç”¨åŸºç¡€è§†è§‰å®šä½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ¨¡å‹ï¼Œç”¨äºé€šè¿‡è‡ªç„¶è¯­è¨€å’Œä½“ç´ çº§æ³¨é‡Šè¿›è¡Œ3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ä¸­çš„ç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ã€‚ç°æœ‰çš„è§†è§‰å®šä½ä»»åŠ¡é€šå¸¸ä¾èµ–äºè¾¹ç•Œæ¡†ï¼Œè¿™ç§æ–¹æ³•æ— æ³•æ•æ‰åˆ°ç»†ç²’åº¦çš„ç»†èŠ‚ï¼Œå¯¼è‡´ç‰©ä½“è¡¨ç¤ºä¸å‡†ç¡®ã€‚æˆ‘ä»¬å¼•å…¥çš„GroundingOccæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œä»ç²—åˆ°ç»†åœ°é¢„æµ‹ç‰©ä½“ä½ç½®å’Œå ç”¨ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„åŸºå‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-06.html",
    "link_next": "2025-08-08.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "06.08",
        "en": "08/06",
        "zh": "8æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.08",
        "en": "08/08",
        "zh": "8æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}