{
    "date": {
        "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 15",
        "zh": "4æœˆ15æ—¥"
    },
    "time_utc": "2025-04-15 06:16",
    "weekday": 1,
    "issue_id": 3240,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 40,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ prima.cpp - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ´Ğ¾ 70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ CPU/GPU, Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº RAM/VRAM Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Wi-Fi Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Prima.cpp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ mmap Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ»ÑŒÑ†ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ğ´Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Halda Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "è®©å®¶åº­è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºprima.cppçš„åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ™®é€šå®¶åº­è®¾å¤‡ä¸Šè¿è¡Œ70Bè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ··åˆä½¿ç”¨CPUå’ŒGPUï¼Œä¼˜åŒ–å†…å­˜å’Œå¸¦å®½çš„ä½¿ç”¨ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ¡ˆå¯¹é«˜æ€§èƒ½ç¡¬ä»¶çš„ä¾èµ–ã€‚å®ƒé‡‡ç”¨äº†mmapç®¡ç†æ¨¡å‹æƒé‡ï¼Œå¹¶å¼•å…¥äº†ç®¡é“ç¯å¹¶è¡Œå’Œé¢„å–æŠ€æœ¯ï¼Œä»¥å‡å°‘ç£ç›˜åŠ è½½æ—¶é—´ã€‚é€šè¿‡ä¼˜åŒ–è®¡ç®—ã€é€šä¿¡å’Œå†…å­˜ç®¡ç†ï¼Œprima.cppæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä½¿å¾—å…ˆè¿›çš„AIæ¨¡å‹èƒ½å¤Ÿåœ¨å®¶åº­åŠ©æ‰‹ä¸­æ™®åŠã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 25,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FUSION: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "FUSION - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. FUSION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSIONï¼šæ·±åº¦é›†æˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†FUSIONï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨å®Œå…¨çš„è§†è§‰-è¯­è¨€å¯¹é½å’Œé›†æˆèŒƒå¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºLLMè§£ç è¿‡ç¨‹ä¸­çš„åæœŸæ¨¡æ€äº¤äº’ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä¸ªå¤„ç†æµç¨‹ä¸­å®ç°äº†æ·±åº¦ã€åŠ¨æ€çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¼•å¯¼çš„ç»Ÿä¸€è§†è§‰ç¼–ç ï¼Œå°†æ–‡æœ¬ä¿¡æ¯èå…¥è§†è§‰ç¼–ç ï¼Œå®ç°åƒç´ çº§çš„é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é€’å½’å¯¹é½è§£ç ï¼Œèƒ½å¤Ÿåœ¨è§£ç è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬ä¸Šä¸‹æ–‡é€’å½’èšåˆè§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„é—®é¢˜çº§è¯­ä¹‰é›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10068",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
            "url": "https://huggingface.co/papers/2504.10068",
            "abstract": "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.",
            "score": 18,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "bbb7251e84f61649",
            "authors": [
                "Yang Shi",
                "Jiaheng Liu",
                "Yushuo Guan",
                "Zhenhua Wu",
                "Yuanxing Zhang",
                "Zihao Wang",
                "Weihong Lin",
                "Jingyun Hua",
                "Zekun Wang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Wentao Zhang",
                "Fuzheng Zhang",
                "Wenjing Yang",
                "Di Zhang"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10068.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#architecture",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Mavors: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Mavors Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Mavors Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Mavors Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation",
                    "desc": "This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning."
                },
                "zh": {
                    "title": "Mavorsï¼šé•¿è§†é¢‘ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMavorsçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—æ•ˆç‡ä¸ç»†ç²’åº¦æ—¶ç©ºæ¨¡å¼ä¿ç•™ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚Mavorsé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°å¯¹åŸå§‹è§†é¢‘å†…å®¹çš„ç¼–ç ï¼šä¸€æ˜¯ä½¿ç”¨3Då·ç§¯å’Œè§†è§‰å˜æ¢å™¨çš„å†…éƒ¨å—è§†è§‰ç¼–ç å™¨ï¼ˆIVEï¼‰ï¼Œä»¥ä¿ç•™é«˜åˆ†è¾¨ç‡çš„ç©ºé—´ç‰¹å¾ï¼›äºŒæ˜¯é€šè¿‡åŸºäºå˜æ¢å™¨çš„ä¾èµ–å»ºæ¨¡å’Œå—çº§æ—‹è½¬ä½ç½®ç¼–ç çš„å—é—´ç‰¹å¾èšåˆå™¨ï¼ˆIFAï¼‰ï¼Œå»ºç«‹å—ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶è¿˜é€šè¿‡å­å›¾åƒåˆ†è§£å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘ï¼Œä»è€Œç»Ÿä¸€äº†å›¾åƒå’Œè§†é¢‘çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMavorsåœ¨ä¿æŒç©ºé—´ä¿çœŸåº¦å’Œæ—¶é—´è¿ç»­æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦æ—¶ç©ºæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08837",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.08837",
            "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.",
            "score": 17,
            "issue_id": 3237,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "e73823d36c951e4e",
            "authors": [
                "Haozhe Wang",
                "Chao Qu",
                "Zuming Huang",
                "Wei Chu",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "INF.AI",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08837.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#math",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ (SSR) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VL-Rethinker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning",
                    "desc": "This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œç§°ä¸ºé€‰æ‹©æ€§æ ·æœ¬é‡æ”¾ï¼ˆSSRï¼‰ï¼Œä»¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶ç»“åˆå¼ºåˆ¶é‡æ–°æ€è€ƒçš„æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹çš„è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚é€šè¿‡è¿™ä¸¤ç§æŠ€æœ¯çš„ç»“åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹VL-Rethinkeråœ¨å¤šä¸ªæ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚æœ€ç»ˆï¼ŒVL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†å¼€æºçš„æœ€æ–°æ°´å¹³ï¼Œç¼©å°äº†ä¸ç°æœ‰æœ€ä½³æ¨¡å‹çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10479",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "url": "https://huggingface.co/papers/2504.10479",
            "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
            "score": 16,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "51475893ef3c1d8b",
            "authors": [
                "Jinguo Zhu",
                "Weiyun Wang",
                "Zhe Chen",
                "Zhaoyang Liu",
                "Shenglong Ye",
                "Lixin Gu",
                "Yuchen Duan",
                "Hao Tian",
                "Weijie Su",
                "Jie Shao",
                "Zhangwei Gao",
                "Erfei Cui",
                "Yue Cao",
                "Yangzhou Liu",
                "Weiye Xu",
                "Hao Li",
                "Jiahao Wang",
                "Han Lv",
                "Dengnian Chen",
                "Songze Li",
                "Yinan He",
                "Tan Jiang",
                "Jiapeng Luo",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Wenqi Shao",
                "Junjun He",
                "Yingtong Xiong",
                "Wenwen Qu",
                "Peng Sun",
                "Penglong Jiao",
                "Lijun Wu",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Limin Wang",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10479.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "InternVL3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "InternVL3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. InternVL3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ÑĞ´ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (V2PE) Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (MPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 72.2 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMMU Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning with InternVL3",
                    "desc": "InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community."
                },
                "zh": {
                    "title": "InternVL3ï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒçš„æ–°æ ‡æ†",
                    "desc": "InternVL3æ˜¯InternVLç³»åˆ—çš„é‡è¦è¿›å±•ï¼Œé‡‡ç”¨äº†åŸç”Ÿçš„å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„å°†æ–‡æœ¬æ¨¡å‹è½¬å˜ä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¸åŒï¼ŒInternVL3åœ¨å•ä¸€é¢„è®­ç»ƒé˜¶æ®µåŒæ—¶å­¦ä¹ å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ç»è¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼ŒInternVL3åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†72.2çš„åˆ†æ•°ï¼Œæˆä¸ºå¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08942",
            "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
            "url": "https://huggingface.co/papers/2504.08942",
            "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
            "score": 13,
            "issue_id": 3237,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 11",
                "zh": "4æœˆ11æ—¥"
            },
            "hash": "d756012a0eceafb9",
            "authors": [
                "Xing Han LÃ¹",
                "Amirhossein Kazemnejad",
                "Nicholas Meade",
                "Arkil Patel",
                "Dongchan Shin",
                "Alejandra Zambrano",
                "Karolina StaÅ„czak",
                "Peter Shaw",
                "Christopher J. Pal",
                "Siva Reddy"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Google DeepMind",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique MontrÃ©al",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08942.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "AgentRewardBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentRewardBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1302 Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 5 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ 4 LLM, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ° LLM Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Web Agent Evaluation with LLMs",
                    "desc": "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."
                },
                "zh": {
                    "title": "è¯„ä¼°ç½‘ç»œä»£ç†çš„æ–°æ–¹æ³•ï¼šAgentRewardBench",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•è¯„ä¼°ç½‘ç»œä»£ç†çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’æ¥å®Œæˆä»»åŠ¡çš„ä»£ç†ã€‚ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨æ‰©å±•æ–°ä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•å‡†ç¡®è¯†åˆ«æˆåŠŸçš„è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†AgentRewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°ç½‘ç»œä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹1302ä¸ªè½¨è¿¹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰å•ä¸€çš„LLMåœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¿™è¡¨æ˜éœ€è¦å¼€å‘æ›´çµæ´»çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 10,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Upper Confidence Bound Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒçº§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçº§å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è®­ç»ƒæ•°æ®è§†ä¸ºç»Ÿä¸€æ•´ä½“ï¼Œå¿½è§†äº†æ•°æ®åˆ†å¸ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸åŒåˆ†å¸ƒçš„é‡‡æ ·æ¦‚ç‡ï¼Œä¼˜å…ˆè€ƒè™‘é«˜å¹³å‡ä¼˜åŠ¿æˆ–ä½æ ·æœ¬æ•°é‡çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå±•ç¤ºäº†åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥çš„ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 10,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GPT-4o: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç†è§£ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„å¤šæ¨¡æ€æ¨¡å‹GPT-4oåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨å…¨çƒæŒ‡ä»¤éµå¾ªã€ç²¾ç»†ç¼–è¾‘ç²¾åº¦å’Œç”Ÿæˆåæ¨ç†ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚å°½ç®¡ç°æœ‰åŸºå‡†æ˜¾ç¤ºGPT-4oåœ¨å›¾åƒå¤„ç†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å…¶åœ¨æŒ‡ä»¤ç†è§£å’ŒçŸ¥è¯†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚æ¨¡å‹å¸¸å¸¸å¯¹æŒ‡ä»¤è¿›è¡Œå­—é¢è§£é‡Šï¼ŒçŸ¥è¯†çº¦æŸåº”ç”¨ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ¡ä»¶æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¯¹GPT-4oç»Ÿä¸€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„æ™®éå‡è®¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„åŸºå‡†å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œæ¨ç†åŸºç¡€çš„å¤šæ¨¡æ€ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10368",
            "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10368",
            "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.",
            "score": 9,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "c5995fab2f284493",
            "authors": [
                "Wenyuan Zhang",
                "Shuaiyi Nie",
                "Xinghua Zhang",
                "Zefeng Zhang",
                "Tingwen Liu"
            ],
            "affiliations": [
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10368.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸",
                    "desc": "S1-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ 1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LRM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ² 15,5 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ€Ğ°Ğ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LRM."
                },
                "en": {
                    "title": "Evaluating Intuition: S1-Bench for Large Reasoning Models",
                    "desc": "The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„ç›´è§‚æ€ç»´èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†S1-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ›´å€¾å‘äºç›´è§‚çš„ç³»ç»Ÿ1æ€ç»´ï¼Œè€Œéæ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿ2æ¨ç†ã€‚å°½ç®¡LRMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦åˆ†ææ€ç»´çš„ä¾èµ–å¯èƒ½é™åˆ¶äº†å…¶ç³»ç»Ÿ1æ€ç»´èƒ½åŠ›ã€‚ç›®å‰ç¼ºä¹è¯„ä¼°LRMsåœ¨éœ€è¦è¿™ç§èƒ½åŠ›çš„ä»»åŠ¡è¡¨ç°çš„åŸºå‡†ã€‚S1-Benchæä¾›äº†ä¸€ç»„ç®€å•ã€å¤šæ ·ä¸”è‡ªç„¶æ¸…æ™°çš„é—®é¢˜ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LRMsåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10415",
            "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.10415",
            "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.",
            "score": 5,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "15b7c5f49cceef01",
            "authors": [
                "Parshin Shojaee",
                "Ngoc-Hieu Nguyen",
                "Kazem Meidani",
                "Amir Barati Farimani",
                "Khoa D Doan",
                "Chandan K Reddy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "VinUniversity",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10415.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "LLM-SRBench: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-SRBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 239 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LLM-SRBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: LSR-Transform Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ LSR-Synth Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 31.5% ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "LLM-SRBench: A New Frontier in Scientific Equation Discovery",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies."
                },
                "zh": {
                    "title": "ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æ–°åŸºå‡†ï¼šLLM-SRBench",
                    "desc": "ç§‘å­¦æ–¹ç¨‹å‘ç°æ˜¯ç§‘å­¦è¿›æ­¥ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ¨å¯¼å‡ºè‡ªç„¶ç°è±¡çš„è§„å¾‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åˆ©ç”¨åµŒå…¥ç§‘å­¦çŸ¥è¯†è¿›è¡Œå‡è®¾ç”Ÿæˆçš„æ½œåŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„çœŸå®å‘ç°èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰åŸºå‡†å¾€å¾€ä¾èµ–äºå®¹æ˜“è¢«LLMsè®°å¿†çš„å¸¸è§æ–¹ç¨‹ï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡è¢«å¤¸å¤§ã€‚æœ¬æ–‡ä»‹ç»äº†LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•çš„è®°å¿†åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10157",
            "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
            "url": "https://huggingface.co/papers/2504.10157",
            "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "ba0402c49e397963",
            "authors": [
                "Xinnong Zhang",
                "Jiayu Lin",
                "Xinyi Mou",
                "Shiyue Yang",
                "Xiawei Liu",
                "Libo Sun",
                "Hanjia Lyu",
                "Yihang Yang",
                "Weihong Qi",
                "Yue Chen",
                "Guanying Li",
                "Ling Yan",
                "Yao Hu",
                "Siming Chen",
                "Yu Wang",
                "Jingxuan Huang",
                "Jiebo Luo",
                "Shiping Tang",
                "Libo Wu",
                "Baohua Zhou",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "Fudan University",
                "Indiana University",
                "Shanghai Innovation Institute",
                "University of Rochester",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10157.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#social_simulation",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "SocioVerse: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SocioVerse - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ¸Ğ· 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SocioVerse ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "SocioVerse: Enhancing Social Simulations with LLMs",
                    "desc": "This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains."
                },
                "zh": {
                    "title": "SocioVerseï¼šç¤¾ä¼šæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ",
                    "desc": "ç¤¾ä¼šæ¨¡æ‹Ÿæ­£åœ¨é€šè¿‡æ¨¡æ‹Ÿè™šæ‹Ÿä¸ªä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨æ¥æ”¹å˜ä¼ ç»Ÿç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•æ‰ä¸ªä½“å·®å¼‚å’Œé¢„æµ‹ç¾¤ä½“è¡Œä¸ºæ–¹é¢æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç¯å¢ƒã€ç›®æ ‡ç”¨æˆ·ã€äº’åŠ¨æœºåˆ¶å’Œè¡Œä¸ºæ¨¡å¼æ–¹é¢é¢ä¸´å¯¹é½æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SocioVerseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMä»£ç†çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸–ç•Œæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„å¯¹é½ç»„ä»¶å’Œ1000ä¸‡çœŸå®ä¸ªä½“çš„ç”¨æˆ·æ± ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09641",
            "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
            "url": "https://huggingface.co/papers/2504.09641",
            "abstract": "Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "b7c9f390686ff6ef",
            "authors": [
                "Xingjian Zhang",
                "Siwei Wen",
                "Wenjun Wu",
                "Lei Huang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University",
                "Hangzhou International Innovation Institute, Beihang University, Hangzhou, China",
                "SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09641.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#rl",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: TinyLLaVA-Video-R1 Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TinyLLaVA-Video-R1 - Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. TinyLLaVA-Video-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ 'Ğ°Ğ³Ğ°-Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ»ÑÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Empowering Small Models for Big Reasoning in Video Understanding",
                    "desc": "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ä¹Ÿèƒ½æ¨ç†ï¼ŒTinyLLaVA-Video-R1åŠ©åŠ›è§†é¢‘ç†è§£ï¼",
                    "desc": "æœ€è¿‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶åŸºäºé«˜åº¦æ¨ç†å¯†é›†çš„æ•°æ®é›†ï¼Œå¦‚æ•°å­¦å’Œä»£ç ï¼Œä¸”é€šå¸¸é€‰æ‹©å¤§è§„æ¨¡æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ¢ç´¢å°è§„æ¨¡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶è€…ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚æ­¤å¤–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå…¶åœ¨ä¸€èˆ¬é—®ç­”æ•°æ®é›†ä¸Šçš„æ¨ç†è¿‡ç¨‹åŒæ ·å…·æœ‰æ„ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09689",
            "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
            "url": "https://huggingface.co/papers/2504.09689",
            "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent",
            "score": 2,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "11b21970119f2c59",
            "authors": [
                "Jiahao Qiu",
                "Yinghui He",
                "Xinzhe Juan",
                "Yiming Wang",
                "Yuhan Liu",
                "Zixin Yao",
                "Yue Wu",
                "Xun Jiang",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Computer Science, Princeton University",
                "Department of Data Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University",
                "Department of Philosophy, Columbia University",
                "Theta Health Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09689.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#ethics",
                    "#healthcare"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EmoAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. EmoAgent ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: EmoEval, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¸ EmoGuard, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. EmoGuard Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ñ€Ğ¸ÑĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent",
                    "desc": "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."
                },
                "zh": {
                    "title": "EmoAgentï¼šä¿éšœäººæœºäº¤äº’å¿ƒç†å®‰å…¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„äººå·¥æ™ºèƒ½è§’è‰²çš„å…´èµ·ï¼Œç‰¹åˆ«æ˜¯å¯¹å¿ƒç†éšœç¢çš„è„†å¼±ç”¨æˆ·ï¼Œå®‰å…¨é—®é¢˜å¼•èµ·äº†å…³æ³¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†EmoAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå‡è½»äººæœºäº¤äº’ä¸­çš„å¿ƒç†å¥åº·å±å®³ã€‚EmoAgentåŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šEmoEvalæ¨¡æ‹Ÿè™šæ‹Ÿç”¨æˆ·ï¼Œè¯„ä¼°ä¸AIè§’è‰²äº¤äº’å‰åçš„å¿ƒç†å¥åº·å˜åŒ–ï¼Œå¹¶ä½¿ç”¨ç»è¿‡ä¸´åºŠéªŒè¯çš„å¿ƒç†è¯„ä¼°å·¥å…·è¿›è¡Œè¯„ä¼°ã€‚EmoGuardä½œä¸ºä¸­ä»‹ï¼Œç›‘æµ‹ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ï¼Œé¢„æµ‹æ½œåœ¨ä¼¤å®³ï¼Œå¹¶æä¾›çº æ­£åé¦ˆï¼Œä»¥é™ä½é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09763",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
            "url": "https://huggingface.co/papers/2504.09763",
            "abstract": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.",
            "score": 1,
            "issue_id": 3240,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "c4ae0eadf040035f",
            "authors": [
                "Zaid Khan",
                "Elias Stengel-Eskin",
                "Archiki Prasad",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09763.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ EFA (Executable Functional Abstraction) - Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ EFAGen, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ EFA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞµĞµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. EFAGen ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ EFA Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ EFA Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Automating Advanced Math Problem Generation with EFAs",
                    "desc": "This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners."
                },
                "zh": {
                    "title": "è‡ªåŠ¨ç”Ÿæˆé«˜çº§æ•°å­¦é—®é¢˜çš„å¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡ï¼ˆEFAï¼‰çš„ç¨‹åºï¼Œè¿™äº›ç¨‹åºèƒ½å¤Ÿä»ç‰¹å®šçš„æ•°å­¦é—®é¢˜å®ä¾‹ä¸­æ¨å¯¼å‡ºæŠ½è±¡è¿‡ç¨‹ï¼Œå¹¶ç”Ÿæˆæ–°çš„ç›¸å…³å®ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ„å»ºé«˜çº§æ•°å­¦é—®é¢˜çš„EFAçš„æ–¹æ³•ï¼Œç§°ä¸ºEFAGenï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®ç§å­æ•°å­¦é—®é¢˜åŠå…¶é€æ­¥è§£å†³æ–¹æ¡ˆç”Ÿæˆå€™é€‰EFAç¨‹åºã€‚é€šè¿‡å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œæˆ‘ä»¬å®šä¹‰äº†æœ‰æ•ˆEFAå¿…é¡»å…·å¤‡çš„å±æ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›æµ‹è¯•ä½œä¸ºå¯éªŒè¯çš„å¥–åŠ±æ¥è®­ç»ƒLLMï¼Œæé«˜å…¶EFAç¼–å†™èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¯æ˜äº†EFAGenç”Ÿæˆçš„EFAèƒ½å¤Ÿä¿æŒä¸ç§å­é—®é¢˜çš„ä¸€è‡´æ€§ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé€‚åˆå­¦ä¹ è€…çš„ä¸åŒéš¾åº¦çš„æ•°å­¦é—®é¢˜å˜ä½“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0,
        "#social_simulation": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†è®­ç»ƒè§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æˆæœ¬æ•ˆç›Šç­–ç•¥ã€‚æŠ¥å‘Šå±•ç¤ºäº†ä¸€ä¸ªåä¸º Seaweed-7B çš„ä¸­å‹ç ”ç©¶æ¨¡å‹ï¼Œæ‹¥æœ‰çº¦70äº¿å‚æ•°ï¼Œä½¿ç”¨665,000ä¸ªH100 GPUå°æ—¶ä»å¤´è®­ç»ƒã€‚å°½ç®¡ä½¿ç”¨çš„è®¡ç®—èµ„æºé€‚ä¸­ï¼ŒSeaweed-7B ä»è¡¨ç°å‡ºä¸æ›´å¤§æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è®¾è®¡é€‰æ‹©åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ã€‚æŠ¥å‘Šå¼ºè°ƒäº†æå‡ä¸­å‹æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„å…³é”®è®¾è®¡å†³ç­–ã€‚ç»éªŒä¸Šï¼Œæœ‰ä¸¤ä¸ªè§‚å¯Ÿç»“æœï¼š(1) Seaweed-7B çš„æ€§èƒ½ä¸æˆ–è¶…è¿‡ä½¿ç”¨æ›´å¤šGPUèµ„æºè®­ç»ƒçš„å¤§å‹æ¨¡å‹ï¼›(2) è¯¥æ¨¡å‹å±•ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯é€šè¿‡è½»é‡å¾®è°ƒæˆ–ç»§ç»­è®­ç»ƒæœ‰æ•ˆé€‚åº”å¤šç§ä¸‹æ¸¸åº”ç”¨ã€‚é¡¹ç›®é¡µé¢è§ https://seaweed.video/ã€‚",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "pinyin": "ZhÃ¨ piÄn jÃ¬shÃ¹ bÃ ogÃ o jiÃ¨shÃ o le xÃ¹nliÃ n shÃ¬pÇn shÄ“ngchÃ©ng jÄ«chÇ” mÃ³xÃ­ng de chÃ©ngbÄ›n xiÃ oyÃ¬ cÃ¨lÃ¼Ã¨. BÃ ogÃ o zhÇnshÃ¬ le yÄ«gÃ¨ mÃ­ngwÃ¨i Seaweed-7B de zhÅngxÃ­ng yÃ¡njiÅ« mÃ³xÃ­ng, yÇ’ngyÇ’u yuÄ“ 70 yÃ¬ cÄnshÃ¹, shÇyÃ²ng 665,000 gÃ¨ H100 GPU xiÇoshÃ­ cÃ³ngtÃ³u xÃ¹nliÃ n. JÄ«nrÃ¡n shÇyÃ²ng de jÃ¬suÃ n zÄ«yuÃ¡n shÃ¬guÃ¬ zhÅngdÄ›ng, Seaweed-7B rÃ©ng biÇoxiÃ n chÅ« yÇ” gÃ¨ng dÃ  mÃ³xÃ­ng xiÃ ng bÇmÄ›i de xÃ­ngnÃ©ng. ShÃ¨jÃ¬ xuÇnzÃ© zÃ i zÄ«yuÃ¡n shÃ²uxiÃ n de huÃ¡njÃ¬ng zhÅng yÃ³uwÃ¨i zhÃ²ngyÃ o. BÃ ogÃ o qiÃ¡ngdiÇo le tÃ­shÄ“ng zhÅngxÃ­ng kuÃ²sÃ n mÃ³xÃ­ng xÃ­ngnÃ©ng de guÇnjiÃ n shÃ¨jÃ¬ juÃ©cÃ¨. JÄ«ngyÃ n shÃ ng, yÇ’u liÇng gÃ¨ guÄnchÃ¡ jiÃ©guÇ’: (1) Seaweed-7B de xÃ­ngnÃ©ng yÇ” huÃ² chÄoguÃ² shÇyÃ²ng gÃ¨ng duÅ GPU zÄ«yuÃ¡n xÃ¹nliÃ n de dÃ xÃ­ng mÃ³xÃ­ng; (2) gÃ¨ mÃ³xÃ­ng zhÇnshÃ¬ chÅ« qiÃ¡ngdÃ  de fÃ nhuÃ  nÃ©nglÃ¬, kÄ› tÅngguÃ² qÄ«ngliÃ ng wÄ“itiÃ¡o huÃ² jÃ¬xÃ¹ xÃ¹nliÃ n yÇ’uxiÃ o shÃ¬yÃ¬ng duÅzhÇ’ng xiÃ yÃ³u yÃ¬ngyÃ²ng. XiÃ ngmÃ¹ yÃ¨miÃ n jiÃ n https://seaweed.video/ã€‚",
        "vocab": "[\n    {\"word\": \"æŠ€æœ¯æŠ¥å‘Š\", \"pinyin\": \"jÃ¬shÃ¹ bÃ ogÃ o\", \"trans\": \"technical report\"},\n    {\"word\": \"è§†é¢‘ç”Ÿæˆ\", \"pinyin\": \"shÃ¬pÃ­n shÄ“ngchÃ©ng\", \"trans\": \"video generation\"},\n    {\"word\": \"åŸºç¡€æ¨¡å‹\", \"pinyin\": \"jÄ«chÇ” mÃ³xÃ­ng\", \"trans\": \"foundational model\"},\n    {\"word\": \"æˆæœ¬æ•ˆç›Š\", \"pinyin\": \"chÃ©ngbÄ›n xiÃ oyÃ¬\", \"trans\": \"cost-effectiveness\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"ä¸­å‹\", \"pinyin\": \"zhÅngxÃ­ng\", \"trans\": \"medium-sized\"},\n    {\"word\": \"ç ”ç©¶æ¨¡å‹\", \"pinyin\": \"yÃ¡njiÅ« mÃ³xÃ­ng\", \"trans\": \"research model\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÇ”\", \"trans\": \"parameters\"},\n    {\"word\": \"ä»å¤´è®­ç»ƒ\", \"pinyin\": \"cÃ³ngtÃ³u xÃ¹nliÃ n\", \"trans\": \"train from scratch\"},\n    {\"word\": \"è®¡ç®—èµ„æº\", \"pinyin\": \"jÃ¬suÃ n zÄ«yuÃ¡n\", \"trans\": \"computational resources\"},\n    {\"word\": \"é€‚ä¸­\", \"pinyin\": \"shÃ¬zhÅng\", \"trans\": \"moderate\"},\n    {\"word\": \"åª²ç¾\", \"pinyin\": \"pÃ¬mÄ›i\", \"trans\": \"rival\"},\n    {\"word\": \"è®¾è®¡é€‰æ‹©\", \"pinyin\": \"shÃ¨jÃ¬ xuÇnzÃ©\", \"trans\": \"design choices\"},\n    {\"word\": \"å—é™\", \"pinyin\": \"shÃ²uxiÃ n\", \"trans\": \"constrained\"},\n    {\"word\": \"ç¯å¢ƒ\", \"pinyin\": \"huÃ¡njÃ¬ng\", \"trans\": \"environment\"},\n    {\"word\": \"å°¤ä¸º\", \"pinyin\": \"yÃ³uwÃ©i\", \"trans\": \"especially\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ngyÃ o\", \"trans\": \"important\"},\n    {\"word\": \"å¼ºè°ƒ\", \"pinyin\": \"qiÃ¡ngdiÃ o\", \"trans\": \"emphasize\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"enhance\"},\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ²sÃ n mÃ³xÃ­ng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ­ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"è®¾è®¡å†³ç­–\", \"pinyin\": \"shÃ¨jÃ¬ juÃ©cÃ¨\", \"trans\": \"design decisions\"},\n    {\"word\": \"ç»éªŒ\", \"pinyin\": \"jÄ«ngyÃ n\", \"trans\": \"experience\"},\n    {\"word\": \"è§‚å¯Ÿç»“æœ\", \"pinyin\": \"guÄnchÃ¡ jiÃ©guÇ’\", \"trans\": \"observation results\"},\n    {\"word\": \"æ³›åŒ–èƒ½åŠ›\", \"pinyin\": \"fÃ nhuÃ  nÃ©nglÃ¬\", \"trans\": \"generalization capability\"},\n    {\"word\": \"è½»é‡å¾®è°ƒ\", \"pinyin\": \"qÄ«ngliÃ ng wÄ“itiÃ¡o\", \"trans\": \"lightweight fine-tuning\"},\n    {\"word\": \"ç»§ç»­è®­ç»ƒ\", \"pinyin\": \"jÃ¬xÃ¹ xÃ¹nliÃ n\", \"trans\": \"continued training\"},\n    {\"word\": \"ä¸‹æ¸¸åº”ç”¨\", \"pinyin\": \"xiÃ yÃ³u yÃ¬ngyÃ²ng\", \"trans\": \"downstream applications\"},\n    {\"word\": \"é¡¹ç›®é¡µé¢\", \"pinyin\": \"xiÃ ngmÃ¹ yÃ¨miÃ n\", \"trans\": \"project page\"}\n]",
        "trans": "This technical report introduces cost-effective strategies for training foundational video generation models. The report presents a mid-sized research model named Seaweed-7B, which has approximately 7 billion parameters and was trained from scratch using 665,000 H100 GPU hours. Despite the moderate computational resources used, Seaweed-7B demonstrates performance comparable to larger models. Design choices are particularly important in resource-constrained environments. The report emphasizes key design decisions that enhance the performance of mid-sized diffusion models. Empirically, there are two observations: (1) Seaweed-7B's performance matches or exceeds that of larger models trained with more GPU resources; (2) The model demonstrates strong generalization capabilities and can effectively adapt to various downstream applications through lightweight fine-tuning or continued training. For more information, visit the project page at https://seaweed.video/.",
        "update_ts": "2025-04-14 09:12"
    }
}