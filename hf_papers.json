{
    "date": {
        "ru": "23 —è–Ω–≤–∞—Ä—è",
        "en": "January 23",
        "zh": "1Êúà23Êó•"
    },
    "time_utc": "2025-01-23 17:09",
    "weekday": 3,
    "issue_id": 1832,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12948",
            "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2501.12948",
            "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
            "score": 83,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "cae642107ec57790",
            "authors": [
                "DeepSeek-AI",
                "Daya Guo",
                "Dejian Yang",
                "Haowei Zhang",
                "Junxiao Song",
                "Ruoyu Zhang",
                "Runxin Xu",
                "Qihao Zhu",
                "Shirong Ma",
                "Peiyi Wang",
                "Xiao Bi",
                "Xiaokang Zhang",
                "Xingkai Yu",
                "Yu Wu",
                "Z. F. Wu",
                "Zhibin Gou",
                "Zhihong Shao",
                "Zhuoshu Li",
                "Ziyi Gao",
                "Aixin Liu",
                "Bing Xue",
                "Bingxuan Wang",
                "Bochao Wu",
                "Bei Feng",
                "Chengda Lu",
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chenyu Zhang",
                "Chong Ruan",
                "Damai Dai",
                "Deli Chen",
                "Dongjie Ji",
                "Erhang Li",
                "Fangyun Lin",
                "Fucong Dai",
                "Fuli Luo",
                "Guangbo Hao",
                "Guanting Chen",
                "Guowei Li",
                "H. Zhang",
                "Han Bao",
                "Hanwei Xu",
                "Haocheng Wang",
                "Honghui Ding",
                "Huajian Xin",
                "Huazuo Gao",
                "Hui Qu",
                "Hui Li",
                "Jianzhong Guo",
                "Jiashi Li",
                "Jiawei Wang",
                "Jingchang Chen",
                "Jingyang Yuan",
                "Junjie Qiu",
                "Junlong Li",
                "J. L. Cai",
                "Jiaqi Ni",
                "Jian Liang",
                "Jin Chen",
                "Kai Dong",
                "Kai Hu",
                "Kaige Gao",
                "Kang Guan",
                "Kexin Huang",
                "Kuai Yu",
                "Lean Wang",
                "Lecong Zhang",
                "Liang Zhao",
                "Litong Wang",
                "Liyue Zhang",
                "Lei Xu",
                "Leyi Xia",
                "Mingchuan Zhang",
                "Minghua Zhang",
                "Minghui Tang",
                "Meng Li",
                "Miaojun Wang",
                "Mingming Li",
                "Ning Tian",
                "Panpan Huang",
                "Peng Zhang",
                "Qiancheng Wang",
                "Qinyu Chen",
                "Qiushi Du",
                "Ruiqi Ge",
                "Ruisong Zhang",
                "Ruizhe Pan",
                "Runji Wang",
                "R. J. Chen",
                "R. L. Jin",
                "Ruyi Chen",
                "Shanghao Lu",
                "Shangyan Zhou",
                "Shanhuang Chen",
                "Shengfeng Ye",
                "Shiyu Wang",
                "Shuiping Yu",
                "Shunfeng Zhou",
                "Shuting Pan",
                "S. S. Li",
                "Shuang Zhou",
                "Shaoqing Wu",
                "Shengfeng Ye",
                "Tao Yun",
                "Tian Pei",
                "Tianyu Sun",
                "T. Wang",
                "Wangding Zeng",
                "Wanjia Zhao",
                "Wen Liu",
                "Wenfeng Liang",
                "Wenjun Gao",
                "Wenqin Yu",
                "Wentao Zhang",
                "W. L. Xiao",
                "Wei An",
                "Xiaodong Liu",
                "Xiaohan Wang",
                "Xiaokang Chen",
                "Xiaotao Nie",
                "Xin Cheng",
                "Xin Liu",
                "Xin Xie",
                "Xingchao Liu",
                "Xinyu Yang",
                "Xinyuan Li",
                "Xuecheng Su",
                "Xuheng Lin",
                "X. Q. Li",
                "Xiangyue Jin",
                "Xiaojin Shen",
                "Xiaosha Chen",
                "Xiaowen Sun",
                "Xiaoxiang Wang",
                "Xinnan Song",
                "Xinyi Zhou",
                "Xianzu Wang",
                "Xinxia Shan",
                "Y. K. Li",
                "Y. Q. Wang",
                "Y. X. Wei",
                "Yang Zhang",
                "Yanhong Xu",
                "Yao Li",
                "Yao Zhao",
                "Yaofeng Sun",
                "Yaohui Wang",
                "Yi Yu",
                "Yichao Zhang",
                "Yifan Shi",
                "Yiliang Xiong",
                "Ying He",
                "Yishi Piao",
                "Yisong Wang",
                "Yixuan Tan",
                "Yiyang Ma",
                "Yiyuan Liu",
                "Yongqiang Guo",
                "Yuan Ou",
                "Yuduan Wang",
                "Yue Gong",
                "Yuheng Zou",
                "Yujia He",
                "Yunfan Xiong",
                "Yuxiang Luo",
                "Yuxiang You",
                "Yuxuan Liu",
                "Yuyang Zhou",
                "Y. X. Zhu",
                "Yanhong Xu",
                "Yanping Huang",
                "Yaohui Li",
                "Yi Zheng",
                "Yuchen Zhu",
                "Yunxian Ma",
                "Ying Tang",
                "Yukun Zha",
                "Yuting Yan",
                "Z. Z. Ren",
                "Zehui Ren",
                "Zhangli Sha",
                "Zhe Fu",
                "Zhean Xu",
                "Zhenda Xie",
                "Zhengyan Zhang",
                "Zhewen Hao",
                "Zhicheng Ma",
                "Zhigang Yan",
                "Zhiyu Wu",
                "Zihui Gu",
                "Zijia Zhu",
                "Zijun Liu",
                "Zilin Li",
                "Ziwei Xie",
                "Ziyang Song",
                "Zizheng Pan",
                "Zhen Huang",
                "Zhipeng Xu",
                "Zhongyu Zhang",
                "Zhen Zhang"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12948.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ù–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–Ω–æ–º—É –ò–ò",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π DeepSeek-R1-Zero –∏ DeepSeek-R1. DeepSeek-R1-Zero –æ–±—É—á–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. DeepSeek-R1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏. –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å OpenAI-o1-1217 –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ—Ç–∫—Ä—ã–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with DeepSeek Models",
                    "desc": "This paper presents two reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, developed for enhanced reasoning capabilities. DeepSeek-R1-Zero is trained using large-scale reinforcement learning without any supervised fine-tuning, showcasing impressive reasoning behaviors but facing issues like readability and language mixing. To improve these aspects, DeepSeek-R1 employs a multi-stage training approach and utilizes cold-start data prior to reinforcement learning. The performance of DeepSeek-R1 is on par with existing models like OpenAI-o1-1217, and both models, along with several distilled versions, are made available to the research community."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶Êé®ÁêÜÊ®°ÂûãÁöÑÂàõÊñ∞‰∏éÊåëÊàò",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜÁ¨¨‰∏Ä‰ª£Êé®ÁêÜÊ®°ÂûãDeepSeek-R1-ZeroÂíåDeepSeek-R1„ÄÇDeepSeek-R1-ZeroÊòØÈÄöËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºåÊ≤°ÊúâÁªèËøáÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºåÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÂú®ÂèØËØªÊÄßÂíåËØ≠Ë®ÄÊ∑∑ÂêàÊñπÈù¢Â≠òÂú®‰∏Ä‰∫õÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÂπ∂Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊé®ÁêÜÊÄßËÉΩÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜDeepSeek-R1ÔºåËØ•Ê®°ÂûãÂú®ËøõË°åRL‰πãÂâçÈááÁî®‰∫ÜÂ§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂÜ∑ÂêØÂä®Êï∞ÊçÆ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12909",
            "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
            "url": "https://huggingface.co/papers/2501.12909",
            "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
            "score": 42,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "0b73908eee2c2db3",
            "authors": [
                "Zhenran Xu",
                "Longyue Wang",
                "Jifang Wang",
                "Zhouyi Li",
                "Senbao Shi",
                "Xue Yang",
                "Yiyu Wang",
                "Baotian Hu",
                "Jun Yu",
                "Min Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology (Shenzhen)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12909.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#story_generation",
                    "#3d",
                    "#open_source",
                    "#agents",
                    "#hallucinations"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –∫–∏–Ω–æ—Å—Ç—É–¥–∏—è: –ò–ò-–∞–≥–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—é—Ç —Ñ–∏–ª—å–º—ã –æ—Ç –∏–¥–µ–∏ –¥–æ –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞",
                    "desc": "FilmAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–ª—å–º–æ–≤ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –û–Ω–∞ —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Å—ä–µ–º–æ—á–Ω–æ–π –≥—Ä—É–ø–ø—ã, –≤–∫–ª—é—á–∞—è —Ä–µ–∂–∏—Å—Å–µ—Ä–æ–≤, —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç–æ–≤, –∞–∫—Ç–µ—Ä–æ–≤ –∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤. –°–∏—Å—Ç–µ–º–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —ç—Ç–∞–ø—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ñ–∏–ª—å–º–∞: —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –∏–¥–µ–∏, –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è –∏ –≤—ã–±–æ—Ä –ø–ª–∞–Ω–æ–≤ —Å—ä–µ–º–∫–∏. FilmAgent –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–Ω–æ–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Revolutionizing Film Production with Multi-Agent Collaboration",
                    "desc": "This paper presents FilmAgent, a collaborative framework that utilizes large language models (LLMs) to automate the film production process in 3D virtual environments. FilmAgent employs multiple agents that simulate various roles in filmmaking, such as directors and screenwriters, to collaboratively develop ideas, write scripts, and plan cinematography. The framework enhances decision-making through iterative feedback, which helps to refine scripts and minimize errors. Evaluation results indicate that FilmAgent significantly outperforms traditional methods, demonstrating the effectiveness of multi-agent systems in creative tasks like filmmaking."
                },
                "zh": {
                    "title": "Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÔºåÈù©Êñ∞ËôöÊãüÁîµÂΩ±Âà∂‰Ωú",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫FilmAgentÁöÑÊñ∞ÂûãÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞ËôöÊãüÁîµÂΩ±Âà∂‰ΩúÁöÑËá™Âä®Âåñ„ÄÇFilmAgentÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊ®°ÊãüÂØºÊºî„ÄÅÁºñÂâß„ÄÅÊºîÂëòÂíåÊëÑÂΩ±Â∏àÁ≠â‰∏çÂêåËßíËâ≤ÔºåÊ∂µÁõñÁîµÂΩ±Âà∂‰ΩúÁöÑÂÖ≥ÈîÆÈò∂ÊÆµÔºåÂåÖÊã¨ÂàõÊÑèÂºÄÂèë„ÄÅÂâßÊú¨ÂÜô‰ΩúÂíåÊëÑÂΩ±„ÄÇÈÄöËøáÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑËø≠‰ª£ÂèçÈ¶àÂíå‰øÆËÆ¢ÔºåFilmAgentËÉΩÂ§üÈ™åËØÅ‰∏≠Èó¥ÂâßÊú¨Âπ∂ÂáèÂ∞ëÈîôËØØ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåFilmAgentÂú®Â§ö‰∏™ÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫éÊâÄÊúâÂü∫Á∫øÊ®°ÂûãÔºåËØÅÊòé‰∫ÜÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÂú®ÁîµÂΩ±Âà∂‰Ωú‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12895",
            "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
            "url": "https://huggingface.co/papers/2501.12895",
            "abstract": "Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at https://github.com/yafuly/TPO.",
            "score": 38,
            "issue_id": 1820,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "ebde6f173ad4f6f9",
            "authors": [
                "Yafu Li",
                "Xuyang Hu",
                "Xiaoye Qu",
                "Linjie Li",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12895.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#alignment",
                    "#inference"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ª–µ—Ç—É: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Test-time Preference Optimization (TPO), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. TPO –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –Ω–∞–≥—Ä–∞–¥ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TPO –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞, –ø—Ä–∏—á–µ–º –¥–∞–∂–µ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Llama-3.1-70B-SFT –º–æ–∂–µ—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–æ–≥ –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∞–≥–æ–≤ TPO. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è —Å–æ–±–æ–π –ø—Ä–∞–∫—Ç–∏—á–Ω—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Aligning Language Models with Human Preferences on the Fly",
                    "desc": "This paper presents Test-time Preference Optimization (TPO), a novel framework designed to enhance the alignment of large language model (LLM) outputs with human preferences during inference without the need for retraining. TPO utilizes textual critiques as a form of reward signals, allowing the model to iteratively refine its responses based on human feedback. The results show that TPO can significantly improve the performance of the Llama-3.1-70B-SFT model, enabling it to exceed the performance of the pre-aligned Llama-3.1-70B-Instruct model after just a few optimization steps. Additionally, TPO demonstrates efficient scaling with search width and depth, making it a practical solution for real-time preference alignment in LLMs."
                },
                "zh": {
                    "title": "ÊµãËØïÊó∂ÂÅèÂ•Ω‰ºòÂåñÔºöËÆ©Ê®°ÂûãÊõ¥ÊáÇ‰Ω†",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊÄßËÉΩ‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰∏çÈáçÊñ∞ËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÈöæ‰ª•Âø´ÈÄüÈÄÇÂ∫î‰∫∫Á±ªÂÅèÂ•Ω„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊµãËØïÊó∂ÂÅèÂ•Ω‰ºòÂåñÔºàTPOÔºâÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Â∞ÜLLMÁöÑËæìÂá∫‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÔºåÈÅøÂÖç‰∫ÜÊõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞ÁöÑÈúÄÊ±Ç„ÄÇTPOÈÄöËøáÂ∞ÜÂ•ñÂä±‰ø°Âè∑ËΩ¨Âåñ‰∏∫ÊñáÊú¨ÊâπËØÑÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫ÊñáÊú¨Â•ñÂä±ÔºåÈÄêÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÂìçÂ∫î„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÁªèËøáÂ∞ëÈáèTPOÊ≠•È™§ÂêéÔºåÊúÄÂàùÊú™ÂØπÈΩêÁöÑLlama-3.1-70B-SFTÊ®°ÂûãËÉΩÂ§üË∂ÖË∂äÂ∑≤ÂØπÈΩêÁöÑLlama-3.1-70B-InstructÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13106",
            "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
            "url": "https://huggingface.co/papers/2501.13106",
            "abstract": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) vision-centric alignment stage, which warms up the vision encoder and projector; 2) vision-language pretraining stage, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data. 3) multi-task fine-tuning stage, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding. 4) video-centric fine-tuning, which further improves the model's capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks.",
            "score": 35,
            "issue_id": 1820,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "d22ea6b804e73c9a",
            "authors": [
                "Boqiang Zhang",
                "Kehan Li",
                "Zesen Cheng",
                "Zhiqiang Hu",
                "Yuqian Yuan",
                "Guanzheng Chen",
                "Sicong Leng",
                "Yuming Jiang",
                "Hang Zhang",
                "Xin Li",
                "Peng Jin",
                "Wenqi Zhang",
                "Fan Wang",
                "Lidong Bing",
                "Deli Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13106.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#agi",
                    "#games",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "VideoLLaMA3: –ó—Ä–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ",
                    "desc": "VideoLLaMA3 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ - –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ –∑—Ä–µ–Ω–∏–µ, —á—Ç–æ –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ –≤ –ø–∞—Ä–∞–¥–∏–≥–º–µ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞, —É–¥–µ–ª—è—è –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç. VideoLLaMA3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å–∂–∞—Ç–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
                },
                "en": {
                    "title": "Empowering Image and Video Understanding with Vision-Centric Design",
                    "desc": "VideoLLaMA3 is a cutting-edge multimodal foundation model designed for understanding images and videos. It emphasizes a vision-centric approach, which involves training with high-quality image-text datasets instead of large video-text datasets. The model undergoes four training stages, including alignment, pretraining, fine-tuning, and video-centric fine-tuning, to enhance its capabilities in both image and video comprehension. By adapting the vision encoder to handle varying image sizes and optimizing video token representation, VideoLLaMA3 demonstrates impressive performance across various benchmarks."
                },
                "zh": {
                    "title": "‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£Ê®°Âûã",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫ÜVideoLLaMA3ÔºåËøôÊòØ‰∏Ä‰∏™Êõ¥ÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÔºåÁî®‰∫éÂõæÂÉèÂíåËßÜÈ¢ëÁêÜËß£„ÄÇÂÖ∂Ê†∏ÂøÉËÆæËÆ°ÁêÜÂøµÊòØ‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÔºåÂº∫Ë∞ÉÈ´òË¥®ÈáèÁöÑÂõæÂÉè-ÊñáÊú¨Êï∞ÊçÆÂØπÂõæÂÉèÂíåËßÜÈ¢ëÁêÜËß£ÁöÑÈáçË¶ÅÊÄß„ÄÇVideoLLaMA3ÁöÑËÆ≠ÁªÉÂàÜ‰∏∫Âõõ‰∏™Èò∂ÊÆµÔºåÂåÖÊã¨ËßÜËßâÂØπÈΩê„ÄÅËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉ„ÄÅÂ§ö‰ªªÂä°ÂæÆË∞ÉÂíåËßÜÈ¢ëÂæÆË∞ÉÔºå‰ª•ÊèêÂçáÊ®°ÂûãÂú®ËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÈÄÇÂ∫îÊÄßÂú∞ÁºñÁ†Å‰∏çÂêåÂ§ßÂ∞èÁöÑÂõæÂÉèÂíå‰ºòÂåñËßÜÈ¢ëËæìÂÖ•ÁöÑË°®Á§∫ÔºåVideoLLaMA3Âú®ÂõæÂÉèÂíåËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12599",
            "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
            "url": "https://huggingface.co/papers/2501.12599",
            "abstract": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
            "score": 28,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "427fb9e286a6e3a8",
            "authors": [
                "Kimi Team",
                "Angang Du",
                "Bofei Gao",
                "Bowei Xing",
                "Changjiu Jiang",
                "Cheng Chen",
                "Cheng Li",
                "Chenjun Xiao",
                "Chenzhuang Du",
                "Chonghua Liao",
                "Chuning Tang",
                "Congcong Wang",
                "Dehao Zhang",
                "Enming Yuan",
                "Enzhe Lu",
                "Fengxiang Tang",
                "Flood Sung",
                "Guangda Wei",
                "Guokun Lai",
                "Haiqing Guo",
                "Han Zhu",
                "Hao Ding",
                "Hao Hu",
                "Hao Yang",
                "Hao Zhang",
                "Haotian Yao",
                "Haotian Zhao",
                "Haoyu Lu",
                "Haoze Li",
                "Haozhen Yu",
                "Hongcheng Gao",
                "Huabin Zheng",
                "Huan Yuan",
                "Jia Chen",
                "Jianhang Guo",
                "Jianlin Su",
                "Jianzhou Wang",
                "Jie Zhao",
                "Jin Zhang",
                "Jingyuan Liu",
                "Junjie Yan",
                "Junyan Wu",
                "Lidong Shi",
                "Ling Ye",
                "Longhui Yu",
                "Mengnan Dong",
                "Neo Zhang",
                "Ningchen Ma",
                "Qiwei Pan",
                "Qucheng Gong",
                "Shaowei Liu",
                "Shengling Ma",
                "Shupeng Wei",
                "Sihan Cao",
                "Siying Huang",
                "Tao Jiang",
                "Weihao Gao",
                "Weimin Xiong",
                "Weiran He",
                "Weixiao Huang",
                "Wenhao Wu",
                "Wenyang He",
                "Xianghui Wei",
                "Xianqing Jia",
                "Xingzhe Wu",
                "Xinran Xu",
                "Xinxing Zu",
                "Xinyu Zhou",
                "Xuehai Pan",
                "Y. Charles",
                "Yang Li",
                "Yangyang Hu",
                "Yangyang Liu",
                "Yanru Chen",
                "Yejie Wang",
                "Yibo Liu",
                "Yidao Qin",
                "Yifeng Liu",
                "Ying Yang",
                "Yiping Bao",
                "Yulun Du",
                "Yuxin Wu",
                "Yuzhi Wang",
                "Zaida Zhou",
                "Zhaoji Wang",
                "Zhaowei Li",
                "Zhen Zhu",
                "Zheng Zhang",
                "Zhexu Wang",
                "Zhilin Yang",
                "Zhiqi Huang",
                "Zihao Huang",
                "Ziyao Xu",
                "Zonghan Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.12599.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#reasoning",
                    "#long_context",
                    "#math"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Kimi k1.5 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ RL –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ. –ö–ª—é—á–µ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö, —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Å OpenAI's o1."
                },
                "en": {
                    "title": "Unlocking AI Potential with Reinforcement Learning in LLMs",
                    "desc": "This paper discusses the development of Kimi k1.5, a multi-modal large language model (LLM) that utilizes reinforcement learning (RL) to enhance its training data exploration through reward mechanisms. The authors highlight their innovative RL training techniques and infrastructure optimizations that allow for effective long context scaling and policy optimization without complex methods like Monte Carlo tree search. Kimi k1.5 achieves state-of-the-art performance on various reasoning benchmarks, demonstrating its competitive edge over existing models. Additionally, the paper introduces long2short methods that leverage long-context techniques to significantly improve short-context reasoning results, outperforming other models by a substantial margin."
                },
                "zh": {
                    "title": "Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ™ÅÁ†¥",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜKimi k1.5ÁöÑËÆ≠ÁªÉÂÆûË∑µÔºåËøôÊòØ‰∏ÄÁßçÊúÄÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõË°åËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÈïø‰∏ä‰∏ãÊñáÊâ©Â±ïÂíåÊîπËøõÁöÑÁ≠ñÁï•‰ºòÂåñÔºåÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÊúâÊïàÁöÑRLÊ°ÜÊû∂ÔºåËÄå‰∏ç‰æùËµñ‰∫éÂ§çÊùÇÁöÑÊäÄÊúØÔºåÂ¶ÇËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Âíå‰ª∑ÂÄºÂáΩÊï∞„ÄÇKimi k1.5Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊé®ÁêÜÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁü≠ÈìæÊé®ÁêÜÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂà©Áî®ÈïøÈìæÊäÄÊúØÂèØ‰ª•ÊòæËëóÊèêÂçáÁü≠ÈìæÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑËøõÊ≠•„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13074",
            "title": "Autonomy-of-Experts Models",
            "url": "https://huggingface.co/papers/2501.13074",
            "abstract": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.",
            "score": 26,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "5cf511144ad54091",
            "authors": [
                "Ang Lv",
                "Ruobing Xie",
                "Yining Qian",
                "Songhao Wu",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Di Wang",
                "Rui Yan"
            ],
            "affiliations": [
                "Machine Learning Platform Department, Tencent",
                "Renmin University of China",
                "Southeast University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13074.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ–æ—Ç–±–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª—è–º Mixture-of-Experts (MoE) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Autonomy-of-Experts (AoE). –í AoE —ç–∫—Å–ø–µ—Ä—Ç—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞—é—Ç —Å–µ–±—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö, —á—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–µ. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –æ—Ç 700 –º–ª–Ω –¥–æ 4 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AoE –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ MoE –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Empowering Experts: Self-Selection for Enhanced Learning in MoE Models",
                    "desc": "This paper introduces a new approach called Autonomy-of-Experts (AoE) for Mixture-of-Experts (MoE) models, which traditionally rely on a router to assign tasks to expert modules. The authors argue that the separation of decision-making and execution in MoE leads to poor expert selection and learning inefficiencies. In AoE, experts autonomously evaluate their ability to process inputs based on their internal activations, eliminating the need for a router. By allowing only the most capable experts to participate in processing, AoE enhances expert selection and improves overall model performance while maintaining efficiency."
                },
                "zh": {
                    "title": "Ëá™‰∏ªÈÄâÊã©ÔºåÊèêÂçá‰∏ìÂÆ∂Â≠¶‰π†ÊïàÁéá",
                    "desc": "Ê∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÔºàMoEÔºâÈÄöÂ∏∏‰ΩøÁî®Ë∑ØÁî±Âô®Â∞ÜËæìÂÖ•ÂàÜÈÖçÁªôÁâπÂÆöÁöÑ‰∏ìÂÆ∂Ê®°ÂùóÔºå‰ªÖÊøÄÊ¥ªÈÉ®ÂàÜÂèÇÊï∞ÔºåÈÄöÂ∏∏ÊØîÂØÜÈõÜÊ®°ÂûãË°®Áé∞Êõ¥Â•Ω„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåË∑ØÁî±Âô®ÁöÑÂÜ≥Á≠ñ‰∏é‰∏ìÂÆ∂ÁöÑÊâßË°å‰πãÈó¥ÁöÑÂàÜÁ¶ªÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆ‰ΩÜË¢´ÂøΩËßÜÁöÑÈóÆÈ¢òÔºåÂØºËá¥‰∏ìÂÆ∂ÈÄâÊã©‰∏ç‰Ω≥ÂíåÂ≠¶‰π†ÊïàÊûú‰∏çÁêÜÊÉ≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™‰∏ª‰∏ìÂÆ∂ÔºàAoEÔºâÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑMoEËåÉÂºèÔºåÂÖ∂‰∏≠‰∏ìÂÆ∂Ëá™‰∏ªÈÄâÊã©Ëá™Â∑±Â§ÑÁêÜËæìÂÖ•„ÄÇAoEÂü∫‰∫é‰∏ìÂÆ∂ËÉΩÂ§üÊÑèËØÜÂà∞Ëá™Ë∫´Â§ÑÁêÜËÉΩÂäõÁöÑÊ¥ûÂØüÔºåÈÄöËøáÂÜÖÈÉ®ÊøÄÊ¥ªÁöÑËßÑÊ®°ÂèçÊò†Âá∫Êù•Ôºå‰ªéËÄåÁ°Æ‰øù‰∫ÜÊõ¥Â•ΩÁöÑ‰∏ìÂÆ∂ÈÄâÊã©ÂíåÊúâÊïàÂ≠¶‰π†„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13007",
            "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
            "url": "https://huggingface.co/papers/2501.13007",
            "abstract": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness. To address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a knockout tournament for BoN sampling. Instead of assigning absolute scores, given one math problem, Pairwise RM evaluates two candidate solutions' correctness simultaneously. This approach eliminates the need for arbitrary scoring and enables cross-validation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct \\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from NumiaMath and annotated using gemini-1.5-flash, and train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements over traditional discriminative reward models. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\% challenging problems.",
            "score": 12,
            "issue_id": 1821,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "a34210b73ec25875",
            "authors": [
                "Yantao Liu",
                "Zijun Yao",
                "Rui Min",
                "Yixin Cao",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13007.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#rlhf"
                ],
                "emoji": "üèÜ",
                "ru": {
                    "title": "–ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –≤ LLM",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ø–∞—Ä–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (Pairwise Reward Model) –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å —Ç—É—Ä–Ω–∏—Ä–æ–º –Ω–∞ –≤—ã–±—ã–≤–∞–Ω–∏–µ –¥–ª—è Best-of-N —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –±–∞–ª–ª–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö."
                },
                "en": {
                    "title": "Enhancing Solution Selection with Pairwise Comparisons",
                    "desc": "This paper introduces a new method called Pairwise Reward Model (Pairwise RM) to improve the selection process in Best-of-N (BoN) sampling for Large Language Models (LLMs). Instead of giving arbitrary scores to candidate solutions, Pairwise RM compares two solutions at a time to determine which one is more correct. This method allows for better validation of solutions through direct comparison and eliminates inconsistencies in scoring. The authors also created a large dataset of 443,000 pairwise comparisons to train the model, resulting in significant performance improvements on challenging math problems compared to traditional reward models."
                },
                "zh": {
                    "title": "ÊàêÂØπÂ•ñÂä±Ê®°ÂûãÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄâÊã©ËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ•ñÂä±Ê®°ÂûãÔºåÁß∞‰∏∫ÊàêÂØπÂ•ñÂä±Ê®°ÂûãÔºàPairwise RMÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúÄ‰Ω≥NÔºàBoNÔºâÈááÊ†∑„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÂ∏∏Â∏∏ÁªôÂá∫‰ªªÊÑè‰∏î‰∏ç‰∏ÄËá¥ÁöÑÂàÜÊï∞ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÊàêÂØπÂ•ñÂä±Ê®°ÂûãÈÄöËøáÂêåÊó∂ËØÑ‰º∞‰∏§‰∏™ÂÄôÈÄâËß£ÁöÑÊ≠£Á°ÆÊÄßÔºåÊ∂àÈô§‰∫ÜÂØπ‰ªªÊÑèËØÑÂàÜÁöÑÈúÄÊ±ÇÔºåÂπ∂ÈÄöËøáÂπ∂Ë°åÊØîËæÉÂÆûÁé∞‰∫ÜËß£ÂÜ≥ÊñπÊ°àÁöÑ‰∫§ÂèâÈ™åËØÅ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´443KÊàêÂØπÊØîËæÉÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøáÁõëÁù£ÂæÆË∞ÉËÆ≠ÁªÉ‰∫ÜÊàêÂØπÂ•ñÂä±Ê®°ÂûãÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®Ëß£ÂÜ≥Êï∞Â≠¶ÈóÆÈ¢òÊó∂ÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂà§Âà´Â•ñÂä±Ê®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12570",
            "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
            "url": "https://huggingface.co/papers/2501.12570",
            "abstract": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner",
            "score": 9,
            "issue_id": 1818,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 —è–Ω–≤–∞—Ä—è",
                "en": "January 22",
                "zh": "1Êúà22Êó•"
            },
            "hash": "2cb7e92315bbf3e4",
            "authors": [
                "Haotian Luo",
                "Li Shen",
                "Haiying He",
                "Yibo Wang",
                "Shiwei Liu",
                "Wei Li",
                "Naiqiang Tan",
                "Xiaochun Cao",
                "Dacheng Tao"
            ],
            "affiliations": [
                "China Agriculture University",
                "Didichuxing Co. Ltd",
                "Nanyang Technological University",
                "Shenzhen Campus of Sun Yat-sen University",
                "Tsinghua University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12570.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ OpenAI's O1. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–µ—Ö–Ω–∏–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Length-Harmonizing Fine-Tuning (O1-Pruner), –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ O1-Pruner –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–ª–∏ –¥–∞–∂–µ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Optimizing Long-Thought Reasoning for Efficient Problem Solving",
                    "desc": "This paper discusses a new approach to improve long-thought reasoning in large language models (LLMs) like OpenAI's O1. The authors identify that while these models enhance problem-solving, they also increase inference time due to inefficient token usage. To tackle this, they introduce Length-Harmonizing Fine-Tuning (O1-Pruner), which optimizes the reasoning process by balancing accuracy and efficiency. Their experiments show that O1-Pruner reduces inference overhead and improves accuracy on mathematical reasoning tasks, making it a valuable advancement in LLM performance."
                },
                "zh": {
                    "title": "‰ºòÂåñÊé®ÁêÜÊïàÁéáÔºåÊèêÂçáÂáÜÁ°ÆÊÄßÔºÅ",
                    "desc": "ÊúÄËøëÔºåÈïøÊÄùËÄÉÊé®ÁêÜÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂ¶ÇOpenAIÁöÑO1ÔºåÈááÁî®‰∫ÜÁ±ª‰ºº‰∫∫Á±ªÊÄùËÄÉÂ§çÊùÇÈóÆÈ¢òÁöÑÊâ©Â±ïÊé®ÁêÜËøáÁ®ã„ÄÇËøôÁßçÊé®ÁêÜËåÉÂºèÊòæËëóÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑËß£ÂÜ≥ÈóÆÈ¢òËÉΩÂäõÔºåÂπ∂ÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÈïøÊÄùËÄÉÊé®ÁêÜËøáÁ®ãÂØºËá¥Êé®ÁêÜÊó∂Èó¥Â§ßÂπÖÂ¢ûÂä†„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈïøÂ∫¶ÂçèË∞ÉÂæÆË∞ÉÔºàO1-PrunerÔºâÔºåÊó®Âú®Âú®‰øùÊåÅÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÂáèÂ∞ëÈïøÊÄùËÄÉLLMÁöÑÊé®ÁêÜÂºÄÈîÄ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11067",
            "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
            "url": "https://huggingface.co/papers/2501.11067",
            "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent",
            "score": 5,
            "issue_id": 1820,
            "pub_date": "2025-01-19",
            "pub_date_card": {
                "ru": "19 —è–Ω–≤–∞—Ä—è",
                "en": "January 19",
                "zh": "1Êúà19Êó•"
            },
            "hash": "019b0714b4212a7f",
            "authors": [
                "Elad Levi",
                "Ilan Kadar"
            ],
            "affiliations": [
                "Plurai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11067.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#open_source",
                    "#games",
                    "#optimization",
                    "#graphs",
                    "#benchmark"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "IntellAgent: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ –ò–ò",
                    "desc": "IntellAgent - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º. –û–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–∏—Ç–∏–∫, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞–≥–µ–Ω—Ç–∞. IntellAgent –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞—Ñ–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª–∏—Ç–∏–∫, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É. –°–∏—Å—Ç–µ–º–∞ –≤—ã—è–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–æ–ª–µ–∑–Ω—ã–µ –∏–¥–µ–∏ –¥–ª—è —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Revolutionizing Evaluation of Conversational AI with IntellAgent",
                    "desc": "This paper presents IntellAgent, a new framework for evaluating conversational AI systems, particularly those powered by Large Language Models (LLMs). It addresses the challenges of traditional evaluation methods by automating the creation of diverse benchmarks that simulate real-world interactions. IntellAgent uses a graph-based policy model to analyze the complex relationships and interactions between different policies, providing detailed diagnostics and identifying performance gaps. The open-source nature of IntellAgent encourages collaboration and integration of new features, making it a valuable tool for improving conversational AI systems."
                },
                "zh": {
                    "title": "IntellAgentÔºöÂØπËØùÂºèAIËØÑ‰º∞ÁöÑÊñ∞ËåÉÂºè",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊ≠£Âú®ÊîπÂèò‰∫∫Â∑•Êô∫ËÉΩÔºåÊàê‰∏∫ËÉΩÂ§üËá™‰∏ªËßÑÂàíÂíåÊâßË°å‰ªªÂä°ÁöÑÁ≥ªÁªü„ÄÇÂÆÉ‰ª¨Âú®ÂØπËØùÂºè‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü‰∏≠ÁöÑÂ∫îÁî®Â∞§‰∏∫ÈáçË¶ÅÔºåËøô‰∫õÁ≥ªÁªüÈúÄË¶ÅÂ§ÑÁêÜÂ§öËΩÆÂØπËØù„ÄÅÊï¥ÂêàÁâπÂÆöÈ¢ÜÂüüÁöÑAPIÔºåÂπ∂ÈÅµÂæ™‰∏•Ê†ºÁöÑÊîøÁ≠ñÁ∫¶Êùü„ÄÇÁÑ∂ËÄåÔºåËØÑ‰º∞Ëøô‰∫õÊô∫ËÉΩ‰Ωì‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÊåëÊàòÔºåÂõ†‰∏∫‰º†ÁªüÊñπÊ≥ïÊó†Ê≥ïÊçïÊçâÁé∞ÂÆû‰∏ñÁïå‰∫§‰∫íÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜIntellAgentÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂºÄÊ∫êÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞ÂØπËØùÂºè‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-01-22.html",
    "link_next": "2025-01-24.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1Êúà22Êó•"
    },
    "short_date_next": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1Êúà24Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏§ÁßçÊé®ÁêÜÊ®°ÂûãÔºöDeepSeek-R1-Zero Âíå DeepSeek-R1„ÄÇDeepSeek-R1-Zero ÈÄöËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåÂ±ïÁ§∫‰∫ÜÂá∫Ëâ≤ÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂ≠òÂú®ÂèØËØªÊÄßÂ∑ÆÂíåËØ≠Ë®ÄÊ∑∑ÂêàÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü DeepSeek-R1ÔºåÂÆÉÂú®Âº∫ÂåñÂ≠¶‰π†‰πãÂâçËøõË°åÂ§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂÜ∑ÂêØÂä®Êï∞ÊçÆÂ§ÑÁêÜ„ÄÇDeepSeek-R1 Âú®Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞‰∏é OpenAI-o1-1217 Áõ∏ÂΩì„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫ÜËøô‰∫õÊ®°ÂûãÂíåÂÖ≠‰∏™Âü∫‰∫é Qwen Âíå Llama ÁöÑÂéãÁº©Ê®°Âûã„ÄÇ",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "pinyin": "Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏§ÁßçÊé®ÁêÜÊ®°ÂûãÔºöDeepSeek-R1-Zero Âíå DeepSeek-R1„ÄÇDeepSeek-R1-Zero ÈÄöËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåÂ±ïÁ§∫‰∫ÜÂá∫Ëâ≤ÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂ≠òÂú®ÂèØËØªÊÄßÂ∑ÆÂíåËØ≠Ë®ÄÊ∑∑ÂêàÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü DeepSeek-R1ÔºåÂÆÉÂú®Âº∫ÂåñÂ≠¶‰π†‰πãÂâçËøõË°åÂ§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂÜ∑ÂêØÂä®Êï∞ÊçÆÂ§ÑÁêÜ„ÄÇDeepSeek-R1 Âú®Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞‰∏é OpenAI-o1-1217 Áõ∏ÂΩì„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫ÜËøô‰∫õÊ®°ÂûãÂíåÂÖ≠‰∏™Âü∫‰∫é Qwen Âíå Llama ÁöÑÂéãÁº©Ê®°Âûã„ÄÇ\n\nW«ímen ji√®sh√†o le li«éng zh«íng tuƒ´l«ê m√≥x√≠ng: DeepSeek-R1-Zero h√© DeepSeek-R1. DeepSeek-R1-Zero t≈çnggu√≤ d√† guƒ´m√≥ qi√°ngzh√π xu√©x√≠ x√πnli√†n, zh«énsh√¨ le ch≈´s√® de tuƒ´l«ê n√©ngl√¨, d√†n c√∫nz√†i kƒõd√∫x√¨ng ch√† h√© y«îy√°n h√πnh√© de w√®nt√≠. W√®ile jiƒõju√© zh√®xiƒì w√®nt√≠, w«ímen kƒÅifƒÅ le DeepSeek-R1, tƒÅ z√†i qi√°ngzh√π xu√©x√≠ zhƒ´qi√°n j√¨nx√≠ng du≈ç jiƒìdu√†n x√πnli√†n h√© lƒõng q«êd√≤ng sh√πj√π ch«îl«ê. DeepSeek-R1 z√†i tuƒ´l«ê r√®nw√π sh√†ng de bi«éoxi√†n y«î OpenAI-o1-1217 xiƒÅngdƒÅng. W«ímen kƒÅiyu√°n le zh√®xiƒì m√≥x√≠ng h√© li√π g√® jƒ´y√∫ Qwen h√© Llama de yƒÅsu≈ç m√≥x√≠ng.",
        "vocab": "[{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng‚Äãhu√†‚Äãxu√©‚Äãx√≠', 'trans': 'reinforcement learning'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«énsh√¨', 'trans': 'demonstrate'}, {'word': 'ÂèØËØªÊÄß', 'pinyin': 'kƒõ‚Äãd√∫‚Äãx√¨ng', 'trans': 'readability'}, {'word': 'ËØ≠Ë®ÄÊ∑∑Âêà', 'pinyin': 'y«î‚Äãy√°n‚Äãh√πn‚Äãh√©', 'trans': 'language mixing'}, {'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅifƒÅ', 'trans': 'develop'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç‚Äãjiƒì‚Äãdu√†n', 'trans': 'multi-stage'}, {'word': 'ÂÜ∑ÂêØÂä®', 'pinyin': 'lƒõng‚Äãq«ê‚Äãd√≤ng', 'trans': 'cold start'}, {'word': 'Êï∞ÊçÆÂ§ÑÁêÜ', 'pinyin': 'sh√π‚Äãj√π‚Äãch«î‚Äãl«ê', 'trans': 'data processing'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo‚Äãxi√†n', 'trans': 'performance'}, {'word': 'Áõ∏ÂΩì', 'pinyin': 'xiƒÅng‚ÄãdƒÅng', 'trans': 'equivalent'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi‚Äãyu√°n', 'trans': 'open source'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´‚Äãy√∫', 'trans': 'based on'}, {'word': 'ÂéãÁº©', 'pinyin': 'yƒÅ‚Äãsu≈ç', 'trans': 'compression'}]",
        "trans": "We introduced two reasoning models: DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, trained through large-scale reinforcement learning, demonstrated excellent reasoning capabilities but suffered from poor readability and language mixing issues. To address these problems, we developed DeepSeek-R1, which undergoes multi-stage training and cold start data processing before reinforcement learning. DeepSeek-R1 performs comparably to OpenAI-o1-1217 in reasoning tasks. We have open-sourced these models along with six compressed models based on Qwen and Llama.",
        "update_ts": "2025-01-23 09:10"
    }
}