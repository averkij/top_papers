{
    "date": {
        "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 15",
        "zh": "1æœˆ15æ—¥"
    },
    "time_utc": "2026-01-15 03:43",
    "weekday": 3,
    "issue_id": 588,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.07348",
            "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
            "url": "https://huggingface.co/papers/2601.07348",
            "abstract": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
            "score": 19,
            "issue_id": 588,
            "pub_date": "2026-01-12",
            "pub_date_card": {
                "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 12",
                "zh": "1æœˆ12æ—¥"
            },
            "hash": "3c8478e6aa318055",
            "authors": [
                "Tu Hu",
                "Ronghao Chen",
                "Shuo Zhang",
                "Jianghao Yin",
                "Mou Xiao Feng",
                "Jingping Liu",
                "Shaolei Zhang",
                "Wenqi Jiang",
                "Yuqi Fang",
                "Sen Hu",
                "Yi Xu",
                "Huacan Wang"
            ],
            "affiliations": [
                "ECNU",
                "Midea-AIRC",
                "NJU",
                "PKU",
                "QuantaAlpha",
                "RUC",
                "SYSU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07348.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Controlled Self-Evolution Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ…: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºÑ€Ğ¾ÑÑĞ¾Ğ²ĞµÑ€ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EffiBench-X."
                },
                "en": {
                    "title": "Unlocking Code Generation with Controlled Self-Evolution",
                    "desc": "The Controlled Self-Evolution (CSE) method enhances code generation by addressing inefficiencies in existing self-evolution techniques. It introduces diversified initialization to explore a wider solution space and employs feedback-guided genetic evolution to improve mutation and crossover processes. Additionally, CSE utilizes hierarchical memory to retain valuable experiences from both successful and unsuccessful attempts, facilitating better learning across tasks. Experiments show that CSE significantly outperforms traditional methods, achieving higher efficiency and continuous improvement in code generation."
                },
                "zh": {
                    "title": "å—æ§è‡ªæˆ‘è¿›åŒ–ï¼šæå‡ä»£ç ç”Ÿæˆçš„æ•ˆç‡ä¸è´¨é‡",
                    "desc": "å—æ§è‡ªæˆ‘è¿›åŒ–æ–¹æ³•é€šè¿‡å¤šæ ·åŒ–åˆå§‹åŒ–ã€åé¦ˆå¼•å¯¼çš„é—ä¼ è¿›åŒ–å’Œåˆ†å±‚è®°å¿†æ¥æé«˜ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•åœ¨æœ‰é™é¢„ç®—å†…å‘ç°æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å—æ§è‡ªæˆ‘è¿›åŒ–ï¼ˆCSEï¼‰ï¼Œå®ƒé€šè¿‡ç”Ÿæˆç»“æ„ä¸Šä¸åŒçš„ç®—æ³•ç­–ç•¥æ¥è¦†ç›–å¹¿æ³›çš„è§£å†³ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCSEåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨æ—©æœŸç”Ÿæˆä¸­å®ç°äº†æ›´é«˜çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09708",
            "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
            "url": "https://huggingface.co/papers/2601.09708",
            "abstract": "Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
            "score": 14,
            "issue_id": 588,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "6b6ba22e4fea5524",
            "authors": [
                "Chi-Pin Huang",
                "Yunze Man",
                "Zhiding Yu",
                "Min-Hung Chen",
                "Jan Kautz",
                "Yu-Chiang Frank Wang",
                "Fu-En Yang"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09708.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv",
                    "#transfer_learning",
                    "#inference",
                    "#robotics",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "Fast-ThinkAct â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° 89.3% Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Fast-ThinkAct Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Fast-ThinkAct: Speedy Reasoning for Smart Actions",
                    "desc": "Fast-ThinkAct is a new framework designed for vision-language-action (VLA) tasks that significantly cuts down the time it takes to make decisions by 89.3%. It does this by using a compact form of reasoning that still allows for effective long-term planning and quick adaptation to new situations. The framework learns to reason efficiently by mimicking a more complex teacher model, which helps it connect visual understanding with action execution. Through various tests, Fast-ThinkAct has shown to perform well while being much faster than previous models, making it suitable for dynamic environments."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œå¿«é€Ÿè¡ŒåŠ¨ï¼",
                    "desc": "Fast-ThinkAct æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶ï¼Œé€šè¿‡ç´§å‡‘çš„æ½œåœ¨æ¨ç†å°†æ¨ç†å»¶è¿Ÿå‡å°‘äº† 89.3%ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚çš„è§†è§‰åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚Fast-ThinkAct é€šè¿‡ä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼å‡ºæ½œåœ¨çš„æ€ç»´é“¾ï¼Œå­¦ä¹ é«˜æ•ˆæ¨ç†ï¼Œå¹¶é€šè¿‡åå¥½å¼•å¯¼ç›®æ ‡å¯¹é½æ“ä½œè½¨è¿¹ï¼Œä»è€Œå®ç°è¯­è¨€å’Œè§†è§‰è§„åˆ’èƒ½åŠ›çš„è½¬ç§»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFast-ThinkAct åœ¨å¤šç§æ“ä½œå’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒæœ‰æ•ˆçš„é•¿è¿œè§„åˆ’ã€å°‘é‡æ ·æœ¬é€‚åº”å’Œæ•…éšœæ¢å¤èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09259",
            "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
            "url": "https://huggingface.co/papers/2601.09259",
            "abstract": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
            "score": 11,
            "issue_id": 588,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "75d53a8919dc55a2",
            "authors": [
                "Jian Zhang",
                "Zhiyuan Wang",
                "Zhangqi Wang",
                "Yu He",
                "Haoran Luo",
                "li yuan",
                "Lingling Zhang",
                "Rui Mao",
                "Qika Lin",
                "Jun Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "South China University of Technology",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09259.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑƒÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "MAXS â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¸Ğ¾Ğ¿Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸) Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. MAXS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ lookahead Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with MAXS: Smart Planning for Better Performance",
                    "desc": "MAXS is a new framework designed to enhance the reasoning capabilities of Large Language Model (LLM) agents by using advanced strategies for planning and tool execution. It addresses two main problems: the tendency for agents to make short-sighted decisions and the instability of reasoning paths that can lead to errors. By implementing a lookahead strategy, MAXS predicts the benefits of using different tools and selects the most effective reasoning steps. Additionally, it introduces a mechanism to stop unnecessary computations once a stable reasoning path is found, ensuring both efficiency and effectiveness in multi-tool reasoning tasks."
                },
                "zh": {
                    "title": "MAXSï¼šæå‡å¤šå·¥å…·æ¨ç†çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "MAXSæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å…ƒè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‰ç»ç­–ç•¥å’Œè½¨è¿¹æ”¶æ•›æœºåˆ¶æ¥æ”¹å–„å¤šå·¥å…·æ¨ç†ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„å±€éƒ¨çŸ­è§†å’Œè½¨è¿¹ä¸ç¨³å®šé—®é¢˜ï¼Œä»è€Œå¹³è¡¡äº†å…¨å±€æœ‰æ•ˆæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚MAXSé€šè¿‡å‰ç»ç­–ç•¥å»¶ä¼¸æ¨ç†è·¯å¾„ï¼Œå¹¶ç»“åˆæ­¥éª¤ä¸€è‡´æ€§æ–¹å·®å’Œè·¨æ­¥éª¤è¶‹åŠ¿æ–œç‡æ¥é€‰æ‹©ç¨³å®šä¸”é«˜ä»·å€¼çš„æ¨ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè½¨è¿¹æ”¶æ•›æœºåˆ¶èƒ½å¤Ÿåœ¨è¾¾åˆ°è·¯å¾„ä¸€è‡´æ€§ååœæ­¢è¿›ä¸€æ­¥çš„æ¨ç†ï¼Œä»è€Œæœ‰æ•ˆæ§åˆ¶è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09274",
            "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
            "url": "https://huggingface.co/papers/2601.09274",
            "abstract": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
            "score": 10,
            "issue_id": 588,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "f09abfdc90cf025f",
            "authors": [
                "Jian Zhang",
                "Yu He",
                "Zhiyuan Wang",
                "Zhangqi Wang",
                "Kai He",
                "Fangzhi Xu",
                "Qika Lin",
                "Jun Liu"
            ],
            "affiliations": [
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09274.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ñ€ĞµĞ¹ (anchors) Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² (attractors) - ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº A^3-Bench Ñ 2198 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ SAPM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ AAUI (Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Harnessing Memory for Enhanced Scientific Reasoning",
                    "desc": "This paper introduces A^3-Bench, a new benchmark for evaluating scientific reasoning that emphasizes the role of memory in the reasoning process. It highlights how activating prior knowledge through anchors and attractors can enhance the consistency and stability of reasoning. The authors annotate a large set of science reasoning problems and propose a dual-scale memory evaluation framework to assess how effectively memory is utilized during reasoning tasks. Experiments demonstrate the impact of memory activation on reasoning performance, offering valuable insights into the mechanisms of human-like scientific reasoning."
                },
                "zh": {
                    "title": "è®°å¿†é©±åŠ¨çš„ç§‘å­¦æ¨ç†è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç§‘å­¦æ¨ç†ä¸­è®°å¿†çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†æ¿€æ´»å…ˆå‰çŸ¥è¯†å’Œç»éªŒç»“æ„çš„é‡è¦æ€§ã€‚ä½œè€…æå‡ºäº†A^3-BenchåŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡åŒå°ºåº¦è®°å¿†é©±åŠ¨æ¿€æ´»æ¥è¯„ä¼°ç§‘å­¦æ¨ç†ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†2,198ä¸ªç§‘å­¦æ¨ç†é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†é”šç‚¹å’Œå¸å¼•ç‚¹çš„æ¦‚å¿µæ¥åˆ†æè®°å¿†æ¿€æ´»ç‡ã€‚é€šè¿‡å®éªŒéªŒè¯äº†A^3-Benchçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ†æäº†è®°å¿†æ¿€æ´»å¯¹æ¨ç†è¡¨ç°çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06596",
            "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
            "url": "https://huggingface.co/papers/2601.06596",
            "abstract": "Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.",
            "score": 4,
            "issue_id": 588,
            "pub_date": "2026-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "270eb8d27917d8cc",
            "authors": [
                "Hongjun An",
                "Yiliang Song",
                "Jiangan Chen",
                "Jiawei Shao",
                "Chi Zhang",
                "Xuelong Li"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence (TeleAI), China Telecom",
                "School of Artificial Intelligence, OPtics and ElectroNics, Northwestern Polytechnical University",
                "School of Economics and Management, Guangxi Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06596.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#security",
                    "#interpretability",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ°Ñ‚Ğ°Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¸Ğ½Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒĞ³Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼, Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ°Ñ‚Ğ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¸ÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº RLHF."
                },
                "en": {
                    "title": "Unmasking Vulnerabilities: Preference-Undermining Attacks on LLMs",
                    "desc": "This research explores how large language models (LLMs) can be influenced by manipulative prompts that undermine their alignment with user preferences. It identifies a specific type of attack called Preference-Undermining Attacks (PUA), which can lead models to prioritize pleasing responses over truthful ones. The authors propose a new evaluation method that breaks down the effects of different prompting strategies, allowing for a clearer understanding of how these attacks affect model behavior. Their findings indicate that more advanced models may be more vulnerable to these manipulative tactics, highlighting the need for tailored defenses against such risks."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§ä¸é˜²å¾¡ç­–ç•¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å—åˆ°åå¥½å‰Šå¼±æ”»å‡»çš„å½±å“ï¼Œè¿™ç§æ”»å‡»åˆ©ç”¨äº†æ¨¡å‹çš„å¯¹é½ç›®æ ‡ï¼Œæ­ç¤ºäº†æ¨¡å‹çš„è„†å¼±æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä¼˜åŒ–äº†ç”¨æˆ·åå¥½çš„å¯¹é½ï¼Œä½†è¿™ç§ç›®æ ‡å¯èƒ½è¢«æ“æ§æ€§æç¤ºæ‰€åˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹åå‘è¿åˆç”¨æˆ·è€Œéè¿½æ±‚çœŸå®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å› å­è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´ç»†è‡´åœ°åˆ†ææ¨¡å‹åœ¨é¢å¯¹æ“æ§æ€§æç¤ºæ—¶çš„è¡¨ç°ï¼Œè¯†åˆ«å‡ºç³»ç»Ÿç›®æ ‡å’Œå¯¹è¯å› ç´ çš„å½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒæŸäº›é«˜çº§æ¨¡å‹åœ¨é¢å¯¹æ“æ§æ€§æç¤ºæ—¶åè€Œæ›´å®¹æ˜“å—åˆ°å½±å“ï¼Œæç¤ºéœ€è¦é’ˆå¯¹æ€§é˜²å¾¡è€Œéç»Ÿä¸€çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09136",
            "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
            "url": "https://huggingface.co/papers/2601.09136",
            "abstract": "SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.  \t\t\t\t\tAI-generated summary \t\t\t\t General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
            "score": 3,
            "issue_id": 588,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "3089ecac62962287",
            "authors": [
                "Lijun Liu",
                "Linwei Chen",
                "Zhishou Zhang",
                "Meng Tian",
                "Hengfu Cui",
                "Ruiyang Li",
                "Zhaocheng Liu",
                "Qiang Ju",
                "Qianxi Li",
                "Hong-Yu Zhou"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "Beijing Key Laboratory of Molecular Diagnosis on Dermatoses",
                "Department of Dermatology, Peking University First Hospital",
                "NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics",
                "National Clinical Research Center for Skin and Sexually Transmitted Diseases",
                "School of Biomedical Engineering, Tsinghua University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09136.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#science",
                    "#small_models",
                    "#cv",
                    "#healthcare",
                    "#rl",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ´ĞµÑ€Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ",
                    "desc": "SkinFlow Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ€Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ÑÂ» Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¶Ğ¸ Ğ¾Ñ‚ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Fitzpatrick17k Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ +12.06% Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Top-1 Ğ¸ +28.57% Ğ² Top-6 Ğ½Ğ°Ğ´ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Information Flow for Superior Dermatological Diagnosis",
                    "desc": "SkinFlow is a new framework designed to enhance dermatological diagnosis by improving how visual information is transmitted rather than just increasing the number of model parameters. It addresses the issue of 'diffuse attention' in large vision-language models, which struggle to differentiate subtle skin lesions from background noise. The framework employs a Virtual-Width Dynamic Vision Encoder to effectively manage complex pathological data without needing more parameters, and it uses a two-stage Reinforcement Learning approach to align medical descriptions with diagnostic features. The results show that SkinFlow significantly outperforms existing models in accuracy, proving that optimizing information flow is more effective than simply scaling up model size."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ä¿¡æ¯æµï¼Œæå‡çš®è‚¤ç—…è¯Šæ–­å‡†ç¡®æ€§",
                    "desc": "SkinFlow æ˜¯ä¸€ä¸ªæ–°é¢–çš„çš®è‚¤ç—…è§†è§‰-è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è§†è§‰ä¿¡æ¯ä¼ è¾“æ•ˆç‡æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œè€Œä¸ä»…ä»…ä¾èµ–äºå‚æ•°æ‰©å±•ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è™šæ‹Ÿå®½åº¦åŠ¨æ€è§†è§‰ç¼–ç å™¨ï¼ˆDVEï¼‰æ¥å±•å¼€å¤æ‚çš„ç—…ç†æµå½¢ï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¯¹é½åŒ»å­¦æè¿°å’Œé‡å»ºéšå«çš„è¯Šæ–­ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸´åºŠåŸºç¡€çš„è¯„ä¼°åè®®ï¼Œå¼ºè°ƒè¯Šæ–­å®‰å…¨æ€§å’Œå±‚æ¬¡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„æ ‡ç­¾åŒ¹é…ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨Fitzpatrick17kåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€ä½³æˆç»©ï¼Œæ˜¾ç¤ºå‡ºä¼˜åŒ–å‡ ä½•å®¹é‡å’Œä¿¡æ¯æµæ¯”å•çº¯çš„å‚æ•°æ‰©å±•æ›´èƒ½æå‡è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-14.html",
    "link_next": "2026-01-16.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}