{
    "date": {
        "ru": "20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 20",
        "zh": "12æœˆ20æ—¥"
    },
    "time_utc": "2024-12-20 03:16",
    "weekday": 4,
    "issue_id": 1227,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.15115",
            "title": "Qwen2.5 Technical Report",
            "url": "https://huggingface.co/papers/2412.15115",
            "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
            "score": 22,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "578b15d8a263e387",
            "authors": [
                "Qwen",
                ":",
                "An Yang",
                "Baosong Yang",
                "Beichen Zhang",
                "Binyuan Hui",
                "Bo Zheng",
                "Bowen Yu",
                "Chengyuan Li",
                "Dayiheng Liu",
                "Fei Huang",
                "Haoran Wei",
                "Huan Lin",
                "Jian Yang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jianxin Yang",
                "Jiaxi Yang",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Keming Lu",
                "Keqin Bao",
                "Kexin Yang",
                "Le Yu",
                "Mei Li",
                "Mingfeng Xue",
                "Pei Zhang",
                "Qin Zhu",
                "Rui Men",
                "Runji Lin",
                "Tianhao Li",
                "Tingyu Xia",
                "Xingzhang Ren",
                "Xuancheng Ren",
                "Yang Fan",
                "Yang Su",
                "Yichang Zhang",
                "Yu Wan",
                "Yuqiong Liu",
                "Zeyu Cui",
                "Zhenru Zhang",
                "Zihan Qiu"
            ],
            "affiliations": [
                "Alibaba Cloud Model Studio",
                "Hugging Face Hub",
                "Kaggle",
                "ModelScope"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15115.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal",
                    "#architecture",
                    "#agi",
                    "#dataset",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Qwen2.5: ĞĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2.5, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ 18 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Qwen2.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ ĞµĞµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ."
                },
                "en": {
                    "title": "Qwen2.5: Elevating Language Models with Unmatched Scale and Precision",
                    "desc": "Qwen2.5 is a new series of large language models (LLMs) that have been enhanced through extensive pre-training and post-training processes. The pre-training phase utilized a massive dataset of 18 trillion tokens, significantly improving the model's common sense and reasoning abilities. In the post-training phase, advanced techniques like supervised finetuning and reinforcement learning were applied to refine the model's performance on tasks such as long text generation and instruction following. Qwen2.5 models are available in various sizes and configurations, demonstrating top performance across multiple benchmarks and applications, including specialized models for math and coding."
                },
                "zh": {
                    "title": "Qwen2.5ï¼šæ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚çš„å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Qwen2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨æ»¡è¶³å¤šæ ·åŒ–çš„éœ€æ±‚ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒQwen2.5åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µéƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œé¢„è®­ç»ƒæ•°æ®é›†ä»7ä¸‡äº¿ä¸ªæ ‡è®°æ‰©å±•åˆ°18ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œä¸ºå¸¸è¯†ã€ä¸“å®¶çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æä¾›äº†åšå®åŸºç¡€ã€‚åè®­ç»ƒæ–¹é¢ï¼Œé‡‡ç”¨äº†è¶…è¿‡100ä¸‡æ ·æœ¬çš„å¤æ‚ç›‘ç£å¾®è°ƒå’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†äººç±»åå¥½å’Œé•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚Qwen2.5åœ¨è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯å…¶æ——èˆ°æ¨¡å‹Qwen2.5-72B-Instructåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†è®¸å¤šå¼€æ”¾å’Œä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14835",
            "title": "Progressive Multimodal Reasoning via Active Retrieval",
            "url": "https://huggingface.co/papers/2412.14835",
            "abstract": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.",
            "score": 19,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "749dab304f766614",
            "authors": [
                "Guanting Dong",
                "Chenghao Zhang",
                "Mengjie Deng",
                "Yutao Zhu",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14835.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AR-MCTS: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AR-MCTS - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ (AR) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° (MCTS) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. AR-MCTS Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ AR-MCTS Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with AR-MCTS Framework",
                    "desc": "This paper addresses the challenges faced by multimodal large language models (MLLMs) in performing multi-step reasoning tasks. The authors introduce a new framework called AR-MCTS, which combines Active Retrieval (AR) and Monte Carlo Tree Search (MCTS) to enhance the reasoning capabilities of MLLMs. By utilizing a unified retrieval module, the framework retrieves essential insights from a hybrid-modal corpus to assist in solving complex problems. The approach not only improves the diversity and reliability of reasoning but also incorporates a process reward model for automatic verification of multimodal reasoning tasks, demonstrating significant performance improvements in experiments."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„AR-MCTSæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAR-MCTSçš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸»åŠ¨æ£€ç´¢å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢æ¥é€æ­¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„æ£€ç´¢æ¨¡å—ï¼Œä»æ··åˆæ¨¡æ€æ£€ç´¢åº“ä¸­æå–è§£å†³å¤æ‚æ¨ç†é—®é¢˜çš„å…³é”®æ”¯æŒä¿¡æ¯ã€‚ä¸ºäº†å¼¥è¡¥è‡ªåŠ¨åŒ–å¤šæ¨¡æ€æ¨ç†éªŒè¯çš„ä¸è¶³ï¼Œæˆ‘ä»¬ç»“åˆäº†MCTSç®—æ³•å’Œä¸»åŠ¨æ£€ç´¢æœºåˆ¶ï¼Œå®ç°äº†é€æ­¥æ³¨é‡Šçš„è‡ªåŠ¨ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAR-MCTSæ¡†æ¶åœ¨æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œä¼˜åŒ–äº†é‡‡æ ·çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14475",
            "title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval",
            "url": "https://huggingface.co/papers/2412.14475",
            "abstract": "Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70times more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field.",
            "score": 13,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "9cc225c1e0ce01c5",
            "authors": [
                "Junjie Zhou",
                "Zheng Liu",
                "Ze Liu",
                "Shitao Xiao",
                "Yueze Wang",
                "Bo Zhao",
                "Chen Jason Zhang",
                "Defu Lian",
                "Yongping Xiong"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Shanghai Jiaotong University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14475.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "MegaPairs: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MegaPairs - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (CIR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MegaPairs: Unlocking Multimodal Retrieval with Synthetic Data",
                    "desc": "This paper presents MegaPairs, a new method for creating training data for multimodal retrieval tasks, which combine images and text. By using vision language models (VLMs) and a large collection of open-domain images, MegaPairs generates a synthetic dataset that significantly enhances the training process. The results show that models trained with MegaPairs outperform those trained on much larger existing datasets, achieving state-of-the-art performance in various benchmarks. The authors plan to make their dataset and models publicly available to support further advancements in multimodal retrieval research."
                },
                "zh": {
                    "title": "MegaPairsï¼šæå‡å¤šæ¨¡æ€æ£€ç´¢çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMegaPairsçš„æ–°æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ£€ç´¢é¢†åŸŸä¸­è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¼€æ”¾åŸŸå›¾åƒï¼Œç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMegaPairsç”Ÿæˆçš„æ•°æ®è´¨é‡é«˜ï¼Œä½¿å¾—å¤šæ¨¡æ€æ£€ç´¢å™¨çš„æ€§èƒ½æ˜¾è‘—è¶…è¿‡äº†åŸºäºç°æœ‰æ•°æ®é›†è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡2600ä¸‡çš„è®­ç»ƒå®ä¾‹ï¼Œå¹¶è®­ç»ƒäº†å¤šç§è§„æ¨¡çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15204",
            "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
            "url": "https://huggingface.co/papers/2412.15204",
            "abstract": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",
            "score": 6,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "6cf25f1f8b2e5710",
            "authors": [
                "Yushi Bai",
                "Shangqing Tu",
                "Jiajie Zhang",
                "Hao Peng",
                "Xiaozhi Wang",
                "Xin Lv",
                "Shulin Cao",
                "Jiazheng Xu",
                "Lei Hou",
                "Yuxiao Dong",
                "Jie Tang",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15204.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LongBench v2: Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "LongBench v2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 503 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ² ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 8 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¾ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ»Ğ¾Ğ². Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 100 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 50.1% Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1-preview Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 57.7%, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ½Ğ° 4%."
                },
                "en": {
                    "title": "LongBench v2: Elevating LLMs' Long-Context Reasoning Skills",
                    "desc": "This paper presents LongBench v2, a benchmark for evaluating large language models (LLMs) on long-context tasks that require advanced reasoning and understanding. It includes 503 multiple-choice questions with contexts ranging from 8,000 to 2 million words, covering various tasks like question answering and dialogue understanding. The benchmark was developed using data from nearly 100 educated individuals to ensure quality and difficulty, with human experts achieving only 53.7% accuracy under time constraints. The findings show that while the best LLMs perform around 50.1% accuracy, a model with enhanced reasoning capabilities can exceed human performance, emphasizing the need for improved reasoning in LLMs for long-context challenges."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›ï¼ŒæŒ‘æˆ˜é•¿ä¸Šä¸‹æ–‡é—®é¢˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†LongBench v2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†é•¿ä¸Šä¸‹æ–‡é—®é¢˜çš„èƒ½åŠ›ã€‚è¿™äº›é—®é¢˜éœ€è¦æ·±å…¥ç†è§£å’Œæ¨ç†ï¼Œæ¶µç›–äº†å¤šä¸ªç°å®ä¸–ç•Œçš„ä»»åŠ¡ã€‚LongBench v2åŒ…å«503ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ–‡æœ¬é•¿åº¦ä»8000åˆ°200ä¸‡å­—ä¸ç­‰ï¼Œæ¶‰åŠå…­ä¸ªä¸»è¦ä»»åŠ¡ç±»åˆ«ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨ç›´æ¥å›ç­”é—®é¢˜æ—¶çš„å‡†ç¡®ç‡ä»…ä¸º50.1%ï¼Œè€Œç»è¿‡æ›´é•¿æ¨ç†çš„o1-previewæ¨¡å‹åˆ™è¾¾åˆ°äº†57.7%ï¼Œè¶…è¶Šäº†äººç±»åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14642",
            "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
            "url": "https://huggingface.co/papers/2412.14642",
            "abstract": "In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are available through https://github.com/phenixace/TOMG-Bench.",
            "score": 2,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "d6b4853faa2e7839",
            "authors": [
                "Jiatong Li",
                "Junxian Li",
                "Yunqing Liu",
                "Dongzhan Zhou",
                "Qing Li"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14642.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ TOMG-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¿Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 25 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Benchmarking LLMs for Molecule Generation Excellence",
                    "desc": "This paper introduces TOMG-Bench, a benchmark designed to assess the ability of large language models (LLMs) in generating molecules. It includes three main tasks: molecule editing, optimization, and customized generation, each with multiple subtasks and a substantial dataset of test samples. An automated evaluation system is developed to measure the quality and accuracy of the generated molecules, highlighting the challenges in open-domain molecule generation. The study also shows that with the help of a specialized dataset for instruction tuning, Llama3.1-8B significantly outperforms other LLMs, indicating potential advancements in text-guided molecule discovery."
                },
                "zh": {
                    "title": "å¼€æ”¾åˆ†å­ç”Ÿæˆçš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†æ–‡æœ¬åŸºç¡€çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†ï¼ˆTOMG-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾é¢†åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†ã€‚TOMG-BenchåŒ…å«ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šåˆ†å­ç¼–è¾‘ï¼ˆMolEditï¼‰ã€åˆ†å­ä¼˜åŒ–ï¼ˆMolOptï¼‰å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆï¼ˆMolCustomï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡ä¸‹åˆæœ‰ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡åŒ…å«5000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚ä¸ºäº†åº”å¯¹å¼€æ”¾åˆ†å­ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿï¼Œä»¥æµ‹é‡ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºäº†25ä¸ªLLMçš„å½“å‰å±€é™æ€§å’Œæ½œåœ¨æ”¹è¿›é¢†åŸŸï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨OpenMolInsæ•°æ®é›†ï¼ŒLlama3.1-8Båœ¨TOMG-Benchä¸Šè¶…è¶Šäº†æ‰€æœ‰å¼€æºé€šç”¨LLMï¼Œç”šè‡³æ¯”GPT-3.5-turboé«˜å‡º46.5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15213",
            "title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
            "url": "https://huggingface.co/papers/2412.15213",
            "abstract": "Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.",
            "score": 2,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "7e00a5665592fb4d",
            "authors": [
                "Qihao Liu",
                "Xi Yin",
                "Alan Yuille",
                "Andrew Brown",
                "Mannat Singh"
            ],
            "affiliations": [
                "GenAI, Meta",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15213.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "CrossFlow: ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ CrossFlow. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, CrossFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³ÑƒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±ĞµÑĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ¹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. CrossFlow Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Cross-Modal Media Generation with CrossFlow",
                    "desc": "This paper introduces CrossFlow, a new framework for cross-modal flow matching that allows direct mapping between different media distributions without relying on Gaussian noise. Unlike traditional diffusion models, CrossFlow eliminates the need for a conditioning mechanism, simplifying the training process. The authors demonstrate that using Variational Encoders enhances the model's performance and enables Classifier-free guidance. Results show that CrossFlow not only outperforms standard flow matching in text-to-image tasks but also excels in various other cross-modal and intra-modal mapping tasks, indicating its broad applicability in media generation."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€æµåŒ¹é…çš„æ–°æ€è·¯",
                    "desc": "æ‰©æ•£æ¨¡å‹åŠå…¶æ¨å¹¿çš„æµåŒ¹é…åœ¨åª’ä½“ç”Ÿæˆé¢†åŸŸäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚ä¼ ç»Ÿæ–¹æ³•æ˜¯ä»ç®€å•çš„é«˜æ–¯å™ªå£°æºåˆ†å¸ƒå­¦ä¹ åˆ°ç›®æ ‡åª’ä½“åˆ†å¸ƒçš„å¤æ‚æ˜ å°„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œæ¢ç´¢å¦‚ä½•ç›´æ¥ä»ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒæ˜ å°„åˆ°å¦ä¸€ç§æ¨¡æ€çš„åˆ†å¸ƒï¼Œçœå»å™ªå£°åˆ†å¸ƒå’Œæ¡ä»¶æœºåˆ¶çš„éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†CrossFlowæ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15216",
            "title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
            "url": "https://huggingface.co/papers/2412.15216",
            "abstract": "We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.",
            "score": 1,
            "issue_id": 1227,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "ee62a21bee761d14",
            "authors": [
                "Enis Simsar",
                "Alessio Tonioni",
                "Yongqin Xian",
                "Thomas Hofmann",
                "Federico Tombari"
            ],
            "affiliations": [
                "ETH Zurich",
                "Google Switzerland",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15216.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Cycle Edit Consistency (CEC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ-Ğ¿Ñ€Ğ°Ğ²ĞºĞ°, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Revolutionizing Image Editing: Unsupervised Learning Without Ground Truth",
                    "desc": "This paper presents an unsupervised model for instruction-based image editing that does not require ground-truth edited images for training. Traditional supervised methods rely on datasets with input images, edited images, and edit instructions, which can introduce biases and limit how well the model can generalize. The authors introduce a new editing mechanism called Cycle Edit Consistency (CEC), which allows for simultaneous forward and backward edits, ensuring consistency in both image and attention spaces. Their approach demonstrates improved performance across various edits, highlighting the potential for scaling instruction-based image editing without the constraints of existing datasets."
                },
                "zh": {
                    "title": "æ— ç›‘ç£å›¾åƒç¼–è¾‘ï¼šæ‰“ç ´ä¼ ç»Ÿé™åˆ¶",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œè®­ç»ƒæ—¶ä¸éœ€è¦çœŸå®çš„ç¼–è¾‘å›¾åƒã€‚ç°æœ‰çš„ç›‘ç£æ–¹æ³•ä¾èµ–äºåŒ…å«è¾“å…¥å›¾åƒã€ç¼–è¾‘å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤çš„ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½å¼•å…¥åå·®å¹¶é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ç¼–è¾‘æœºåˆ¶â€”â€”å¾ªç¯ç¼–è¾‘ä¸€è‡´æ€§ï¼ˆCECï¼‰ï¼Œåœ¨ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­åº”ç”¨å‰å‘å’Œåå‘ç¼–è¾‘ï¼Œä»è€Œåœ¨å›¾åƒå’Œæ³¨æ„åŠ›ç©ºé—´ä¸­å¼ºåˆ¶ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œè¿™ç§æ— ç›‘ç£æŠ€æœ¯åœ¨æ›´å¹¿æ³›çš„ç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ä¿çœŸåº¦å’Œç²¾ç¡®åº¦ï¼Œæ ‡å¿—ç€åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘çš„é‡å¤§è¿›å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-19.html",
    "link_next": "2024-12-23.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬æ¯å¤©éƒ½ä¸ç”µè„‘äº’åŠ¨ï¼Œæ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»è¿˜æ˜¯å·¥ä½œã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ”¹è¿›ï¼Œèƒ½å¤Ÿä¸ç¯å¢ƒäº’åŠ¨å¹¶äº§ç”Ÿå½±å“çš„AIä»£ç†ä¹Ÿè¿…é€Ÿå‘å±•ã€‚ä½†æ˜¯ï¼ŒAIä»£ç†åœ¨åŠ é€Ÿæˆ–è‡ªä¸»æ‰§è¡Œå·¥ä½œä»»åŠ¡æ–¹é¢çš„è¡¨ç°å¦‚ä½•ï¼Ÿè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå¯¹å¸Œæœ›åœ¨å·¥ä½œæµç¨‹ä¸­é‡‡ç”¨AIçš„è¡Œä¸šä»¥åŠç†è§£AIé‡‡ç”¨å¯¹åŠ³åŠ¨åŠ›å¸‚åœºå½±å“çš„ç»æµæ”¿ç­–éƒ½æœ‰é‡è¦æ„ä¹‰ã€‚ä¸ºäº†è¡¡é‡è¿™äº›LLMä»£ç†åœ¨æ‰§è¡Œç°å®ä¸–ç•Œä¸“ä¸šä»»åŠ¡æ–¹é¢çš„è¡¨ç°ï¼Œæˆ‘ä»¬ä»‹ç»äº†TheAgentCompanyï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°ä¸æ•°å­—å·¥ä½œè€…äº’åŠ¨æ–¹å¼ç›¸ä¼¼çš„AIä»£ç†çš„å¯æ‰©å±•åŸºå‡†ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè‡ªåŒ…å«çš„ç¯å¢ƒï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªå°å‹è½¯ä»¶å…¬å¸ï¼Œå¹¶åˆ›å»ºäº†å„ç§ä»»åŠ¡ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œæœ€å…·ç«äº‰åŠ›çš„ä»£ç†å¯ä»¥è‡ªä¸»å®Œæˆ24%çš„ä»»åŠ¡ã€‚è¿™è¡¨æ˜ï¼Œåœ¨æ¨¡æ‹ŸçœŸå®å·¥ä½œç¯å¢ƒä¸­ï¼Œè¾ƒç®€å•çš„ä»»åŠ¡å¯ä»¥è¢«è‡ªåŠ¨è§£å†³ï¼Œä½†æ›´å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä»è¶…å‡ºå½“å‰ç³»ç»Ÿçš„èƒ½åŠ›ã€‚",
        "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
        "pinyin": "WÇ’men mÄ›itiÄn dÅu yÇ” diÃ nnÇo hÃ¹dÃ²ng, wÃºlÃ¹n shÃ¬ rÃ¬chÃ¡ng shÄ“nghuÃ³ hÃ¡ishÃ¬ gÅngzuÃ². SuÃ­zhe dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) de gÇijÃ¬n, nÃ©nggÃ²u yÇ” huÃ¡njÃ¬ng hÃ¹dÃ²ng bÃ¬ng chÇnshÄ“ng yÇngxiÇng de AI dÃ ilÇ yÄ› xÃ¹nsÃ¹ fÄzhÇn. DÃ nshÃ¬, AI dÃ ilÇ zÃ i jiÄsÃ¹ huÃ² zÃ¬zhÇ” zhÃ­xÃ­ng gÅngzuÃ² rÃ¨nwÃ¹ fÄngmiÃ n de biÇoxiÃ n rÃºhÃ©? ZhÃ¨ ge wÃ¨ntÃ­ de dÃ¡'Ã n duÃ¬ xÄ«wÃ ng zÃ i gÅngzuÃ² liÃºchÃ©ng zhÅng qÇ”yÃ²ng AI de hÃ¡ngyÃ¨ yÇjiÇ lÇjiÄ› AI qÇ”yÃ²ng duÃ¬ lÃ¡odÃ²nglÃ¬ shÃ¬chÇng yÇngxiÇng de jÄ«ngjÃ¬ zhÃ¨ngcÃ¨ dÅu yÇ’u zhÃ²ngyÃ o yÃ¬yÃ¬. WÃ¨ile hÃ©ngliÃ¡ng zhÃ¨xiÄ“ LLM dÃ ilÇ zÃ i zhÃ­xÃ­ng xiÃ nshÃ¬ shÃ¬jiÃ¨ zhuÄnyÃ¨ rÃ¨nwÃ¹ fÄngmiÃ n de biÇoxiÃ n, wÇ’men jiÃ¨shÃ o le TheAgentCompany, yÄ«gÃ¨ yÃ²ngyÃº pÃ­ngguÄ yÇ” shÃ¹zÃ¬ gÅngzuÃ²zhÄ› hÃ¹dÃ²ng fÄngshÃ¬ xiÄngsÃ¬ de AI dÃ ilÇ de kÄ› kuÃ²zhÇn jÄ«zhÇ”n. WÇ’men gÃ²uchÃ©ng le yÄ«gÃ¨ zÃ¬ bÄohÃ¡n de huÃ¡njÃ¬ng, mÃ³nÇ le yÄ«gÃ¨ xiÇo xÃ­ng ruÇnjiÃ n gÅngsÄ«, bÃ¬ng chuÃ ngjiÃ n le gÃ¨zhÇ’ng rÃ¨nwÃ¹. CÃ¨shÃ¬ jiÃ©guÇ’ xiÇnshÃ¬, zuÃ¬ jÃ¹ yÇngzhÃ n lÃ¬ de dÃ ilÇ kÄ›yÇ zÃ¬zhÇ” wÃ¡nchÃ©ng 24% de rÃ¨nwÃ¹. ZhÃ¨ biÇomÃ­ng, zÃ i mÃ³nÇ zhÄ“nshÃ­ gÅngzuÃ² huÃ¡njÃ¬ng zhÅng, jiÃ o qiÇnjiÃ n de rÃ¨nwÃ¹ kÄ›yÇ bÃ¨i zÃ¬dÃ²ng jiÄ›juÃ©, dÃ n gÃ¨ng fÃ¹zÃ¡ de chÃ¡ng shÃ­jiÄn rÃ¨nwÃ¹ rÃ©ng chÄochÅ« dÄngqiÃ¡n xÃ¬tÇ’ng de nÃ©nglÃ¬.",
        "vocab": "[\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹dÃ²ng\", \"trans\": \"interact\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language models\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ ilÇ\", \"trans\": \"agent\"},\n    {\"word\": \"è¿…é€Ÿ\", \"pinyin\": \"xÃ¹nsÃ¹\", \"trans\": \"rapidly\"},\n    {\"word\": \"è‡ªä¸»\", \"pinyin\": \"zÃ¬zhÇ”\", \"trans\": \"autonomously\"},\n    {\"word\": \"æ‰§è¡Œ\", \"pinyin\": \"zhÃ­xÃ­ng\", \"trans\": \"execute\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"è¡Œä¸š\", \"pinyin\": \"hÃ¡ngyÃ¨\", \"trans\": \"industry\"},\n    {\"word\": \"é‡‡ç”¨\", \"pinyin\": \"cÇiyÃ²ng\", \"trans\": \"adopt\"},\n    {\"word\": \"å·¥ä½œæµç¨‹\", \"pinyin\": \"gÅngzuÃ² liÃºchÃ©ng\", \"trans\": \"workflow\"},\n    {\"word\": \"åŠ³åŠ¨åŠ›\", \"pinyin\": \"lÃ¡odÃ²nglÃ¬\", \"trans\": \"labor force\"},\n    {\"word\": \"å¸‚åœº\", \"pinyin\": \"shÃ¬chÇng\", \"trans\": \"market\"},\n    {\"word\": \"å½±å“\", \"pinyin\": \"yÇngxiÇng\", \"trans\": \"impact\"},\n    {\"word\": \"ç»æµ\", \"pinyin\": \"jÄ«ngjÃ¬\", \"trans\": \"economic\"},\n    {\"word\": \"æ”¿ç­–\", \"pinyin\": \"zhÃ¨ngcÃ¨\", \"trans\": \"policy\"},\n    {\"word\": \"è¡¡é‡\", \"pinyin\": \"hÃ©ngliÃ¡ng\", \"trans\": \"measure\"},\n    {\"word\": \"ç°å®ä¸–ç•Œ\", \"pinyin\": \"xiÃ nshÃ­ shÃ¬jiÃ¨\", \"trans\": \"real world\"},\n    {\"word\": \"ä¸“ä¸š\", \"pinyin\": \"zhuÄnyÃ¨\", \"trans\": \"professional\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å¯æ‰©å±•\", \"pinyin\": \"kÄ› kuÃ²zhÇn\", \"trans\": \"scalable\"},\n    {\"word\": \"æ•°å­—å·¥ä½œè€…\", \"pinyin\": \"shÃ¹zÃ¬ gÅngzuÃ²zhÄ›\", \"trans\": \"digital worker\"},\n    {\"word\": \"è‡ªåŒ…å«\", \"pinyin\": \"zÃ¬ bÄohÃ¡n\", \"trans\": \"self-contained\"},\n    {\"word\": \"æ¨¡æ‹Ÿ\", \"pinyin\": \"mÃ³nÇ\", \"trans\": \"simulate\"},\n    {\"word\": \"è½¯ä»¶å…¬å¸\", \"pinyin\": \"ruÇnjiÃ n gÅngsÄ«\", \"trans\": \"software company\"},\n    {\"word\": \"ç«äº‰åŠ›\", \"pinyin\": \"jÃ¬ngzhÄ“nglÃ¬\", \"trans\": \"competitiveness\"},\n    {\"word\": \"å®Œæˆ\", \"pinyin\": \"wÃ¡nchÃ©ng\", \"trans\": \"complete\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇomÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ›juÃ©\", \"trans\": \"resolve\"},\n    {\"word\": \"è¶…å‡º\", \"pinyin\": \"chÄochÅ«\", \"trans\": \"exceed\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"capability\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬tÇ’ng\", \"trans\": \"system\"}\n]",
        "trans": "We interact with computers daily, whether it's in our daily lives or at work. As large language models (LLMs) improve, AI agents capable of interacting with the environment and generating impact are also rapidly developing. However, how well do these AI agents perform in accelerating or autonomously executing work tasks? The answer to this question is crucial for industries looking to adopt AI in their workflows, as well as for economic policies aiming to understand the impact of AI adoption on the labor market. To measure the performance of these LLM agents in executing real-world professional tasks, we introduce TheAgentCompany, a scalable benchmark for evaluating AI agents that interact in a manner similar to digital workers. We constructed a self-contained environment simulating a small software company and created a variety of tasks. Test results indicate that the most competitive agents can autonomously complete 24% of the tasks. This suggests that in a simulated real-world work environment, simpler tasks can be automated, but more complex, long-term tasks remain beyond the current capabilities of the systems.",
        "update_ts": "2024-12-19 09:11"
    }
}