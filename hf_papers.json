{
    "date": {
        "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 12",
        "zh": "12æœˆ12æ—¥"
    },
    "time_utc": "2024-12-12 05:11",
    "weekday": 3,
    "issue_id": 1082,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.07760",
            "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
            "url": "https://huggingface.co/papers/2412.07760",
            "abstract": "Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: https://jianhongbai.github.io/SynCamMaster/.",
            "score": 17,
            "issue_id": 1081,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "5ac69027d8ae0669",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xintao Wang",
                "Ziyang Yuan",
                "Xiao Fu",
                "Zuozhu Liu",
                "Haoji Hu",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "Kuaishou Technology",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07760.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#3d",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ°Ğ¼ Ğ¸Ğ· Unreal Engine. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ¿ÑƒÑĞº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° SynCamVideo-Dataset."
                },
                "en": {
                    "title": "Dynamic Consistency in Multi-View Video Generation",
                    "desc": "This paper explores the use of video diffusion models to create videos that maintain dynamic consistency from multiple viewpoints, which is important for applications like virtual filming. The authors propose a new module that enhances existing text-to-video models, allowing them to generate videos that are consistent in appearance and geometry across different camera angles. They introduce a multi-view synchronization module to ensure that the content remains coherent, even when viewed from various perspectives. Additionally, they present a hybrid training approach that combines different types of video data to improve the model's performance and release a new dataset for multi-view synchronized videos."
                },
                "zh": {
                    "title": "å®ç°å¤šè§†è§’è§†é¢‘çš„ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•æ˜¾ç¤ºå‡ºåœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒä¸‰ç»´ä¸€è‡´æ€§æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶è¿™äº›æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹ç¡®ä¿åŠ¨æ€ä¸€è‡´æ€§çš„æ½œåŠ›ï¼Œè¿™å¯¹äºè™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨éå¸¸é‡è¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ä»ä»»æ„è§†è§’ç”Ÿæˆå¼€æ”¾ä¸–ç•Œè§†é¢‘ï¼Œå¹¶å¼•å…¥å…­è‡ªç”±åº¦ç›¸æœºå§¿æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯æ’æ‹”æ¨¡å—ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼Œä»¥å®ç°å¤šç›¸æœºè§†é¢‘ç”Ÿæˆï¼Œå¹¶ç¡®ä¿ä¸åŒè§†è§’ä¸‹å†…å®¹çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08580",
            "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations",
            "url": "https://huggingface.co/papers/2412.08580",
            "abstract": "Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes. Based on LAION-SG, we train a new foundation model SDXL-SG to incorporate structural annotation information into the generation process. Extensive experiments show advanced models trained on our LAION-SG boast significant performance improvements in complex scene generation over models on existing datasets. We also introduce CompSG-Bench, a benchmark that evaluates models on compositional image generation, establishing a new standard for this domain.",
            "score": 7,
            "issue_id": 1081,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "07b05e5ae44a52c7",
            "authors": [
                "Zejian Li",
                "Chenye Meng",
                "Yize Li",
                "Ling Yang",
                "Shengyuan Zhang",
                "Jiarui Ma",
                "Jiayi Li",
                "Guang Yang",
                "Changyuan Yang",
                "Zhiyuan Yang",
                "Jinxiong Chang",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Alibaba Group",
                "Ant Group",
                "Jiangnan University",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08580.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LAION-SG Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑÑ†ĞµĞ½. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SDXL-SG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CompSG-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Structured Scene Graphs",
                    "desc": "This paper addresses the challenges faced by text-to-image (T2I) models in generating complex images with multiple objects and their relationships. The authors identify that existing datasets lack detailed annotations for inter-object relationships, which hampers model performance. To overcome this, they introduce LAION-SG, a new dataset that includes comprehensive scene graph annotations, enhancing the understanding of object attributes and relationships. They also present a new model, SDXL-SG, trained on this dataset, which shows significant improvements in generating intricate scenes, along with a new benchmark, CompSG-Bench, for evaluating compositional image generation."
                },
                "zh": {
                    "title": "æ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œæå‡å›¾åƒç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ€è¿‘åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„T2Iæ¨¡å‹åœ¨ç”ŸæˆåŒ…å«å¤šä¸ªå¯¹è±¡å’Œå¤æ‚å…³ç³»çš„å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªé—®é¢˜æºäºç°æœ‰å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†çš„å±€é™æ€§ï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹ç²¾ç¡®çš„å¯¹è±¡é—´å…³ç³»æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†LAION-SGï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é«˜è´¨é‡ç»“æ„æ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºå¤æ‚åœºæ™¯ä¸­çš„è¯­ä¹‰ç»“æ„ï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†æ–°çš„åŸºç¡€æ¨¡å‹SDXL-SGã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07797",
            "title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation",
            "url": "https://huggingface.co/papers/2412.07797",
            "abstract": "In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.",
            "score": 5,
            "issue_id": 1080,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "ff9bb8b603f9d972",
            "authors": [
                "Dongjie Fu"
            ],
            "affiliations": [
                "Mogo AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07797.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Mogo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mogo Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Mogo Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ RVQ-VAE Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mogo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸ BERT-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Mogo: Revolutionizing Text-to-Motion with High-Quality 3D Generation",
                    "desc": "This paper introduces Mogo, a new architecture for generating high-quality 3D human motions from text. Mogo combines a hierarchical residual vector quantization variational autoencoder (RVQ-VAE) with a hierarchical causal transformer to produce continuous and cyclic motion sequences efficiently. Unlike existing Bert-type models, Mogo maintains the streaming output capability of GPT-type models while improving performance in out-of-distribution scenarios. Experimental results show that Mogo not only generates longer motion sequences but also achieves superior quality metrics compared to both GPT-type and Bert-type models."
                },
                "zh": {
                    "title": "Mogoï¼šé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Däººç±»åŠ¨ä½œçš„åˆ›æ–°æ¶æ„",
                    "desc": "åœ¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆé¢†åŸŸï¼ŒBertç±»å‹çš„æ¨¡å‹ï¼ˆå¦‚MoMask, MMMï¼‰è™½ç„¶è¾“å‡ºè´¨é‡è¾ƒé«˜ï¼Œä½†ç¼ºä¹æµå¼è¾“å‡ºèƒ½åŠ›ï¼Œæ— æ³•æ»¡è¶³è§†é¢‘æ¸¸æˆå’Œå¤šåª’ä½“ç¯å¢ƒçš„éœ€æ±‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPTç±»å‹çš„è‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚T2M-GPTï¼‰å…·å¤‡è¿™ä¸€ç‰¹æ€§ï¼Œä½†åœ¨ç”Ÿæˆè´¨é‡ä¸Šç¨é€Šä¸€ç­¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¶æ„Mogoï¼ˆMotion Only Generate Onceï¼‰ï¼Œå®ƒé€šè¿‡è®­ç»ƒå•ä¸€çš„å˜æ¢å™¨æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„3Däººç±»åŠ¨ä½œã€‚Mogoç»“åˆäº†é«˜ç²¾åº¦çš„å±‚æ¬¡æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨å’Œå±‚æ¬¡å› æœå˜æ¢å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿ç»­ä¸”å¾ªç¯çš„åŠ¨ä½œåºåˆ—ï¼Œè¶…è¶Šäº†ç°æœ‰æ•°æ®é›†çš„é™åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07825",
            "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2412.07825",
            "abstract": "3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.",
            "score": 2,
            "issue_id": 1082,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "91db0c60d08d4efd",
            "authors": [
                "Wufei Ma",
                "Haoyu Chen",
                "Guofeng Zhang",
                "Celso M de Melo",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07825.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "3DSRBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ LMM",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğº 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 3DSRBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2772 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ 12 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ FlipEval Ğ´Ğ»Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LMM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… 3D Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹ÑĞ¾Ñ‚Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing 3D Spatial Reasoning in AI Models",
                    "desc": "This paper introduces 3DSRBench, a new benchmark designed to evaluate 3D spatial reasoning in large multi-modal models (LMMs). It includes 2,772 annotated visual question-answer pairs that cover various question types related to spatial relationships in 3D environments. The study highlights the limitations of current LMMs in understanding aspects like height, orientation, and location, especially when dealing with images taken from uncommon viewpoints. By providing a structured evaluation framework, this work aims to enhance the development of models with improved 3D reasoning capabilities."
                },
                "zh": {
                    "title": "æ¨åŠ¨3Dç©ºé—´æ¨ç†çš„æœªæ¥å‘å±•",
                    "desc": "3Dç©ºé—´æ¨ç†æ˜¯åˆ†æå’Œç†è§£ä¸‰ç»´ç©ºé—´ä¸­ç‰©ä½“ä½ç½®ã€æ–¹å‘å’Œç©ºé—´å…³ç³»çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªå…¨é¢çš„3Dç©ºé—´æ¨ç†åŸºå‡†ï¼Œ3DSRBenchï¼ŒåŒ…å«2772ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–12ç§é—®é¢˜ç±»å‹ã€‚æˆ‘ä»¬é€šè¿‡å¹³è¡¡æ•°æ®åˆ†å¸ƒå’Œé‡‡ç”¨æ–°é¢–çš„FlipEvalç­–ç•¥ï¼Œå¯¹3Dç©ºé—´æ¨ç†èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨3Dæ„è¯†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹å‘å±•æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-11.html",
    "link_next": "2024-12-13.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚",
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ i mÇ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (codeLLMs) zÃ i dÃ i mÇ shÄ“ng chÃ©ng fÄng miÃ n de jÃ¬n zhÃ n. yÇ qiÃ¡n de dÃ i mÇ xiÄng guÄn jÄ« zhÇ”n cÃ¨ shÃ¬ zhÇ” yÃ o guÄn zhÃ¹ shÄ“ng chÃ©ng zhÃ¨ng quÃ¨ de dÃ i mÇ piÃ n duÃ n, dÃ n hÅ« lÃ¼Ã¨ le yÇ” rÃ©n lÃ¨i piÄn hÃ o de yÄ« zhÃ¬ xÃ¬ng. wÃ¨i le mÃ­ bÇ” zhÃ¨ yÄ« chÄ jÃ¹, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ mÃ­ng wÃ¨i CodeArena de yÃ¡n gÃ© rÃ©n gÅng biÄn zhÃ¬ jÄ« zhÇ”n cÃ¨ shÃ¬, mÃ³ nÇ zhÄ“n shÃ­ shÃ¬ jiÃ¨ biÄn mÇ rÃ¨n wÃ¹ de fÃº zÃ  xÃ¬ng hÃ© duÅ yÃ ng xÃ¬ng. tÅng guÃ² xÃ¬ tÇ’ng shÃ­ yÃ n, zuÃ² zhÄ› fÄ xiÃ n zÃ i kÄi yuÃ¡n dÃ i mÇ LLMs hÃ© zhuÄn yÇ’u LLMs zhÄ« jiÄn cÃºn zÃ i xiÇn zhÃ¹ de xÃ¬ng nÃ©ng chÄ jÃ¹, qiÃ¡ng diÃ o le rÃ©n lÃ¨i piÄn hÃ o yÄ« zhÃ¬ xÃ¬ng de zhÃ²ng yÃ o xÃ¬ng.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'ä»¥å‰', 'pinyin': 'yÇ qiÃ¡n', 'trans': 'before'},\n{'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'},\n{'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ« zhÇ”n cÃ¨ shÃ¬', 'trans': 'benchmark test'},\n{'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'main'},\n{'word': 'å…³æ³¨', 'pinyin': 'guÄn zhÃ¹', 'trans': 'focus on'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'æ­£ç¡®', 'pinyin': 'zhÃ¨ng quÃ¨', 'trans': 'correct'},\n{'word': 'ç‰‡æ®µ', 'pinyin': 'piÃ n duÃ n', 'trans': 'segment'},\n{'word': 'å¿½ç•¥', 'pinyin': 'hÅ« lÃ¼Ã¨', 'trans': 'ignore'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},\n{'word': 'å¼¥è¡¥', 'pinyin': 'mÃ­ bÇ”', 'trans': 'make up for'},\n{'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'ä¸¥æ ¼', 'pinyin': 'yÃ¡n gÃ©', 'trans': 'strict'},\n{'word': 'äººå·¥ç¼–åˆ¶', 'pinyin': 'rÃ©n gÅng biÄn zhÃ¬', 'trans': 'artificially compiled'},\n{'word': 'æ¨¡æ‹Ÿ', 'pinyin': 'mÃ³ nÇ', 'trans': 'simulate'},\n{'word': 'çœŸå®ä¸–ç•Œ', 'pinyin': 'zhÄ“n shÃ­ shÃ¬ jiÃ¨', 'trans': 'real world'},\n{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},\n{'word': 'å¤šæ ·æ€§', 'pinyin': 'duÅ yÃ ng xÃ¬ng', 'trans': 'diversity'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open source'},\n{'word': 'ä¸“æœ‰', 'pinyin': 'zhuÄn yÇ’u', 'trans': 'proprietary'},\n{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'å¼ºè°ƒ', 'pinyin': 'qiÃ¡ng diÃ o', 'trans': 'emphasize'},\n{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}]",
        "trans": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.",
        "update_ts": "2024-12-11 09:11"
    }
}