{
    "date": {
        "ru": "1 мая",
        "en": "May 1",
        "zh": "5月1日"
    },
    "time_utc": "2025-05-01 05:12",
    "weekday": 3,
    "issue_id": 3528,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.21776",
            "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
            "url": "https://huggingface.co/papers/2504.21776",
            "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
            "score": 14,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "61ce82abe42f584a",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Guanting Dong",
                "Hongjin Qian",
                "Yutao Zhu",
                "Yongkang Wu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "BAAI",
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21776.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#science",
                    "#agents",
                    "#rlhf",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "WebThinker: Расширение возможностей ИИ для глубоких веб-исследований",
                    "desc": "WebThinker - это глубокая исследовательская система, которая расширяет возможности больших языковых моделей (LLM) для автономного поиска в интернете и составления исследовательских отчетов. Система включает модуль Deep Web Explorer для динамического поиска и извлечения информации из веб-страниц, а также стратегию автономного мышления, поиска и составления черновиков. WebThinker использует обучение с подкреплением на основе прямой оптимизации предпочтений (DPO) для улучшения использования исследовательских инструментов. Эксперименты показывают, что WebThinker значительно превосходит существующие методы на сложных задачах рассуждения и генерации научных отчетов."
                },
                "en": {
                    "title": "Empowering LRMs with Real-Time Web Research Capabilities",
                    "desc": "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."
                },
                "zh": {
                    "title": "WebThinker：让推理模型更智能的研究助手",
                    "desc": "大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在长时间推理方面表现出色，但它们依赖静态内部知识，限制了在复杂知识密集型任务中的表现。为了解决这个问题，我们提出了WebThinker，一个深度研究代理，能够让LRMs自主搜索网络、浏览网页并在推理过程中撰写研究报告。WebThinker集成了深网探索模块，使LRMs在遇到知识空白时能够动态搜索和提取信息。通过引入基于强化学习的训练策略，我们的实验表明WebThinker在复杂推理基准和科学报告生成任务中显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21233",
            "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
            "url": "https://huggingface.co/papers/2504.21233",
            "abstract": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.",
            "score": 10,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "0b800a9195884db4",
            "authors": [
                "Haoran Xu",
                "Baolin Peng",
                "Hany Awadalla",
                "Dongdong Chen",
                "Yen-Chun Chen",
                "Mei Gao",
                "Young Jin Kim",
                "Yunsheng Li",
                "Liliang Ren",
                "Yelong Shen",
                "Shuohang Wang",
                "Weijian Xu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21233.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие потенциала малых языковых моделей в задачах рассуждения",
                    "desc": "Статья представляет систематический подход к улучшению способностей рассуждения в малых языковых моделях (SLM). Авторы предлагают четырехэтапный рецепт обучения, включающий дистилляцию данных, дообучение на высококачественных данных с цепочками рассуждений, оптимизацию прямого предпочтения и обучение с подкреплением. Применение этого метода к модели Phi-4-Mini (3.8 млрд параметров) позволило превзойти более крупные модели в задачах математических рассуждений. Результаты подтверждают эффективность предложенного подхода для развития сильных способностей рассуждения даже в ресурсно-ограниченных малых моделях."
                },
                "en": {
                    "title": "Unlocking Reasoning Power in Small Models",
                    "desc": "This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning."
                },
                "zh": {
                    "title": "小模型也能强推理！",
                    "desc": "本论文探讨了如何通过链式思维（CoT）来提升小型语言模型（SLM）的推理能力。尽管大型语言模型（LLM）在生成中间推理步骤方面表现良好，但小型模型由于容量限制，提升推理能力仍然具有挑战性。研究表明，通过从LLM生成的合成数据进行蒸馏，可以显著改善SLM的推理能力。我们提出了一种系统的训练方案，包括四个步骤，最终在Phi-4-Mini模型上实现了超越更大模型的推理表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20966",
            "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
            "url": "https://huggingface.co/papers/2504.20966",
            "abstract": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.",
            "score": 7,
            "issue_id": 3526,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 апреля",
                "en": "April 29",
                "zh": "4月29日"
            },
            "hash": "cb610c1427bdf307",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Softpick: улучшенная активация для эффективных трансформеров",
                    "desc": "Статья представляет softpick - новый метод активации для механизмов внимания в трансформерах, заменяющий softmax. Softpick устраняет проблему 'attention sink' и чрезмерных активаций, сохраняя производительность на уровне softmax. Эксперименты показывают, что softpick создает более разреженные карты внимания и состояния с меньшим эксцессом. Модели с softpick превосходят softmax при квантовании, особенно при низкой битовой точности."
                },
                "en": {
                    "title": "Softpick: A Smarter Alternative to Softmax for Transformers",
                    "desc": "This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability."
                },
                "zh": {
                    "title": "softpick：提升Transformer注意力的新选择",
                    "desc": "本文介绍了一种名为softpick的新方法，它可以替代transformer注意力机制中的softmax。softpick不需要将权重归一化为1，能够消除注意力沉没和大规模激活。实验表明，使用softpick的模型在标准基准测试中与softmax表现相当，但注意力沉没率为0%，并且生成的隐藏状态具有更低的峰度。softpick在量化和低精度训练中表现优越，尤其在较低位数精度下具有明显优势，展示了其在稀疏性优化和可解释性方面的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21318",
            "title": "Phi-4-reasoning Technical Report",
            "url": "https://huggingface.co/papers/2504.21318",
            "abstract": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",
            "score": 5,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "7004ae060f674e9a",
            "authors": [
                "Marah Abdin",
                "Sahaj Agarwal",
                "Ahmed Awadallah",
                "Vidhisha Balachandran",
                "Harkirat Behl",
                "Lingjiao Chen",
                "Gustavo de Rosa",
                "Suriya Gunasekar",
                "Mojan Javaheripi",
                "Neel Joshi",
                "Piero Kauffmann",
                "Yash Lara",
                "Caio César Teodoro Mendes",
                "Arindam Mitra",
                "Besmira Nushi",
                "Dimitris Papailiopoulos",
                "Olli Saarikivi",
                "Shital Shah",
                "Vaishnavi Shrivastava",
                "Vibhav Vineet",
                "Yue Wu",
                "Safoora Yousefi",
                "Guoqing Zheng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21318.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#dataset",
                    "#math",
                    "#transfer_learning",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Мощная модель рассуждений на основе тщательно отобранных данных",
                    "desc": "Исследователи представили Phi-4-reasoning - модель с 14 миллиардами параметров, обученную для сложных задач рассуждения. Модель была обучена с помощью контролируемой тонкой настройки на тщательно отобранных обучающих примерах и демонстрациях рассуждений. Phi-4-reasoning генерирует подробные цепочки рассуждений, эффективно используя вычислительные ресурсы во время вывода. Модель превосходит по производительности значительно более крупные модели с открытыми весами на различных задачах рассуждения, включая математику, научное мышление, программирование и пространственное понимание."
                },
                "en": {
                    "title": "Unlocking Complex Reasoning with Phi-4-Reasoning",
                    "desc": "The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models."
                },
                "zh": {
                    "title": "推理模型的新突破：Phi-4-reasoning",
                    "desc": "本文介绍了Phi-4-reasoning，这是一个拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过对精心挑选的“可教”提示进行监督微调训练，生成详细的推理链，有效利用推理时的计算能力。我们还开发了Phi-4-reasoning-plus，通过基于结果的强化学习进一步增强，能够生成更长的推理轨迹，从而提高性能。综合评估显示，这两个模型在数学、科学推理、编码等多个任务上均优于更大的开放权重模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19720",
            "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
            "url": "https://huggingface.co/papers/2504.19720",
            "abstract": "Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",
            "score": 5,
            "issue_id": 3526,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "e74f8b7af65e09fd",
            "authors": [
                "Ranran Zhen",
                "Juntao Li",
                "Yixin Ji",
                "Zhenlin Yang",
                "Tong Liu",
                "Qingrong Xia",
                "Xinyu Duan",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Min Zhang"
            ],
            "affiliations": [
                "Huawei Cloud",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19720.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение инференса больших языковых моделей: от экземпляра до кластера",
                    "desc": "Статья представляет собой обзор методов оптимизации инференса больших языковых моделей (LLM). Авторы рассматривают подходы на уровне отдельных экземпляров, включая размещение модели и управление запросами. Также описываются стратегии на уровне кластеров, такие как балансировка нагрузки и облачные решения. Особое внимание уделяется новым сценариям применения LLM и вспомогательным методам оптимизации."
                },
                "en": {
                    "title": "Optimizing LLMs: Balancing Performance and Efficiency in Generative AI",
                    "desc": "This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services."
                },
                "zh": {
                    "title": "推动大型语言模型推理服务的研究进展",
                    "desc": "大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其庞大的参数量和注意力机制的高计算需求导致了内存开销大，影响了推理服务的低延迟和高吞吐量。本文全面调查了应对这些挑战的方法，包括实例级和集群级的策略，以及新兴场景的方向。我们讨论了模型部署、请求调度、解码长度预测等实例级方法，以及GPU集群部署和多实例负载均衡等集群级策略。最后，我们提出了未来研究的潜在方向，以推动LLM推理服务的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18904",
            "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
            "url": "https://huggingface.co/papers/2504.18904",
            "abstract": "Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.",
            "score": 5,
            "issue_id": 3525,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 апреля",
                "en": "April 26",
                "zh": "4月26日"
            },
            "hash": "c724dcb5ceb5df7b",
            "authors": [
                "Haoran Geng",
                "Feishi Wang",
                "Songlin Wei",
                "Yuyang Li",
                "Bangjun Wang",
                "Boshi An",
                "Charlie Tianyue Cheng",
                "Haozhe Lou",
                "Peihao Li",
                "Yen-Jen Wang",
                "Yutong Liang",
                "Dylan Goetting",
                "Chaoyi Xu",
                "Haozhe Chen",
                "Yuxi Qian",
                "Yiran Geng",
                "Jiageng Mao",
                "Weikang Wan",
                "Mingtong Zhang",
                "Jiangran Lyu",
                "Siheng Zhao",
                "Jiazhao Zhang",
                "Jialiang Zhang",
                "Chengyang Zhao",
                "Haoran Lu",
                "Yufei Ding",
                "Ran Gong",
                "Yuran Wang",
                "Yuxuan Kuang",
                "Ruihai Wu",
                "Baoxiong Jia",
                "Carlo Sferrazza",
                "Hao Dong",
                "Siyuan Huang",
                "Yue Wang",
                "Jitendra Malik",
                "Pieter Abbeel"
            ],
            "affiliations": [
                "BIGAI",
                "CMU",
                "PKU",
                "Stanford",
                "UC Berkeley",
                "UCLA",
                "UIUC",
                "UMich",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18904.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RoboVerse: универсальная платформа для масштабирования и стандартизации обучения роботов",
                    "desc": "RoboVerse - это комплексная платформа для робототехники, включающая симуляционную среду, синтетический набор данных и унифицированные бенчмарки. Платформа поддерживает множество симуляторов и роботизированных воплощений, обеспечивая плавный переход между различными средами. Синтетический набор данных отличается высокой точностью физики и фотореалистичным рендерингом. RoboVerse предлагает унифицированные бенчмарки для имитационного обучения и обучения с подкреплением, позволяющие оценивать различные уровни обобщения."
                },
                "en": {
                    "title": "RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks",
                    "desc": "This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field."
                },
                "zh": {
                    "title": "RoboVerse：推动机器人学习的强大框架",
                    "desc": "本论文介绍了RoboVerse，这是一个综合框架，旨在解决机器人领域数据收集和评估的挑战。RoboVerse包括一个模拟平台、一个合成数据集和统一的基准测试，支持多种模拟器和机器人形态。通过高保真物理和逼真的渲染，合成数据集提供了高质量和多样性的数据。实验结果表明，RoboVerse显著提升了模仿学习、强化学习和从模拟到现实的转移性能，验证了其数据集和基准的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21850",
            "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
            "url": "https://huggingface.co/papers/2504.21850",
            "abstract": "Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.",
            "score": 4,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "3db97f245360deb4",
            "authors": [
                "Xindi Wu",
                "Hee Seung Hwang",
                "Polina Kirichenko",
                "Olga Russakovsky"
            ],
            "affiliations": [
                "Meta AI",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21850.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "COMPACT: эффективное обучение MLLM сложным визуально-языковым задачам",
                    "desc": "Статья представляет новый метод обучения мультимодальных больших языковых моделей (MLLM) под названием COMPACT. Этот подход генерирует набор данных для обучения, контролируя композиционную сложность примеров. COMPACT позволяет MLLM эффективнее обучаться сложным задачам, комбинируя атомарные возможности. Метод достигает сопоставимых результатов с LLaVA-665k, используя менее 10% данных, и превосходит его на сложных мультизадачных тестах."
                },
                "en": {
                    "title": "Unlocking Complex Tasks with Efficient Compositional Training",
                    "desc": "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."
                },
                "zh": {
                    "title": "提升复杂视觉语言任务的能力",
                    "desc": "多模态大型语言模型（MLLMs）在简单的视觉语言任务中表现出色，但在需要多种能力的复杂任务中却面临挑战。传统的视觉指令调优（VIT）主要关注数据量的扩大，而忽视了训练示例的组合复杂性。我们提出了COMPACT（组合原子到复杂视觉能力调优），它生成一个明确控制训练示例组合复杂性的训练数据集。COMPACT使得MLLMs能够更高效地学习复杂能力，并在多个基准测试中表现出色，尤其是在涉及复杂多能力任务时。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21855",
            "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
            "url": "https://huggingface.co/papers/2504.21855",
            "abstract": "In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.",
            "score": 2,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "5d8989ce0c77aa23",
            "authors": [
                "Qihao Liu",
                "Ju He",
                "Qihang Yu",
                "Liang-Chieh Chen",
                "Alan Yuille"
            ],
            "affiliations": [
                "Independent Researcher",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21855.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games",
                    "#video",
                    "#small_models"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "ReVision: физика в помощь ИИ для создания реалистичных видео",
                    "desc": "ReVision - это новый фреймворк для улучшения генерации видео с использованием трехмерных физических знаний. Он состоит из трех этапов: генерация грубого видео, извлечение 2D и 3D признаков для создания объектно-ориентированного представления, и уточнение движения с помощью параметризованной физической модели. ReVision значительно повышает качество генерируемых видео с точки зрения сложных движений и взаимодействий, превосходя даже более крупные модели."
                },
                "en": {
                    "title": "ReVision: Enhancing Video Generation with 3D Physical Knowledge",
                    "desc": "The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation."
                },
                "zh": {
                    "title": "通过3D物理知识提升视频生成的真实感与可控性",
                    "desc": "近年来，视频生成技术取得了显著进展，但在生成复杂动作和交互方面仍面临挑战。为了解决这些问题，我们提出了ReVision，这是一个可插拔的框架，能够将参数化的三维物理知识集成到预训练的条件视频生成模型中，从而显著提升生成高质量视频的能力。ReVision包括三个阶段：首先使用视频扩散模型生成粗略视频，然后提取2D和3D特征构建三维物体中心表示，最后通过参数化物理先验模型精炼运动序列，反馈到视频扩散模型中以生成一致的运动视频。我们的实验表明，ReVision在复杂视频生成上表现优异，甚至以较少的参数超越了大型模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21039",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
            "url": "https://huggingface.co/papers/2504.21039",
            "abstract": "As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",
            "score": 2,
            "issue_id": 3528,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "a66fbdd0c4cc7250",
            "authors": [
                "Paul Kassianik",
                "Baturay Saglam",
                "Alexander Chen",
                "Blaine Nelson",
                "Anu Vellore",
                "Massimo Aufiero",
                "Fraser Burch",
                "Dhruv Kedia",
                "Avi Zohary",
                "Sajana Weerawardhena",
                "Aman Priyanshu",
                "Adam Swanda",
                "Amy Chang",
                "Hyrum Anderson",
                "Kojin Oshiba",
                "Omar Santos",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "affiliations": [
                "Foundation AI Cisco Systems Inc.",
                "Security & Trust Organization Cisco Systems Inc.",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21039.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#security"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Специализированная языковая модель открывает новые горизонты в кибербезопасности",
                    "desc": "Статья представляет Foundation-Sec-8B - языковую модель, специализированную на кибербезопасности. Модель основана на архитектуре Llama 3.1 и дообучена на специально подобранном корпусе текстов по кибербезопасности. Foundation-Sec-8B показывает результаты, сравнимые с Llama 3.1-70B и GPT-4o-mini в некоторых задачах кибербезопасности. Авторы публикуют модель, чтобы ускорить развитие и внедрение ИИ-инструментов в сфере кибербезопасности."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Foundation-Sec-8B",
                    "desc": "This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors."
                },
                "zh": {
                    "title": "推动网络安全的AI工具进步",
                    "desc": "随着基于变换器的大型语言模型（LLMs）在社会中的广泛应用，它们在软件工程、创意写作和数字艺术等领域带来了革命性的变化。然而，由于缺乏专业的训练数据和表示网络安全特定知识的复杂性，它们在网络安全领域的应用仍然有限。为了解决这些问题，我们提出了Foundation-Sec-8B，这是一个专注于网络安全的LLM，基于Llama 3.1架构，并通过在精心策划的网络安全语料库上进行持续预训练来增强。我们在多个网络安全基准测试中评估了Foundation-Sec-8B，结果显示它在某些网络安全特定任务上与Llama 3.1-70B和GPT-4o-mini相匹配。"
                }
            }
        }
    ],
    "link_prev": "2025-04-30.html",
    "link_next": "2025-05-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4月30日"
    },
    "short_date_next": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5月2日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的检索增强生成（RAG）框架，称为UniversalRAG。它能从不同模态和粒度的知识源中检索和整合信息。现有的RAG方法主要局限于单一模态的语料库，而UniversalRAG通过模态感知路由机制动态选择最合适的模态特定语料库进行检索。此外，它还根据查询的复杂性和范围，对每种模态进行多级粒度的组织。研究在8个多模态基准上验证了UniversalRAG的优越性。",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "pinyin": "这篇文章介绍了一种新的检索增强生成（RAG）框架，称为UniversalRAG。它能从不同模态和粒度的知识源中检索和整合信息。现有的RAG方法主要局限于单一模态的语料库，而UniversalRAG通过模态感知路由机制动态选择最合适的模态特定语料库进行检索。此外，它还根据查询的复杂性和范围，对每种模态进行多级粒度的组织。研究在8个多模态基准上验证了UniversalRAG的优越性。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de jiǎn suǒ zēng qiáng shēng chéng (RAG) kuàng jià, chēng wéi UniversalRAG. tā néng cóng bù tóng mó tài hé lì dù de zhī shi yuán zhōng jiǎn suǒ hé zhěng hé xìn xī. xiàn yǒu de RAG fāng fǎ zhǔ yào jú xiàn yī dàn yī mó tài de yǔ liào kù, ér UniversalRAG tōng guò mó tài gǎn jué lù yóu jī zhì dòng tài xuǎn zé zuì hé shì de mó tài tè dìng yǔ liào kù jìn xíng jiǎn suǒ. cǐ wài, tā hái gēn jù chá xún de fú zà xìng hé fàn wéi, duì měi zhǒng mó tài jìn xíng duō jí lì dù de zǔ zhī. yán jiū zài 8 gè duō mó tài bǐ zhǔn shàng yàn zhèng le UniversalRAG de yōu yuè xìng.",
        "vocab": "[\n    {\"word\": \"检索\", \"pinyin\": \"jiǎnsuǒ\", \"trans\": \"retrieval\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēngqiáng\", \"trans\": \"enhancement\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generation\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"粒度\", \"pinyin\": \"lì dù\", \"trans\": \"granularity\"},\n    {\"word\": \"知识源\", \"pinyin\": \"zhīshi yuán\", \"trans\": \"knowledge source\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integration\"},\n    {\"word\": \"局限于\", \"pinyin\": \"jú xiàn yú\", \"trans\": \"limited to\"},\n    {\"word\": \"单一\", \"pinyin\": \"dān yī\", \"trans\": \"single\"},\n    {\"word\": \"语料库\", \"pinyin\": \"yǔ liào kù\", \"trans\": \"corpus\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎnzhī\", \"trans\": \"perception\"},\n    {\"word\": \"路由\", \"pinyin\": \"lù yóu\", \"trans\": \"routing\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamic\"},\n    {\"word\": \"选择\", \"pinyin\": \"xuǎnzé\", \"trans\": \"selection\"},\n    {\"word\": \"合适\", \"pinyin\": \"héshì\", \"trans\": \"appropriate\"},\n    {\"word\": \"特定\", \"pinyin\": \"tèdìng\", \"trans\": \"specific\"},\n    {\"word\": \"查询\", \"pinyin\": \"cháxún\", \"trans\": \"query\"},\n    {\"word\": \"复杂性\", \"pinyin\": \"fùzáxìng\", \"trans\": \"complexity\"},\n    {\"word\": \"范围\", \"pinyin\": \"fànwéi\", \"trans\": \"scope\"},\n    {\"word\": \"多级\", \"pinyin\": \"duōjí\", \"trans\": \"multi-level\"},\n    {\"word\": \"组织\", \"pinyin\": \"zǔzhī\", \"trans\": \"organization\"},\n    {\"word\": \"研究\", \"pinyin\": \"yánjiū\", \"trans\": \"research\"},\n    {\"word\": \"验证\", \"pinyin\": \"yànzhèng\", \"trans\": \"validation\"},\n    {\"word\": \"优越性\", \"pinyin\": \"yōuyuèxìng\", \"trans\": \"superiority\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"}\n]",
        "trans": "This article introduces a new Retrieval-Augmented Generation (RAG) framework called UniversalRAG. It can retrieve and integrate information from knowledge sources of different modalities and granularities. Existing RAG methods are mainly limited to single-modality corpora, while UniversalRAG dynamically selects the most suitable modality-specific corpus for retrieval through a modality-aware routing mechanism. Additionally, it organizes each modality at multiple granularity levels based on the complexity and scope of the query. The superiority of UniversalRAG has been validated on 8 multimodal benchmarks.",
        "update_ts": "2025-04-30 09:12"
    }
}