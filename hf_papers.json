{
    "date": {
        "ru": "3 Ğ¸ÑĞ»Ñ",
        "en": "July 3",
        "zh": "7æœˆ3æ—¥"
    },
    "time_utc": "2025-07-03 18:16",
    "weekday": 3,
    "issue_id": 4631,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01949",
            "title": "Kwai Keye-VL Technical Report",
            "url": "https://huggingface.co/papers/2507.01949",
            "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.",
            "score": 92,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "ca23195c7fa1bb87",
            "authors": [
                "Kwai Keye Team",
                "Biao Yang",
                "Bin Wen",
                "Changyi Liu",
                "Chenglong Chu",
                "Chengru Song",
                "Chongling Rao",
                "Chuan Yi",
                "Da Li",
                "Dunju Zang",
                "Fan Yang",
                "Guorui Zhou",
                "Hao Peng",
                "Haojie Ding",
                "Jiaming Huang",
                "Jiangxia Cao",
                "Jiankang Chen",
                "Jingyun Hua",
                "Jin Ouyang",
                "Kaibing Chen",
                "Kaiyu Jiang",
                "Kaiyu Tang",
                "Kun Gai",
                "Shengnan Zhang",
                "Siyang Mao",
                "Sui Huang",
                "Tianke Zhang",
                "Tingting Gao",
                "Wei Chen",
                "Wei Yuan",
                "Xiangyu Wu",
                "Xiao Hu",
                "Xingyu Lu",
                "Yang Zhou",
                "Yi-Fan Zhang",
                "Yiping Yang",
                "Yulong Chen",
                "Zhenhua Wu",
                "Zhenyu Li",
                "Zhixin Ling",
                "Ziming Li",
                "Dehua Ma",
                "Di Xu",
                "Haixuan Gao",
                "Hang Li",
                "Jiawei Guo",
                "Jing Wang",
                "Lejian Ren",
                "Muhao Wei",
                "Qianqian Wang",
                "Qigen Hu",
                "Shiyao Wang",
                "Tao Yu",
                "Xinchen Luo",
                "Yan Li",
                "Yiming Liang",
                "Yuhang Hu",
                "Zeyi Lu",
                "Zhuoran Yang",
                "Zixing Zhang"
            ],
            "affiliations": [
                "Kuaishou Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01949.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Kwai Keye-VL: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Kwai Keye-VL - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 600 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Keye-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Short-Video Understanding with Keye-VL",
                    "desc": "This paper presents Kwai Keye-VL, a multimodal large language model designed to improve understanding of short-form videos, which are increasingly popular. Keye-VL is built on a vast dataset of over 600 billion tokens, focusing on video content, and employs a unique training strategy that includes a four-stage pre-training and a two-phase post-training process. The model enhances its reasoning abilities through a novel data mixture that encourages different modes of thinking, followed by reinforcement learning to refine its outputs. Evaluations demonstrate that Keye-VL outperforms existing models on video benchmarks while maintaining strong performance on general image tasks."
                },
                "zh": {
                    "title": "çŸ­è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šKwai Keye-VL",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKwai Keye-VLçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºçŸ­è§†é¢‘ç†è§£ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰80äº¿ä¸ªå‚æ•°ï¼Œæ—¨åœ¨æå‡å¯¹åŠ¨æ€ã€ä¿¡æ¯å¯†é›†å‹çŸ­è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨è§†è§‰-è¯­è¨€èƒ½åŠ›ã€‚Keye-VLçš„å¼€å‘åŸºäºä¸¤ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šä¸€ä¸ªè¶…è¿‡6000äº¿ä¸ªæ ‡è®°çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œç‰¹åˆ«å¼ºè°ƒè§†é¢‘å†…å®¹ï¼Œä»¥åŠä¸€ç§åˆ›æ–°çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬å››é˜¶æ®µçš„é¢„è®­ç»ƒå’Œä¸¤é˜¶æ®µçš„åè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¯¹é½æ­¥éª¤ï¼ŒKeye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨ä¸€èˆ¬å›¾åƒä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01945",
            "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
            "url": "https://huggingface.co/papers/2507.01945",
            "abstract": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.",
            "score": 60,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "cf167e3958c2df99",
            "authors": [
                "Nan Chen",
                "Mengqi Huang",
                "Yihao Meng",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01945.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "LongAnimation: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LongAnimation. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğµ. LongAnimation Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ SketchDiT Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (DGLM) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… (14 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²), Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… (Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 500 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²) Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Achieving Long-Term Color Consistency in Animation Colorization",
                    "desc": "This paper presents LongAnimation, a framework designed to automate the colorization of long animations, addressing the high labor costs associated with traditional methods. It critiques existing approaches that focus on short-term colorization and local feature fusion, which often overlook the importance of global color consistency. The proposed method utilizes a Dynamic Global-Local Memory (DGLM) to dynamically integrate global features with current generation data, ensuring a cohesive color palette throughout the animation. Additionally, a Color Consistency Reward is introduced to enhance the smoothness of transitions between video segments, demonstrating effectiveness in both short-term and long-term animation colorization tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€å…¨å±€-å±€éƒ¨èŒƒå¼å®ç°åŠ¨ç”»è‰²å½©ä¸€è‡´æ€§",
                    "desc": "åŠ¨ç”»ä¸Šè‰²æ˜¯åŠ¨ç”»äº§ä¸šç”Ÿäº§ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œé•¿æ—¶é—´åŠ¨ç”»çš„ä¸Šè‰²æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼ŒåŸºäºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è‡ªåŠ¨åŒ–é•¿æ—¶é—´åŠ¨ç”»ä¸Šè‰²å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨çŸ­æœŸä¸Šè‰²ï¼Œé‡‡ç”¨å±€éƒ¨èŒƒå¼æ¥å®ç°å±€éƒ¨ç‰‡æ®µä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œä½†å¿½è§†äº†å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´é•¿æœŸè‰²å½©ä¸€è‡´æ€§ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€å…¨å±€-å±€éƒ¨èŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€æå–ä¸å½“å‰ç”Ÿæˆç›¸å…³çš„å…¨å±€è‰²å½©ä¸€è‡´ç‰¹å¾ï¼Œæå‡ºäº†LongAnimationæ¡†æ¶ï¼Œæœ‰æ•ˆç»´æŠ¤äº†çŸ­æœŸå’Œé•¿æœŸçš„è‰²å½©ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01634",
            "title": "Depth Anything at Any Condition",
            "url": "https://huggingface.co/papers/2507.01634",
            "abstract": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.   Project Page: https://ghost233lism.github.io/depthanything-AC-page   Code: https://github.com/HVision-NKU/DepthAnythingAC",
            "score": 34,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "48dee9247e2393f0",
            "authors": [
                "Boyuan Sun",
                "Modi Jin",
                "Bowen Yin",
                "Qibin Hou"
            ],
            "affiliations": [
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01634.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»ÑĞ±Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…",
                    "desc": "DepthAnything-AC - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ»Ğ¸ÑÑŒ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹."
                },
                "en": {
                    "title": "Mastering Depth Estimation in Any Environment",
                    "desc": "DepthAnything-AC is a monocular depth estimation model designed to perform well in various challenging environmental conditions. It addresses the limitations of previous models that struggle with issues like lighting changes and weather effects. The model uses an unsupervised consistency regularization approach, allowing it to learn effectively from a small amount of unlabeled data. Additionally, it incorporates a Spatial Distance Constraint to improve the accuracy of depth estimation by focusing on the relationships between different image patches."
                },
                "zh": {
                    "title": "åœ¨ä»»ä½•æ¡ä»¶ä¸‹çš„æ·±åº¦ä¼°è®¡æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDepth Anything at Any Conditionï¼ˆDepthAnything-ACï¼‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒæ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ·±åº¦ä¼°è®¡ã€‚ä»¥å¾€çš„æ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨ä¸€èˆ¬åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¦‚å…‰ç…§å˜åŒ–å’Œæ¶åŠ£å¤©æ°”ä¸‹ï¼Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºå’Œä»å—æŸå›¾åƒç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾çš„å›°éš¾ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£ä¸€è‡´æ€§æ­£åˆ™åŒ–å¾®è°ƒæ–¹æ³•ï¼Œä»…éœ€å°‘é‡æœªæ ‡è®°æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDepthAnything-ACåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†é›¶æ ·æœ¬èƒ½åŠ›ï¼ŒåŒ…æ‹¬çœŸå®ä¸–ç•Œçš„æ¶åŠ£å¤©æ°”åŸºå‡†å’ŒåˆæˆæŸååŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01925",
            "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
            "url": "https://huggingface.co/papers/2507.01925",
            "abstract": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.",
            "score": 18,
            "issue_id": 4618,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "28708b74dd1e7612",
            "authors": [
                "Yifan Zhong",
                "Fengshuo Bai",
                "Shaofei Cai",
                "Xuchuan Huang",
                "Zhang Chen",
                "Xiaowei Zhang",
                "Yuanfei Wang",
                "Shaoyang Guo",
                "Tianrui Guan",
                "Ka Nam Lui",
                "Zhiquan Qi",
                "Yitao Liang",
                "Yuanpei Chen",
                "Yaodong Yang"
            ],
            "affiliations": [
                "Institute for AI, Peking University",
                "PKU-PsiBot Joint Lab",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01925.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#reasoning",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº VLA, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ğ´, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ VLA."
                },
                "en": {
                    "title": "Unifying Vision-Language-Action Models through Action Tokenization",
                    "desc": "This paper discusses the progress of vision-language-action (VLA) models, which integrate visual and linguistic inputs to perform actions in the physical world. It identifies a common framework among these models, where a series of VLA modules process inputs to generate action tokens that convey actionable information. The authors categorize these action tokens into various types, such as language descriptions and trajectories, highlighting the importance of how they are formulated. The survey aims to clarify the role of action tokens in VLA development, assess their strengths and weaknesses, and suggest future research directions to enhance the effectiveness of VLA models."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è¡ŒåŠ¨æ ‡è®°ï¼Œæ¨åŠ¨VLAæ¨¡å‹å‘å±•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚å°½ç®¡å½“å‰çš„VLAæ¨¡å‹æ–¹æ³•å¤šæ ·ï¼Œä½†å¯ä»¥ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸‹ï¼Œå¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œç”Ÿæˆé€æ­¥ç¼–ç çš„è¡ŒåŠ¨æ ‡è®°ã€‚æ–‡ç« è¿˜æŒ‡å‡ºï¼ŒVLAæ¨¡å‹çš„ä¸»è¦è®¾è®¡é€‰æ‹©åœ¨äºè¡ŒåŠ¨æ ‡è®°çš„å½¢å¼åŒ–æ–¹å¼ï¼ŒåŒ…æ‹¬è¯­è¨€æè¿°ã€ä»£ç ã€å¯ç”¨æ€§ã€è½¨è¿¹ã€ç›®æ ‡çŠ¶æ€ç­‰ã€‚é€šè¿‡å¯¹ç°æœ‰VLAç ”ç©¶çš„åˆ†ç±»å’Œè§£è¯»ï¼Œæœ¬æ–‡æ—¨åœ¨è¯†åˆ«æ”¹è¿›é¢†åŸŸï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01953",
            "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
            "url": "https://huggingface.co/papers/2507.01953",
            "abstract": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.",
            "score": 11,
            "issue_id": 4617,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "de2dfee54bea9af5",
            "authors": [
                "Yukang Cao",
                "Chenyang Si",
                "Jinghao Wang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01953.jpg",
            "data": {
                "categories": [
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "FreeMorph: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "FreeMorph - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, FreeMorph Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑˆĞ°Ğ³Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FreeMorph Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 10-50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Morphing with Tuning-Free Efficiency",
                    "desc": "FreeMorph is a novel method for image morphing that does not require tuning, making it efficient and effective for images with different meanings or layouts. It overcomes limitations of previous techniques that needed fine-tuning of diffusion models, which could lead to time delays and inconsistencies. The method introduces a guidance-aware spherical interpolation to enhance the quality of transitions and reduce identity loss, while also employing a step-oriented variation trend for smoother morphing. Evaluations show that FreeMorph is significantly faster and achieves superior results compared to existing methods, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "FreeMorphï¼šæ— éœ€è°ƒä¼˜çš„é«˜æ•ˆå›¾åƒå˜å½¢æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†FreeMorphï¼Œè¿™æ˜¯ä¸€ç§é¦–ä¸ªæ— éœ€è°ƒä¼˜çš„å›¾åƒå˜å½¢æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒè¯­ä¹‰æˆ–å¸ƒå±€çš„è¾“å…¥ã€‚ä¸ä¾èµ–äºå¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFreeMorphæ— éœ€é’ˆå¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜ä¿çœŸåœ°å®ç°å›¾åƒå˜å½¢ã€‚å°½ç®¡è°ƒä¼˜è‡ªç”±çš„æ–¹æ³•åœ¨æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†ç”±äºå¤šæ­¥å»å™ªè¿‡ç¨‹çš„éçº¿æ€§ç‰¹æ€§å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åå·®ï¼Œä¿æŒé«˜è´¨é‡ç»“æœä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æŒ‡å¯¼æ„ŸçŸ¥çš„çƒé¢æ’å€¼è®¾è®¡å’Œæ­¥éª¤å¯¼å‘çš„å˜åŒ–è¶‹åŠ¿ï¼ŒæˆåŠŸè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½¿FreeMorphåœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•å¿«10åˆ°50å€ï¼Œå¹¶å»ºç«‹äº†å›¾åƒå˜å½¢çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01957",
            "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
            "url": "https://huggingface.co/papers/2507.01957",
            "abstract": "We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256times256 res.) and 1024 to 48 (512times512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4times lower latency than previous parallelized autoregressive models.",
            "score": 10,
            "issue_id": 4621,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "b2594c8c1eebcb0c",
            "authors": [
                "Zhuoyang Zhang",
                "Luke J. Huang",
                "Chengyue Wu",
                "Shang Yang",
                "Kelly Peng",
                "Yao Lu",
                "Song Han"
            ],
            "affiliations": [
                "First Intelligence",
                "MIT",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01957.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Locality-aware Parallel Decoding (LPD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 3.4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Accelerating Image Generation with Locality-aware Parallel Decoding",
                    "desc": "This paper introduces Locality-aware Parallel Decoding (LPD) to improve the speed of autoregressive image generation. Traditional methods face high latency due to memory constraints when predicting the next patch of an image. The authors propose two innovative techniques: a flexible architecture for parallelized autoregressive modeling and a locality-aware generation ordering that optimizes the order of patch generation. These advancements significantly reduce the number of generation steps and latency while maintaining high image quality, outperforming previous models."
                },
                "zh": {
                    "title": "åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§å±€éƒ¨æ„ŸçŸ¥å¹¶è¡Œè§£ç ï¼ˆLPDï¼‰æ–¹æ³•ï¼Œä»¥åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚ä¼ ç»Ÿçš„è‡ªå›å½’å›¾åƒç”Ÿæˆä¾èµ–äºä¸‹ä¸€ä¸ªè¡¥ä¸çš„é¢„æµ‹ï¼Œè¿™ä¸€è¿‡ç¨‹å—å†…å­˜é™åˆ¶ï¼Œå¯¼è‡´å»¶è¿Ÿè¾ƒé«˜ã€‚æˆ‘ä»¬å¼•å…¥äº†çµæ´»çš„å¹¶è¡Œè‡ªå›å½’å»ºæ¨¡å’Œå±€éƒ¨æ„ŸçŸ¥ç”Ÿæˆé¡ºåºä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œä»¥å®ç°é«˜å¹¶è¡Œæ€§å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆæ­¥éª¤ä»256å‡å°‘åˆ°20ï¼Œå¹¶åœ¨ä¸å½±å“è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01544",
            "title": "MARVIS: Modality Adaptive Reasoning over VISualizations",
            "url": "https://huggingface.co/papers/2507.01544",
            "abstract": "Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16\\% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis",
            "score": 7,
            "issue_id": 4624,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "547bee35865d6ddd",
            "authors": [
                "Benjamin Feuer",
                "Lennart Purucker",
                "Oussama Elachqar",
                "Chinmay Hegde"
            ],
            "affiliations": [
                "NYU",
                "Oumi.AI",
                "University of Freiburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01544.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "MARVIS: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MARVIS - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ VLM Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸. MARVIS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Gemini Ğ½Ğ° 16% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ."
                },
                "en": {
                    "title": "Unlocking Versatility in Vision-Language Models with MARVIS",
                    "desc": "This paper introduces MARVIS, a novel method that enhances small vision-language models by allowing them to predict various data modalities without the need for training. MARVIS converts latent embeddings into visual representations, enabling the models to apply their reasoning capabilities effectively across different domains. The approach demonstrates competitive performance in areas like vision, audio, and biological data, outperforming existing models like Gemini by 16% on average. Importantly, MARVIS maintains privacy by not requiring any domain-specific training or exposing personally identifiable information."
                },
                "zh": {
                    "title": "MARVISï¼šå°å‹æ¨¡å‹çš„å¤šæ¨¡æ€é¢„æµ‹æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMARVISçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§æ•°æ®æ¨¡æ€ä¸Šçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚MARVISé€šè¿‡å°†æ½œåœ¨åµŒå…¥ç©ºé—´è½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´å’Œç»†ç²’åº¦æ¨ç†èƒ½åŠ›ï¼ŒæˆåŠŸè§£è¯»å’Œåˆ©ç”¨è¿™äº›è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æ— éœ€ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨è§†è§‰ã€éŸ³é¢‘ã€ç”Ÿç‰©å’Œè¡¨æ ¼æ•°æ®ç­‰é¢†åŸŸä¸­å®ç°ç«äº‰æ€§çš„æ€§èƒ½ã€‚MARVISåœ¨ä¸æš´éœ²ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å•ä¸ª3Bå‚æ•°æ¨¡å‹çš„è¡¨ç°è¶…è¶Šäº†Geminiï¼Œæ¥è¿‘ä¸“ä¸šæ–¹æ³•çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22868",
            "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
            "url": "https://huggingface.co/papers/2506.22868",
            "abstract": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.",
            "score": 4,
            "issue_id": 4621,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "94371810be905c93",
            "authors": [
                "Junsung Lee",
                "Junoh Kang",
                "Bohyung Han"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22868.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "STR-Match: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "STR-Match - ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ STR. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ 1D Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. STR-Match Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°."
                },
                "en": {
                    "title": "Enhancing Video Editing with STR-Match: Coherence Meets Quality",
                    "desc": "STR-Match is a novel video editing algorithm that enhances the quality and coherence of AI-generated videos. It utilizes a unique STR score to assess the relevance of pixels across time and space, ensuring that the edited videos maintain visual appeal and temporal consistency. By employing 2D spatial attention and 1D temporal attention, STR-Match avoids the complexity of 3D attention mechanisms while still achieving impressive results. The method is training-free and effectively handles significant domain transformations, outperforming existing techniques in visual quality and spatiotemporal coherence."
                },
                "zh": {
                    "title": "STR-Matchï¼šæ—¶ç©ºä¸€è‡´çš„è§†è§‰è§†é¢‘ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "STR-Matchæ˜¯ä¸€ç§æ— è®­ç»ƒçš„è§†é¢‘ç¼–è¾‘ç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººä¸”æ—¶ç©ºä¸€è‡´çš„è§†é¢‘ã€‚å®ƒé€šè¿‡å¼•å…¥æ–°é¢–çš„STRè¯„åˆ†ï¼Œåˆ©ç”¨2Dç©ºé—´æ³¨æ„åŠ›å’Œ1Dæ—¶é—´æ¨¡å—æ¥æ•æ‰ç›¸é‚»å¸§ä¹‹é—´çš„åƒç´ ç›¸å…³æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSTR-Matché¿å…äº†è®¡ç®—å¼€é”€å¤§çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—çš„é¢†åŸŸè½¬æ¢ä¸‹ä¿æŒè§†é¢‘çš„å…³é”®è§†è§‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTR-Matchåœ¨è§†è§‰è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23552",
            "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
            "url": "https://huggingface.co/papers/2506.23552",
            "abstract": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web",
            "score": 3,
            "issue_id": 4615,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "69157bacab4dea7d",
            "authors": [
                "Mingi Kwon",
                "Joonghyuk Shin",
                "Jaeseok Jung",
                "Jaesik Park",
                "Youngjung Uh"
            ],
            "affiliations": [
                "Seoul National University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23552.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°",
                    "desc": "JAM-Flow - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ»Ğ¸Ñ†ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ flow matching Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-Modal Diffusion Transformer (MM-DiT) Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Motion-DiT Ğ¸ Audio-DiT. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. JAM-Flow Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·."
                },
                "en": {
                    "title": "Unified Synthesis of Speech and Facial Motion with JAM-Flow",
                    "desc": "This paper presents JAM-Flow, a new framework that combines facial motion and speech synthesis into one model. It uses advanced techniques like flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to allow for effective interaction between audio and visual data. The model includes specialized components for handling both motion and audio, ensuring that each modality retains its unique characteristics while working together. By training with a unique inpainting-style objective, JAM-Flow can generate synchronized talking heads from various inputs, making it a significant advancement in multi-modal generative modeling."
                },
                "zh": {
                    "title": "ç»Ÿä¸€é¢éƒ¨è¿åŠ¨ä¸è¯­éŸ³çš„ç”Ÿæˆæ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºJAM-Flowçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶åˆæˆé¢éƒ¨è¿åŠ¨å’Œè¯­éŸ³ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æµåŒ¹é…å’Œæ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼ˆMM-DiTï¼‰æ¶æ„ï¼Œé›†æˆäº†ä¸“é—¨çš„è¿åŠ¨å’ŒéŸ³é¢‘æ¨¡å—ã€‚é€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„åŠ›å±‚ï¼Œè¿™äº›æ¨¡å—å®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™äº†å„è‡ªæ¨¡æ€çš„ä¼˜åŠ¿ã€‚JAM-Flowæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°æ–‡æœ¬é©±åŠ¨çš„åŒæ­¥äººå¤´ç”Ÿæˆå’ŒéŸ³é¢‘é©±åŠ¨çš„åŠ¨ç”»ç­‰ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00472",
            "title": "ARIG: Autoregressive Interactive Head Generation for Real-time\n  Conversations",
            "url": "https://huggingface.co/papers/2507.00472",
            "abstract": "Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.",
            "score": 2,
            "issue_id": 4628,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "083f371459a06cdd",
            "authors": [
                "Ying Guo",
                "Xi Liu",
                "Cheng Zhen",
                "Pengfei Yan",
                "Xiaoming Wei"
            ],
            "affiliations": [
                "Vision AI Department, Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00472.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ARIG: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ARIG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ (IBU) Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° (CSU). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Real-Time Realism in Virtual Conversations with ARIG",
                    "desc": "This paper presents a new method for generating realistic interactions in virtual agents, focusing on head motion during conversations. The proposed autoregressive framework, called ARIG, allows for real-time motion prediction by modeling it as a continuous process rather than using discrete codes. It enhances interaction realism by incorporating interactive behavior understanding (IBU) and conversational state understanding (CSU), which analyze both short-term and long-term conversational dynamics. The results from extensive experiments demonstrate that ARIG significantly improves the quality and smoothness of virtual agent interactions compared to previous methods."
                },
                "zh": {
                    "title": "æå‡è™šæ‹Ÿä»£ç†äº¤äº’çœŸå®æ„Ÿçš„è‡ªå›å½’ç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†äº¤äº’å¼å¤´éƒ¨ç”Ÿæˆï¼Œä»¥æé«˜è™šæ‹Ÿä»£ç†çš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªå›å½’ï¼ˆARï¼‰çš„æ¡†æ¶ARIGï¼Œé€šè¿‡éå‘é‡é‡åŒ–çš„ARè¿‡ç¨‹æ¥å»ºæ¨¡è¿åŠ¨é¢„æµ‹ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¿ç»­ç©ºé—´é¢„æµ‹ã€‚ä¸ºäº†å¢å¼ºäº¤äº’çš„çœŸå®æ„Ÿï¼Œæˆ‘ä»¬å¼ºè°ƒäº†äº¤äº’è¡Œä¸ºç†è§£ï¼ˆIBUï¼‰å’Œè¯¦ç»†çš„å¯¹è¯çŠ¶æ€ç†è§£ï¼ˆCSUï¼‰ï¼Œé€šè¿‡åŒè½¨åŒæ¨¡æ€ä¿¡å·æ¥æ€»ç»“çŸ­æœŸè¡Œä¸ºï¼Œå¹¶å¯¹é•¿æœŸè¡Œä¸ºè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨å®æ—¶ç”Ÿæˆå’Œäº¤äº’çœŸå®æ„Ÿæ–¹é¢çš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-02.html",
    "link_next": "2025-07-04.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.07",
        "en": "07/04",
        "zh": "7æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}