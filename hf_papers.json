{
    "date": {
        "ru": "9 мая",
        "en": "May 9",
        "zh": "5月9日"
    },
    "time_utc": "2025-05-09 07:11",
    "weekday": 4,
    "issue_id": 3675,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04620",
            "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
            "url": "https://huggingface.co/papers/2505.04620",
            "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/",
            "score": 35,
            "issue_id": 3671,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "57991e528141671e",
            "authors": [
                "Hao Fei",
                "Yuan Zhou",
                "Juncheng Li",
                "Xiangtai Li",
                "Qingshan Xu",
                "Bobo Li",
                "Shengqiong Wu",
                "Yaoting Wang",
                "Junbao Zhou",
                "Jiahao Meng",
                "Qingyu Shi",
                "Zhiyuan Zhou",
                "Liangtao Shi",
                "Minghe Gao",
                "Daoan Zhang",
                "Zhiqi Ge",
                "Weiming Wu",
                "Siliang Tang",
                "Kaihang Pan",
                "Yaobo Ye",
                "Haobo Yuan",
                "Tao Zhang",
                "Tianjie Ju",
                "Zixiang Meng",
                "Shilin Xu",
                "Liyu Jia",
                "Wentao Hu",
                "Meng Luo",
                "Jiebo Luo",
                "Tat-Seng Chua",
                "Shuicheng Yan",
                "Hanwang Zhang"
            ],
            "affiliations": [
                "HFUT",
                "KAUST",
                "NJU",
                "NTU",
                "NUS",
                "PKU",
                "SJTU",
                "UR",
                "WHU",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04620.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый подход к оценке мультимодальных ИИ-систем на пути к AGI",
                    "desc": "Статья описывает новую систему оценки мультимодальных больших языковых моделей (MLLM) под названием General-Level. Эта система определяет 5 уровней производительности и обобщаемости MLLM, предлагая методологию для сравнения моделей и оценки прогресса существующих систем. Ключевым понятием в этой системе является концепция Синергии, которая измеряет согласованность возможностей моделей в понимании и генерации контента, а также в работе с различными модальностями. Для поддержки этой системы оценки авторы представляют General-Bench - набор из более чем 700 задач и 325 800 примеров, охватывающий широкий спектр навыков, модальностей и форматов."
                },
                "en": {
                    "title": "Towards Multimodal Generalists: Evaluating MLLM Progress",
                    "desc": "The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "迈向真正的多模态通用人工智能",
                    "desc": "多模态大型语言模型（MLLM）正在快速发展，得益于大型语言模型（LLM）的先进能力。现有的MLLM正朝着多模态通用主义者的方向演变，不仅能够理解多种模态，还能在不同模态之间生成内容。本文提出了一种新的评估框架——General-Level，定义了MLLM性能和通用性的五个等级，以便比较不同模型的能力。通过General-Bench，我们提供了一个更广泛的技能和任务评估，揭示了当前多模态通用模型在实现真正人工智能方面的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02847",
            "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.02847",
            "abstract": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.",
            "score": 13,
            "issue_id": 3671,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "9204f0ca97eb8bc7",
            "authors": [
                "Bang Zhang",
                "Ruotian Ma",
                "Qingxuan Jiang",
                "Peisong Wang",
                "Jiaqi Chen",
                "Zheng Xie",
                "Xingyu Chen",
                "Yue Wang",
                "Fanghua Ye",
                "Jian Li",
                "Yifan Yang",
                "Zhaopeng Tu",
                "Xiaolong Li"
            ],
            "affiliations": [
                "Hunyuan AI Digital Human, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02847.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#alignment",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAGE: Измерение эмпатии и социального интеллекта языковых моделей",
                    "desc": "Статья представляет SAGE - новую систему оценки способности больших языковых моделей (LLM) понимать человека, а не просто текст. SAGE использует 'Разумного Агента', который симулирует человеческие эмоции и мысли во время взаимодействия, обеспечивая более реалистичную оценку тестируемой модели в многоходовых диалогах. Эксперименты показали, что эмоциональные оценки SAGE коррелируют с психологическими метриками. Система также используется для создания публичного рейтинга LLM, выявляющего значительные различия между передовыми и базовыми моделями."
                },
                "en": {
                    "title": "Measuring Empathy in AI: The SAGE Framework",
                    "desc": "The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems."
                },
                "zh": {
                    "title": "评估语言模型的情感理解能力",
                    "desc": "本文介绍了一种名为SAGE的自动评估框架，用于测量大型语言模型（LLM）对人类情感和社交认知的理解能力。SAGE通过模拟人类情感变化和内心想法，提供了更真实的多轮对话评估。实验结果表明，SAGE的情感评分与心理学评估工具的评分高度相关，验证了其心理学的真实性。该框架还建立了一个公开的Sentient排行榜，揭示了不同模型之间的显著差距，推动了对更具同理心和社交能力的语言代理的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05315",
            "title": "Scalable Chain of Thoughts via Elastic Reasoning",
            "url": "https://huggingface.co/papers/2505.05315",
            "abstract": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.",
            "score": 11,
            "issue_id": 3672,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "0ce3be6057da3ed2",
            "authors": [
                "Yuhui Xu",
                "Hanze Dong",
                "Lei Wang",
                "Doyen Sahoo",
                "Junnan Li",
                "Caiming Xiong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.05315.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эластичное рассуждение: эффективные цепочки мысли в условиях ограниченных ресурсов",
                    "desc": "Эта статья представляет новый подход под названием 'Эластичное рассуждение' для крупных моделей рассуждения (LRM). Метод разделяет процесс рассуждения на две фазы - мышление и решение - с независимыми бюджетами, что позволяет эффективно работать в условиях ограниченных ресурсов. Авторы предлагают стратегию обучения с ограниченным бюджетом, интегрированную в GRPO, которая учит модель адаптивно рассуждать при сокращении процесса мышления. Эксперименты на математических и программистских задачах показывают, что 'Эластичное рассуждение' работает надежно при строгих ограничениях бюджета и производит более краткие и эффективные рассуждения даже в неограниченных условиях."
                },
                "en": {
                    "title": "Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models",
                    "desc": "This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs."
                },
                "zh": {
                    "title": "弹性推理：可控推理的新解决方案",
                    "desc": "大型推理模型（LRMs）在复杂任务上取得了显著进展，但其输出长度不受控制，给实际应用带来了挑战。我们提出了一种名为弹性推理的新框架，将推理过程分为思考和解决两个阶段，并为每个阶段分配独立的预算。在测试时，弹性推理优先考虑解决方案的完整性，从而在资源紧张的情况下显著提高可靠性。我们的实验证明，弹性推理在严格的预算限制下表现出色，同时训练成本显著低于基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05474",
            "title": "3D Scene Generation: A Survey",
            "url": "https://huggingface.co/papers/2505.05474",
            "abstract": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.",
            "score": 5,
            "issue_id": 3672,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "06bda1a6228b8f26",
            "authors": [
                "Beichen Wen",
                "Haozhe Xie",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore 637335"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05474.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#multimodal",
                    "#synthetic",
                    "#survey"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Новые горизонты в генерации трехмерных сцен: от процедурных методов к нейронным сетям",
                    "desc": "Эта статья представляет собой обзор современных методов генерации трехмерных сцен. Авторы систематизируют подходы по четырем парадигмам: процедурная генерация, нейронная генерация на основе 3D, генерация на основе изображений и генерация на основе видео. В работе анализируются технические основы, компромиссы и репрезентативные результаты каждого подхода, а также рассматриваются наборы данных, протоколы оценки и прикладные задачи. Статья завершается обсуждением ключевых проблем и перспективных направлений в области генерации 3D-сцен, включая повышение точности, физически корректную и интерактивную генерацию."
                },
                "en": {
                    "title": "Advancing 3D Scene Generation with Deep Learning",
                    "desc": "This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation."
                },
                "zh": {
                    "title": "3D场景生成的未来方向与挑战",
                    "desc": "3D场景生成旨在合成具有空间结构、语义意义和照片真实感的环境，广泛应用于沉浸式媒体、机器人、自动驾驶和具身人工智能等领域。早期基于程序规则的方法虽然具有可扩展性，但多样性有限。近年来，深度生成模型（如GAN和扩散模型）以及3D表示（如NeRF和3D高斯）取得了进展，使得能够学习真实世界场景的分布，从而提高了生成的真实感、多样性和视图一致性。本文综述了最新的3D场景生成方法，分析了其技术基础、权衡和代表性结果，并讨论了生成能力、3D表示、数据和评估等关键挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03981",
            "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
            "url": "https://huggingface.co/papers/2505.03981",
            "abstract": "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.",
            "score": 4,
            "issue_id": 3672,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "0e6c2f37e1536f9f",
            "authors": [
                "Qianchu Liu",
                "Sheng Zhang",
                "Guanghui Qin",
                "Timothy Ossowski",
                "Yu Gu",
                "Ying Jin",
                "Sid Kiblawi",
                "Sam Preston",
                "Mu Wei",
                "Paul Vozila",
                "Tristan Naumann",
                "Hoifung Poon"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03981.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#transfer_learning",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обобщаемые рассуждения: от текста к мультимодальности и специализированным доменам",
                    "desc": "Эта статья исследует возможность обобщения способностей к рассуждению на различные модальности и домены. Авторы представляют X-Reasoner - мультимодальную модель, обученную только на текстовых данных общего домена для обобщаемых рассуждений. Эксперименты показывают, что X-Reasoner успешно переносит навыки рассуждения на мультимодальные и узкоспециализированные задачи, превосходя существующие модели. Также представлена медицинская версия модели - X-Reasoner-Med, достигающая новых рекордных результатов на ряде текстовых и мультимодальных медицинских бенчмарков."
                },
                "en": {
                    "title": "Unlocking Generalizable Reasoning Across Modalities with X-Reasoner",
                    "desc": "This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks."
                },
                "zh": {
                    "title": "推理能力的跨模态推广",
                    "desc": "最近的专有模型（如o3）展示了强大的多模态推理能力。然而，大多数现有的开源研究主要集中在训练仅基于文本的推理模型，评估也主要限于数学和一般领域任务。因此，如何有效地将推理能力扩展到文本输入和一般领域之外仍然不清楚。本文探讨了一个基本的研究问题：推理是否可以跨模态和领域进行推广？"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05071",
            "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
            "url": "https://huggingface.co/papers/2505.05071",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.",
            "score": 2,
            "issue_id": 3675,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "4251cc9ddf64d2b8",
            "authors": [
                "Chunyu Xie",
                "Bin Wang",
                "Fanjing Kong",
                "Jincheng Li",
                "Dawei Liang",
                "Gengshen Zhang",
                "Dawei Leng",
                "Yuhui Yin"
            ],
            "affiliations": [
                "360 AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05071.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "FG-CLIP: Точное понимание изображений на новом уровне",
                    "desc": "Статья представляет Fine-Grained CLIP (FG-CLIP) - улучшенную версию модели CLIP для более детального понимания изображений. FG-CLIP использует 1,6 миллиарда пар изображение-текст с длинными подписями для захвата семантических деталей. Модель обучается на наборе данных из 12 миллионов изображений с 40 миллионами ограничивающих рамок и детальными подписями. FG-CLIP превосходит оригинальный CLIP и другие современные методы в различных задачах, включая детальное понимание изображений и мультимодальные бенчмарки."
                },
                "en": {
                    "title": "Unlocking Fine-Grained Understanding with FG-CLIP",
                    "desc": "The paper introduces Fine-Grained CLIP (FG-CLIP), an enhancement of the original CLIP model aimed at improving fine-grained understanding in multimodal tasks. FG-CLIP achieves this by generating a massive dataset of 1.6 billion long caption-image pairs, which helps capture detailed semantic information. Additionally, it constructs a high-quality dataset with 12 million images and 40 million bounding boxes, ensuring that the model learns from context-rich representations. By incorporating 10 million hard negative samples, FG-CLIP enhances its ability to differentiate subtle semantic differences, leading to superior performance in various tasks compared to the original CLIP and other leading models."
                },
                "zh": {
                    "title": "细粒度理解的新突破：FG-CLIP",
                    "desc": "对比语言-图像预训练（CLIP）在多模态任务中表现出色，但在细粒度理解方面存在困难。为了解决这个问题，我们提出了细粒度CLIP（FG-CLIP），通过三项关键创新来增强细粒度理解。首先，我们利用大型多模态模型生成16亿对长标题-图像对，以捕捉全局语义细节。其次，构建了一个高质量的数据集，包含1200万张图像和4000万个区域特定的边界框，确保精确且丰富的上下文表示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19314",
            "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
            "url": "https://huggingface.co/papers/2504.19314",
            "abstract": "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.",
            "score": 2,
            "issue_id": 3673,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 апреля",
                "en": "April 27",
                "zh": "4月27日"
            },
            "hash": "06aff0f566bd3817",
            "authors": [
                "Peilin Zhou",
                "Bruce Leon",
                "Xiang Ying",
                "Can Zhang",
                "Yifan Shao",
                "Qichen Ye",
                "Dading Chong",
                "Zhiling Jin",
                "Chenxuan Xie",
                "Meng Cao",
                "Yuxin Gu",
                "Sixin Hong",
                "Jing Ren",
                "Jian Chen",
                "Chao Liu",
                "Yining Hua"
            ],
            "affiliations": [
                "Alibaba Group",
                "HSBC",
                "Harvard T.H. Chan School of Public Health",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "MBZUAI",
                "Mindverse AI",
                "NIO",
                "Peking University",
                "Zhejiang University",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19314.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "BrowseComp-ZH: испытание языковых моделей в китайском интернете",
                    "desc": "Статья представляет BrowseComp-ZH - новый бенчмарк для оценки способностей языковых моделей (ЯМ) работать с китайским веб-контентом. Бенчмарк состоит из 289 сложных многоэтапных вопросов в 11 различных областях. Несмотря на свои сильные разговорные и поисковые возможности, большинство современных ЯМ показывают низкую точность на этом тесте. Результаты демонстрируют, что для успеха в BrowseComp-ZH требуются не только эффективные стратегии поиска, но и сложные рассуждения и согласование информации."
                },
                "en": {
                    "title": "Evaluating LLMs: The Challenge of Chinese Web Browsing",
                    "desc": "This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments."
                },
                "zh": {
                    "title": "中文网络智能体评估新基准：BrowseComp-ZH",
                    "desc": "随着大型语言模型（LLMs）逐渐演变为使用工具的智能体，实时浏览网络的能力成为衡量其推理和检索能力的重要标准。现有的基准测试如BrowseComp主要集中在英语，忽视了其他主要信息生态系统（尤其是中文）在语言、基础设施和审查方面的复杂性。为了解决这一问题，我们推出了BrowseComp-ZH，这是一个专门为全面评估中文网络上的LLM智能体而设计的高难度基准测试。该基准测试包含289个跨越11个不同领域的多跳问题，旨在考察模型的检索策略、推理能力和信息整合能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05288",
            "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
            "url": "https://huggingface.co/papers/2505.05288",
            "abstract": "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.",
            "score": 1,
            "issue_id": 3675,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "7ffec9bccc965d17",
            "authors": [
                "Ahmed Abdelreheem",
                "Filippo Aleotti",
                "Jamie Watson",
                "Zawar Qureshi",
                "Abdelrahman Eldesokey",
                "Peter Wonka",
                "Gabriel Brostow",
                "Sara Vicente",
                "Guillermo Garcia-Hernando"
            ],
            "affiliations": [
                "KAUST",
                "Niantic Spatial",
                "UCL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05288.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#3d",
                    "#survey",
                    "#dataset"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Языковое управление размещением объектов в реальных 3D-сценах",
                    "desc": "Статья представляет новую задачу размещения объектов в реальных 3D-сценах с помощью языковых инструкций. Модель получает облако точек 3D-сцены, 3D-модель объекта и текстовое описание желаемого размещения. Задача требует рассуждений о 3D-геометрических отношениях и свободном пространстве, что отличает ее от других задач локализации в 3D. Авторы предлагают новый бенчмарк, протокол оценки, датасет для обучения 3D языковых моделей и базовый метод решения."
                },
                "en": {
                    "title": "Placing Objects with Words in 3D Spaces!",
                    "desc": "This paper presents a new task called Language-Guided Object Placement in Real 3D Scenes, which involves placing a 3D object based on a textual description. The model must analyze a point cloud of a 3D scene and determine a suitable location for the object that aligns with the given prompt. This task is particularly challenging due to the ambiguity of multiple valid placements and the need for understanding 3D spatial relationships. The authors introduce a benchmark, evaluation protocol, and a dataset to train 3D language models, aiming to enhance the evaluation of generalist 3D models."
                },
                "zh": {
                    "title": "语言引导的3D物体放置新挑战",
                    "desc": "我们提出了一项新任务：在真实3D场景中进行语言引导的物体放置。我们的模型接收一个3D场景的点云、一个3D资产和一个文本提示，任务是找到一个符合提示的有效放置位置。与其他语言引导的3D场景定位任务相比，这项任务具有特定的挑战性，因为它存在多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出新的基准和评估协议来开启这一任务，并引入了一个新的数据集用于训练3D大语言模型，以及第一个非平凡的基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05467",
            "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
            "url": "https://huggingface.co/papers/2505.05467",
            "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.",
            "score": 0,
            "issue_id": 3674,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "c876dfdb1d290930",
            "authors": [
                "Haibo Wang",
                "Bo Feng",
                "Zhengfeng Lai",
                "Mingze Xu",
                "Shiyu Li",
                "Weifeng Ge",
                "Afshin Dehghan",
                "Meng Cao",
                "Ping Huang"
            ],
            "affiliations": [
                "Apple",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05467.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "StreamBridge: Революция в потоковом понимании видео",
                    "desc": "StreamBridge - это фреймворк, который превращает офлайн-модели Video-LLM в потоковые. Он решает проблемы многоэтапного понимания в реальном времени и отсутствия механизмов проактивного реагирования. Фреймворк использует буфер памяти с компрессией и легковесную модель активации для интеграции с существующими Video-LLM. Для поддержки StreamBridge создан датасет Stream-IT, предназначенный для обучения потокового понимания видео."
                },
                "en": {
                    "title": "Transforming Video-LLMs for Real-Time Streaming Success",
                    "desc": "StreamBridge is a framework designed to enhance offline Video-LLMs for real-time streaming applications. It tackles two main issues: the need for effective multi-turn interactions and the ability to provide proactive responses. By using a memory buffer with round-decayed compression, it allows models to handle longer contexts in conversations. Additionally, StreamBridge introduces a lightweight activation model that integrates easily with existing Video-LLMs, and it is supported by the Stream-IT dataset, which is specifically created for streaming video understanding tasks."
                },
                "zh": {
                    "title": "StreamBridge：流媒体视频理解的新突破",
                    "desc": "StreamBridge是一个简单而有效的框架，可以将离线视频大语言模型（Video-LLMs）转变为支持流媒体的模型。它解决了在在线场景中适应现有模型的两个基本挑战：多轮实时理解能力有限和缺乏主动响应机制。具体来说，StreamBridge结合了内存缓冲区和逐轮衰减压缩策略，支持长上下文的多轮交互，并且采用了轻量级的解耦激活模型，能够轻松集成到现有的视频大语言模型中，实现持续的主动响应。此外，我们构建了Stream-IT，这是一个针对流媒体视频理解的大规模数据集，包含交错的视频-文本序列和多样的指令格式。"
                }
            }
        }
    ],
    "link_prev": "2025-05-08.html",
    "link_next": "2025-05-12.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "08.05",
        "en": "05/08",
        "zh": "5月8日"
    },
    "short_date_next": {
        "ru": "12.05",
        "en": "05/12",
        "zh": "5月12日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLMs）搜索能力的重要性。最近的研究使用强化学习（RL）与实时搜索引擎互动来改进LLMs的搜索能力，但面临文档质量不可控和API费用高昂的挑战。为解决这些问题，作者提出了ZeroSearch，一种不需要与实际搜索引擎互动的RL框架。通过轻量级的监督微调和基于课程的滚动策略，ZeroSearch能够有效提升LLMs的搜索能力，并且在不同参数规模的模型上表现良好。",
        "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
        "pinyin": "Zhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLMs) sōusuǒ nénglì de zhòngyàoxìng. Zuìjìn de yánjiū shǐyòng qiángzhì xuéxí (RL) yǔ shíshí sōusuǒ yǐnqíng hùdòng lái gǎijìn LLMs de sōusuǒ nénglì, dàn miànlín wénjiàn zhìliàng bù kě kòng hé API fèiyòng gāo'áng de tiǎozhàn. Wèi jiějué zhèxiē wèntí, zuòzhě tíchū le ZeroSearch, yīzhǒng bù xūyào yǔ shíjì sōusuǒ yǐnqíng hùdòng de RL kuàngjià. Tōngguò qīngliàngjí de jiàndū wēitiáo hé jīyú kèchéng de gǔndòng cèlüè, ZeroSearch nénggòu yǒuxiào tíshēng LLMs de sōusuǒ nénglì, bìngqiě zài bùtóng cānshù guīmó de móxíng shàng biǎoxiàn liánghǎo.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improve'},\n{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '搜索', 'pinyin': 'sōu suǒ', 'trans': 'search'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'},\n{'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'},\n{'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'},\n{'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '不可控', 'pinyin': 'bù kě kòng', 'trans': 'uncontrollable'},\n{'word': 'API', 'pinyin': 'API', 'trans': 'API'},\n{'word': '费用', 'pinyin': 'fèi yòng', 'trans': 'cost'},\n{'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '轻量级', 'pinyin': 'qīng liàng jí', 'trans': 'lightweight'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'},\n{'word': '课程', 'pinyin': 'kè chéng', 'trans': 'course'},\n{'word': '滚动', 'pinyin': 'gǔn dòng', 'trans': 'rolling'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '良好', 'pinyin': 'liáng hǎo', 'trans': 'good'},\n{'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameter'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'}]",
        "trans": "This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes.",
        "update_ts": "2025-05-08 09:12"
    }
}