{
    "date": {
        "ru": "18 февраля",
        "en": "February 18",
        "zh": "2月18日"
    },
    "time_utc": "2025-02-18 20:11",
    "weekday": 1,
    "issue_id": 2280,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.11089",
            "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
            "url": "https://huggingface.co/papers/2502.11089",
            "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
            "score": 51,
            "issue_id": 2271,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "aa26756957a07b01",
            "authors": [
                "Jingyang Yuan",
                "Huazuo Gao",
                "Damai Dai",
                "Junyu Luo",
                "Liang Zhao",
                "Zhengyan Zhang",
                "Zhenda Xie",
                "Y. X. Wei",
                "Lean Wang",
                "Zhiping Xiao",
                "Yuqing Wang",
                "Chong Ruan",
                "Ming Zhang",
                "Wenfeng Liang",
                "Wangding Zeng"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11089.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное внимание для длинного контекста",
                    "desc": "Статья представляет NSA - новый механизм нативно обучаемого разреженного внимания для эффективного моделирования длинного контекста в языковых моделях. NSA использует динамическую иерархическую стратегию разреженности, сочетающую сжатие токенов на грубом уровне с точным выбором токенов на детальном уровне. Авторы достигают значительного ускорения за счет оптимизации алгоритма и реализации для современного оборудования. Эксперименты показывают, что модель с NSA сохраняет или превосходит модели с полным вниманием на различных задачах, при этом обеспечивая существенное ускорение на последовательностях длиной 64 тысячи токенов."
                },
                "en": {
                    "title": "Efficient Long-Context Modeling with Natively Trainable Sparse Attention",
                    "desc": "This paper introduces NSA, a new Sparse Attention mechanism designed for efficient long-context modeling in language models. NSA combines coarse-grained token compression with fine-grained token selection to enhance both global context and local detail while reducing computational costs. The method features innovations that optimize performance on modern hardware and allow for end-to-end training, which cuts down on pretraining time without losing effectiveness. Experimental results demonstrate that NSA not only matches but often surpasses traditional Full Attention models in various tasks, proving its efficiency and capability in handling long sequences."
                },
                "zh": {
                    "title": "高效长上下文建模的新突破",
                    "desc": "长上下文建模对下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著的挑战。稀疏注意力提供了一种有前景的方向，可以在保持模型能力的同时提高效率。我们提出了NSA，一种原生可训练的稀疏注意力机制，结合了算法创新和硬件优化，以实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的令牌压缩和细粒度的令牌选择，既保留了全局上下文意识，又保持了局部精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12152",
            "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
            "url": "https://huggingface.co/papers/2502.12152",
            "abstract": "Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. Project page: https://humanoid-getup.github.io/",
            "score": 28,
            "issue_id": 2266,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "c87382b187d7b745",
            "authors": [
                "Xialin He",
                "Runpei Dong",
                "Zixuan Chen",
                "Saurabh Gupta"
            ],
            "affiliations": [
                "Simon Fraser University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12152.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Роботы учатся вставать: прорыв в адаптивном управлении гуманоидами",
                    "desc": "Эта статья описывает разработку системы машинного обучения для создания контроллеров, позволяющих гуманоидным роботам вставать из различных положений на разных поверхностях. Авторы применяют двухэтапный подход с использованием куррикулума: сначала находится оптимальная траектория подъема, а затем она оптимизируется для плавности и устойчивости. Система успешно протестирована на реальном гуманоидном роботе G1 в различных условиях, включая скользкие и наклонные поверхности. Это первая успешная демонстрация обученных стратегий подъема для гуманоидных роботов человеческого размера в реальном мире."
                },
                "en": {
                    "title": "Learning to Get Up: Humanoid Robots Rise to the Challenge!",
                    "desc": "This paper presents a novel learning framework for enabling humanoid robots to recover from falls by getting up from various positions and terrains. The approach consists of a two-phase curriculum where the first phase focuses on discovering effective getting-up trajectories with minimal constraints, while the second phase refines these motions to ensure they are smooth and robust. The framework addresses the complexities of contact patterns and collision geometry, which are critical for the getting-up task. The results demonstrate successful real-world applications, allowing a humanoid robot to rise from both face-up and face-down positions on challenging surfaces."
                },
                "zh": {
                    "title": "人形机器人自主起身的创新学习框架",
                    "desc": "本文提出了一种学习框架，使人形机器人能够从不同的姿势和地形中自行站起。由于人形机器人在跌倒后可能处于多种复杂的姿势，设计控制器非常困难。我们采用了两阶段的方法，第一阶段发现适合的起身轨迹，第二阶段则将这些轨迹优化为平滑且稳健的动作。实验表明，该方法使得人形机器人能够在多种真实环境中成功站起。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12115",
            "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
            "url": "https://huggingface.co/papers/2502.12115",
            "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.",
            "score": 21,
            "issue_id": 2266,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "0b2638455d4393b0",
            "authors": [
                "Samuel Miserendino",
                "Michele Wang",
                "Tejal Patwardhan",
                "Johannes Heidecke"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12115.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "💻",
                "ru": {
                    "title": "SWE-Lancer: Измеряем возможности ИИ в реальных задачах разработки ПО",
                    "desc": "SWE-Lancer - это новый бенчмарк для оценки систем искусственного интеллекта в области разработки программного обеспечения. Он включает более 1400 реальных задач с платформы Upwork общей стоимостью 1 миллион долларов. Задачи разделены на инженерные (от исправления ошибок до разработки крупных функций) и управленческие. Оценка производится с помощью автоматических тестов и сравнения с решениями опытных разработчиков."
                },
                "en": {
                    "title": "Unlocking AI's Potential in Freelance Software Engineering",
                    "desc": "SWE-Lancer is a new benchmark that includes over 1,400 freelance software engineering tasks sourced from Upwork, with a total value of $1 million. It features both independent tasks, like bug fixes and large feature implementations, and managerial tasks that require decision-making between different technical proposals. The tasks are rigorously evaluated, with independent tasks tested by experienced engineers and managerial decisions compared to those made by original engineering managers. Despite the evaluation, current advanced models struggle to complete most tasks, highlighting the need for further research in AI's economic implications."
                },
                "zh": {
                    "title": "SWE-Lancer：推动AI模型经济影响研究的基准",
                    "desc": "我们介绍了SWE-Lancer，这是一个包含1400多个来自Upwork的自由软件工程任务的基准，任务总价值达到100万美元。SWE-Lancer包括独立的工程任务，如50个bug修复和价值32000美元的功能实现，以及管理任务，模型需要在技术实施提案中进行选择。独立任务通过经验丰富的软件工程师进行三重验证的端到端测试来评分，而管理决策则与原雇佣的工程经理的选择进行比较。我们评估了模型的表现，发现前沿模型仍然无法解决大多数任务，SWE-Lancer的开源将促进未来的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11190",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "url": "https://huggingface.co/papers/2502.11190",
            "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "score": 15,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "83a57c7c971f5060",
            "authors": [
                "Haoming Xu",
                "Ningyuan Zhao",
                "Liming Yang",
                "Sendong Zhao",
                "Shumin Deng",
                "Mengru Wang",
                "Bryan Hooi",
                "Nay Oo",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "National University of Singapore",
                "Tsinghua University",
                "Xiamen University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11190.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#hallucinations",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ReLearn: эффективное разобучение без потери качества",
                    "desc": "В статье представлен новый метод ReLearn для эффективного разобучения больших языковых моделей. В отличие от существующих подходов, ReLearn использует аугментацию данных и дообучение, что позволяет избежать нарушения когерентности текста. Авторы также предлагают новые метрики оценки: KFR, KRR и LS. Эксперименты показывают, что ReLearn успешно удаляет целевые знания, сохраняя при этом высокое качество генерации текста."
                },
                "en": {
                    "title": "ReLearn: Effective Unlearning Without Sacrificing Coherence",
                    "desc": "This paper introduces ReLearn, a novel approach for unlearning in large language models that avoids the pitfalls of reverse optimization, which can harm the model's ability to predict subsequent tokens. ReLearn employs a data augmentation and fine-tuning pipeline that effectively targets specific knowledge for removal while maintaining the fluency and relevance of generated text. The authors propose new evaluation metrics, including Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR), to assess the balance between forgetting and retaining knowledge, alongside a Linguistic Score (LS) for output quality. Experimental results demonstrate that ReLearn achieves its unlearning goals without sacrificing the coherence of the model's text generation capabilities."
                },
                "zh": {
                    "title": "ReLearn：高效去学习与语言生成的完美结合",
                    "desc": "当前的大型语言模型的去学习方法通常依赖于反向优化来降低目标词的概率。然而，这种方法会干扰后续词的预测，导致模型性能和语言连贯性下降。此外，现有的评估指标过于强调上下文遗忘，而对响应的流畅性和相关性评估不足。为了解决这些问题，我们提出了ReLearn，一个有效的去学习的数据增强和微调流程，并引入了全面的评估框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12148",
            "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2502.12148",
            "abstract": "The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow",
            "score": 14,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "09e80125d90fd3df",
            "authors": [
                "Ling Yang",
                "Xinchen Zhang",
                "Ye Tian",
                "Chenming Shang",
                "Minghao Xu",
                "Wentao Zhang",
                "Bin Cui"
            ],
            "affiliations": [
                "Mila - Quebec AI Institute",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12148.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "🌉",
                "ru": {
                    "title": "HermesFlow: мост между пониманием и генерацией в мультимодальных ИИ",
                    "desc": "Статья представляет новый фреймворк HermesFlow для мультимодальных больших языковых моделей (MLLM). Авторы обнаружили, что способности MLLM к пониманию обычно превосходят их генеративные возможности. HermesFlow призван устранить этот разрыв, используя гомологичные данные предпочтений для обучения. Фреймворк применяет методы Pair-DPO и итеративной оптимизации для эффективного выравнивания мультимодального понимания и генерации. Эксперименты показывают превосходство HermesFlow над существующими методами в сокращении разрыва между пониманием и генерацией в MLLM."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Understanding and Generation in MLLMs with HermesFlow",
                    "desc": "This paper discusses the advancements in Multimodal Large Language Models (MLLMs) and identifies a key issue: these models often understand information better than they can generate it. The authors introduce HermesFlow, a new framework that aims to improve the balance between understanding and generation in MLLMs. By using homologous data to create preference data for both tasks, HermesFlow employs Pair-DPO and self-play optimization to align these capabilities more effectively. Experimental results show that HermesFlow significantly reduces the performance gap between understanding and generation, suggesting its potential as a foundational model for future multimodal applications."
                },
                "zh": {
                    "title": "HermesFlow：缩小理解与生成的差距",
                    "desc": "这篇论文探讨了自回归范式在多模态大型语言模型（MLLMs）中的成功，特别是像Show-o、Transfusion和Emu3这样的模型在图像理解和生成方面的进展。研究发现，MLLMs的理解能力通常强于生成能力，两者之间存在显著差距。为了解决这个问题，论文提出了HermesFlow框架，通过使用同源数据来优化理解和生成之间的对齐。实验结果表明，HermesFlow在缩小多模态理解与生成之间的差距方面优于之前的方法，展示了其作为下一代多模态基础模型对齐框架的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11196",
            "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
            "url": "https://huggingface.co/papers/2502.11196",
            "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.",
            "score": 11,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "d97eafe0888b9da4",
            "authors": [
                "Yixin Ou",
                "Yunzhi Yao",
                "Ningyu Zhang",
                "Hui Jin",
                "Jiacheng Sun",
                "Shumin Deng",
                "Zhenguo Li",
                "Huajun Chen"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "National University of Singapore, NUS-NCS Joint Lab, Singapore",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11196.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны обучения нейросетей: эволюция цепей знаний в LLM",
                    "desc": "Это исследование посвящено изучению механизмов усвоения новых знаний в больших языковых моделях (LLM). Авторы анализируют эволюцию 'цепей знаний' - вычислительных подграфов, ответственных за хранение и обработку информации. Они обнаружили, что усвоение новых знаний зависит от их связи с уже имеющимися, а эволюция цепей знаний проходит фазы формирования и оптимизации. Результаты могут помочь улучшить стратегии непрерывного предобучения моделей."
                },
                "en": {
                    "title": "Understanding Knowledge Integration in Large Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) learn and store new knowledge within their neural networks. It introduces the concept of knowledge circuit evolution, which refers to the computational pathways that help LLMs process and retain information. The study finds that new knowledge is better integrated when it relates to what the model already knows, and that the process of knowledge circuit evolution shifts from forming new connections to optimizing existing ones. Additionally, the research reveals that this evolution tends to move from deeper to shallower layers of the model, offering insights that could improve continual pre-training methods for LLMs."
                },
                "zh": {
                    "title": "理解大型语言模型的知识内化",
                    "desc": "大型语言模型（LLMs）在知识密集型任务中表现出色，但它们在理解如何内化新知识方面存在重要缺口。本文通过知识电路演化的视角，识别出促进知识存储和处理的计算子图。我们的系统分析表明，新知识的获取受已有知识相关性的影响，知识电路的演化经历从形成到优化的明显阶段转变，并且演化模式呈现由深到浅的特征。这些发现不仅推动了我们对LLMs中新知识获取机制的理论理解，还有助于改进持续预训练策略，从而提升模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11167",
            "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
            "url": "https://huggingface.co/papers/2502.11167",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE.",
            "score": 10,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "99e183fd31de3be0",
            "authors": [
                "Bohan Lyu",
                "Siqiao Huang",
                "Zichen Liang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua",
                "Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11167.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#plp",
                    "#dataset",
                    "#agi",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM как виртуальные исполнители кода: возможности и ограничения",
                    "desc": "Исследователи представили SURGE - комплексный бенчмарк для оценки способности больших языковых моделей (LLM) предсказывать результаты выполнения кода без его фактического запуска. Бенчмарк охватывает восемь ключевых аспектов, включая задачи на разных языках программирования, анализ репозиториев и научные вычисления. Оценка различных LLM на SURGE показала, что модели могут предсказывать результаты выполнения кода в некоторых случаях, но имеют ограничения в качестве универсальных суррогатных исполнителей. Исследование дает эмпирическое представление о возможности использования LLM в качестве суррогатных исполнителей кода."
                },
                "en": {
                    "title": "Exploring LLMs as Surrogate Code Executors with SURGE",
                    "desc": "This paper explores the potential of large language models (LLMs) to act as surrogate code executors, which means predicting the output of code without actually running it. The authors introduce a benchmark called SURGE, which tests LLMs on various programming tasks, including multi-language support and analysis of complex algorithms. They evaluate different LLMs to see how well they can predict code execution results and identify common errors in their predictions. The results show that while LLMs can succeed in some scenarios, they still have significant limitations in general-purpose code execution prediction."
                },
                "zh": {
                    "title": "探索大型语言模型作为代码执行器的潜力",
                    "desc": "大型语言模型（LLMs）在代码理解和生成方面表现出色，但它们能否作为通用的替代代码执行器来预测程序的输出和行为仍然是一个重要问题。为此，我们提出了SURGE，一个涵盖八个关键方面的综合基准，包括多语言编程任务和高成本科学计算等。我们对多种开源和专有的LLMs进行了评估，并研究了模型规模和训练数据规模对替代执行准确性的影响。研究结果表明，尽管LLMs在某些情况下能够预测代码执行结果，但在通用替代执行方面仍存在局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08745",
            "title": "IHEval: Evaluating Language Models on Following the Instruction Hierarchy",
            "url": "https://huggingface.co/papers/2502.08745",
            "abstract": "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.",
            "score": 9,
            "issue_id": 2279,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "b61f4ac9281295ed",
            "authors": [
                "Zhihan Zhang",
                "Shiyang Li",
                "Zixuan Zhang",
                "Xin Liu",
                "Haoming Jiang",
                "Xianfeng Tang",
                "Yifan Gao",
                "Zheng Li",
                "Haodong Wang",
                "Zhaoxuan Tan",
                "Yichuan Li",
                "Qingyu Yin",
                "Bing Yin",
                "Meng Jiang"
            ],
            "affiliations": [
                "Amazon",
                "University of Notre Dame",
                "Worcester Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08745.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Иерархия инструкций: ахиллесова пята языковых моделей",
                    "desc": "Статья представляет новый бенчмарк IHEval для оценки способности языковых моделей следовать иерархии инструкций. Бенчмарк содержит 3,538 примеров в девяти задачах, охватывающих случаи согласованных и конфликтующих инструкций разных приоритетов. Оценка популярных ЯМ показала их трудности в распознавании приоритетов инструкций и резкое снижение производительности при конфликтующих инструкциях. Лучшая модель с открытым исходным кодом достигла лишь 48% точности в разрешении таких конфликтов."
                },
                "en": {
                    "title": "Enhancing Language Models: Prioritizing Instructions for Better Performance",
                    "desc": "This paper discusses the importance of instruction hierarchy in language models (LMs), which helps prioritize system messages, user messages, and conversation history. The authors introduce IHEval, a new benchmark with 3,538 examples across nine tasks to evaluate how well LMs follow these instruction priorities. Their findings reveal that popular LMs struggle significantly when faced with conflicting instructions, showing a notable drop in performance. The results indicate a critical need for improvements in LMs to better handle instruction hierarchies and conflicts in the future."
                },
                "zh": {
                    "title": "提升语言模型的指令理解能力",
                    "desc": "本文探讨了指令层级的重要性，指令层级从系统消息到用户消息、对话历史和工具输出建立了优先顺序。这一主题在语言模型（LMs）中受到的关注有限，缺乏全面的基准来评估模型遵循指令层级的能力。为此，我们引入了IHEval，这是一个新颖的基准，包含3,538个示例，涵盖了不同优先级指令一致或冲突的九个任务。我们的评估显示，流行的语言模型在识别指令优先级方面存在困难，尤其在面对冲突指令时，性能显著下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10458",
            "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
            "url": "https://huggingface.co/papers/2502.10458",
            "abstract": "This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the LLM decoder shares the same input feature space with diffusion decoders that use the corresponding LLM encoder for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.",
            "score": 9,
            "issue_id": 2270,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "bd036baca649b696",
            "authors": [
                "Zhenxing Mi",
                "Kuan-Chieh Wang",
                "Guocheng Qian",
                "Hanrong Ye",
                "Runtao Liu",
                "Sergey Tulyakov",
                "Kfir Aberman",
                "Dan Xu"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST)",
                "Snap Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10458.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ThinkDiff: мультимодальное рассуждение для диффузионных моделей",
                    "desc": "ThinkDiff - это новая парадигма выравнивания, которая наделяет диффузионные модели текст-изображение способностями мультимодального понимания и рассуждения в контексте, интегрируя сильные стороны моделей зрения-языка (VLM). Метод использует обучение зрению-языку как прокси-задачу, выравнивая VLM с декодером энкодер-декодерной большой языковой модели (LLM) вместо диффузионного декодера. ThinkDiff эффективно раскрывает возможности понимания, рассуждения и композиции в диффузионных моделях без сложного обучения и наборов данных. Эксперименты показывают значительное улучшение точности с 19.2% до 46.3% на сложном бенчмарке CoBSAT для генерации мультимодальных рассуждений в контексте."
                },
                "en": {
                    "title": "Empowering Diffusion Models with Multimodal Reasoning",
                    "desc": "ThinkDiff is a new approach that enhances text-to-image diffusion models by integrating vision-language models (VLMs) for better understanding and reasoning. Unlike previous methods that focused on pixel-level accuracy, ThinkDiff emphasizes in-context reasoning, overcoming challenges related to complex datasets. It aligns VLMs with the decoder of a large language model (LLM), simplifying the training process and improving performance. Experiments show that ThinkDiff significantly boosts accuracy on multimodal reasoning tasks and excels in creating coherent images from multiple inputs."
                },
                "zh": {
                    "title": "ThinkDiff：提升文本到图像的推理能力",
                    "desc": "本文提出了ThinkDiff，这是一种新颖的对齐范式，旨在通过整合视觉-语言模型（VLMs）的优势，增强文本到图像扩散模型的多模态上下文理解和推理能力。现有的多模态扩散微调方法主要关注像素级重建，而忽视了上下文推理，并受到推理基础数据集复杂性和有限性的限制。ThinkDiff通过将视觉-语言训练作为代理任务，解决了这些挑战，将VLM与编码器-解码器大型语言模型（LLM）的解码器对齐，而不是扩散解码器。实验表明，ThinkDiff在多模态上下文推理生成的CoBSAT基准测试中，准确率从19.2%显著提高到46.3%，仅需在4个A100 GPU上训练5小时。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12146",
            "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
            "url": "https://huggingface.co/papers/2502.12146",
            "abstract": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
            "score": 9,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "25a69f8cf847067e",
            "authors": [
                "Ye Tian",
                "Ling Yang",
                "Xinchen Zhang",
                "Yunhai Tong",
                "Mengdi Wang",
                "Bin Cui"
            ],
            "affiliations": [
                "Peking University",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12146.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#diffusion",
                    "#alignment",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Оптимизация траекторий для повышения точности диффузионных моделей",
                    "desc": "Авторы предлагают метод Diffusion-Sharpening для улучшения точности генеративных диффузионных моделей. Этот подход оптимизирует траектории сэмплирования во время обучения, используя интегралы по путям и обратную связь по вознаграждению. Diffusion-Sharpening демонстрирует превосходную эффективность обучения и вывода по сравнению с существующими методами. Эксперименты показывают, что предложенный метод превосходит другие подходы по различным метрикам, включая согласованность текста и предпочтения людей."
                },
                "en": {
                    "title": "Optimize Sampling Paths for Better Model Alignment!",
                    "desc": "The paper introduces Diffusion-Sharpening, a novel fine-tuning method that improves the alignment of machine learning models by optimizing the paths taken during sampling. Unlike traditional reinforcement learning (RL) methods that focus on individual training steps, this approach considers the entire trajectory, which enhances overall performance. By employing a path integral framework, Diffusion-Sharpening efficiently selects the best trajectories while minimizing inference costs. Experimental results show that this method not only converges faster but also achieves better performance than existing RL-based and trajectory optimization techniques across various evaluation metrics."
                },
                "zh": {
                    "title": "扩散锐化：高效的微调新方法",
                    "desc": "我们提出了一种名为扩散锐化（Diffusion-Sharpening）的微调方法，通过优化采样轨迹来增强下游对齐。现有的基于强化学习的微调方法主要关注单个训练时间步，忽视了轨迹级别的对齐，而最近的采样轨迹优化方法则带来了显著的推理成本。扩散锐化通过使用路径积分框架在训练过程中选择最佳轨迹，利用奖励反馈并摊销推理成本，从而克服了这些问题。我们的实验表明，扩散锐化在训练效率和推理效率上均优于现有的微调方法，提供了一种可扩展且高效的未来扩散模型微调解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09061",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "url": "https://huggingface.co/papers/2502.09061",
            "abstract": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.",
            "score": 9,
            "issue_id": 2264,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4a44947deeb14cf4",
            "authors": [
                "Debangshu Banerjee",
                "Tarun Suresh",
                "Shubham Ugare",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс между ограничениями и рассуждением в языковых моделях",
                    "desc": "Эта статья исследует проблему генерации синтаксически и семантически корректных выходных данных языковыми моделями (LLM) для задач, требующих формального рассуждения. Авторы предлагают теоретическое объяснение, почему строгое ограничение выходных данных LLM может снижать их способности к рассуждению. Они представляют алгоритм CRANE, который балансирует между корректностью ограниченной генерации и гибкостью неограниченной генерации. Эксперименты показывают, что CRANE значительно превосходит существующие методы ограниченного и неограниченного декодирования на сложных задачах символьного рассуждения."
                },
                "en": {
                    "title": "Balancing Correctness and Reasoning in LLM Outputs with CRANE",
                    "desc": "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."
                },
                "zh": {
                    "title": "平衡推理能力与生成正确性的创新解码算法",
                    "desc": "本研究探讨了如何在生成代码和符号数学推理等任务中，确保大型语言模型（LLM）输出的语法和语义正确性。我们发现，过于严格的语法约束会降低模型的推理能力。为了解决这个问题，我们提出了一种新的解码算法CRANE，通过增加精心设计的额外规则，既能保持输出的正确性，又能增强推理能力。实验结果表明，CRANE在多个基准测试中显著优于现有的解码策略，提升了符号推理任务的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11831",
            "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
            "url": "https://huggingface.co/papers/2502.11831",
            "abstract": "We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.",
            "score": 6,
            "issue_id": 2270,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "269bf0ee605c4537",
            "authors": [
                "Quentin Garrido",
                "Nicolas Ballas",
                "Mahmoud Assran",
                "Adrien Bardes",
                "Laurent Najman",
                "Michael Rabbat",
                "Emmanuel Dupoux",
                "Yann LeCun"
            ],
            "affiliations": [
                "EHESS",
                "FAIR at Meta",
                "Univ Gustave Eiffel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11831.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Интуитивная физика возникает в нейросетях без предварительного программирования",
                    "desc": "Исследователи изучали возникновение интуитивного понимания физики в нейронных сетях, обученных предсказывать скрытые области в видео. Используя метод нарушения ожиданий, они обнаружили, что модели, предсказывающие результаты в выученном пространстве представлений, демонстрируют понимание базовых физических свойств. Напротив, модели, работающие с пиксельным пространством, и мультимодальные языковые модели показали результаты близкие к случайным. Исследование показывает, что для приобретения интуитивного понимания физики достаточно совместного обучения абстрактному пространству представлений и предсказания недостающих частей сенсорного ввода."
                },
                "en": {
                    "title": "Learning Intuitive Physics Through Video Prediction",
                    "desc": "This paper explores how deep neural networks can learn intuitive physics by predicting missing parts of videos. The researchers found that models trained in a learned representation space can grasp concepts like object permanence and shape consistency. In contrast, models that operate directly on pixel data or rely on text reasoning perform poorly. The study suggests that understanding intuitive physics can emerge from training on sensory input without needing pre-existing core knowledge."
                },
                "zh": {
                    "title": "通过预测学习直观物理理解",
                    "desc": "本研究探讨了通用深度神经网络模型在预测自然视频中被遮挡区域时，如何产生直观物理理解。我们发现，经过训练的模型在学习的表示空间中预测结果时，能够理解物体的持久性和形状一致性等直观物理特性。相比之下，在像素空间中进行视频预测的模型和通过文本推理的多模态大型语言模型，其表现接近随机水平。我们的研究表明，联合学习抽象表示空间并预测感官输入的缺失部分，足以获得直观物理的理解，甚至在仅用一周独特视频训练的模型也能表现出超出随机的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11438",
            "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
            "url": "https://huggingface.co/papers/2502.11438",
            "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.",
            "score": 6,
            "issue_id": 2264,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "93b545df707b1538",
            "authors": [
                "Jimin Lee",
                "Ingeol Baek",
                "Byeongjeong Kim",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11438.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самоусиление ИИ в преобразовании текста в SQL",
                    "desc": "SAFE-SQL - это новый подход к преобразованию естественного языка в SQL-запросы. Он использует большие языковые модели для генерации и фильтрации релевантных примеров, улучшая процесс обучения в контексте. SAFE-SQL превосходит предыдущие методы в задачах zero-shot и few-shot, достигая более высокой точности выполнения запросов. Особенно эффективен в сложных и ранее не встречавшихся сценариях, где традиционные методы часто не справляются."
                },
                "en": {
                    "title": "Transforming Language to SQL with Self-Augmented Learning",
                    "desc": "This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches."
                },
                "zh": {
                    "title": "自我增强，提升Text-to-SQL的准确性",
                    "desc": "本文提出了一种新的框架SAFE-SQL，用于将自然语言问题转换为可执行的SQL查询。该框架通过自我增强的上下文学习和细粒度示例选择来提高SQL生成的质量。SAFE-SQL首先生成多个与测试输入相关的Text-to-SQL示例，然后通过三种相关性评估对这些示例进行过滤，从而构建高质量的学习示例。与传统的零样本和少样本方法相比，SAFE-SQL在执行准确性上取得了显著提升，尤其在困难和未见过的场景中表现更佳。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12135",
            "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
            "url": "https://huggingface.co/papers/2502.12135",
            "abstract": "With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate.",
            "score": 5,
            "issue_id": 2270,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "ff42a91250856a78",
            "authors": [
                "Chaoyue Song",
                "Jianfeng Zhang",
                "Xiu Li",
                "Fan Yang",
                "Yiwen Chen",
                "Zhongcong Xu",
                "Jun Hao Liew",
                "Xiaoyang Guo",
                "Fayao Liu",
                "Jiashi Feng",
                "Guosheng Lin"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute for Inforcomm Research, A*STAR",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12135.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Магия оживления 3D: от статики к реалистичной анимации",
                    "desc": "MagicArticulate - это фреймворк для автоматического преобразования статичных 3D-моделей в готовые к анимации версии. Авторы представили Articulation-XL - крупномасштабный набор данных с более чем 33 тысячами аннотированных 3D-моделей. Они предложили новый метод генерации скелета, использующий авторегрессионный трансформер для обработки различного количества костей и суставов. Для предсказания весов скиннинга применяется функциональный процесс диффузии с учетом объемных геодезических расстояний между вершинами и суставами."
                },
                "en": {
                    "title": "Transforming 3D Models for Realistic Animation with MagicArticulate",
                    "desc": "This paper introduces MagicArticulate, a framework designed to convert static 3D models into articulation-ready versions for realistic animation. It addresses the challenges of manual annotation by providing Articulation-XL, a large-scale benchmark with over 33,000 3D models and high-quality articulation annotations. The framework employs a novel skeleton generation method using an auto-regressive transformer to manage varying bone structures and their dependencies. Additionally, it utilizes a functional diffusion process to predict skinning weights, resulting in superior performance in generating articulated models compared to existing methods."
                },
                "zh": {
                    "title": "自动化3D模型关节化的革命性框架",
                    "desc": "随着3D内容创作的快速增长，自动将静态3D模型转换为可进行真实动画的关节模型的需求日益增加。传统方法依赖于手动标注，既耗时又费力，且缺乏大规模基准测试限制了基于学习的解决方案的发展。我们提出了MagicArticulate框架，能够自动将静态3D模型转化为适合关节动画的资产。我们的主要贡献包括建立了Articulation-XL基准、提出了一种新颖的骨架生成方法，并使用功能扩散过程预测蒙皮权重，显著提升了动画质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 5,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "🐣",
                "ru": {
                    "title": "Извлечение информации на плечах гигантов: как IE модели могут использовать ресурсы LLM",
                    "desc": "Исследователи представили новый подход к извлечению информации (IE) с использованием ресурсов больших языковых моделей (LLM). Метод под названием 'извлечение следующих токенов' (NTE) позволяет переформулировать задачу предсказания следующего токена в задачу извлечения уже присутствующих в контексте токенов. Модель Cuckoo, обученная на 102,6 млн примеров извлекательных данных, показывает лучшие результаты в условиях малого количества обучающих примеров по сравнению с существующими предобученными IE моделями. Этот подход позволяет IE моделям автоматически развиваться вместе с улучшениями в подготовке данных для LLM без дополнительных ручных усилий."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "利用LLM提升信息提取模型的性能",
                    "desc": "本文探讨了如何利用大型语言模型（LLM）来提升信息提取（IE）模型的性能。我们提出了一种新的提取方法，称为下一标记提取（NTE），通过将下一个标记预测转化为对上下文中已存在标记的提取，从而使IE模型能够利用LLM的资源。我们开发的Cuckoo模型在少量样本的情况下，能够有效适应传统和复杂的指令跟随IE任务，并且表现优于现有的预训练IE模型。Cuckoo作为一个“搭便车者”，能够随着LLM数据准备的进步而自然演变，无需额外的人工努力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11157",
            "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
            "url": "https://huggingface.co/papers/2502.11157",
            "abstract": "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.",
            "score": 4,
            "issue_id": 2272,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "df0803abac7073f4",
            "authors": [
                "Jianyuan Zhong",
                "Zeju Li",
                "Zhijian Xu",
                "Xiangyu Wen",
                "Qiang Xu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11157.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Dyve: умное обнаружение ошибок в ИИ-рассуждениях",
                    "desc": "Представлен Dyve - динамический верификатор процессов, улучшающий обнаружение ошибок рассуждения в больших языковых моделях. Он сочетает быстрое и медленное мышление, вдохновленное теорией Канемана о двух системах. Dyve использует новую технику пошаговой фильтрации консенсуса для создания качественных обучающих сигналов из зашумленных данных. Эксперименты на наборах данных ProcessBench и MATH показывают, что Dyve значительно превосходит существующие верификаторы на основе процессов."
                },
                "en": {
                    "title": "Dyve: Enhancing Language Model Reasoning with Dual Thinking Strategies",
                    "desc": "Dyve is a dynamic process verifier designed to improve error detection in large language models by utilizing two types of reasoning: fast (System 1) and slow (System 2) thinking. It applies quick, token-level checks for simple tasks while employing in-depth analysis for more complex processes. The system uses a unique method of step-wise consensus filtering, which combines Monte Carlo estimation with evaluations from large language models to generate reliable supervision signals from noisy data. Experiments show that Dyve outperforms current process verifiers and enhances performance in competitive settings."
                },
                "zh": {
                    "title": "Dyve：提升语言模型推理的动态验证器",
                    "desc": "Dyve是一种动态过程验证器，旨在提高大型语言模型中的推理错误检测能力。它结合了快速思维和慢速思维，灵感来源于卡尼曼的系统理论。Dyve根据任务的复杂性，灵活地应用即时的token级确认（系统1）和全面分析（系统2）。通过一种新颖的逐步共识过滤过程监督技术，Dyve从嘈杂数据中提取高质量的监督信号，实验结果表明其在多个数据集上显著优于现有的过程验证器。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11775",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "url": "https://huggingface.co/papers/2502.11775",
            "abstract": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.",
            "score": 4,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "ff6f52de37532ca7",
            "authors": [
                "Guangzhi Sun",
                "Yudong Yang",
                "Jimin Zhuang",
                "Changli Tang",
                "Yixuan Li",
                "Wei Li",
                "Zejun MA",
                "Chao Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua university",
                "Univeristy of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11775.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение рассуждений в мультимодальных языковых моделях для понимания видео",
                    "desc": "Статья представляет video-SALMONN-o1 - первую открытую аудиовизуальную языковую модель с улучшенными способностями рассуждения для общего понимания видео. Авторы разработали набор данных с интенсивными рассуждениями и метод оптимизации предпочтений процесса (pDPO) для эффективного обучения на многомодальных входных данных. Также был создан бенчмарк RivaBench для оценки способностей моделей к рассуждению при анализе видео. video-SALMONN-o1 демонстрирует значительные улучшения точности по сравнению с базовыми моделями на различных задачах понимания видео."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Enhanced Reasoning",
                    "desc": "This paper introduces video-SALMONN-o1, an innovative open-source audio-visual large language model (LLM) aimed at improving general video understanding. It addresses the gap in reasoning capabilities for video content by creating a specialized dataset with complex audio-visual questions and detailed solutions. The authors also present process direct preference optimization (pDPO), a method that enhances reward modeling for multimodal inputs through contrastive step selection. The model demonstrates significant accuracy improvements over existing benchmarks, showcasing its effectiveness in tasks like synthetic video detection without prior training."
                },
                "zh": {
                    "title": "视频理解的新突破：video-SALMONN-o1",
                    "desc": "这篇论文提出了video-SALMONN-o1，这是第一个开源的增强推理音视频大语言模型，旨在解决一般视频理解任务。为了提升其推理能力，研究团队开发了一个包含具有挑战性的音视频问题和逐步解决方案的推理密集型数据集。论文还提出了过程直接偏好优化（pDPO），利用对比步骤选择实现针对多模态输入的高效步骤级奖励建模。此外，RivaBench作为第一个推理密集型视频理解基准，提供了超过4000个高质量的问题-答案对，涵盖了多种场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 4,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты в доказательном программировании",
                    "desc": "Статья посвящена проблеме обучения языковых моделей программированию, ориентированному на доказательства. Авторы предлагают метод синтетического расширения данных для решения проблемы нехватки корпусов на языках доказательного программирования. Они создают модель PoPilot, которая превосходит GPT-4 на 64% в задачах проектного уровня. Метод также позволяет улучшить результаты GPT-4 на 54% путем исправления его выходных данных."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "合成数据增强，提升证明编程能力！",
                    "desc": "现有的语言模型在面向证明的编程中面临数据稀缺的问题，主要体现在两个方面：缺乏足够的面向证明编程语言（如F*）的语料库，以及缺少大规模的项目级证明实现，无法教会模型复杂的推理过程。我们提出了一种基于合成数据增强的方法，专注于项目级的面向证明编程，既用于生成也用于修复。该方法通过合成基本的面向证明编程问题来解决数据稀缺问题，并结合多样化的编码数据以提高推理能力，同时在现有代码库中创建新的证明和修复数据。我们的14B参数模型PoPilot经过微调后，在项目级面向证明编程中超越了GPT-4o模型64%的性能，并通过修复其输出提高了54%的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11357",
            "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
            "url": "https://huggingface.co/papers/2502.11357",
            "abstract": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.",
            "score": 3,
            "issue_id": 2277,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "26d055a62173bda2",
            "authors": [
                "Vardaan Pahuja",
                "Yadong Lu",
                "Corby Rosset",
                "Boyu Gou",
                "Arindam Mitra",
                "Spencer Whitehead",
                "Yu Su",
                "Ahmed Awadallah"
            ],
            "affiliations": [
                "Microsoft Research, Redmond",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11357.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#synthetic",
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Масштабные данные - ключ к совершенствованию веб-агентов",
                    "desc": "Статья представляет новый подход к созданию крупномасштабного набора данных для обучения мультимодальных веб-агентов. Авторы разработали масштабируемый метод синтеза более 94 тысяч успешных мультимодальных веб-траекторий, охватывающих 49 тысяч уникальных URL-адресов. На основе этого набора данных был обучен веб-агент Explorer, показавший высокую производительность на различных бенчмарках. Исследование подчеркивает важность увеличения объема данных для улучшения возможностей веб-агентов."
                },
                "en": {
                    "title": "Unlocking Web Tasks with Scalable Multimodal Datasets",
                    "desc": "This paper presents a solution to the challenge of training large multimodal models (LMMs) for web tasks by creating a vast and diverse dataset of web trajectories. The dataset includes over 94,000 successful multimodal web interactions, which were synthesized through extensive web exploration, making it cost-effective to produce. The authors introduce Explorer, a multimodal web agent trained on this dataset, which shows improved performance on various benchmarks compared to previous models. The study emphasizes the importance of data scaling in enhancing the capabilities of web agents, aiming to make advanced LMM research more accessible to the community."
                },
                "zh": {
                    "title": "合成多样化数据集，提升多模态代理能力",
                    "desc": "本文介绍了一种新方法，用于合成大型多模态模型（LMM）所需的多样化轨迹级数据集。我们创建了一个包含超过94,000个成功的多模态网络轨迹的数据集，涵盖49,000个独特的URL和720,000个截图。通过广泛的网络探索和任务意图的细化，我们能够以低成本收集多样化的数据。我们的实验表明，数据规模的扩大是提升网络代理能力的关键因素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11748",
            "title": "ILIAS: Instance-Level Image retrieval At Scale",
            "url": "https://huggingface.co/papers/2502.11748",
            "abstract": "This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/",
            "score": 3,
            "issue_id": 2277,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "59967e50364f64f1",
            "authors": [
                "Giorgos Kordopatis-Zilos",
                "Vladan Stojnić",
                "Anna Manko",
                "Pavel Šuma",
                "Nikolaos-Antonios Ypsilantis",
                "Nikos Efthymiadis",
                "Zakaria Laskar",
                "Jiří Matas",
                "Ondřej Chum",
                "Giorgos Tolias"
            ],
            "affiliations": [
                "VRG, FEE, Czech Technical University in Prague"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11748.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ILIAS: Новый стандарт для оценки масштабного поиска изображений",
                    "desc": "ILIAS - это новый набор данных для тестирования поиска изображений на уровне экземпляров в крупном масштабе. Он разработан для оценки способности современных и будущих моделей основания и методов поиска распознавать конкретные объекты. ILIAS включает запросы и положительные изображения для 1000 экземпляров объектов, собранных вручную для отражения сложных условий и разнообразных доменов. Тестирование проводится на фоне 100 миллионов отвлекающих изображений из YFCC100M, что позволяет оценить эффективность моделей в различных сценариях."
                },
                "en": {
                    "title": "ILIAS: Advancing Instance-Level Image Retrieval with Scale and Diversity",
                    "desc": "This paper presents ILIAS, a novel dataset aimed at enhancing instance-level image retrieval capabilities. It features a large-scale collection of 1,000 object instances with diverse domains and challenging conditions, evaluated against 100 million distractor images. The dataset allows for benchmarking of various models, revealing that domain-specific fine-tuning can lead to performance drops in broader contexts. Additionally, the study highlights the importance of local descriptors in retrieval tasks and notes that vision-language models perform comparably in text-to-image and image-to-image retrieval scenarios."
                },
                "zh": {
                    "title": "ILIAS：大规模实例级图像检索的新标准",
                    "desc": "本研究介绍了ILIAS，这是一个用于大规模实例级图像检索的新测试数据集。它旨在评估当前和未来基础模型及检索技术识别特定对象的能力。ILIAS的优势在于其大规模、领域多样性、准确的真实标签，以及尚未饱和的性能表现。数据集中包含1000个对象实例的查询和正样本图像，手动收集以捕捉具有挑战性的条件和多样的领域。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10550",
            "title": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.10550",
            "abstract": "Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our contributions establish a unified framework for advancing memory RL research, driving the development of more reliable systems for real-world applications. The code is available at https://sites.google.com/view/memorybenchrobots/.",
            "score": 3,
            "issue_id": 2272,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "99224cee50b48e48",
            "authors": [
                "Egor Cherepanov",
                "Nikita Kachaev",
                "Alexey K. Kovalev",
                "Aleksandr I. Panov"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "MIPT, Dolgoprudny, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10550.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#agents",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "MIKASA: универсальный бенчмарк для оценки памяти RL-агентов",
                    "desc": "Статья представляет MIKASA - новый комплексный бенчмарк для оценки возможностей памяти агентов обучения с подкреплением. Авторы предлагают классификацию задач, требующих интенсивного использования памяти, и создают набор тестов MIKASA-Base для систематической оценки агентов с улучшенной памятью. Также разработан MIKASA-Robo - набор из 32 задач для оценки памяти в манипуляциях роботов. Этот бенчмарк призван способствовать развитию исследований в области обучения с подкреплением с использованием памяти."
                },
                "en": {
                    "title": "MIKASA: Advancing Memory in Reinforcement Learning for Robotics",
                    "desc": "This paper addresses the importance of memory in reinforcement learning (RL) for agents performing complex tasks that require understanding of time and space. It highlights the lack of standardized benchmarks to evaluate memory capabilities in RL, especially in tabletop robotic manipulation scenarios. To fill this gap, the authors introduce MIKASA, a benchmark suite designed to assess memory-intensive skills in agents. MIKASA includes a classification framework, a unified benchmark called MIKASA-Base, and a set of 32 specific tasks in MIKASA-Robo to systematically evaluate memory-enhanced agents."
                },
                "zh": {
                    "title": "提升智能体记忆能力的统一基准",
                    "desc": "本文探讨了记忆在强化学习（RL）中的重要性，尤其是在处理复杂任务时。我们提出了MIKASA，一个全面的基准测试套件，用于评估智能体的记忆能力。MIKASA包括一个分类框架和两个基准，分别用于系统评估记忆增强智能体和设计32个记忆密集型任务。我们的工作为记忆强化学习研究提供了统一的框架，推动了更可靠系统的开发。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12054",
            "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
            "url": "https://huggingface.co/papers/2502.12054",
            "abstract": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.",
            "score": 3,
            "issue_id": 2269,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "4aaf92e2d2fd9766",
            "authors": [
                "Xinyu Zhang",
                "Yuxuan Dong",
                "Yanrui Wu",
                "Jiaxing Huang",
                "Chengyou Jia",
                "Basura Fernando",
                "Mike Zheng Shou",
                "Lingling Zhang",
                "Jun Liu"
            ],
            "affiliations": [
                "Institute of High-Performance Computing, A*STAR",
                "Show Lab, National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12054.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PhysReason: испытание физикой для искусственного интеллекта",
                    "desc": "Статья представляет PhysReason - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к физическому рассуждению. Бенчмарк состоит из 1200 задач разной сложности, требующих в среднем 8.1 шагов решения. Авторы предлагают систему оценки решений и выявляют основные проблемы моделей в применении физических теорем, понимании процессов, вычислениях и анализе условий. Даже лучшие модели показывают результат ниже 60% на сложных задачах."
                },
                "en": {
                    "title": "PhysReason: A New Benchmark for Physics-Based Reasoning in AI",
                    "desc": "This paper introduces PhysReason, a new benchmark designed to evaluate the physics-based reasoning abilities of large language models. It consists of 1,200 problems, with a mix of knowledge-based and reasoning-based tasks, categorized into three difficulty levels. The benchmark highlights the complexity of physics reasoning, requiring multiple solution steps, with hard problems demanding an average of 15.6 steps. The study also identifies key challenges in physics reasoning, such as theorem application and process understanding, providing insights into the limitations of current models."
                },
                "zh": {
                    "title": "物理推理能力的新基准",
                    "desc": "大型语言模型在数学和逻辑推理方面表现出色，但在物理推理方面的评估仍然不足。我们提出了PhysReason，这是一个包含1200个问题的基准测试，其中75%是推理类问题，分为简单、中等和困难三个难度级别。通过引入物理解题自动评分框架，我们能够有效评估模型在物理定理应用、过程理解、计算和条件分析等方面的能力。我们的研究表明，当前顶尖模型在物理推理任务上的表现仍有待提高，尤其是在困难问题上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11330",
            "title": "System Message Generation for User Preferences using Open-Source Models",
            "url": "https://huggingface.co/papers/2502.11330",
            "abstract": "System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.",
            "score": 3,
            "issue_id": 2267,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "36ca5a9ceb25e7fa",
            "authors": [
                "Minbyul Jeong",
                "Jungho Cho",
                "Minsoo Khang",
                "Dawoon Jung",
                "Teakgyu Hong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.11330.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SysGen: улучшение соответствия ответов LLM через генерацию системных сообщений",
                    "desc": "Эта статья представляет SysGen - новый метод для генерации системных сообщений для больших языковых моделей (LLM). SysGen создает системные сообщения, которые лучше соответствуют ответам ассистента, используя наборы данных для обучения с учителем без исходных системных сообщений. Обучение на данных SysGen значительно улучшило соответствие ответов модели системным сообщениям и инструкциям пользователя, что было продемонстрировано на различных моделях с открытым исходным кодом. Качественный анализ подчеркивает важность разнообразных системных сообщений для лучшей адаптации к различным контекстам."
                },
                "en": {
                    "title": "Enhancing LLM Responses with SysGen: Better System Messages for Better Alignment",
                    "desc": "This paper presents SysGen, a new method for generating system messages that help large language models (LLMs) respond more accurately to user instructions. System messages are crucial for guiding LLMs in their interactions, but there is a lack of publicly available data that includes these messages. SysGen addresses this issue by creating a pipeline that generates system messages from existing supervised fine-tuning datasets, leading to improved alignment of model responses. The results show that training with SysGen data enhances the performance of various open-source models while keeping their effectiveness on other benchmarks largely unchanged."
                },
                "zh": {
                    "title": "SysGen：提升语言模型响应对齐性的系统消息生成",
                    "desc": "本论文介绍了SysGen，一个用于生成系统消息的管道，旨在提高大型语言模型（LLMs）与用户指令的对齐度。系统消息在与LLMs的交互中起着重要作用，能够帮助用户指定角色和任务。通过在没有系统消息的监督微调数据集上进行训练，SysGen显著改善了模型响应的对齐性。我们的分析表明，多样化的系统消息对于在不同上下文中实现更好的适应性至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11098",
            "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2502.11098",
            "abstract": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.",
            "score": 3,
            "issue_id": 2265,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "9aaac2e7c7b495a4",
            "authors": [
                "Zhao Wang",
                "Sota Moriyama",
                "Wei-Yao Wang",
                "Briti Gangopadhyay",
                "Shingo Takamatsu"
            ],
            "affiliations": [
                "Sony Group Corporation, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Структурированное общение и иерархические действия для эффективного взаимодействия ИИ-агентов",
                    "desc": "В статье представлена новая система TalkHier для улучшения взаимодействия между агентами на основе больших языковых моделей (LLM). TalkHier вводит структурированный протокол коммуникации и иерархическую систему уточнения для решения проблем неверных выводов и предвзятости. Система превосходит современные методы в различных задачах, включая ответы на вопросы и генерацию рекламных текстов. TalkHier демонстрирует потенциал для установления нового стандарта в многоагентных системах на основе LLM."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Collaboration with TalkHier Framework",
                    "desc": "This paper introduces Talk Hierarchically, Act Structurally (TalkHier), a new framework designed to improve communication and collaboration among multi-agent systems using large language models (LLMs). It features a structured communication protocol that enhances context understanding and a hierarchical refinement system to correct errors and biases in agent outputs. The framework outperforms existing state-of-the-art models in various tasks, demonstrating its effectiveness in open-domain question answering and targeted text generation. Overall, TalkHier aims to establish a new benchmark for LLM-based multi-agent systems, promoting better teamwork and adaptability among agents."
                },
                "zh": {
                    "title": "结构化交流，分层行动的智能体协作新标准",
                    "desc": "本文提出了一种新的框架，称为Talk Structurally, Act Hierarchically（TalkHier），旨在改善大语言模型（LLM）多智能体系统中的通信和协作。该框架引入了一种结构化的通信协议，以便在复杂任务中进行丰富的上下文交流，并建立了一个分层的精炼系统，以解决错误输出、虚假信息和偏见等问题。实验结果表明，TalkHier在多个任务上超越了现有的最先进技术，包括开放领域问答和特定领域选择性提问等。该研究为LLM-MA系统设定了新的标准，推动了更有效、灵活和协作的多智能体框架的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10454",
            "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
            "url": "https://huggingface.co/papers/2502.10454",
            "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of \"proof by counterexamples\" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.",
            "score": 3,
            "issue_id": 2265,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "1821254437fc158d",
            "authors": [
                "Yinghui Li",
                "Jiayi Kuang",
                "Haojing Huang",
                "Zhikun Xu",
                "Xinnian Liang",
                "Yi Yu",
                "Wenlian Lu",
                "Yangning Li",
                "Xiaoyu Tan",
                "Chao Qu",
                "Ying Shen",
                "Hai-Tao Zheng",
                "Philip S. Yu"
            ],
            "affiliations": [
                "ARC Lab, Arizona State University",
                "Bytedance Inc.",
                "INFLY TECH (Shanghai) Co., Ltd.",
                "Peng Cheng Laboratory",
                "School of Mathematical Science, Fudan University",
                "Sun-Yat Sen University",
                "Tsinghua University",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10454.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Контрпримеры как ключ к улучшению математических способностей ИИ",
                    "desc": "Статья исследует способность больших языковых моделей (LLM) генерировать математические доказательства. Авторы утверждают, что текущие LLM ограничены в глубоком понимании теорем и предлагают метод обучения на контрпримерах. Они создали бенчмарк CounterMATH для оценки способности LLM доказывать утверждения через контрпримеры. Эксперименты показывают, что улучшение навыков рассуждения на основе контрпримеров критически важно для повышения общих математических способностей LLM."
                },
                "en": {
                    "title": "Enhancing LLMs' Mathematical Proofs through Counterexamples",
                    "desc": "This paper discusses the limitations of current Large Language Models (LLMs) in generating mathematical proofs, emphasizing their dependence on prior exposure to proof processes during training. The authors introduce a new benchmark called CounterMATH, which challenges LLMs to prove mathematical statements by providing counterexamples, thereby testing their understanding of mathematical concepts. They also present a data engineering framework to enhance the training data for LLMs, aiming to improve their reasoning capabilities. The findings suggest that enhancing counterexample-driven reasoning is essential for advancing the mathematical proficiency of LLMs."
                },
                "zh": {
                    "title": "通过反例提升数学推理能力",
                    "desc": "本论文探讨了利用大型语言模型（LLMs）进行数学证明生成的能力。我们认为，当前LLMs的证明能力主要依赖于其在训练过程中是否接触过相关的证明过程，这限制了它们对数学定理和相关概念的深入理解。我们提出了一种基于反例的证明方法，旨在通过反例增强LLMs的数学推理和证明能力。为此，我们手动创建了一个高质量的数学基准CounterMATH，以评估LLMs在提供反例时的数学概念掌握情况。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11085",
            "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
            "url": "https://huggingface.co/papers/2502.11085",
            "abstract": "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.",
            "score": 2,
            "issue_id": 2270,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "de635e01a182309d",
            "authors": [
                "Yasir Ghunaim",
                "Hasan Abed Al Kader Hammoud",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11085.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Качество важнее количества в предобучении моделей для предсказания атомных свойств",
                    "desc": "Эта статья ставит под сомнение современную парадигму в предсказании атомных свойств, связывающую прогресс с увеличением размеров датасетов и вычислительных ресурсов. Авторы демонстрируют, что предобучение на тщательно отобранном, релевантном задаче датасете может сравниться или даже превзойти крупномасштабное предобучение, используя всего 1/24 вычислительных затрат. Они вводят новую метрику - Индекс Химического Сходства (CSI), вдохновленную расстоянием Фреше в компьютерном зрении, для молекулярных графов. Исследование показывает, что модели, предобученные на меньшем, но целевом датасете, стабильно превосходят модели, предобученные на огромных смешанных датасетах."
                },
                "en": {
                    "title": "Quality Over Quantity in Atomic Property Prediction",
                    "desc": "This paper argues that the success of predicting atomic properties does not solely depend on having large datasets and powerful computing resources. Instead, it demonstrates that pretraining on a well-chosen, relevant dataset can achieve better results while using significantly less computational power. The authors introduce the Chemical Similarity Index (CSI), a new metric that measures how well the pretraining dataset aligns with the specific task. Their research shows that using a focused dataset can lead to superior model performance compared to using larger, less relevant datasets, emphasizing that the quality of data is more important than its quantity."
                },
                "zh": {
                    "title": "质量胜于数量：原子属性预测的新视角",
                    "desc": "这篇论文挑战了原子属性预测领域的传统观念，认为进步与数据集规模和计算资源的增加有关。我们展示了在精心选择的、与任务相关的数据集上进行预训练，可以匹配甚至超越大规模预训练，同时计算成本仅为1/24。我们引入了化学相似性指数（CSI），这是一个新颖的指标，用于量化上游预训练数据集与下游任务之间的对齐程度。研究结果表明，选择最相关的数据集可以显著提高模型性能，强调了在原子属性预测中，质量往往优于数量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09509",
            "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling",
            "url": "https://huggingface.co/papers/2502.09509",
            "abstract": "Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.",
            "score": 1,
            "issue_id": 2280,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "d034cfa8688142a4",
            "authors": [
                "Theodoros Kouzelis",
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Nikos Komodakis"
            ],
            "affiliations": [
                "Archimedes, Athena RC, Greece",
                "IACM-Forth, Greece",
                "National Technical University of Athens, Greece",
                "University of Crete, Greece",
                "valaio.ai, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09509.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эквивариантные автоэнкодеры для улучшения генеративных моделей изображений",
                    "desc": "Статья представляет EQ-VAE - новый подход к регуляризации автоэнкодеров для генеративных моделей изображений. Метод обеспечивает эквивариантность латентного пространства к семантически-сохраняющим преобразованиям, таким как масштабирование и вращение. EQ-VAE улучшает производительность современных генеративных моделей, включая DiT, SiT, REPA и MaskGIT. Предложенный метод совместим как с непрерывными, так и с дискретными автоэнкодерами."
                },
                "en": {
                    "title": "Enhancing Image Synthesis with EQ-VAE: Simplifying Latent Spaces for Better Generative Performance",
                    "desc": "This paper introduces EQ-VAE, a novel regularization technique designed to improve latent generative models used for image synthesis. The authors highlight that traditional autoencoders struggle with maintaining equivariance to transformations like scaling and rotation, which complicates the latent space and affects generative quality. By enforcing equivariance in the latent space, EQ-VAE simplifies the structure while preserving the quality of image reconstruction. The results show significant performance improvements in various state-of-the-art generative models, demonstrating EQ-VAE's effectiveness and versatility across different types of autoencoders."
                },
                "zh": {
                    "title": "EQ-VAE：提升潜在生成模型性能的关键",
                    "desc": "潜在生成模型已成为高质量图像合成的主要方法。这些模型使用自编码器将图像压缩到潜在空间，然后通过生成模型学习潜在分布。我们发现现有的自编码器在语义保持变换（如缩放和旋转）方面缺乏等变性，导致复杂的潜在空间，影响生成性能。为了解决这个问题，我们提出了EQ-VAE，这是一种简单的正则化方法，可以在潜在空间中强制等变性，从而降低复杂性而不降低重建质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09969",
            "title": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning",
            "url": "https://huggingface.co/papers/2502.09969",
            "abstract": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.",
            "score": 1,
            "issue_id": 2278,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "6530066ad48b1daf",
            "authors": [
                "Ishika Agarwal",
                "Dilek Hakkani-Tür"
            ],
            "affiliations": [
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09969.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#small_models"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективная оценка влияния данных для языковых моделей с помощью малых нейросетей",
                    "desc": "Статья представляет новый метод оценки влияния данных на языковые модели, называемый InfluenceNetwork. Этот подход использует небольшие нейронные сети для оценки значений влияния, что позволяет сократить вычислительные затраты до 99%. Авторы демонстрируют, что модели размером всего 0,0027% от полноразмерных языковых моделей могут эффективно оценивать влияние. Метод NN-CIFT применяется для выбора подмножества данных при дообучении инструкций, показывая производительность на уровне современных методов функций влияния при значительном ускорении."
                },
                "en": {
                    "title": "Efficient Influence Estimation with InfluenceNetwork",
                    "desc": "This paper introduces the InfluenceNetwork, a small neural network designed to efficiently estimate influence values in model training. Traditional methods are computationally expensive and struggle with large datasets, but the InfluenceNetwork achieves up to a 99% reduction in costs. The proposed method, NN-CIFT, allows for effective subset selection in instruction fine-tuning without sacrificing performance compared to existing influence functions. The authors also provide a detailed analysis of hyperparameters to optimize the performance of their approach."
                },
                "zh": {
                    "title": "小型神经网络实现高效影响估计",
                    "desc": "影响函数在模型训练中提供了重要的见解，但现有方法计算成本高且泛化能力有限。本文提出了一种小型神经网络，称为影响网络（InfluenceNetwork），用于估计影响值，成本降低高达99%。我们的方法（NN-CIFT）在选择子集进行指令微调的下游任务中表现良好，且与传统影响函数相比，性能没有妥协。通过对超参数的深入分析，我们证明了小模型也能有效估计影响值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08820",
            "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model",
            "url": "https://huggingface.co/papers/2502.08820",
            "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks.",
            "score": 1,
            "issue_id": 2274,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "f3f1c79c06903edb",
            "authors": [
                "Emre Can Acikgoz",
                "Jeremiah Greer",
                "Akul Datta",
                "Ze Yang",
                "William Zeng",
                "Oussama Elachqar",
                "Emmanouil Koukoumidis",
                "Dilek Hakkani-Tür",
                "Gokhan Tur"
            ],
            "affiliations": [
                "Oumi",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08820.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#agi",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "CALM: универсальный подход к созданию разговорных агентов нового поколения",
                    "desc": "Эта статья представляет новый подход CALM (Conversational Agentic Language Model), объединяющий возможности диалоговых систем и языковых агентов. Авторы создали многозадачный датасет CALM-IT, сочетающий многоходовые рассуждения ReAct с использованием сложных API. На его основе обучены модели CALM 8B, 70B и 405B, превосходящие специализированные модели на трех бенчмарках: MultiWOZ 2.4, BFCL V3 и API-Bank. CALM решает проблему разрыва между системами задачно-ориентированного диалога (TOD) и языковыми агентами (LA), объединяя их сильные стороны."
                },
                "en": {
                    "title": "CALM: Bridging the Gap in Conversational AI",
                    "desc": "This paper discusses the development of CALM, a new model that combines the strengths of Language Agents (LA) and task-oriented dialogue (TOD) systems. Traditional TOD systems struggle with limited API training and maintaining user intent over multiple interactions, while LAs lack robust multi-turn management. The authors introduce CALM-IT, a multi-task dataset that integrates reasoning and API usage, allowing for better training of conversational agents. The results show that CALM models significantly outperform existing specialized models on key benchmarks, demonstrating the effectiveness of this unified approach."
                },
                "zh": {
                    "title": "CALM：对话与代理能力的统一模型",
                    "desc": "本文介绍了一种新的对话代理模型CALM（Conversational Agentic Language Model），旨在解决传统任务导向对话系统（TOD）和语言代理（LA）在多轮对话管理和功能调用方面的不足。当前的系统通常在有限的目标API上训练，导致在与新服务交互时需要新的数据来维持质量，而语言代理则未能有效保持用户意图。我们通过创建CALM-IT数据集，将多轮推理与复杂API使用相结合，训练了三种不同规模的CALM模型，结果显示这些模型在多个基准测试中超越了现有的领域特定模型。该研究表明，CALM模型能够有效整合对话能力和代理能力，推动对话系统的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08441",
            "title": "Better Embeddings with Coupled Adam",
            "url": "https://huggingface.co/papers/2502.08441",
            "abstract": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.",
            "score": 1,
            "issue_id": 2271,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "4357e7dc6b15b5b7",
            "authors": [
                "Felix Stollenwerk",
                "Tobias Stollenwerk"
            ],
            "affiliations": [
                "AI Sweden",
                "Forschungszentrum Jülich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08441.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Улучшение вложений слов с помощью Coupled Adam",
                    "desc": "Статья исследует проблему анизотропии в представлениях слов, обучаемых большими языковыми моделями (LLM). Авторы утверждают, что второй момент в оптимизаторе Adam является причиной анизотропных вложений. Они предлагают модифицированный оптимизатор под названием Coupled Adam для смягчения этой проблемы. Эксперименты показывают, что Coupled Adam значительно улучшает качество вложений и приводит к лучшим результатам как на этапе обучения, так и при решении последующих задач на достаточно больших наборах данных."
                },
                "en": {
                    "title": "Enhancing Embedding Quality with Coupled Adam",
                    "desc": "This paper addresses the issue of anisotropic embeddings in large language models (LLMs), which can negatively impact their performance. The authors identify that the second moment in the Adam optimizer contributes to this anisotropy. To counteract this, they propose a new optimizer called Coupled Adam, which modifies the way embeddings are learned. Experimental results show that Coupled Adam not only enhances the quality of embeddings but also improves performance in both upstream and downstream tasks when applied to sufficiently large datasets."
                },
                "zh": {
                    "title": "改进优化器，提升嵌入质量",
                    "desc": "尽管大型语言模型（LLMs）具有出色的能力，但它们学习的词表示存在一种不理想且尚未充分理解的特征，即各向异性。本文认为，Adam优化器中的二阶矩是导致各向异性嵌入的原因，并提出了一种名为Coupled Adam的改进优化器来缓解这一问题。我们的实验表明，Coupled Adam显著提高了嵌入的质量，同时在足够大的数据集上也提升了上游和下游的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09083",
            "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
            "url": "https://huggingface.co/papers/2502.09083",
            "abstract": "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",
            "score": 1,
            "issue_id": 2270,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "ecf7018a52bbede5",
            "authors": [
                "Greta Warren",
                "Irina Shklovski",
                "Isabelle Augenstein"
            ],
            "affiliations": [
                "Linköping University, Linköping, Sweden",
                "University of Copenhagen, Copenhagen, Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09083.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#ethics",
                    "#healthcare",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Объяснимая автоматизация для эффективной проверки фактов",
                    "desc": "Статья посвящена актуальной проблеме автоматизированной проверки фактов в эпоху больших языковых моделей и генеративного ИИ. Авторы провели интервью с профессиональными фактчекерами, чтобы понять, как они оценивают доказательства и принимают решения. Исследование выявило потребность в объяснимых системах автоматизированной проверки фактов, которые могли бы интегрироваться в рабочие процессы фактчекеров. Результаты показывают необходимость в прозрачных объяснениях, отражающих ход рассуждений модели, ссылающихся на конкретные доказательства и указывающих на неопределенности и пробелы в информации."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Automated Fact-Checking with Human-Centric Explanations",
                    "desc": "This paper addresses the growing challenge of misinformation in online media by exploring the role of automated fact-checking systems. It highlights the necessity for these systems to provide clear explanations that align with how human fact-checkers evaluate evidence and make decisions. Through interviews with fact-checking professionals, the study identifies specific requirements for explanations that can enhance the integration of automated tools into fact-checking workflows. The findings reveal critical gaps in current explanation practices and suggest criteria for developing more effective automated fact-checking solutions."
                },
                "zh": {
                    "title": "提升自动化事实核查的解释能力",
                    "desc": "这篇论文探讨了大型语言模型和生成性人工智能在在线媒体中的广泛应用，强调了自动化事实核查的必要性，以帮助核查员应对日益增加和复杂化的虚假信息。研究通过与事实核查专业人士的半结构化访谈，分析了核查员如何评估证据、做出决策以及解释他们的过程。论文还考察了核查员在实践中如何使用自动化工具，并识别了他们对自动化事实核查工具的解释需求。研究结果显示，核查员在解释方面存在未满足的需求，并确定了可复制的事实核查解释的重要标准，包括追踪模型的推理路径、引用具体证据以及突出不确定性和信息缺口。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08826",
            "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2502.08826",
            "abstract": "Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
            "score": 0,
            "issue_id": 2279,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "e299bbaebf315923",
            "authors": [
                "Mohammad Mahdi Abootorabi",
                "Amirhosein Zobeiri",
                "Mahdi Dehghani",
                "Mohammadali Mohammadkhani",
                "Bardia Mohammadi",
                "Omid Ghahroodi",
                "Mahdieh Soleymani Baghshah",
                "Ehsaneddin Asgari"
            ],
            "affiliations": [
                "College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran",
                "Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran",
                "Computer Engineering Department, Sharif University of Technology, Tehran, Iran",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08826.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rag",
                    "#multimodal",
                    "#survey",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мультимодальный RAG: Новый фронтир в обогащении языковых моделей",
                    "desc": "Эта статья представляет собой обзор систем мультимодального поиска и генерации (Multimodal RAG), которые интегрируют внешнюю динамическую информацию из различных модальностей для улучшения работы больших языковых моделей. Авторы анализируют методологии, наборы данных, метрики и инновации в области мультимодального RAG. Рассматриваются уникальные проблемы кросс-модального выравнивания и рассуждения, отличающие мультимодальный RAG от традиционного одномодального. Статья также обсуждает открытые вызовы и будущие направления исследований в этой развивающейся области."
                },
                "en": {
                    "title": "Enhancing AI with Multimodal Retrieval-Augmented Generation",
                    "desc": "This paper discusses the limitations of Large Language Models (LLMs) in handling hallucinations and outdated information due to their static training data. It introduces Retrieval-Augmented Generation (RAG) as a solution that incorporates external, dynamic information to improve the accuracy and relevance of generated content. The paper further explores Multimodal RAG, which combines various data types like text, images, and audio to enhance output quality, while addressing the challenges of cross-modal alignment and reasoning. It provides a comprehensive analysis of methodologies, evaluation metrics, and future research directions to advance the development of more reliable AI systems that utilize multimodal knowledge effectively."
                },
                "zh": {
                    "title": "多模态RAG：提升生成能力的未来之路",
                    "desc": "大型语言模型（LLMs）在处理幻觉和过时知识方面存在困难，因为它们依赖于静态训练数据。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，从而增强事实和更新的基础。最近的多模态学习进展导致了多模态RAG的发展，结合了文本、图像、音频和视频等多种模态，以增强生成的输出。然而，跨模态对齐和推理为多模态RAG带来了独特的挑战，使其与传统的单模态RAG有所不同。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11177",
            "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
            "url": "https://huggingface.co/papers/2502.11177",
            "abstract": "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.",
            "score": 0,
            "issue_id": 2273,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "d72ce28b4092ab0f",
            "authors": [
                "Wanli Yang",
                "Fei Sun",
                "Jiajun Tan",
                "Xinyu Ma",
                "Qi Cao",
                "Dawei Yin",
                "Huawei Shen",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Baidu Inc.",
                "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS",
                "Huawei",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11177.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Переосмысление редактирования языковых моделей: от иллюзии к реальности",
                    "desc": "Статья посвящена исследованию эффективности редактирования моделей в задачах вопросно-ответных систем. Авторы предлагают новый бенчмарк QAEdit и стандартизированную систему оценки для более точного измерения производительности методов редактирования. Результаты показывают, что существующие методы работают значительно хуже, чем сообщалось ранее, в основном из-за проблем в практиках оценки. Исследование выявляет ключевые проблемы и предлагает рекомендации для улучшения надежности и практичности методов редактирования моделей."
                },
                "en": {
                    "title": "Rethinking Model Editing: Bridging Theory and Real-World Performance",
                    "desc": "This paper investigates the effectiveness of model editing techniques in question answering (QA) systems, particularly focusing on large language models (LLMs). The authors introduce QAEdit, a new benchmark and evaluation framework to rigorously assess how well these editing methods correct errors in LLMs. Their findings reveal that current editing methods perform significantly worse in real-world scenarios than previously reported, highlighting flaws in past evaluation practices. By simulating real-world conditions, they show that existing approaches struggle with even a small number of edits, prompting a reevaluation of model editing methods and their assessment."
                },
                "zh": {
                    "title": "重新审视模型编辑的有效性与评估方法",
                    "desc": "本论文探讨了模型编辑在问答系统中的有效性，尤其是在真实应用中的表现。我们提出了QAEdit，一个新的基准测试，旨在评估现有编辑方法在纠正大型语言模型（LLM）错误时的效果。实验结果显示，当前的编辑方法在实际应用中的表现远低于之前的报告，只有38.5%的准确率。通过模块分析和控制实验，我们发现评估实践中的问题导致了这种性能下降，并提出了一个严格的评估框架，以推动模型编辑研究的可靠性和实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11574",
            "title": "Large Language Models and Mathematical Reasoning Failures",
            "url": "https://huggingface.co/papers/2502.11574",
            "abstract": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.",
            "score": 0,
            "issue_id": 2268,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "478a2c4575e67b28",
            "authors": [
                "Johan Boye",
                "Birger Moell"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11574.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Раскрывая ограничения математического мышления ИИ",
                    "desc": "В статье исследуются способности крупных языковых моделей (LLM) к математическим рассуждениям на основе 50 новых задач уровня старшей школы. Авторы анализируют не только правильность ответов, но и ход решения, выявляя ошибки в рассуждениях. Оценка восьми современных моделей показала, что даже при улучшении точности ответов, все модели демонстрируют ошибки в пространственном мышлении, стратегическом планировании и арифметике. Результаты подчеркивают важность оценки процесса рассуждений, а не только ответов, и указывают на необходимость улучшения способностей LLM к структурированному мышлению и обработке ограничений."
                },
                "en": {
                    "title": "Evaluating Reasoning, Not Just Answers in LLMs",
                    "desc": "This paper examines how well large language models (LLMs) can solve high-school-level math word problems by focusing on their reasoning abilities. It evaluates eight advanced models, revealing that while some newer models show better accuracy, they still struggle with spatial reasoning, strategic planning, and arithmetic. The analysis identifies common reasoning failures, such as making incorrect assumptions and having difficulty with multi-step deductions. The findings stress the importance of assessing the reasoning process in addition to the final answers, highlighting the need for improvements in LLMs' structured reasoning skills."
                },
                "zh": {
                    "title": "评估推理过程，超越答案正确性",
                    "desc": "本论文研究了大型语言模型（LLMs）在数学推理方面的能力，使用了50个新构建的高中水平的文字问题。与以往只关注答案正确性的研究不同，我们严格分析了最终答案和解决步骤，以识别推理失败。评估了包括Mixtral、Llama、Gemini、GPT-4o和OpenAI的o1变体在内的八个最先进模型，发现尽管新模型（如o3-mini、deepseek-r1）在准确性上更高，但所有模型在空间推理、战略规划和算术方面都存在错误。我们的结果强调了评估推理过程的重要性，而不仅仅是答案，并警告不要高估LLMs的解决问题能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11578",
            "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
            "url": "https://huggingface.co/papers/2502.11578",
            "abstract": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.",
            "score": 0,
            "issue_id": 2268,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "039b4ffb618ff3b1",
            "authors": [
                "Birger Moell",
                "Johan Boye"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11578.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#science"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Измерение сложности языка как индикатор возможностей языковых моделей",
                    "desc": "Статья исследует способность современных языковых моделей (LLM) выполнять задачи измерения сложности языка, в частности вычисление метрики читабельности LIX и среднего расстояния зависимостей (ADD). Эксперименты проводились на шведских эссе уровня старшей школы и университета. Результаты показывают, что модель ChatGPT-o1-mini демонстрирует наилучшую производительность в обеих задачах. Обнаружена сильная корреляция между точностью вычисления LIX и общей производительностью моделей на бенчмарке MMLU, что предполагает возможность использования задач измерения сложности языка как прокси для оценки общих возможностей LLM."
                },
                "en": {
                    "title": "Evaluating Language Complexity as a Proxy for LLM Performance",
                    "desc": "This paper explores how well large language models (LLMs) can measure language complexity using specific metrics like the LIX readability score and Average Dependency Distance (ADD). The authors tested these models on Swedish essays from high school and university students to see how accurately they could compute LIX scores and perform dependency parsing. The results showed that while all models had some success, ChatGPT-o1-mini was the most reliable, achieving the best accuracy in both tasks. Furthermore, a strong correlation was found between the models' LIX computation accuracy and their performance on the MMLU benchmark, indicating that language complexity measurements can be useful for evaluating LLM capabilities without needing extensive datasets."
                },
                "zh": {
                    "title": "语言复杂性测量：评估大型语言模型的新方法",
                    "desc": "本文研究了大型语言模型（LLMs）在语言复杂性测量任务中的表现，特别是计算LIX可读性指标和平均依赖距离（ADD）。我们使用瑞典高中和大学的论文来评估模型计算LIX分数和进行依赖解析的能力，并将结果与已建立的基准进行比较。研究发现，尽管所有模型在这些任务上都有一定能力，但ChatGPT-o1-mini在LIX计算和依赖解析中表现最为一致，准确率最高。此外，我们观察到模型在计算LIX时的准确性与其在大规模多任务语言理解（MMLU）基准上的整体表现之间存在显著的负相关关系。"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2月19日"
    },
    "categories": {
        "#dataset": 20,
        "#data": 8,
        "#benchmark": 19,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 2,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 10,
        "#math": 4,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 22,
        "#robotics": 2,
        "#agi": 3,
        "#games": 1,
        "#interpretability": 4,
        "#reasoning": 12,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 22,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 6,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了人形机器人自动跌倒恢复的重要性。设计控制器让机器人站起来很难，因为跌倒后机器人可能处于各种姿态，且地形复杂。文章提出了一个学习框架，让机器人在不同姿态和地形下站起来。框架分两阶段，先找到站起来的路径，再优化动作使其平稳可靠。实验中，机器人在不同地面和姿态下成功站起来，这是首次在真实环境中成功应用的学习方法。",
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "pinyin": "这篇文章讨论了人形机器人自动跌倒恢复的重要性。\nZhè piān wénzhāng tǎolùn le rénxíng jīqìrén zìdòng diēdǎo huīfù de zhòngyàoxìng.\n\n设计控制器让机器人站起来很难，因为跌倒后机器人可能处于各种姿态，且地形复杂。\nShèjì kòngzhìqì ràng jīqìrén zhàn qǐlái hěn nán, yīnwèi diēdǎo hòu jīqìrén kěnéng chǔyú gèzhǒng zītài, qiě dìxíng fùzá.\n\n文章提出了一个学习框架，让机器人在不同姿态和地形下站起来。\nWénzhāng tíchū le yīgè xuéxí kuàngjià, ràng jīqìrén zài bùtóng zītài hé dìxíng xià zhàn qǐlái.\n\n框架分两阶段，先找到站起来的路径，再优化动作使其平稳可靠。\nKuàngjià fēn liǎng jiēduàn, xiān zhǎo dào zhàn qǐlái de lùjìng, zài yōuhuà dòngzuò shǐ qí píngwěn kěkào.\n\n实验中，机器人在不同地面和姿态下成功站起来，这是首次在真实环境中成功应用的学习方法。\nShíyàn zhōng, jīqìrén zài bùtóng dìmiàn hé zītài xià chénggōng zhàn qǐlái, zhè shì shǒucì zài zhēnshí huánjìng zhōng chénggōng yìngyòng de xuéxí fāngfǎ.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '人形机器人', 'pinyin': 'rén xíng jī qì rén', 'trans': 'humanoid robot'},\n{'word': '自动', 'pinyin': 'zì dòng', 'trans': 'automatic'},\n{'word': '跌倒', 'pinyin': 'diē dǎo', 'trans': 'fall down'},\n{'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'},\n{'word': '控制器', 'pinyin': 'kòng zhì qì', 'trans': 'controller'},\n{'word': '站起来', 'pinyin': 'zhàn qǐ lái', 'trans': 'stand up'},\n{'word': '姿态', 'pinyin': 'zī tài', 'trans': 'posture'},\n{'word': '地形', 'pinyin': 'dì xíng', 'trans': 'terrain'},\n{'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'},\n{'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'},\n{'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'},\n{'word': '平稳', 'pinyin': 'píng wěn', 'trans': 'stable'},\n{'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '地面', 'pinyin': 'dì miàn', 'trans': 'ground'},\n{'word': '成功', 'pinyin': 'chéng gōng', 'trans': 'success'},\n{'word': '首次', 'pinyin': 'shǒu cì', 'trans': 'first time'},\n{'word': '真实', 'pinyin': 'zhēn shí', 'trans': 'real'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]",
        "trans": "This article discusses the importance of automatic fall recovery for humanoid robots. Designing a controller to make a robot stand up after a fall is challenging because the robot may be in various postures and the terrain can be complex. The article proposes a learning framework that enables the robot to stand up from different postures and terrains. The framework consists of two stages: first, finding a path to stand up, and then optimizing the actions to make them smooth and reliable. In experiments, the robot successfully stood up from various surfaces and postures, marking the first successful application of a learning method in a real-world environment.",
        "update_ts": "2025-02-18 09:11"
    }
}