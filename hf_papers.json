{
    "date": {
        "ru": "31 Ğ¸ÑĞ»Ñ",
        "en": "July 31",
        "zh": "7æœˆ31æ—¥"
    },
    "time_utc": "2025-07-31 05:20",
    "weekday": 3,
    "issue_id": 5104,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.22827",
            "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
            "url": "https://huggingface.co/papers/2507.22827",
            "abstract": "A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder.",
            "score": 33,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "a09c860f7fa98aea",
            "authors": [
                "Yilei Jiang",
                "Yaozhi Zheng",
                "Yuxuan Wan",
                "Jiaming Han",
                "Qunzhong Wang",
                "Michael R. Lyu",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "2ARISE Lab",
                "CUHK 1MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22827.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#interpretability",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ² ĞºĞ¾Ğ´ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ HTML/CSS ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Transforming UI Designs into Code with Modular Intelligence",
                    "desc": "This paper presents a modular multi-agent framework designed to enhance the process of converting user interface (UI) designs into front-end code. It integrates vision-language models to accurately identify UI components, employs hierarchical layout planning to organize these components, and utilizes adaptive prompt-based synthesis for code generation. By breaking down the UI-to-code generation into three distinct stagesâ€”grounding, planning, and generationâ€”the framework improves the robustness and interpretability of the output compared to traditional end-to-end methods. The authors also introduce a scalable data engine that generates large datasets of image-code pairs, which are used to fine-tune a vision-language model, resulting in improved performance in layout accuracy and code quality."
                },
                "zh": {
                    "title": "æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡UIåˆ°ä»£ç ç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå°†ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡è½¬åŒ–ä¸ºå‰ç«¯ä»£ç ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€å±‚æ¬¡å¸ƒå±€è§„åˆ’å’Œè‡ªé€‚åº”æç¤ºåˆæˆï¼Œåˆ†ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„é˜¶æ®µï¼šåŸºç¡€ã€è§„åˆ’å’Œç”Ÿæˆã€‚åŸºç¡€ä»£ç†ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ£€æµ‹å’Œæ ‡è®°UIç»„ä»¶ï¼Œè§„åˆ’ä»£ç†æ„å»ºå±‚æ¬¡å¸ƒå±€ï¼Œç”Ÿæˆä»£ç†åˆ™é€šè¿‡è‡ªé€‚åº”æç¤ºåˆæˆç”ŸæˆHTML/CSSä»£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸ƒå±€å‡†ç¡®æ€§ã€ç»“æ„ä¸€è‡´æ€§å’Œä»£ç æ­£ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21493",
            "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
            "url": "https://huggingface.co/papers/2507.21493",
            "abstract": "BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.",
            "score": 19,
            "issue_id": 5102,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "28c2cc57bc1db4c8",
            "authors": [
                "Longwen Zhang",
                "Qixuan Zhang",
                "Haoran Jiang",
                "Yinuo Bai",
                "Wei Yang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21493.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "BANG: Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "BANG - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ 'Generative Exploded Dynamics', ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. BANG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ 3D-ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹."
                },
                "en": {
                    "title": "BANG: Intuitive 3D Creation through Generative Decomposition",
                    "desc": "BANG is a generative approach that enhances 3D creation by allowing users to intuitively decompose and manipulate 3D objects at a part level. It employs latent diffusion models and a temporal attention mechanism to ensure smooth transitions and maintain the coherence of geometric and semantic properties during the decomposition process. The system allows for precise control through spatial prompts, enabling users to specify which parts to manipulate, and can integrate with multimodal models for enhanced creativity. BANG's capabilities support detailed geometry generation and are particularly useful in applications like 3D printing, where it facilitates the creation of separable parts for easy assembly."
                },
                "zh": {
                    "title": "BANGï¼šç›´è§‚çš„3Då¯¹è±¡åˆ†è§£ä¸åˆ›ä½œæ–°æ–¹æ³•",
                    "desc": "BANGæ˜¯ä¸€ç§ç”Ÿæˆæ€§æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°3Då¯¹è±¡çš„ç›´è§‚éƒ¨åˆ†çº§åˆ†è§£å’Œæ“ä½œï¼Œæå‡3Dåˆ›ä½œæµç¨‹ã€‚å®ƒé€šè¿‡â€œç”Ÿæˆæ€§çˆ†ç‚¸åŠ¨æ€â€æŠ€æœ¯ï¼Œåˆ›å»ºè¾“å…¥å‡ ä½•ä½“çš„å¹³æ»‘çˆ†ç‚¸çŠ¶æ€åºåˆ—ï¼Œé€æ­¥åˆ†ç¦»éƒ¨ä»¶ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚BANGä½¿ç”¨ç»è¿‡é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„çˆ†ç‚¸è§†å›¾é€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿åˆ†è§£è¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ï¼Œç¡®ä¿æ—¶é—´ä¸Šçš„å¹³æ»‘è¿‡æ¸¡å’Œä¸€è‡´æ€§ï¼Œæå¤§åœ°å¢å¼ºäº†3Dåˆ›ä½œçš„çµæ´»æ€§å’Œç›´è§‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22448",
            "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
            "url": "https://huggingface.co/papers/2507.22448",
            "abstract": "Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.",
            "score": 12,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "1023abbaabd95fa0",
            "authors": [
                "Jingwei Zuo",
                "Maksim Velikanov",
                "Ilyas Chahed",
                "Younes Belkada",
                "Dhia Eddine Rhayem",
                "Guillaume Kunsch",
                "Hakim Hacid",
                "Hamza Yous",
                "Brahim Farhat",
                "Ibrahim Khadraoui",
                "Mugariya Farooq",
                "Giulia Campesan",
                "Ruxandra Cojocaru",
                "Yasser Djilali",
                "Shi Hu",
                "Iheb Chaabane",
                "Puneesh Khanna",
                "Mohamed El Amine Seddik",
                "Ngoc Dung Huynh",
                "Phuc Le Khac",
                "Leen AlQadi",
                "Billel Mokeddem",
                "Mohamed Chami",
                "Abdalgader Abubaker",
                "Mikhail Lubinets",
                "Kacper Piskorski",
                "Slim Frikha"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22448.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#agi",
                    "#architecture",
                    "#science",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Falcon-H1: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰ÑŒ Ğ² Ğ¼Ğ¸Ñ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Falcon-H1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ…. Falcon-H1 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ñ‚ 0,5B Ğ´Ğ¾ 34B. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Falcon-H1: Redefining Efficiency in Language Models",
                    "desc": "Falcon-H1 introduces a new series of large language models that utilize a hybrid architecture, merging Transformer-based attention with State Space Models for enhanced performance and efficiency. This innovative design allows the models to handle long-context memory better while maintaining computational efficiency. The models are available in various sizes and configurations, demonstrating state-of-the-art capabilities across multiple tasks, including reasoning and multilingual processing. By outperforming larger models with fewer parameters, Falcon-H1 sets a new standard in the field of AI language models."
                },
                "zh": {
                    "title": "Falcon-H1ï¼šé«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆ",
                    "desc": "Falcon-H1æ˜¯ä¸€ç³»åˆ—æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¶æ„ï¼Œç»“åˆäº†åŸºäºTransformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚è¿™ç§è®¾è®¡ä½¿å¾—Falcon-H1åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å’Œä¼˜è¶Šçš„é•¿æ—¶è®°å¿†èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„Falconæ¨¡å‹ä¸åŒï¼ŒFalcon-H1æä¾›äº†å¤šç§é…ç½®ï¼Œèƒ½å¤Ÿåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç«äº‰ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä»¥å¼€æ”¾æºä»£ç çš„æ–¹å¼å‘å¸ƒï¼Œä½“ç°äº†æˆ‘ä»¬å¯¹å¯åŠæ€§å’Œå½±å“åŠ›çš„æ‰¿è¯ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22886",
            "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
            "url": "https://huggingface.co/papers/2507.22886",
            "abstract": "Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.",
            "score": 4,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "99373a83d84e0212",
            "authors": [
                "Kaining Ying",
                "Henghui Ding",
                "Guanquan Jie",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22886.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "OmniAVS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 2,098 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 59,458 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. OISA - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OISA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° OmniAVS Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Segmentation with OmniAVS and OISA",
                    "desc": "This paper introduces Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset designed to enhance audio-visual segmentation by incorporating diverse multimodal expressions. It features 2,098 videos and 59,458 multimodal referring expressions, which include text, speech, sound, and visual cues. The study also presents the Omnimodal Instructed Segmentation Assistant (OISA), which utilizes a Multimodal Large Language Model (MLLM) to improve reasoning and understanding of complex audiovisual content. Experimental results demonstrate that OISA significantly outperforms existing segmentation methods on the OmniAVS dataset and shows competitive performance on related tasks."
                },
                "zh": {
                    "title": "å…¨æ¨¡æ€éŸ³è§†é¢‘åˆ†å‰²çš„åˆ›æ–°ä¸çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†å…¨æ¨¡æ€å¼•ç”¨éŸ³è§†é¢‘åˆ†å‰²ï¼ˆOmniAVSï¼‰å’Œå…¨æ¨¡æ€æŒ‡ä»¤åˆ†å‰²åŠ©æ‰‹ï¼ˆOISAï¼‰ï¼Œæ—¨åœ¨æå‡éŸ³è§†é¢‘åˆ†å‰²çš„èƒ½åŠ›ã€‚OmniAVSæ˜¯ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«2098ä¸ªè§†é¢‘å’Œ59458ä¸ªå¤šæ¨¡æ€å¼•ç”¨è¡¨è¾¾ï¼Œå…·æœ‰8ç§çµæ´»ç»“åˆæ–‡æœ¬ã€è¯­éŸ³ã€å£°éŸ³å’Œè§†è§‰çº¿ç´¢çš„å¤šæ¨¡æ€è¡¨è¾¾ç±»å‹ã€‚OISAåˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç†è§£å¤æ‚çº¿ç´¢å¹¶è¿›è¡Œæ¨ç†åˆ†å‰²ï¼Œä»è€Œè§£å†³å¤šæ¨¡æ€æ¨ç†å’ŒéŸ³è§†é¢‘å†…å®¹çš„ç»†è‡´ç†è§£é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOISAåœ¨OmniAVSä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…¶ä»–ç›¸å…³ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†ç«äº‰æ€§ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22853",
            "title": "Repair-R1: Better Test Before Repair",
            "url": "https://huggingface.co/papers/2507.22853",
            "abstract": "Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.  \t\t\t\t\tAI-generated summary \t\t\t\t APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
            "score": 3,
            "issue_id": 5103,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "3f84dd190fcc7d27",
            "authors": [
                "Haichuan Hu",
                "Xiaochen Xie",
                "Quanjun Zhang"
            ],
            "affiliations": [
                "Alibaba Cloud",
                "Nanjing University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22853.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Repair-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Repair-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ (APR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ² Ñ„Ğ°Ğ·Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Repair-R1 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Automated Program Repair with Test-Driven Training",
                    "desc": "Repair-R1 is a novel approach to Automated Program Repair (APR) that enhances the repair process by incorporating test cases during the training phase. Unlike traditional methods that only use tests after generating patches, Repair-R1 prioritizes test generation before the repair, allowing the model to create targeted tests that identify defects more effectively. This method leverages reinforcement learning to optimize both test generation and bug repair simultaneously, leading to improved performance metrics. Experimental results show significant increases in repair success rates, test generation success, and overall test coverage compared to standard models."
                },
                "zh": {
                    "title": "Repair-R1ï¼šä¼˜å…ˆç”Ÿæˆæµ‹è¯•ï¼Œæå‡è‡ªåŠ¨ä¿®å¤æ•ˆæœ",
                    "desc": "Repair-R1 æ˜¯ä¸€ç§è‡ªåŠ¨ç¨‹åºä¿®å¤æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†æµ‹è¯•ç”¨ä¾‹æ•´åˆåˆ°è®­ç»ƒé˜¶æ®µæ¥å¢å¼ºä¿®å¤æ•ˆæœã€‚è¯¥æ–¹æ³•ä¼˜å…ˆç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç„¶åå†è¿›è¡Œä¿®å¤ï¼Œä»è€Œæé«˜äº†ä¿®å¤æˆåŠŸç‡å’Œæµ‹è¯•ç”ŸæˆæˆåŠŸç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRepair-R1 åœ¨ä¿®å¤ä¹‹å‰ç”Ÿæˆèƒ½å¤ŸåŒºåˆ†ç¼ºé™·è¡Œä¸ºçš„æµ‹è¯•ç”¨ä¾‹ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°å®šä½ç¼ºé™·å¹¶ç†è§£å…¶æ ¹æœ¬åŸå› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepair-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤å’Œæµ‹è¯•çš„æˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20976",
            "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision",
            "url": "https://huggingface.co/papers/2507.20976",
            "abstract": "A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA",
            "score": 2,
            "issue_id": 5103,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ»Ñ",
                "en": "July 28",
                "zh": "7æœˆ28æ—¥"
            },
            "hash": "c48709f829f62b89",
            "authors": [
                "Xiao Fang",
                "Minhyek Jeon",
                "Zheyang Qin",
                "Stanislav Panev",
                "Celso de Melo",
                "Shuowen Hu",
                "Shayok Chakraborty",
                "Fernando De la Torre"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Florida State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20976.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ² Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LDM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Bridging Domain Gaps for Better Vehicle Detection in Aerial Imagery",
                    "desc": "This paper presents a new framework for improving vehicle detection in aerial images by using fine-tuned latent diffusion models (LDMs). The challenge addressed is the difficulty of models trained in one area to perform well in different geographic regions due to varying conditions. The proposed method enhances training by generating high-quality synthetic aerial images and their labels, effectively bridging the gap between different domains. Experimental results demonstrate significant performance gains in vehicle detection accuracy compared to existing methods, showcasing the effectiveness of this multi-stage, multi-modal approach."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€çŸ¥è¯†è½¬ç§»æå‡èˆªç©ºå›¾åƒè½¦è¾†æ£€æµ‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µã€å¤šæ¨¡æ€çš„çŸ¥è¯†è½¬ç§»æ¡†æ¶ï¼Œåˆ©ç”¨å¾®è°ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥æ”¹å–„ä¸åŒé¢†åŸŸçš„èˆªç©ºå›¾åƒä¸­çš„è½¦è¾†æ£€æµ‹ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„èˆªç©ºå›¾åƒåŠå…¶æ ‡ç­¾ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†æ£€æµ‹å™¨çš„è®­ç»ƒï¼Œè§£å†³äº†æ¨¡å‹åœ¨ä¸åŒåœ°ç†åŒºåŸŸé—´çš„æ³›åŒ–é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’Œå…¶ä»–é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨AP50æŒ‡æ ‡ä¸Šæé«˜äº†4-23%ã€6-10%ã€7-40%åŠè¶…è¿‡50%çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ¥è‡ªæ–°è¥¿å…°å’ŒçŠ¹ä»–å·çš„ä¸¤ä¸ªæ–°æ ‡æ³¨çš„èˆªç©ºæ•°æ®é›†ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-30.html",
    "link_next": "2025-08-01.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.07",
        "en": "07/30",
        "zh": "7æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}