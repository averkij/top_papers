{
    "date": {
        "ru": "13 марта",
        "en": "March 13",
        "zh": "3月13日"
    },
    "time_utc": "2025-03-13 21:10",
    "weekday": 3,
    "issue_id": 2695,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.09566",
            "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
            "url": "https://huggingface.co/papers/2503.09566",
            "abstract": "The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.",
            "score": 31,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "6952e94de20936ce",
            "authors": [
                "Lingmin Ran",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09566.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Ускорение видео-диффузии: эффективность через поэтапность",
                    "desc": "Статья представляет TPDiff - унифицированную систему для повышения эффективности обучения и вывода видео-диффузионных моделей. Авторы предлагают разделить процесс диффузии на несколько этапов, постепенно увеличивая частоту кадров, что оптимизирует вычислительные ресурсы. Для обучения многоэтапной диффузионной модели вводится специальная структура обучения: поэтапная диффузия. Экспериментальные оценки подтверждают универсальность метода, демонстрируя сокращение затрат на обучение на 50% и повышение эффективности вывода в 1,5 раза."
                },
                "en": {
                    "title": "Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!",
                    "desc": "This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed."
                },
                "zh": {
                    "title": "优化视频扩散模型的计算效率",
                    "desc": "视频扩散模型的发展面临着巨大的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程具有固有的减少熵的特性。考虑到视频模态中的帧间冗余，在高熵阶段保持全帧率是没有必要的。基于这一见解，我们提出了TPDiff框架，通过将扩散过程分为多个阶段，逐步提高帧率，从而优化计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09573",
            "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
            "url": "https://huggingface.co/papers/2503.09573",
            "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
            "score": 29,
            "issue_id": 2678,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "32f097e93cbf5f3a",
            "authors": [
                "Marianne Arriola",
                "Aaron Gokaslan",
                "Justin T Chiu",
                "Zhihan Yang",
                "Zhixuan Qi",
                "Jiaqi Han",
                "Subham Sekhar Sahoo",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Cohere, NY, USA",
                "Cornell Tech, NY, USA",
                "Stanford University, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09573.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Блочные диффузионные модели: лучшее из двух миров в языковом моделировании",
                    "desc": "Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования."
                },
                "en": {
                    "title": "Block Diffusion: The Future of Flexible Language Generation",
                    "desc": "This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths."
                },
                "zh": {
                    "title": "块扩散模型：灵活生成与高效推理的结合",
                    "desc": "扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09151",
            "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
            "url": "https://huggingface.co/papers/2503.09151",
            "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/",
            "score": 23,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "e1463c182b0fe8e9",
            "authors": [
                "Hyeonho Jeong",
                "Suhyeon Lee",
                "Jong Chul Ye"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09151.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в многоракурсной видеогенерации без 4D-датасетов",
                    "desc": "Reangle-A-Video - это новый подход к генерации синхронизированных многоракурсных видео из одного входного видео. Метод использует двухэтапный процесс: обучение многоракурсному движению и согласованный перевод изображения в изображения с разных ракурсов. В отличие от традиционных методов, Reangle-A-Video не требует больших 4D-датасетов, а использует существующие модели диффузии для изображений и видео. Эксперименты показывают, что этот метод превосходит существующие подходы в задачах статического переноса ракурса и динамического управления камерой."
                },
                "en": {
                    "title": "Transforming Single Videos into Multi-View Masterpieces!",
                    "desc": "Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation."
                },
                "zh": {
                    "title": "Reangle-A-Video：单视频生成多视角同步视频的新方法",
                    "desc": "我们提出了Reangle-A-Video，这是一个统一框架，用于从单个输入视频生成同步的多视角视频。与主流方法不同，我们的方法将多视角视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。Reangle-A-Video的操作分为两个阶段：首先，通过自监督方式对图像到视频的扩散变换器进行同步微调，以提取视角不变的运动；其次，在推理时使用DUSt3R进行跨视角一致性指导，将输入视频的第一帧变形并修复为不同的摄像机视角，生成多视角一致的起始图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09601",
            "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling",
            "url": "https://huggingface.co/papers/2503.09601",
            "abstract": "Score Distillation Sampling (SDS) has emerged as an effective technique for leveraging 2D diffusion priors for tasks such as text-to-3D generation. While powerful, SDS struggles with achieving fine-grained alignment to user intent. To overcome this, we introduce RewardSDS, a novel approach that weights noise samples based on alignment scores from a reward model, producing a weighted SDS loss. This loss prioritizes gradients from noise samples that yield aligned high-reward output. Our approach is broadly applicable and can extend SDS-based methods. In particular, we demonstrate its applicability to Variational Score Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks, showing significant improvements over SDS and VSD on a diverse set of metrics measuring generation quality and alignment to desired reward models, enabling state-of-the-art performance. Project page is available at https://itaychachy. github.io/reward-sds/.",
            "score": 10,
            "issue_id": 2683,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "4cbf8c2d009fe092",
            "authors": [
                "Itay Chachy",
                "Guy Yariv",
                "Sagie Benaim"
            ],
            "affiliations": [
                "Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09601.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#diffusion",
                    "#3d",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точная генерация контента с помощью взвешенного обучения",
                    "desc": "Статья представляет новый метод RewardSDS, улучшающий технику Score Distillation Sampling (SDS) для генерации контента. RewardSDS использует модель вознаграждения для взвешивания шумовых сэмплов, что позволяет лучше соответствовать намерениям пользователя. Авторы также представляют RewardVSD - расширение метода Variational Score Distillation (VSD). Эксперименты показывают значительное улучшение качества генерации и соответствия заданным критериям для задач генерации изображений и 3D-моделей."
                },
                "en": {
                    "title": "Aligning User Intent with Reward-Driven Sampling",
                    "desc": "This paper presents RewardSDS, an innovative method that enhances Score Distillation Sampling (SDS) by incorporating alignment scores from a reward model. By weighting noise samples according to their alignment with user intent, RewardSDS generates a more effective weighted loss function. This approach not only improves the performance of SDS but also extends its application to Variational Score Distillation (VSD) through the introduction of RewardVSD. The authors demonstrate that their methods significantly outperform existing techniques in generating high-quality outputs across various tasks, including text-to-image and text-to-3D generation."
                },
                "zh": {
                    "title": "提升生成对齐度的新方法",
                    "desc": "得分蒸馏采样（SDS）是一种有效利用二维扩散先验进行文本到三维生成的技术。然而，SDS在实现与用户意图的细致对齐方面存在困难。为了解决这个问题，我们提出了RewardSDS，这是一种基于奖励模型的对齐得分加权噪声样本的新方法，从而生成加权的SDS损失。我们的研究表明，RewardSDS和RewardVSD在文本到图像、二维编辑和文本到三维生成任务上显著提高了生成质量和与期望奖励模型的对齐度，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08525",
            "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
            "url": "https://huggingface.co/papers/2503.08525",
            "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.",
            "score": 10,
            "issue_id": 2681,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 марта",
                "en": "March 11",
                "zh": "3月11日"
            },
            "hash": "e82260bba3e835b4",
            "authors": [
                "Tong Wei",
                "Yijun Yang",
                "Junliang Xing",
                "Yuanchun Shi",
                "Zongqing Lu",
                "Deheng Ye"
            ],
            "affiliations": [
                "Peking University",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08525.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#reasoning",
                    "#video",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Управляемое обучение рассуждениям для визуально-языковых моделей",
                    "desc": "Исследование посвящено применению обучения с подкреплением для улучшения рассуждений в визуально-языковых моделях. Авторы обнаружили феномен 'коллапса мыслей' при использовании наград, основанных только на результатах действий. Для решения этой проблемы предложен метод GTR (Guided Thought Reinforcement), который автоматически оценивает и уточняет рассуждения агента на каждом шаге обучения. Эксперименты показали, что GTR значительно улучшает производительность и обобщающую способность модели LLaVA-7b в различных визуальных средах."
                },
                "en": {
                    "title": "Enhancing VLMs with Guided Thought Reinforcement",
                    "desc": "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."
                },
                "zh": {
                    "title": "引导思维强化：提升视觉语言模型的推理能力",
                    "desc": "本研究探讨了可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中的应用，尤其是在复杂的视觉环境中进行目标导向的推理。我们发现，当奖励仅基于行动结果时，RL无法有效激励VLM的思维链推理，导致思维崩溃现象，表现为代理的思维多样性迅速下降和推理不完整。为了解决这一问题，我们提出了一种自动纠正器，能够在每个RL步骤中评估和改进代理的推理过程。通过实验，我们的引导思维强化（GTR）框架显著提高了LLaVA-7b模型在各种视觉环境中的表现和泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04388",
            "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
            "url": "https://huggingface.co/papers/2503.04388",
            "abstract": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for LLMs. Additionally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .",
            "score": 10,
            "issue_id": 2682,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "9b3fb8251a206c0d",
            "authors": [
                "Shahar Levy",
                "Nir Mazor",
                "Lihi Shalmon",
                "Michael Hassid",
                "Gabriel Stanovsky"
            ],
            "affiliations": [
                "School of Computer Science and Engineering The Hebrew University of Jerusalem, Jerusalem, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04388.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#rag"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Больше документов - больше проблем для языковых моделей",
                    "desc": "Статья исследует влияние количества документов на производительность языковых моделей в задачах генерации с извлечением (RAG). Авторы создали специальные наборы данных, где сохраняли длину контекста и позицию релевантной информации постоянными, но варьировали число документов. Результаты показали, что увеличение количества документов в RAG создает значительные трудности для языковых моделей. Также было установлено, что обработка множества документов - это отдельная проблема от работы с длинными контекстами."
                },
                "en": {
                    "title": "More Documents, More Challenges for LLMs!",
                    "desc": "This paper investigates how the number of documents retrieved in Retrieval-augmented generation (RAG) impacts the performance of large language models (LLMs). The authors conduct experiments using custom datasets focused on multi-hop question answering, ensuring that context length remains constant while varying the number of documents. Their findings reveal that increasing the number of documents can significantly hinder the performance of LLMs, indicating that managing multiple documents presents unique challenges distinct from those associated with long contexts. The study contributes to the understanding of RAG by providing datasets and code for further research."
                },
                "zh": {
                    "title": "文档数量影响LLM性能的挑战",
                    "desc": "检索增强生成（RAG）为大型语言模型（LLMs）提供相关文档。尽管之前的研究指出检索过多文档可能会降低性能，但并未明确控制上下文长度来研究文档数量对性能的影响。我们在多跳问答任务的自定义数据集上评估了各种语言模型，发现增加文档数量在RAG设置中对LLMs造成了显著挑战。此外，我们的结果表明，处理多个文档与处理长上下文是两个不同的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06955",
            "title": "Motion Anything: Any to Motion Generation",
            "url": "https://huggingface.co/papers/2503.06955",
            "abstract": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything",
            "score": 7,
            "issue_id": 2680,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "9199d2d99b75d862",
            "authors": [
                "Zeyu Zhang",
                "Yiran Wang",
                "Wei Mao",
                "Danning Li",
                "Rui Zhao",
                "Biao Wu",
                "Zirui Song",
                "Bohan Zhuang",
                "Ian Reid",
                "Richard Hartley"
            ],
            "affiliations": [
                "ANU",
                "Google",
                "JD.com",
                "MBZUAI",
                "McGill",
                "Tencent",
                "USYD",
                "UTS",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06955.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#synthetic",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Универсальная генерация движений с мультимодальным контролем",
                    "desc": "Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D."
                },
                "en": {
                    "title": "Revolutionizing Motion Generation with Multimodal Control",
                    "desc": "This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods."
                },
                "zh": {
                    "title": "多模态运动生成的新突破",
                    "desc": "本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07103",
            "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
            "url": "https://huggingface.co/papers/2503.07103",
            "abstract": "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.",
            "score": 6,
            "issue_id": 2683,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "4f07119680bb80a1",
            "authors": [
                "Alessandro Giagnorio",
                "Antonio Mastropaolo",
                "Saima Afrin",
                "Massimiliano Di Penta",
                "Gabriele Bavota"
            ],
            "affiliations": [
                "Software Institute Università della Svizzera italiana, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07103.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Квантование LLM для эффективной генерации кода",
                    "desc": "Статья посвящена исследованию эффективности квантования больших языковых моделей (LLM) для генерации кода. Авторы изучают возможность сжатия моделей до 2 бит на параметр, используя современные методы квантования. Эксперименты показывают, что 4-битное квантование позволяет сократить объем памяти на 70% без существенной потери производительности. При более экстремальном квантовании (3 и 2 бита) использование специфичного для кода набора данных для калибровки помогает ограничить снижение эффективности."
                },
                "en": {
                    "title": "Optimizing Code Generation with Extreme Quantization Techniques",
                    "desc": "This paper explores the use of quantization techniques to reduce the memory footprint of large language models (LLMs) used for code generation. It builds on previous work by examining larger models with up to 34 billion parameters and applying advanced quantization methods that compress model precision down to 2 bits. The study finds that using 4-bit quantization can reduce memory usage by 70% without significantly impacting performance. Furthermore, it highlights the importance of using code-specific calibration datasets to maintain effectiveness even with more aggressive quantization levels."
                },
                "zh": {
                    "title": "量化技术助力大型语言模型的高效代码生成",
                    "desc": "大型语言模型（LLMs）在代码生成方面表现出色，能够根据自然语言描述自动实现需求。模型的有效性通常随着参数数量的增加而提高，但较大的模型在部署时会面临内存和碳足迹的挑战。为了解决这个问题，研究者们提出了量化技术，以减少LLM代码生成器的内存占用，同时保持其有效性。本文通过研究更新的、参数高达34B的LLM，探索了更极端的量化技术，发现4位精度的量化可以将内存占用减少70%，而性能几乎没有显著下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06573",
            "title": "WildIFEval: Instruction Following in the Wild",
            "url": "https://huggingface.co/papers/2503.06573",
            "abstract": "Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions.",
            "score": 5,
            "issue_id": 2683,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 марта",
                "en": "March 9",
                "zh": "3月9日"
            },
            "hash": "a2fa7b2b917156fd",
            "authors": [
                "Gili Lior",
                "Asaf Yehudai",
                "Ariel Gera",
                "Liat Ein-Dor"
            ],
            "affiliations": [
                "IBM Research",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06573.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый вызов для ИИ: следование сложным инструкциям",
                    "desc": "Статья представляет новый набор данных WildIFEval, содержащий 12000 реальных пользовательских инструкций с разнообразными ограничениями. Авторы классифицируют эти ограничения на 8 категорий и используют набор данных для оценки способности современных языковых моделей следовать сложным инструкциям. Результаты показывают, что производительность всех моделей снижается с увеличением числа ограничений. Исследование выявляет большой потенциал для улучшения моделей в выполнении сложных инструкций с множественными ограничениями."
                },
                "en": {
                    "title": "WildIFEval: Benchmarking LLMs on Complex User Instructions",
                    "desc": "This paper presents WildIFEval, a new dataset designed to evaluate how well large language models (LLMs) can follow user instructions with multiple constraints. The dataset contains 12,000 real user prompts that include a variety of constraints categorized into eight classes, reflecting the complexity of real-world instructions. Experiments show that as the number of constraints increases, the performance of all tested LLMs declines, indicating a significant area for improvement. The study highlights the importance of the type of constraint in affecting model performance and aims to encourage further research in this challenging area of instruction-following."
                },
                "zh": {
                    "title": "多重约束下的指令遵循挑战",
                    "desc": "最近的大型语言模型（LLMs）在遵循用户指令方面取得了显著成功，但处理具有多个约束的指令仍然是一个重大挑战。我们提出了WildIFEval，这是一个包含12000个真实用户指令的大规模数据集，涵盖了多样化的多重约束条件。与之前的数据集不同，我们的收集涵盖了广泛的词汇和主题约束，并将这些约束分为八个高层类别，以捕捉它们在现实场景中的分布和动态。通过WildIFEval，我们进行了广泛的实验，以基准测试领先LLMs的指令遵循能力，发现所有评估的模型在约束数量增加时性能下降，表明这些任务仍有很大的改进空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09402",
            "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
            "url": "https://huggingface.co/papers/2503.09402",
            "abstract": "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.",
            "score": 4,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "6a25ed0e2c069e4f",
            "authors": [
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09402.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#games",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VLog: Пересказ видео через словарь событий",
                    "desc": "VLog - это новая система понимания видео, которая использует словарь событий для описания повседневной деятельности человека. Она основана на языковой модели GPT-2 и включает в себя генеративную модель поиска, иерархический словарь и стратегию обновления словаря. VLog способна генерировать краткие и точные описания видео, учитывая контекст и логические связи между событиями. Эффективность системы была продемонстрирована на нескольких наборах данных, включая специально созданный набор VidCap-Eval."
                },
                "en": {
                    "title": "VLog: Revolutionizing Video Narration with Hierarchical Vocabulary",
                    "desc": "The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets."
                },
                "zh": {
                    "title": "VLog：视频理解的新视角",
                    "desc": "本论文介绍了一种名为VLog的视频理解框架，旨在将视频叙述定义为词汇，超越现有生成视频语言模型中的子词词汇。VLog基于轻量级语言模型GPT-2，具有三项关键创新：生成检索模型、层次词汇和词汇更新策略。生成检索模型结合了语言模型的复杂推理能力和对比检索的高效相似性搜索。通过在EgoSchema、COIN和HiREST数据集上的实验，验证了VLog在生成简洁、上下文准确的叙述方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09579",
            "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
            "url": "https://huggingface.co/papers/2503.09579",
            "abstract": "Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.",
            "score": 3,
            "issue_id": 2682,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "2c884c8c6aab1cc4",
            "authors": [
                "Yingfa Chen",
                "Yutong Wu",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China",
                "SIST, University of Science and Technology Beijing, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09579.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация LLM: больше параметров, меньше головок внимания",
                    "desc": "Статья исследует влияние длины контекста и конфигурации головок внимания на эффективность и производительность больших языковых моделей (LLM). Авторы проводят систематическое сравнение моделей с различными размерами параметров, длинами контекста и конфигурациями головок внимания. Они расширяют существующие методы масштабирования, чтобы оптимизировать LLM как на этапе обучения, так и при выводе. Результаты показывают, что при обработке длинных последовательностей большая модель с меньшим количеством головок внимания может достичь лучших результатов при меньших вычислительных затратах и использовании памяти."
                },
                "en": {
                    "title": "Optimizing LLMs: Less Heads, More Efficiency!",
                    "desc": "This paper investigates how different configurations of Transformer-based large language models (LLMs) affect their performance and resource efficiency. It highlights the importance of context length and attention head settings, which have been largely ignored in previous research. By comparing various model sizes and configurations, the authors propose new scaling methods that optimize both training and inference costs. Their results indicate that larger models with fewer attention heads can perform better and use less computational and memory resources when handling long sequences."
                },
                "zh": {
                    "title": "优化大型语言模型的构建与成本",
                    "desc": "本文探讨了如何构建高效的基于Transformer的大型语言模型（LLMs），重点在于最大化模型的语言能力，同时降低训练和部署成本。研究比较了不同参数大小、上下文长度和注意力头配置对模型性能、计算成本和内存成本的影响。结果表明，在处理较长序列时，较大的模型配合较少的注意力头可以实现更低的损失，同时降低计算和内存成本。我们的研究为开发实用的LLMs提供了重要的见解，尤其是在长上下文处理的场景中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09419",
            "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
            "url": "https://huggingface.co/papers/2503.09419",
            "abstract": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM",
            "score": 3,
            "issue_id": 2680,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "1c497a991b18da6a",
            "authors": [
                "Yifan Zhou",
                "Zeqi Xiao",
                "Shuai Yang",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09419.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей",
                    "desc": "Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений."
                },
                "en": {
                    "title": "Achieving Consistency in Latent Diffusion Models with Shift-Equivariance",
                    "desc": "Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs."
                },
                "zh": {
                    "title": "提升潜在扩散模型的一致性",
                    "desc": "潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08681",
            "title": "Self-Taught Self-Correction for Small Language Models",
            "url": "https://huggingface.co/papers/2503.08681",
            "abstract": "Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.",
            "score": 3,
            "issue_id": 2686,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 марта",
                "en": "March 11",
                "zh": "3月11日"
            },
            "hash": "1886b6cebec24540",
            "authors": [
                "Viktor Moskvoretskii",
                "Chris Biemann",
                "Irina Nikishina"
            ],
            "affiliations": [
                "HSE University",
                "Skoltech",
                "University of Hamburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08681.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#optimization",
                    "#alignment",
                    "#training"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "Самообучение самокоррекции: новый подход к улучшению малых языковых моделей",
                    "desc": "Эта статья представляет алгоритм Self-Taught Self-Correction (STaSC) для обучения малых языковых моделей (SLM) самокоррекции. В отличие от предыдущих подходов, использующих внешние инструменты или крупные проприетарные модели, STaSC применяет итеративное дообучение на самогенерированных данных. Эксперименты на задаче вопросно-ответного анализа показывают значительное улучшение производительности моделей. Авторы также анализируют механизмы самокоррекции и влияние различных аспектов дизайна алгоритма на динамику обучения и общую эффективность."
                },
                "en": {
                    "title": "Empowering Small Models with Self-Correction",
                    "desc": "This paper addresses the issue of error-proneness in large language models (LLMs) by focusing on self-correction capabilities in smaller language models (SLMs). The authors propose a novel algorithm called Self-Taught Self-Correction (STaSC), which utilizes iterative fine-tuning with self-generated data to enhance the self-correction process. Experimental results indicate that STaSC significantly improves performance on a question-answering task, showcasing its effectiveness. Additionally, the paper analyzes the mechanisms behind self-correction and how various design choices influence learning dynamics and outcomes."
                },
                "zh": {
                    "title": "小型模型的自我纠正新方法",
                    "desc": "尽管大型语言模型在各种任务中表现出色，但它们仍然容易出错。本文探讨了如何通过自我生成的数据对小型语言模型进行迭代微调，以实现自我纠正。我们提出了自我学习自我纠正（STaSC）算法，该算法结合了多种设计选择。实验结果表明，STaSC在问答任务中有效地学习了自我纠正，显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07588",
            "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
            "url": "https://huggingface.co/papers/2503.07588",
            "abstract": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.",
            "score": 3,
            "issue_id": 2684,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "b3cad6b7241db7fa",
            "authors": [
                "Junwei Luo",
                "Yingying Zhang",
                "Xue Yang",
                "Kang Wu",
                "Qi Zhu",
                "Lei Liang",
                "Jingdong Chen",
                "Yansheng Li"
            ],
            "affiliations": [
                "Ant Group",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07588.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#survey",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Умное сжатие для анализа гигантских спутниковых снимков",
                    "desc": "Статья представляет новый метод для эффективного анализа крупномасштабных спутниковых снимков с использованием больших языково-визуальных моделей (LVLM). Авторы предлагают технику выборочного удаления токенов на основе текстовых подсказок и динамической пирамиды изображений для сохранения важных деталей при снижении вычислительной сложности. Метод включает модуль фокусировки на регионах (RFM) и стратегию отбора фрагментов изображения от грубого к точному. Также представлен новый датасет LRS-VQA для оценки способностей LVLM в работе с крупными спутниковыми снимками."
                },
                "en": {
                    "title": "Efficiently Unlocking Insights from Gigapixel Remote Sensing Images",
                    "desc": "This paper addresses the challenges of understanding large Remote Sensing Images (RSIs) using Large Vision-Language Models (LVLMs). It introduces a novel text-guided token pruning method combined with a Dynamic Image Pyramid (DIP) to efficiently process gigapixel images without losing important details. The proposed Region Focus Module (RFM) helps in identifying essential vision tokens, while the coarse-to-fine strategy reduces computational costs. Additionally, the authors present a new benchmark, LRS-VQA, to evaluate LVLMs on large RSIs, demonstrating that their method outperforms existing strategies in terms of efficiency and effectiveness."
                },
                "zh": {
                    "title": "高效处理大型遥感图像的视觉-语言理解方法",
                    "desc": "本论文提出了一种高效的视觉-语言理解方法，专门针对大型遥感图像（RSIs）。我们的方法结合了动态图像金字塔（DIP）和文本引导的标记修剪技术，以减少计算复杂度并保留图像细节。通过区域聚焦模块（RFM），我们能够识别关键的视觉标记，从而优化图像处理过程。此外，我们还构建了一个新的基准数据集LRS-VQA，以评估现有大型视觉-语言模型的感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09590",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
            "url": "https://huggingface.co/papers/2503.09590",
            "abstract": "Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm.",
            "score": 2,
            "issue_id": 2689,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "82b85fd9bb1c9a17",
            "authors": [
                "Md Mohaiminul Islam",
                "Tushar Nagarajan",
                "Huiyu Wang",
                "Gedas Bertasius",
                "Lorenzo Torresani"
            ],
            "affiliations": [
                "Meta AI",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09590.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#benchmark",
                    "#long_context",
                    "#architecture"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "BIMBA: умное сжатие видео для эффективного анализа большими языковыми моделями",
                    "desc": "Статья представляет BIMBA - эффективную модель пространства состояний для обработки длинных видео в задаче видео-вопрос-ответ (VQA). Модель использует алгоритм выборочного сканирования для эффективного выбора критической информации из видео высокой размерности и преобразования ее в сокращенную последовательность токенов. Это позволяет эффективно обрабатывать длинные видео большими языковыми моделями (LLM). BIMBA достигает наилучших результатов на нескольких бенчмарках для длинных видео в задаче VQA."
                },
                "en": {
                    "title": "BIMBA: Efficiently Answering Questions from Long Videos",
                    "desc": "This paper addresses the challenge of Video Question Answering (VQA) in long videos, where extracting relevant information from numerous frames is difficult. The authors introduce BIMBA, a state-space model that efficiently selects critical information from high-dimensional video data. Unlike previous methods that often miss important events due to redundancy, BIMBA uses a selective scan algorithm to create a reduced token sequence for processing by large language models (LLMs). The results show that BIMBA achieves state-of-the-art performance on various long-form VQA benchmarks, demonstrating its effectiveness in handling long videos."
                },
                "zh": {
                    "title": "BIMBA：高效处理长视频问答的创新模型",
                    "desc": "视频问答（VQA）在长视频中面临提取相关信息和建模长距离依赖的挑战。自注意力机制虽然能处理序列建模，但在处理大量时空标记时计算成本过高。以往的方法通常依赖压缩策略来降低计算成本，但这些简单的方法往往会过度表示冗余信息，错过重要事件。我们提出了BIMBA，一个高效的状态空间模型，能够从高维视频中选择关键信息，并将其转化为简化的标记序列，以便高效处理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09427",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "url": "https://huggingface.co/papers/2503.09427",
            "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "score": 2,
            "issue_id": 2677,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "491beb48064068d2",
            "authors": [
                "Yaorui Shi",
                "Jiaqi Yang",
                "Sihang Li",
                "Junfeng Fang",
                "Xiang Wang",
                "Zhiyuan Liu",
                "Yang Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09427.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#transfer_learning",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Единая модель для анализа клеток и текста",
                    "desc": "scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день."
                },
                "en": {
                    "title": "Bridging Cells and Text: The Power of scMMGPT",
                    "desc": "This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models."
                },
                "zh": {
                    "title": "单细胞多模态生成预训练变换器的创新应用",
                    "desc": "预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05397",
            "title": "Multi Agent based Medical Assistant for Edge Devices",
            "url": "https://huggingface.co/papers/2503.05397",
            "abstract": "Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.",
            "score": 2,
            "issue_id": 2685,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 марта",
                "en": "March 7",
                "zh": "3月7日"
            },
            "hash": "0d34c4bc75fe4355",
            "authors": [
                "Sakharam Gawade",
                "Shivam Akhouri",
                "Chinmay Kulkarni",
                "Jagdish Samant",
                "Pragya Sahu",
                "Aastik",
                "Jai Pahal",
                "Saswat Meher"
            ],
            "affiliations": [
                "Samsung Research Institute Bangalore, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05397.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#healthcare",
                    "#small_models",
                    "#agents"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Многоагентный медицинский помощник: эффективность без компромиссов",
                    "desc": "Статья представляет инновационную систему многоагентного медицинского помощника, работающего на устройстве пользователя. Эта система преодолевает ограничения, связанные с конфиденциальностью, задержками и зависимостью от интернета, характерные для крупных языковых моделей в здравоохранении. Используя меньшие, специализированные агенты, система оптимизирует ресурсы и обеспечивает масштабируемость. Помощник, основанный на модели Qwen Code Instruct 2.5 7B, предлагает комплексное решение для медицинских нужд, включая запись на прием, мониторинг здоровья и напоминания о приеме лекарств."
                },
                "en": {
                    "title": "Empowering Healthcare with On-Device Multi-Agent Systems",
                    "desc": "This paper presents a novel on-device multi-agent healthcare assistant that addresses key challenges in using Large Action Models (LAMs) in healthcare, such as privacy, latency, and internet dependency. By employing smaller, task-specific agents, the system enhances resource optimization, scalability, and performance. The assistant offers various healthcare functionalities, including appointment scheduling, health monitoring, and medication reminders, all while maintaining a lightweight design for on-device use. The system demonstrates impressive performance metrics, achieving high RougeL scores for its planning and calling tasks, showcasing its potential for user-centric healthcare solutions."
                },
                "zh": {
                    "title": "智能医疗助手：隐私、安全、高效的解决方案",
                    "desc": "大型行动模型（LAMs）在智能自动化领域取得了革命性进展，但在医疗保健中的应用面临隐私、延迟和对互联网依赖等挑战。本文介绍了一种基于设备的多智能体医疗助手，克服了这些限制。该系统利用较小的、特定任务的智能体来优化资源，确保可扩展性和高性能。我们提出的系统作为一站式医疗解决方案，具备预约、健康监测、用药提醒和每日健康报告等功能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09600",
            "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
            "url": "https://huggingface.co/papers/2503.09600",
            "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.",
            "score": 1,
            "issue_id": 2683,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "4d7741ddea8c8c48",
            "authors": [
                "Jihao Zhao",
                "Zhiyuan Ji",
                "Zhaoxin Fan",
                "Hanyu Wang",
                "Simin Niu",
                "Bo Tang",
                "Feiyu Xiong",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai, China",
                "School of Information, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09600.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rag",
                    "#optimization"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умное разбиение текста для улучшения RAG систем",
                    "desc": "Эта статья представляет новый подход к разбиению текста на фрагменты в системах RAG. Авторы вводят метод двойной метрики для оценки качества разбиения и показывают ограничения традиционных методов. Они предлагают фреймворк Mixture-of-Chunkers (MoC), использующий языковые модели для создания регулярных выражений для извлечения фрагментов. Эксперименты демонстрируют, что предложенные метрики и фреймворк MoC эффективно решают проблемы разбиения текста и улучшают работу систем RAG."
                },
                "en": {
                    "title": "Enhancing Chunking for Better Retrieval-Augmented Generation",
                    "desc": "This paper discusses the importance of text chunking in Retrieval-Augmented Generation (RAG) systems that use large language models (LLMs). It introduces a new evaluation method with two metrics, Boundary Clarity and Chunk Stickiness, to measure the quality of text chunks. The authors point out the limitations of existing chunking methods and propose a new framework called Mixture-of-Chunkers (MoC) that improves chunking precision while maintaining computational efficiency. Through experiments, they show that their metrics and MoC framework enhance the overall performance of RAG systems by effectively addressing chunking challenges."
                },
                "zh": {
                    "title": "提升RAG系统性能的分块创新",
                    "desc": "本文探讨了检索增强生成（RAG）模型中文本分块的重要性。我们提出了一种双指标评估方法，包括边界清晰度和分块粘性，以量化分块质量。通过这一评估方法，我们揭示了传统和语义分块在处理复杂上下文时的局限性，并强调了将大型语言模型（LLMs）整合到分块过程中的必要性。为了解决基于LLMs的方法在计算效率和分块精度之间的权衡，我们设计了一个关注粒度的混合分块器（MoC）框架，显著提升了RAG系统的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09410",
            "title": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC",
            "url": "https://huggingface.co/papers/2503.09410",
            "abstract": "Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.",
            "score": 0,
            "issue_id": 2686,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "2ccf3518b5892591",
            "authors": [
                "Jiale Wang",
                "Chen Zhao",
                "Wei Ke",
                "Tong Zhang"
            ],
            "affiliations": [
                "EPFL",
                "University of Chinese Academy of Sciences",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09410.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#diffusion",
                    "#cv",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🎲",
                "ru": {
                    "title": "Повышение обобщающей способности RANSAC с помощью диффузионной модели Монте-Карло",
                    "desc": "Эта статья представляет новый подход к улучшению алгоритма RANSAC с использованием глубокого обучения. Авторы предлагают метод диффузии с применением метода Монте-Карло для генерации разнообразных обучающих данных, симулирующих реальные шумные условия. Эксперименты на наборах данных ScanNet и MegaDepth показывают, что предложенный подход значительно улучшает способность обобщения RANSAC на основе машинного обучения. Исследование включает подробный анализ эффективности ключевых компонентов предложенной системы."
                },
                "en": {
                    "title": "Enhancing RANSAC Robustness with Monte Carlo Diffusion",
                    "desc": "This paper presents a new method to improve the robustness of learning-based Random Sample Consensus (RANSAC) against noisy data. The authors propose a diffusion-based approach that adds noise to clean data, mimicking real-world conditions for better training. By integrating Monte Carlo sampling, they create diverse data distributions, enhancing the model's ability to generalize to unseen data. Experimental results show that this method significantly boosts the performance of learning-based RANSAC in feature matching tasks."
                },
                "zh": {
                    "title": "基于扩散的RANSAC：提升鲁棒性与泛化能力",
                    "desc": "随机样本一致性（RANSAC）是一种用于从噪声数据中稳健估计参数模型的基本方法。现有的基于学习的RANSAC方法利用深度学习增强其对异常值的鲁棒性，但这些方法在相同算法生成的数据上进行训练和测试，导致在推理时对分布外数据的泛化能力有限。本文提出了一种新颖的基于扩散的范式，通过逐步向真实数据中注入噪声，模拟训练基于学习的RANSAC所需的噪声条件。我们还结合了蒙特卡洛采样，以在多个阶段引入不同类型的随机性，从而增强数据的多样性，评估结果表明该方法显著提高了学习型RANSAC的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08674",
            "title": "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields",
            "url": "https://huggingface.co/papers/2503.08674",
            "abstract": "Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.",
            "score": 0,
            "issue_id": 2693,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 марта",
                "en": "March 11",
                "zh": "3月11日"
            },
            "hash": "444f0805a7ed179b",
            "authors": [
                "Tobias Kreiman",
                "Aditi S. Krishnapriyan"
            ],
            "affiliations": [
                "LBNL",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08674.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#transfer_learning",
                    "#dataset",
                    "#graphs",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Преодоление границ обучения: новые стратегии для машинных силовых полей",
                    "desc": "Статья посвящена исследованию машинных силовых полей (MLFFs) как альтернативы дорогостоящим квантово-механическим молекулярным симуляциям. Авторы изучают проблемы обобщения MLFFs за пределами обучающих распределений и предлагают два новых метода для смягчения эффектов смещения распределения. Первый метод основан на спектральной теории графов и модифицирует рёбра тестовых графов. Второй метод улучшает представления для систем вне распределения, используя вспомогательную целевую функцию."
                },
                "en": {
                    "title": "Enhancing MLFFs: Bridging the Gap in Chemical Space Generalization",
                    "desc": "This paper discusses Machine Learning Force Fields (MLFFs) as a cost-effective alternative to traditional quantum mechanical simulations for molecular modeling. It highlights the challenges MLFFs face when generalizing to chemical spaces that differ from their training data, often leading to overfitting. The authors propose two innovative test-time refinement strategies to improve the performance of MLFFs on out-of-distribution data without relying on expensive reference labels. Their findings suggest that with better training techniques, MLFFs can effectively model a wider range of chemical environments."
                },
                "zh": {
                    "title": "提升机器学习力场的泛化能力",
                    "desc": "机器学习力场（MLFFs）是一种有前景的替代方案，用于昂贵的量子力学分子模拟。本文研究了MLFFs在训练分布之外的泛化能力，发现当前的监督训练方法可能导致过拟合，无法有效学习分布外系统的表示。为了解决这一问题，提出了两种新的测试时优化策略，旨在减少分布转移带来的误差。实验结果表明，这些策略显著提高了在分布外系统上的表现，表明MLFFs有潜力建模多样的化学空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05333",
            "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?",
            "url": "https://huggingface.co/papers/2503.05333",
            "abstract": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.",
            "score": 0,
            "issue_id": 2690,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 марта",
                "en": "March 7",
                "zh": "3月7日"
            },
            "hash": "c9dae1097c1be3c2",
            "authors": [
                "Martin Spitznagel",
                "Jan Vaillant",
                "Janis Keuper"
            ],
            "affiliations": [
                "Herrenknecht AG",
                "Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany",
                "University of Mannheim, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05333.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#optimization",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Генеративные модели в физическом моделировании: потенциал и ограничения",
                    "desc": "Статья исследует потенциал генеративных моделей в контексте физических симуляций. Авторы предлагают набор данных из 300 тысяч пар изображений и базовые оценки для трех различных задач физического моделирования. Исследование направлено на выяснение способности генеративных моделей изучать сложные физические отношения из пар входных и выходных изображений. Также рассматривается возможность ускорения процесса по сравнению с симуляциями на основе дифференциальных уравнений."
                },
                "en": {
                    "title": "Revolutionizing Physical Simulations with Generative Models",
                    "desc": "This paper explores the use of generative learning models for image-to-image translation in the context of physical simulations. It presents a dataset of 300,000 image pairs and establishes a benchmark for evaluating how well these models can learn complex physical relationships. The authors investigate the potential speed improvements that can be achieved by using generative models instead of traditional differential equation-based simulations. However, while some models show promise for faster simulations, they often struggle with maintaining physical accuracy, highlighting the need for new approaches to ensure correctness in physical simulations."
                },
                "zh": {
                    "title": "探索生成模型在物理模拟中的潜力",
                    "desc": "这篇论文探讨了生成学习模型在物理模拟中的应用，尤其是图像到图像的转换能力。研究者提供了一个包含30万对图像的数据集，并针对三种不同的物理模拟任务进行了基准评估。论文提出了两个研究问题：生成模型是否能够从输入输出图像对中学习复杂的物理关系？通过替代基于微分方程的模拟，能实现多大的加速？结果显示，尽管当前模型在加速方面有潜力，但在物理正确性方面存在明显的局限性，因此需要新的方法来确保物理的准确性。"
                }
            }
        }
    ],
    "link_prev": "2025-03-12.html",
    "link_next": "2025-03-14.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3月12日"
    },
    "short_date_next": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3月14日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 1,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 12,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 4,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了视频扩散模型的发展面临的计算需求挑战。为了缓解这一问题，研究人员提出了TPDiff框架，通过分阶段增加帧率来优化计算效率。该框架只在最后阶段使用全帧率，从而提高训练和推理效率。实验结果显示，这种方法可以减少50%的训练成本，并提高1.5倍的推理效率。",
        "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
        "pinyin": "这篇文章讨论了视频扩散模型的发展面临的计算需求挑战。\nZhè piān wénzhāng tǎolùn le shìpín kuòsàn móxíng de fāzhǎn miànlín de jìsuàn xūqiú tiǎozhàn.\n\n为了缓解这一问题，研究人员提出了TPDiff框架，通过分阶段增加帧率来优化计算效率。\nWèile huǎnjiě zhè yī wèntí, yánjiū rényuán tíchū le TP Diff kuàngjià, tōngguò fēn jiēduàn zēngjiā zhēnlǜ lái yōuhuà jìsuàn xiàolǜ.\n\n该框架只在最后阶段使用全帧率，从而提高训练和推理效率。\nGǎi kuàngjià zhǐ zài zuìhòu jiēduàn shǐyòng quán zhēnlǜ, cóng'ér tígāo xùnliàn hé tuīlǐ xiàolǜ.\n\n实验结果显示，这种方法可以减少50%的训练成本，并提高1.5倍的推理效率。\nShíyàn jiéguǒ xiǎnshì, zhè zhǒng fāngfǎ kěyǐ jiǎnshǎo 50% de xùnliàn chéngběn, bìng tígāo 1.5 bèi de tuīlǐ xiàolǜ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"development\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"calculation\"},\n    {\"word\": \"需求\", \"pinyin\": \"xū qiú\", \"trans\": \"demand\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"缓解\", \"pinyin\": \"huǎn jiě\", \"trans\": \"alleviate\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"分阶段\", \"pinyin\": \"fēn jiē duàn\", \"trans\": \"in stages\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"帧率\", \"pinyin\": \"zhèn lǜ\", \"trans\": \"frame rate\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"阶段\", \"pinyin\": \"jiē duàn\", \"trans\": \"stage\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"全帧率\", \"pinyin\": \"quán zhèn lǜ\", \"trans\": \"full frame rate\"},\n    {\"word\": \"从而\", \"pinyin\": \"cóng ér\", \"trans\": \"thus\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"increase\"}\n]",
        "trans": "This article discusses the computational challenges faced in the development of video diffusion models. To mitigate this issue, researchers have proposed the TPDiff framework, which optimizes computational efficiency by incrementally increasing the frame rate in stages. The framework only uses the full frame rate in the final stage, thereby enhancing both training and inference efficiency. Experimental results show that this method can reduce training costs by 50% and improve inference efficiency by 1.5 times.",
        "update_ts": "2025-03-13 09:11"
    }
}