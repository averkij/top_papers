{
    "date": {
        "ru": "16 октября",
        "en": "October 16",
        "zh": "10月16日"
    },
    "time_utc": "2025-10-16 07:12",
    "weekday": 3,
    "issue_id": 6449,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.13554",
            "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
            "url": "https://huggingface.co/papers/2510.13554",
            "abstract": "Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps. This work positions attention as a privileged substrate that renders the internal logic of LLMs legible, not merely as a byproduct of computation, but as a mechanistic blueprint of reasoning itself. We first distinguish attention heads between locally and globally focused information processing and reveal that locally focused heads produce a sawtooth pattern near the diagonal indicating phrasal chunks, while globally focused heads expose tokens that exert broad downstream influence over future tokens. We formalize these with two metrics: 1) Windowed Average Attention Distance, which measures the extent of backward attention within a clipped window; 2) Future Attention Influence, which quantifies a token's global importance as the average attention it receives from subsequent tokens. Taken together, these signals reveal a recurring preplan-and-anchor mechanism, where the model first performs a long-range contextual reference to generate an introductory token, which is immediately followed by or coincides with a semantic anchor token that organizes subsequent reasoning. Leveraging these insights, we introduce three novel RL strategies that dynamically perform targeted credit assignment to critical nodes (preplan tokens, anchor tokens, and their temporal coupling) and show consistent performance gains across various reasoning tasks. By aligning optimization with the model's intrinsic reasoning rhythm, we aim to transform opaque optimization into an actionable structure-aware process, hoping to offer a potential step toward more transparent and effective optimization of LLM reasoning.",
            "score": 36,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "ae0c2f486679f7f1",
            "authors": [
                "Yang Li",
                "Zhichen Dong",
                "Yuhan Sun",
                "Weixun Wang",
                "Shaopan Xiong",
                "Yijia Luo",
                "Jiashun Liu",
                "Han Lu",
                "Jiamang Wang",
                "Wenbo Su",
                "Bo Zheng",
                "Junchi Yan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13554.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Внимание как карта рассуждений: направленное обучение через ключевые токены",
                    "desc": "Исследователи анализируют механизмы attention в LLM, чтобы понять, как модели рассуждают, выявляя паттерн «планирование-и-якорь». Они разделяют attention heads на локально и глобально сфокусированные, измеряя влияние токенов через специальные метрики. Обнаружено, что модели сначала делают дальний контекстный поиск для генерации вводного токена, затем создают семантический «якорь», организующий дальнейшие рассуждения. На основе этих инсайтов предложены три новые RL-стратегии, которые присваивают reward именно критическим токенам, улучшая производительность на задачах рассуждения."
                },
                "en": {
                    "title": "Unlocking LLM Reasoning with Targeted Attention Strategies",
                    "desc": "This paper investigates how attention mechanisms in Large Language Models (LLMs) can clarify their reasoning processes. It identifies two types of attention heads: those that focus locally on specific phrases and those that have a global influence on future tokens. By introducing metrics like Windowed Average Attention Distance and Future Attention Influence, the authors reveal a structured reasoning pattern that involves preplanning and anchoring tokens. The study proposes new Reinforcement Learning strategies that assign credit to these critical tokens, leading to improved performance in reasoning tasks by aligning optimization with the model's inherent reasoning structure."
                },
                "zh": {
                    "title": "揭示推理模式，提升模型性能的创新策略",
                    "desc": "本文分析了大型语言模型（LLMs）中的注意力机制，以揭示其推理模式，并提出了新的强化学习（RL）策略，通过关注关键标记来提高性能。研究表明，注意力不仅是计算的副产品，而是推理的机制蓝图。我们区分了局部和全局关注的信息处理，发现局部关注的头部产生锯齿形模式，而全局关注的头部则揭示了对未来标记的广泛影响。基于这些发现，我们提出了三种新的RL策略，动态地对关键节点进行有针对性的信用分配，从而在各种推理任务中实现了一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13344",
            "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
            "url": "https://huggingface.co/papers/2510.13344",
            "abstract": "UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html",
            "score": 33,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "a52820c83c08467e",
            "authors": [
                "Zhenyu Liu",
                "Yunxin Li",
                "Xuanyu Zhang",
                "Qixun Teng",
                "Shenyuan Jiang",
                "Xinyu Chen",
                "Haoyuan Shi",
                "Jinchao Li",
                "Qi Wang",
                "Haolan Chen",
                "Fanbo Meng",
                "Mingjun Zhao",
                "Yu Xu",
                "Yancheng He",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China",
                "Shenzhen Loop Area Institute, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13344.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#games",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#audio",
                    "#training"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Единая модель для речи и музыки через динамическую Mixture-of-Experts",
                    "desc": "UniMoE-Audio — это унифицированная модель для генерации речи и музыки, основанная на Dynamic-Capacity Mixture-of-Experts архитектуре. Модель решает проблемы дисбаланса данных и конфликтов между задачами через трёхэтапную стратегию обучения: независимую тренировку специалистов, интеграцию в MoE-архитектуру и совместное обучение. Архитектура включает динамическое распределение экспертов через Top-P routing, domain-специфичные эксперты, общие эксперты и null-эксперты для адаптивного пропуска вычислений. Модель достигает state-of-the-art результатов на бенчмарках по генерации речи и музыки, демонстрируя улучшенную кросс-доменную синергию без деградации производительности."
                },
                "en": {
                    "title": "Unifying Speech and Music Generation with Dynamic Experts",
                    "desc": "UniMoE-Audio is a novel model designed for generating both speech and music using a Dynamic-Capacity Mixture-of-Experts framework. It addresses the challenges of data imbalance and task conflicts that have historically separated these two domains. The model employs a Top-P routing strategy to dynamically allocate experts, allowing for both domain-specific and shared knowledge. Through a three-stage training process, UniMoE-Audio achieves state-of-the-art performance while enhancing synergy between speech and music generation tasks."
                },
                "zh": {
                    "title": "统一音频生成的未来",
                    "desc": "UniMoE-Audio 是一个统一的语音和音乐生成模型，采用动态容量混合专家框架，旨在解决数据不平衡和任务冲突的问题。该模型通过引入 Top-P 路由策略，实现动态专家数量分配，并设计了混合专家结构，以便于处理特定领域和通用特征。为了应对数据不平衡，UniMoE-Audio 采用了三阶段的训练课程，逐步提升模型的性能。实验结果表明，该模型在语音和音乐生成的主要基准测试中表现出色，并且在跨领域协同学习方面优于传统的联合训练方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13678",
            "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
            "url": "https://huggingface.co/papers/2510.13678",
            "abstract": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100times faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
            "score": 32,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "ef944b6ef4d97bd1",
            "authors": [
                "Xinyang Li",
                "Tengfei Wang",
                "Zixiao Gu",
                "Shengchuan Zhang",
                "Chunchao Guo",
                "Liujuan Cao"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University",
                "Tencent",
                "Yes Lab, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13678.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Мгновенная генерация 3D-миров: скорость и качество вместе",
                    "desc": "FlashWorld — это генеративная модель, которая создаёт 3D-сцены из одного изображения или текстового промпта за считанные секунды, работая в 10-100 раз быстрее предыдущих методов. Ключевая инновация заключается в переходе от классического подхода с генерацией multi-view изображений к прямому созданию 3D Gaussian представлений. Модель использует двухэтапное обучение: сначала dual-mode pre-training для поддержки обоих режимов генерации, затем cross-mode post-training для улучшения визуального качества путём дистилляции знаний между режимами. Результат — высококачественная 3D-генерация с сохранением геометрической консистентности и отличной обобщающей способностью."
                },
                "en": {
                    "title": "FlashWorld: Fast and High-Quality 3D Scene Generation",
                    "desc": "FlashWorld is a generative model that creates high-quality 3D scenes from single images or text prompts in a fraction of the time compared to previous methods. It innovatively combines multi-view-oriented and 3D-oriented generation techniques, allowing for faster rendering while maintaining 3D consistency. The model employs a dual-mode pre-training phase and a cross-mode post-training phase to enhance visual quality and reduce denoising steps during inference. By utilizing a large dataset of single-view images and text prompts, FlashWorld improves its ability to generalize to new inputs effectively."
                },
                "zh": {
                    "title": "FlashWorld：快速生成高质量3D场景的创新模型",
                    "desc": "FlashWorld是一种生成模型，可以快速从单张图像或文本提示生成高质量的3D场景。它结合了多视角导向和3D导向的生成方法，使得生成速度比以往快10到100倍，同时保持优越的渲染质量。该模型通过双模式预训练和交叉模式后训练，有效整合了两种方法的优点，确保了3D一致性并提升了视觉质量。实验结果表明，FlashWorld在生成效率和效果上都优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13795",
            "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
            "url": "https://huggingface.co/papers/2510.13795",
            "abstract": "A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.  \t\t\t\t\tAI-generated summary \t\t\t\t Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
            "score": 30,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "9896f21bc1e1b5ba",
            "authors": [
                "Yi Zhang",
                "Bolin Ni",
                "Xin-Sheng Chen",
                "Heng-Rui Zhang",
                "Yongming Rao",
                "Houwen Peng",
                "Qinglin Lu",
                "Han Hu",
                "Meng-Hao Guo",
                "Shi-Min Hu"
            ],
            "affiliations": [
                "Beihang University",
                "Tencent Hunyuan Team",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13795.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#dataset",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🐝",
                "ru": {
                    "title": "Качественные данные — ключ к открытым мультимодальным моделям",
                    "desc": "Исследователи создали новый датасет Honey-Data-15M из 15 миллионов пар вопрос-ответ для обучения мультимодальных LLM. Датасет включает улучшенные данные с Chain-of-Thought рассуждениями двух уровней сложности и прошёл тщательную очистку от шума. На основе этих данных обучена модель Bee-8B, которая достигла state-of-the-art результатов среди полностью открытых моделей. Работа доказывает, что фокус на качестве данных позволяет открытым моделям конкурировать с полузакрытыми аналогами вроде InternVL3.5-8B."
                },
                "en": {
                    "title": "Elevating Open Models with Quality Data",
                    "desc": "This paper presents a new dataset called Honey-Data-15M, which contains 15 million question-answer pairs designed to improve the performance of fully open multimodal large language models (MLLMs). The dataset is enhanced with a dual-level Chain-of-Thought (CoT) enrichment strategy and processed through a data curation pipeline named HoneyPipe, which is part of the DataStudio framework. By training the Bee-8B model on this curated dataset, the authors achieve state-of-the-art results that rival those of semi-open models. The work emphasizes the importance of high-quality data for supervised fine-tuning to enhance the capabilities of open-source MLLMs."
                },
                "zh": {
                    "title": "提升开放模型性能的关键在于数据质量",
                    "desc": "本文提出了一种新的数据集和数据处理管道，以提高完全开放的多模态大语言模型（MLLMs）的性能。我们引入了Honey-Data-15M数据集，包含约1500万个问答对，并通过多种清洗技术和双层思维链（CoT）增强策略进行处理。我们还开发了HoneyPipe数据处理管道和DataStudio框架，为社区提供透明且可适应的数据处理方法。实验结果表明，基于Honey-Data-15M训练的Bee-8B模型在完全开放的MLLMs中达到了新的最先进水平，性能与半开放模型相当，甚至在某些情况下超越了它们。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13747",
            "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
            "url": "https://huggingface.co/papers/2510.13747",
            "abstract": "InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
            "score": 25,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "4bcb34dcd1975ae1",
            "authors": [
                "Wenwen Tong",
                "Hewei Guo",
                "Dongchuan Ran",
                "Jiangnan Chen",
                "Jiefan Lu",
                "Kaibin Wang",
                "Keqiang Li",
                "Xiaoxu Zhu",
                "Jiakui Li",
                "Kehan Li",
                "Xueheng Li",
                "Lumin Li",
                "Chenxu Guo",
                "Jiasheng Zhou",
                "Jiandong Chen",
                "Xianye Wu",
                "Jiahao Wang",
                "Silei Wu",
                "Lei Chen",
                "Hanming Deng",
                "Yuxuan Song",
                "Dinghao Zhou",
                "Guiping Zhong",
                "Ken Zheng",
                "Shiyin Kang",
                "Lewei Lu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13747.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#small_models",
                    "#long_context",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#audio",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Легковесная омнимодальная модель с долговременной памятью для естественного аудиовизуального взаимодействия",
                    "desc": "InteractiveOmni — это унифицированная омнимодальная большая языковая модель размером от 4B до 8B параметров, которая объединяет визуальный энкодер, аудио-энкодер, LLM и речевой декодер для понимания и генерации контента. Модель обучается по многоэтапной стратегии, включающей предобучение для омнимодального понимания и дообучение на речевых диалогах и аудиовизуальном взаимодействии. Авторы создали специальные датасеты для многораундовых диалогов и бенчмарки для оценки долговременной памяти и речевого взаимодействия. InteractiveOmni-4B достигает результатов сопоставимых с моделями в два раза большего размера и превосходит другие открытые модели в задачах понимания изображений, аудио, видео и генерации речи."
                },
                "en": {
                    "title": "Revolutionizing Multi-Turn Interactions with InteractiveOmni",
                    "desc": "InteractiveOmni is a cutting-edge omni-modal large language model designed for audio-visual interactions, capable of understanding and generating speech across multiple turns. It integrates various components like vision and audio encoders with a language model and speech decoder, allowing it to perform complex tasks efficiently. The model employs a multi-stage training strategy to enhance its cross-modal capabilities, ensuring it can handle intricate conversations effectively. With its impressive performance and reduced parameter size, InteractiveOmni sets a new standard for lightweight models in the field of interactive AI systems."
                },
                "zh": {
                    "title": "全模态交互的智能新纪元",
                    "desc": "InteractiveOmni是一种统一的全模态大型语言模型，专注于音频-视觉的多轮交互。该模型集成了视觉编码器、音频编码器、大型语言模型和语音解码器，旨在实现全面的理解和生成能力。通过多阶段训练策略，InteractiveOmni能够有效处理复杂的多轮对话，并在长时间记忆和语音交互方面表现出色。实验结果表明，InteractiveOmni在多个基准测试中超越了领先的开源模型，提供了更智能的音频-视觉交互体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13626",
            "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
            "url": "https://huggingface.co/papers/2510.13626",
            "abstract": "State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
            "score": 21,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "2e3c1031d59c2710",
            "authors": [
                "Senyu Fei",
                "Siyin Wang",
                "Junhao Shi",
                "Zihao Dai",
                "Jikun Cai",
                "Pengfang Qian",
                "Li Ji",
                "Xinzhe He",
                "Shiduo Zhang",
                "Zhaoye Fei",
                "Jinlan Fu",
                "Jingjing Gong",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Shanghai Innovation Institute",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13626.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#video",
                    "#interpretability",
                    "#security"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Хрупкость VLA моделей: высокие бенчмарки скрывают критические уязвимости",
                    "desc": "Исследователи провели систематический анализ уязвимостей современных Visual-Language-Action (VLA) моделей для робототехники. Несмотря на впечатляющие результаты на бенчмарках (до 95% успеха), модели оказались крайне чувствительны к возмущениям: изменение ракурса камеры или начальной позы робота снижает производительность до 30%. Особенно тревожно, что модели практически игнорируют языковые инструкции, хотя должны на них опираться. Работа показывает, что высокие показатели на бенчмарках не гарантируют реальной надёжности и требуется пересмотр методов оценки VLA моделей."
                },
                "en": {
                    "title": "Unmasking the Fragility of Visual-Language-Action Models",
                    "desc": "This paper investigates the vulnerabilities of state-of-the-art Visual-Language-Action (VLA) models in robotic manipulation tasks. Despite achieving high benchmark scores, these models show significant weaknesses when faced with changes in camera angles and robot starting positions, leading to drastic drops in performance. The study systematically tests various perturbations, revealing that the models are particularly sensitive to environmental changes while largely ignoring language instructions. The findings suggest that high performance on benchmarks does not guarantee robustness, emphasizing the need for better evaluation methods that reflect real-world conditions."
                },
                "zh": {
                    "title": "高分不等于高可靠性，VLA模型需更严谨评估",
                    "desc": "本研究分析了视觉-语言-动作（VLA）模型在机器人操作中的脆弱性。尽管这些模型在基准测试中表现出色，但在不同的扰动下，尤其是相机视角和机器人初始状态的变化时，表现却极为不稳定。我们的分析显示，模型对扰动因素极为敏感，性能在轻微扰动下可能从95%降至30%以下。更令人惊讶的是，模型对语言指令的变化几乎没有反应，往往完全忽视这些指令。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07944",
            "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
            "url": "https://huggingface.co/papers/2510.07944",
            "abstract": "CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
            "score": 20,
            "issue_id": 6445,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "73e48c906fb522f5",
            "authors": [
                "Tianrui Zhang",
                "Yichen Liu",
                "Zilin Guo",
                "Yuxin Guo",
                "Jingcheng Ni",
                "Chenjing Ding",
                "Dan Xu",
                "Lewei Lu",
                "Zehuan Wu"
            ],
            "affiliations": [
                "Sensetime Research",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07944.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#optimization",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "4D-реконструкция динамических сцен через видео диффузию",
                    "desc": "Статья представляет CVD-STORM — модель для генерации видео с нескольких ракурсов, которая также оценивает глубину динамических сцен. Авторы улучшили стандартный VAE, добавив задачу 4D-реконструкции, что позволило модели лучше кодировать пространственные структуры и временную динамику. Интеграция такого VAE в процесс видео-диффузии значительно повышает качество генерации по метрикам FID и FVD. Модель особенно полезна для автономного вождения, где требуется не только реалистичное видео, но и геометрическая информация о сцене."
                },
                "en": {
                    "title": "Enhancing Video Generation with Depth Estimation Using CVD-STORM",
                    "desc": "CVD-STORM is a novel cross-view video diffusion model designed to improve the quality of video generation while also providing depth estimation for dynamic scenes. It employs a spatial-temporal reconstruction Variational Autoencoder (VAE) that enhances the model's ability to capture 3D structures and temporal dynamics through a fine-tuning process. By integrating this VAE into the video diffusion framework, CVD-STORM achieves significant improvements in video generation metrics such as FID and FVD. The model also utilizes a Gaussian Splatting Decoder to effectively reconstruct dynamic scenes, offering valuable geometric insights for better scene understanding."
                },
                "zh": {
                    "title": "CVD-STORM：提升视频生成与深度估计的创新模型",
                    "desc": "CVD-STORM是一种跨视角视频扩散模型，结合了时空重建的变分自编码器（VAE），旨在提升视频生成质量并为动态场景提供深度估计。该模型通过辅助的4D重建任务对VAE进行微调，从而增强其编码三维结构和时间动态的能力。随后，将该VAE集成到视频扩散过程中，显著提高了生成质量。实验结果表明，该模型在FID和FVD指标上均有显著提升，同时联合训练的高斯点云解码器有效重建动态场景，为全面理解场景提供了有价值的几何信息。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13809",
            "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.13809",
            "abstract": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
            "score": 18,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "74cd3b0ff8137389",
            "authors": [
                "Sihui Ji",
                "Xi Chen",
                "Xin Tao",
                "Pengfei Wan",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13809.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#rl",
                    "#video",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Обучение видео-моделей физическим законам через предпочтения",
                    "desc": "PhysMaster улучшает генерацию видео, добавляя понимание физических законов через специальный PhysEncoder. Система использует reinforcement learning и Direct Preference Optimization, чтобы научить модель генерировать физически правдоподобные видео из статичных изображений. PhysEncoder извлекает физическую информацию из входного изображения (позиции объектов, их взаимодействия) и использует её как дополнительное условие при генерации. Это универсальное plug-in решение, которое можно применять к различным физическим сценариям, приближая video generation модели к роли настоящих world models."
                },
                "en": {
                    "title": "Enhancing Video Realism with Physics-Aware Generation",
                    "desc": "PhysMaster is a novel approach to video generation that incorporates physical knowledge to enhance the realism of generated videos. It utilizes a component called PhysEncoder, which extracts physical information from input images to guide the video generation process. By employing reinforcement learning and Direct Preference Optimization, PhysMaster optimizes the model's understanding of physical dynamics, ensuring that the generated videos are not only visually appealing but also adhere to the laws of physics. This method demonstrates the potential for creating more accurate world models that can be applied to various physical scenarios."
                },
                "zh": {
                    "title": "PhysMaster：提升视频生成的物理意识",
                    "desc": "PhysMaster 是一种通过整合物理知识来增强视频生成的模型。它使用 PhysEncoder 编码物理信息，以提高视频生成模型的物理意识。该模型采用强化学习和直接偏好优化（DPO）来优化物理表示，确保生成的视频符合物理规律。PhysMaster 提供了一种通用的解决方案，适用于各种物理过程的表示学习，能够广泛应用于物理意识视频生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13804",
            "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
            "url": "https://huggingface.co/papers/2510.13804",
            "abstract": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
            "score": 18,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "85e2081e6a4e6525",
            "authors": [
                "Xinchen Zhang",
                "Xiaoying Zhang",
                "Youbin Wu",
                "Yanbin Cao",
                "Renrui Zhang",
                "Ruihang Chu",
                "Ling Yang",
                "Yujiu Yang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13804.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальный верификатор для надежной проверки визуальных результатов в мультимодальных моделях",
                    "desc": "Статья представляет Generative Universal Verifier — новую концепцию для улучшения мультимодального рассуждения в vision-language моделях через визуальную верификацию. Авторы создали бенчмарк ViVerBench из 16 категорий задач и обучили OmniVerifier-7B — первую универсальную генеративную модель для проверки визуальных результатов, показавшую прирост +8.3 на бенчмарке. Предложена парадигма OmniVerifier-TTS для последовательного test-time scaling, которая итеративно улучшает генерацию и редактирование изображений через детальную оптимизацию. Метод превосходит существующие подходы типа Best-of-N на бенчмарках T2I-ReasonBench (+3.7) и GenEval++ (+4.3), обеспечивая более надежные и контролируемые системы рассуждений."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reliable Visual Verification",
                    "desc": "The Generative Universal Verifier is a new tool that improves how machines understand and generate visual information alongside text. It introduces ViVerBench, a benchmark for testing how well models can verify visual outcomes, revealing that current models struggle compared to human performance. The paper also presents OmniVerifier-7B, a generative verifier that enhances visual verification capabilities and shows significant improvements in benchmark scores. Additionally, OmniVerifier-TTS offers a method for refining image generation and editing, leading to better overall performance in multimodal reasoning tasks."
                },
                "zh": {
                    "title": "提升多模态推理的可靠性与生成能力",
                    "desc": "本文介绍了一种新概念——生成通用验证器（Generative Universal Verifier），旨在提升多模态推理能力。我们构建了ViVerBench，这是一个涵盖16类关键任务的基准，用于评估多模态推理中的视觉结果。通过训练OmniVerifier-7B，我们识别出视觉验证中的三种基本能力，并展示它们的协同作用。最后，我们提出了OmniVerifier-TTS，通过迭代优化提升生成能力，推动了更可靠的多模态推理系统的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04767",
            "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
            "url": "https://huggingface.co/papers/2510.04767",
            "abstract": "Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.",
            "score": 18,
            "issue_id": 6445,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "b9d87711f8ddf258",
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Seunghyuk Oh",
                "Minjae Lee",
                "Yuchen Zeng",
                "Shuibai Zhang",
                "Coleman Hooper",
                "Yuezhou Hu",
                "Hyung Il Koo",
                "Nam Ik Cho",
                "Kangwook Lee"
            ],
            "affiliations": [
                "FuriosaAI",
                "KRAFTON AI",
                "Microsoft Research",
                "Seoul National University",
                "UC Berkeley",
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04767.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Проблема скорости против качества в параллельном декодировании диффузионных LLM",
                    "desc": "Статья исследует проблемы параллельного декодирования в диффузионных языковых моделях (dLLM), которые теоретически могут ускорить генерацию текста. Авторы показывают, что предположение об условной независимости токенов при параллельном декодировании игнорирует зависимости между токенами, что неизбежно ухудшает качество генерации. Представлен новый бенчмарк ParallelBench, специально разработанный для оценки dLLM на задачах, тривиальных для людей и авторегрессионных моделей, но сложных для диффузионных моделей. Исследование выявляет фундаментальный компромисс между скоростью и качеством, подчеркивая необходимость разработки новых методов декодирования для эффективных dLLM."
                },
                "en": {
                    "title": "Enhancing Quality in Fast Decoding: The Need for Innovation in dLLMs",
                    "desc": "This paper discusses the challenges of using parallel decoding in diffusion language models (dLLMs), which can lead to a decline in the quality of generated text due to overlooked token dependencies. The authors highlight that while dLLMs can speed up inference, the assumption of conditional independence can harm performance when token relationships are strong. They introduce ParallelBench, a new benchmark designed to evaluate dLLMs under realistic tasks that are easy for humans but difficult for dLLMs using parallel decoding. The study reveals that current parallel decoding methods do not effectively adjust to task complexity, resulting in significant quality loss, emphasizing the need for better decoding strategies."
                },
                "zh": {
                    "title": "平行解码的挑战与新基准的必要性",
                    "desc": "在扩散大语言模型（dLLMs）中，平行解码会导致生成质量下降，因为它忽略了令牌之间的依赖关系。这种条件独立假设使得在强依赖关系的情况下，平行解码的效果不佳。为了应对这一问题，本文首先进行了信息论分析，并提出了ParallelBench基准，专门用于评估dLLMs在平行解码下的表现。我们的研究表明，现有的平行解码策略在任务难度变化时难以调整并保持质量，强调了开发新解码方法的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13802",
            "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
            "url": "https://huggingface.co/papers/2510.13802",
            "abstract": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
            "score": 16,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "2a519ed83bf3da2a",
            "authors": [
                "Xinhang Liu",
                "Yuxi Xiao",
                "Donny Y. Chen",
                "Jiashi Feng",
                "Yu-Wing Tai",
                "Chi-Keung Tang",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Dartmouth College",
                "HKUST",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13802.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Отследить всё: предсказание траекторий всех пикселей видео за один проход",
                    "desc": "Статья представляет Trace Anything — нейросеть, которая предсказывает траектории движения всех пикселей в видео за один проход. Модель представляет видео как поле траекторий, где каждому пикселю соответствует непрерывная 3D-траектория, параметризованная B-сплайнами с контрольными точками. Обученная на масштабных 4D данных, система достигает state-of-the-art результатов в оценке траекторных полей и превосходит существующие методы по эффективности. Модель демонстрирует emergent abilities, включая предсказание движения в будущем и пространственно-временное слияние данных."
                },
                "en": {
                    "title": "Predicting Video Trajectories with Efficiency and Precision",
                    "desc": "The paper introduces Trace Anything, a neural network designed to predict video trajectories efficiently in a single pass. It utilizes a novel representation called Trajectory Field, which maps each pixel in a video to a continuous 3D trajectory function over time. This approach allows the model to generate control points for B-splines, enabling accurate trajectory predictions at any moment. The results show that Trace Anything not only achieves state-of-the-art performance but also demonstrates significant efficiency and advanced capabilities like motion forecasting and goal-conditioned manipulation."
                },
                "zh": {
                    "title": "单次预测，轨迹追踪的未来",
                    "desc": "本文提出了一种名为Trace Anything的神经网络，用于在单次前向传播中预测视频的轨迹场。该方法通过将视频表示为每个像素的连续三维轨迹函数，来有效建模和预测视频中的动态。实验结果表明，Trace Anything在轨迹场估计的基准测试中表现出色，并在点跟踪基准上也具有竞争力。此外，该模型在效率上有显著提升，能够实现目标条件的操作、运动预测和时空融合等新兴能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13621",
            "title": "The Role of Computing Resources in Publishing Foundation Model Research",
            "url": "https://huggingface.co/papers/2510.13621",
            "abstract": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/",
            "score": 10,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "f9f73f3064dbf2e4",
            "authors": [
                "Yuexing Hao",
                "Yue Huang",
                "Haoran Zhang",
                "Chenyang Zhao",
                "Zhenwen Liang",
                "Paul Pu Liang",
                "Yue Zhao",
                "Lichao Sun",
                "Saleh Kalantari",
                "Xiangliang Zhang",
                "Marzyeh Ghassemi"
            ],
            "affiliations": [
                "CSE, University of Notre Dame, South Bend, 46556, USA",
                "Computer Science Department, Lehigh University, Bethlehem, 18015, USA",
                "Computer Science Department, University of California, Los Angeles, 90095, USA",
                "Cornell University, Ithaca, 14850, USA",
                "EECS, MIT, Cambridge, 02135, USA",
                "School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13621.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#ethics"
                ],
                "emoji": "💰",
                "ru": {
                    "title": "Деньги решают: как вычислительные ресурсы влияют на исследования foundation models",
                    "desc": "Исследование анализирует связь между вычислительными ресурсами и научным прогрессом в области foundation models (больших базовых моделей). Авторы изучили 6517 статей и опросили 229 исследователей, обнаружив корреляцию между доступом к GPU и национальным финансированием, а также цитируемостью работ. Интересно, что не было найдено сильной связи между ресурсами и типом организации (академическая или промышленная), доменом или методологией исследования. Авторы рекомендуют создавать общедоступные и доступные по цене вычислительные платформы, чтобы снизить барьер входа для исследователей с ограниченными ресурсами."
                },
                "en": {
                    "title": "Empowering AI Research Through Shared Computing Resources",
                    "desc": "This paper investigates how computing resources, such as GPUs and funding, influence the progress of foundation model research in AI. By analyzing 6517 papers and surveying 229 authors, the study finds a strong correlation between increased computing resources and national funding and citations. However, it reveals that these resources do not significantly impact the research environment, domain, or methodology. The authors recommend creating shared computing resources to support under-resourced researchers, promoting diversity and innovation in the field."
                },
                "zh": {
                    "title": "计算资源与基础模型研究的关系",
                    "desc": "本论文探讨了计算资源与基础模型研究的科学进展之间的关系。我们分析了6517篇2022至2024年间发表的基础模型论文，并调查了229位第一作者对计算资源影响的看法。研究发现，计算资源的增加与国家资金分配和引用次数相关，但与研究环境、领域或研究方法没有显著相关性。我们建议个人和机构应专注于创建共享和可负担的计算机会，以降低资源不足研究者的进入门槛。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13515",
            "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
            "url": "https://huggingface.co/papers/2510.13515",
            "abstract": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
            "score": 10,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "4e7810d5695a73ea",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Kaichen Zhang",
                "Xiang An",
                "Ziyong Feng",
                "Yueyi Zhang",
                "Weidong Cai",
                "Jiankang Deng",
                "Lidong Bing"
            ],
            "affiliations": [
                "Imperial College London",
                "LMMs-Lab Team",
                "M.R.L. Team",
                "MiroMind AI",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "MLLM как судья для умного майнинга hard negatives в мультимодальных эмбеддингах",
                    "desc": "Статья представляет UniME-V2 — универсальную мультимодальную модель эмбеддингов, которая использует мультимодальные большие языковые модели (MLLMs) для улучшения обучения представлений. Ключевая идея заключается в использовании MLLM в качестве судьи для оценки семантического соответствия пар запрос-кандидат и генерации мягких оценок совпадения. Эти оценки помогают находить разнообразные и качественные hard negatives, а также служат мягкими метками для обучения, что повышает способность модели различать семантически близкие объекты. Метод достигает state-of-the-art результатов на бенчмарке MMEB и задачах поиска, также предложена модель-ранкер UniME-V2-Reranker для дополнительного улучшения."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with Smart Negative Mining",
                    "desc": "The paper introduces a new model called Universal Multimodal Embedding version 2 (UniME-V2) that improves how machines understand and represent different types of data. It uses advanced machine learning language models (MLLMs) to find and evaluate hard negative examples, which are crucial for training. By generating soft semantic matching scores, the model can better distinguish between similar candidates and improve its ability to identify relevant information. The results show that UniME-V2 outperforms existing methods in various tasks, making it a significant advancement in multimodal representation learning."
                },
                "zh": {
                    "title": "提升多模态表示学习的创新模型",
                    "desc": "本文提出了一种新颖的通用多模态嵌入模型（UniME-V2），旨在通过识别多样化的高质量困难负样本来增强表示学习。该模型利用多语言大模型（MLLMs）来评估查询-候选对的语义对齐，并生成软语义匹配分数，从而改善区分能力。通过构建潜在的困难负样本集，UniME-V2能够有效减轻假负样本的影响，并识别出多样化的高质量困难负样本。实验结果表明，该方法在多个检索任务上达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13759",
            "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
            "url": "https://huggingface.co/papers/2510.13759",
            "abstract": "Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
            "score": 9,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "9e262f2fe4ddde55",
            "authors": [
                "Kai Zou",
                "Ziqi Huang",
                "Yuhao Dong",
                "Shulin Tian",
                "Dian Zheng",
                "Hongbo Liu",
                "Jingwen He",
                "Bin Liu",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13759.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Двусторонняя синергия понимания и генерации изображений",
                    "desc": "Uni-MMMU — это бенчмарк для оценки unified мультимодальных моделей, которые одновременно понимают и генерируют изображения. Существующие тесты оценивают эти способности изолированно, но Uni-MMMU проверяет их реальную интеграцию в восьми областях, включая науку, программирование и математику. Каждая задача требует от моделей либо использовать понимание концепций для точной визуальной генерации, либо применять генерацию как инструмент для аналитического рассуждения. Результаты показывают существенные различия в производительности разных типов моделей и раскрывают, когда и как визуальное понимание и генерация усиливают друг друга."
                },
                "en": {
                    "title": "Bridging Visual Understanding and Generation with Uni-MMMU",
                    "desc": "Uni-MMMU is a new benchmark designed to assess how well visual understanding and generation work together across different fields. It focuses on tasks that require both skills, rather than evaluating them separately. The benchmark includes eight domains, such as science and mathematics, where models must use their understanding to create visuals or use visuals to enhance reasoning. By testing various models, Uni-MMMU highlights important differences in performance and shows how these two abilities can support each other, paving the way for better unified models."
                },
                "zh": {
                    "title": "视觉理解与生成的双向协同评估",
                    "desc": "Uni-MMMU是一个基准测试，评估视觉理解与生成之间的双向协同，涵盖多个领域。该基准系统地揭示了生成与理解之间的相互作用，特别是在科学、编程、数学和谜题等推理中心领域。每个任务都要求模型利用概念理解来指导精确的视觉合成，或利用生成作为分析推理的认知支架。通过对最先进的统一模型进行广泛评估，Uni-MMMU揭示了性能差异和跨模态依赖，为统一模型的进一步发展提供了可靠的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10977",
            "title": "Revisiting Model Interpolation for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2510.10977",
            "abstract": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at https://github.com/wutaiqiang/MI{Github}.",
            "score": 8,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            },
            "hash": "bdc53b166ac44504",
            "authors": [
                "Taiqiang Wu",
                "Runming Yang",
                "Tao Liu",
                "Jiahao Wang",
                "Ngai Wong"
            ],
            "affiliations": [
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10977.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Три стадии интерполяции: простое слияние моделей побеждает сложные методы",
                    "desc": "Исследователи изучили простейший метод слияния моделей - прямую интерполяцию весов между Instruct и Thinking моделями. Они обнаружили, что интерполяция моделей проходит через три различных стадии эволюции с уникальным поведением на траектории рассуждений. Стратегически подобранная интерполированная модель неожиданно превосходит сложные baseline методы слияния как по эффективности, так и по результативности. Работа предлагает практический фреймворк для создания моделей с точно заданными способностями к рассуждениям при оптимальном балансе производительности и затрат."
                },
                "en": {
                    "title": "Unlocking Efficient Reasoning through Model Interpolation",
                    "desc": "This paper explores the concept of model merging, particularly focusing on Instruct and Thinking models, to enhance reasoning efficiency. The authors analyze a basic method of directly interpolating weights from two models, revealing a three-stage evolution in the reasoning process. Their findings indicate that a well-interpolated model can outperform more complex merging techniques in terms of both efficiency and effectiveness. The research provides a structured approach to model interpolation, enabling the development of models with specific reasoning strengths, supported by comprehensive experiments and ablation studies."
                },
                "zh": {
                    "title": "模型插值：高效推理的新路径",
                    "desc": "本文探讨了模型合并，特别是在指令和思维模型上的应用，展示了其在高效推理方面的卓越表现。我们系统地回顾了最简单的合并方法，即直接插值两个权重，并观察到模型插值遵循三阶段的演变范式，具有不同的推理轨迹特征。这些动态为在性能与成本之间的权衡提供了原则性指导。实证结果表明，经过战略性插值的模型在效率和有效性上超越了复杂的模型合并基线，进一步通过广泛的消融研究验证了我们的发现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10921",
            "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
            "url": "https://huggingface.co/papers/2510.10921",
            "abstract": "FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.",
            "score": 8,
            "issue_id": 6445,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            },
            "hash": "48bb15d507d38753",
            "authors": [
                "Chunyu Xie",
                "Bin Wang",
                "Fanjing Kong",
                "Jincheng Li",
                "Dawei Liang",
                "Ji Ao",
                "Dawei Leng",
                "Yuhui Yin"
            ],
            "affiliations": [
                "360.cn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10921.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Детальное двуязычное выравнивание изображений и текста",
                    "desc": "FG-CLIP 2 — это двуязычная vision-language модель для английского и китайского языков, которая улучшает детальное выравнивание визуального и текстового контента. Модель использует богатый fine-grained supervision, включая сопоставление регионов с текстом и моделирование длинных описаний, а также новую функцию потерь TIC (Textual Intra-modal Contrastive) для различения семантически похожих описаний. В отличие от CLIP, который хорошо работает с глобальным выравниванием, FG-CLIP 2 лучше захватывает детали объектов, их атрибуты и пространственные отношения. Модель достигает state-of-the-art результатов на 29 датасетах в 8 задачах и включает новый бенчмарк для оценки китайского мультимодального понимания."
                },
                "en": {
                    "title": "Enhancing Bilingual Vision-Language Alignment with FG-CLIP 2",
                    "desc": "FG-CLIP 2 is a bilingual vision-language model that improves the alignment between visual content and text descriptions in both English and Chinese. It addresses the limitations of existing models by using rich supervision techniques, such as region-text matching and long-caption modeling, to enhance fine-grained understanding. The model introduces a new loss function called Textual Intra-modal Contrastive (TIC) loss, which helps differentiate between similar captions more effectively. With extensive training on a diverse dataset and rigorous evaluation, FG-CLIP 2 achieves state-of-the-art performance across various tasks and datasets, making it a significant advancement in bilingual multimodal understanding."
                },
                "zh": {
                    "title": "双语视觉-语言模型的细粒度对齐新突破",
                    "desc": "FG-CLIP 2 是一种双语视觉-语言模型，旨在提高英语和中文之间的细粒度对齐能力。该模型通过丰富的监督学习和新的文本内对比损失（TIC损失）来实现这一目标，能够更好地捕捉视觉内容与语言描述之间的细微差别。FG-CLIP 2 在多个数据集和任务上表现出色，超越了现有的模型，特别是在双语理解方面。我们还推出了一个新的基准测试，以评估中文的多模态理解能力，推动未来的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13778",
            "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
            "url": "https://huggingface.co/papers/2510.13778",
            "abstract": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
            "score": 7,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "9f1a9178757021cf",
            "authors": [
                "Xinyi Chen",
                "Yilun Chen",
                "Yanwei Fu",
                "Ning Gao",
                "Jiaya Jia",
                "Weiyang Jin",
                "Hao Li",
                "Yao Mu",
                "Jiangmiao Pang",
                "Yu Qiao",
                "Yang Tian",
                "Bin Wang",
                "Bolun Wang",
                "Fangjing Wang",
                "Hanqing Wang",
                "Tai Wang",
                "Ziqin Wang",
                "Xueyuan Wei",
                "Chao Wu",
                "Shuai Yang",
                "Jinhui Ye",
                "Junqiu Yu",
                "Jia Zeng",
                "Jingjing Zhang",
                "Jinyu Zhang",
                "Shi Zhang",
                "Feng Zheng",
                "Bowen Zhou",
                "Yangkun Zhu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13778.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Роботы учатся действовать через понимание пространства",
                    "desc": "InternVLA-M1 — это единая система для управления роботами, которая связывает инструкции с действиями через пространственное понимание сцены. Модель обучается в два этапа: сначала учится определять «где действовать» на 2.3 миллионах примеров пространственного рассуждения, затем учится «как действовать» для конкретного робота. Система показывает улучшения от 4% до 20% в различных симуляциях и реальных задачах, особенно при работе с новыми объектами. Ключевая идея — использование spatial grounding как моста между языковыми командами и физическими действиями робота."
                },
                "en": {
                    "title": "Empowering Robots with Spatially Guided Intelligence",
                    "desc": "The paper presents InternVLA-M1, a framework that enhances robots' ability to follow instructions by integrating spatial grounding with vision-language-action training. This approach involves a two-stage process: first, pre-training the model on a large dataset to understand where to act based on spatial reasoning, and second, fine-tuning it to determine how to act using spatial prompts. The results show significant performance improvements in various robotic tasks, demonstrating the effectiveness of spatial guidance in robot control. Overall, this work emphasizes the importance of spatially informed training for developing versatile and intelligent robots."
                },
                "zh": {
                    "title": "空间引导训练：提升机器人智能的关键",
                    "desc": "本文介绍了一个名为InternVLA-M1的统一框架，旨在提升遵循指令的机器人智能。该框架通过空间引导的视觉-语言-行动训练，建立了指令与机器人动作之间的关键联系。InternVLA-M1采用两阶段流程：首先进行空间引导的预训练，以确定“在哪里行动”；然后进行空间引导的后训练，以生成“如何行动”的具体动作。实验结果表明，该方法在多个任务和模拟中显著提高了机器人的表现，展示了空间引导训练在可扩展和通用机器人中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10274",
            "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
            "url": "https://huggingface.co/papers/2510.10274",
            "abstract": "A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
            "score": 5,
            "issue_id": 6446,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 октября",
                "en": "October 11",
                "zh": "10月11日"
            },
            "hash": "5a799bed54a78313",
            "authors": [
                "Jinliang Zheng",
                "Jianxiong Li",
                "Zhihao Wang",
                "Dongxiu Liu",
                "Xirui Kang",
                "Yuchun Feng",
                "Yinan Zheng",
                "Jiayin Zou",
                "Yilun Chen",
                "Jia Zeng",
                "Ya-Qin Zhang",
                "Jiangmiao Pang",
                "Jingjing Liu",
                "Tai Wang",
                "Xianyuan Zhan"
            ],
            "affiliations": [
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10274.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agi",
                    "#agents",
                    "#training",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мягкие промпты для универсальных роботов",
                    "desc": "Исследователи предложили новый подход Soft Prompt для обучения Vision-Language-Action моделей, которые управляют разными роботами. Метод использует отдельные наборы обучаемых эмбеддингов для каждого источника данных, что позволяет модели эффективно учитывать особенности различных роботизированных платформ. Архитектура X-VLA основана на flow-matching и стандартных Transformer энкодерах с минимальным добавлением параметров. Модель размером 0.9B параметров достигла лучших результатов в 6 симуляциях и на 3 реальных роботах, демонстрируя превосходную гибкость и быструю адаптацию к новым задачам."
                },
                "en": {
                    "title": "Enhancing VLA Models with Soft Prompts for Diverse Robotics",
                    "desc": "This paper introduces a new Soft Prompt method that improves Vision-Language-Action (VLA) models by using learnable embeddings tailored for different robotic data sources. By incorporating prompt learning into cross-embodiment robot learning, the approach allows for better utilization of diverse datasets with minimal additional parameters. The proposed X-VLA architecture employs soft-prompted Transformer encoders, which enhances scalability and simplicity. The results show that X-VLA achieves state-of-the-art performance across various simulations and real-world robots, demonstrating its effectiveness in adapting to different tasks and environments."
                },
                "zh": {
                    "title": "软提示助力视觉-语言-动作模型的突破",
                    "desc": "本文提出了一种新颖的软提示方法，旨在增强视觉-语言-动作（VLA）模型的性能。通过为不同的机器人数据源引入可学习的嵌入，模型能够更好地利用多样化的机器人数据。该方法在多个仿真和真实机器人上进行了评估，显示出在灵活性和适应性方面的卓越表现。我们的X-VLA架构通过软提示的标准Transformer编码器实现了高效的性能和简单的设计。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11958",
            "title": "Direct Multi-Token Decoding",
            "url": "https://huggingface.co/papers/2510.11958",
            "abstract": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.",
            "score": 4,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            },
            "hash": "a9246dd44d38f230",
            "authors": [
                "Xuan Luo",
                "Weizhi Wang",
                "Xifeng Yan"
            ],
            "affiliations": [
                "Department of Computer Science, UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11958.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Генерация нескольких токенов за один проход через поздние слои",
                    "desc": "Исследователи предложили метод Direct Multi-Token Decoding (DMTD), который ускоряет inference больших языковых моделей. Метод основан на гипотезе, что ранние слои понимают контекст, средние обрабатывают задачу, а поздние преобразуют представления в токены. После обработки ранними и средними слоями можно генерировать несколько токенов, используя только поздние слои, избегая повторного прохода через всю сеть. Метод показал двукратное ускорение на модели Qwen3-4B без дополнительных параметров и с минимальной потерей качества."
                },
                "en": {
                    "title": "Speed Up Language Generation with DMTD!",
                    "desc": "Direct Multi-Token Decoding (DMTD) is a new method that speeds up the process of generating text with large language models by only using the late layers of the model for token generation. This approach takes advantage of the fact that early and middle layers have already processed the input, allowing the late layers to efficiently produce multiple tokens without reprocessing. DMTD does not require any extra parameters or complex routines, making it a straightforward enhancement to existing models. Initial results show that a fine-tuned DMTD model can achieve up to a 2x increase in speed with minimal impact on performance, and its effectiveness is expected to grow with larger datasets."
                },
                "zh": {
                    "title": "直接多标记解码：加速推理的新方法",
                    "desc": "直接多标记解码（DMTD）通过仅使用后层进行标记生成，加速了大型语言模型的推理，显著提高了速度且性能损失极小。研究表明，预训练的大型语言模型中的早期、中期和后期层各自承担不同的角色。我们的假设是，一旦早期和中期层处理完输入，生成的隐藏状态就足以支持仅使用后期层生成多个标记，从而避免重复遍历早期和中期层。DMTD方法在不增加额外参数或辅助程序的情况下，已在有限数据集上展示出良好的效果，速度提升可达2倍，且性能损失很小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13602",
            "title": "NOSA: Native and Offloadable Sparse Attention",
            "url": "https://huggingface.co/papers/2510.13602",
            "abstract": "NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2).",
            "score": 3,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "2b3732fd1262cb04",
            "authors": [
                "Yuxiang Huang",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13602.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#long_context",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение декодирования LLM через локальное кэширование",
                    "desc": "NOSA — это новый метод обучаемого разреженного внимания (sparse attention), который решает проблему медленного декодирования в длинных контекстах. Ключевая идея заключается в том, что выбор токенов при разреженном внимании демонстрирует локальность между соседними шагами декодирования, что позволяет выгружать KV-кэш с GPU на CPU. NOSA вводит явные ограничения локальности, разделяя выбор токенов на компоненты, зависящие и не зависящие от запроса, что сокращает передачу данных между CPU и GPU. Результаты показывают ускорение декодирования до 2.3 раза по сравнению с базовым методом при сохранении качества модели."
                },
                "en": {
                    "title": "NOSA: Boosting Decoding Efficiency with Sparse Attention",
                    "desc": "NOSA is a novel framework that improves the efficiency of decoding in large language models (LLMs) by enabling effective offloading of key-value (KV) caches. It leverages trainable sparse attention to minimize memory access while maintaining high performance during long-context processing. The framework introduces locality constraints in token selection, allowing for reduced KV transfers between CPU and GPU, which is crucial for enhancing throughput. Experimental results demonstrate that NOSA achieves significant improvements in decoding speed without sacrificing the quality of the model's outputs."
                },
                "zh": {
                    "title": "NOSA：高效解码的新方法",
                    "desc": "NOSA是一种可训练的稀疏注意力框架，旨在提高解码效率，允许高效的键值（KV）缓存卸载而不影响性能。该方法通过在相邻解码步骤中选择具有强局部性的token，来实现KV缓存的卸载，从而减少内存访问。尽管现有的稀疏注意力方法未能有效减少KV缓存的大小，NOSA通过引入显式的局部性约束，优化了token选择过程，降低了KV传输的成本。经过预训练，NOSA在解码吞吐量上实现了高达2.3倍的提升，同时保持了接近无损的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12560",
            "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
            "url": "https://huggingface.co/papers/2510.12560",
            "abstract": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
            "score": 3,
            "issue_id": 6445,
            "pub_date": "2025-10-14",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            },
            "hash": "7b32811bacd13120",
            "authors": [
                "Xiaoji Zheng",
                "Ziyuan Yang",
                "Yanhao Chen",
                "Yuhang Peng",
                "Yuanrong Tang",
                "Gengyuan Liu",
                "Bokui Chen",
                "Jiangtao Gong"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "The Hong Kong Polytechnic University",
                "Tsinghua University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12560.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#games",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🏎️",
                "ru": {
                    "title": "Конкурентное обучение двух агентов для безопасного автопилота",
                    "desc": "Статья предлагает новый подход к автономному вождению под названием CoIRL-AD, который объединяет imitation learning и reinforcement learning в единую систему с двумя конкурирующими агентами. В отличие от традиционного метода, где сначала применяется IL, а затем RL fine-tuning, здесь оба агента взаимодействуют и обмениваются знаниями во время обучения через механизм конкуренции. Этот подход решает проблему конфликта градиентов и позволяет избежать слабой генерализации IL-моделей и нестабильности RL. Эксперименты на датасете nuScenes показали снижение количества столкновений на 18% и улучшенную работу в редких сценариях."
                },
                "en": {
                    "title": "Revolutionizing Autonomous Driving with CoIRL-AD: A Dual-Policy Approach",
                    "desc": "This paper presents CoIRL-AD, a novel framework that integrates imitation learning (IL) and reinforcement learning (RL) for training autonomous driving models. Unlike traditional methods that use IL for pretraining followed by RL fine-tuning, CoIRL-AD allows IL and RL agents to interact during the training process. This dual-policy approach promotes knowledge sharing and mitigates issues like gradient conflicts, leading to better performance. The results demonstrate a significant reduction in collision rates and improved generalization on challenging driving scenarios, showcasing the effectiveness of this combined training strategy."
                },
                "zh": {
                    "title": "结合模仿学习与强化学习的自动驾驶新方法",
                    "desc": "本论文提出了一种名为CoIRL-AD的双策略框架，旨在结合模仿学习（IL）和强化学习（RL）以提高自动驾驶模型的泛化能力。与传统的两阶段方法不同，CoIRL-AD允许IL和RL代理在训练过程中相互作用，从而促进知识的交流。该框架引入了一种基于竞争的机制，能够有效防止梯度冲突。实验结果表明，与基线相比，CoIRL-AD在nuScenes数据集上减少了18%的碰撞率，并在长尾场景中表现出更强的泛化能力和性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13744",
            "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
            "url": "https://huggingface.co/papers/2510.13744",
            "abstract": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "ac775fe89e3a3d8b",
            "authors": [
                "Shrey Pandit",
                "Austin Xu",
                "Xuan-Phi Nguyen",
                "Yifei Ming",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13744.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Проверка математических рассуждений AI на прочность",
                    "desc": "Исследователи создали Hard2Verify — бенчмарк для оценки верификаторов математических рассуждений LLM, созданный с помощью 500 часов ручной разметки. Верификаторы должны находить ошибки в решениях сложных математических задач, созданных современными языковыми моделями, на уровне отдельных шагов доказательства. Тестирование 29 моделей показало значительное отставание open-source верификаторов от closed-source систем. Работа анализирует причины низкой производительности верификации, влияние вычислительных ресурсов и фундаментальные вопросы самопроверки AI-моделей."
                },
                "en": {
                    "title": "Hard2Verify: Bridging the Gap in LLM Verification",
                    "desc": "The paper introduces Hard2Verify, a benchmark designed to evaluate step-level verifiers for large language model (LLM)-based mathematical reasoning systems. It highlights the importance of strong verifiers that can accurately identify mistakes in mathematical proofs, which are crucial for achieving high performance in competitions like IMO 2025. The benchmark was created through extensive human annotation, involving over 500 hours of labor, to rigorously assess the capabilities of various verification models. The study reveals significant performance gaps between open-source and closed-source verifiers, while also exploring factors that contribute to these discrepancies and the dynamics of verification processes."
                },
                "zh": {
                    "title": "Hard2Verify：评估数学推理的逐步验证器",
                    "desc": "Hard2Verify是一个人类标注的基准，用于评估基于大型语言模型（LLM）的数学推理系统的逐步验证器。该基准强调了开源模型和闭源模型之间的挑战和性能差距。为了在复杂的开放式环境中训练LLM推理器，强大的验证器是必不可少的，它们能够捕捉逐步错误。我们评估了29种生成性批评者和过程奖励模型，结果显示，除了少数优秀的模型外，开源验证器的表现普遍落后于闭源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13282",
            "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
            "url": "https://huggingface.co/papers/2510.13282",
            "abstract": "A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
            "score": 2,
            "issue_id": 6447,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "692908ce4f9b0d08",
            "authors": [
                "JiaKui Hu",
                "Zhengjian Yao",
                "Lujia Jin",
                "Yinghao Chen",
                "Yanye Lu"
            ],
            "affiliations": [
                "Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China",
                "College of Electronic Engineering, National University of Defense Technology, Changsha, China",
                "Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China",
                "JIUTIAN Research, Beijing, China",
                "National Biomedical Imaging Center, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13282.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#synthetic",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Предобучение через распознавание деградации: универсальный подход к восстановлению изображений",
                    "desc": "Статья представляет метод предобучения MaskDCPT для восстановления изображений, который использует классификацию типов деградации как слабый надзор и одновременно выполняет реконструкцию изображения. Архитектура включает энкодер для извлечения признаков из замаскированных изображений низкого качества и два декодера: один классифицирует тип деградации, другой восстанавливает высококачественное изображение. Метод объединяет преимущества masked image modeling и contrastive learning, что позволяет создать универсальное представление для задач restoration. MaskDCPT показывает значительное улучшение для CNN и Transformer архитектур (минимум +3.77 dB PSNR) и включает новый датасет UIR-2.5M с 2.5 миллионами пар изображений с 19 типами деградации."
                },
                "en": {
                    "title": "Enhancing Image Restoration with Masked Degradation Classification",
                    "desc": "The paper presents a novel Masked Degradation Classification Pre-Training method (MaskDCPT) that enhances image restoration by classifying degradation types and reconstructing images. This approach utilizes weak supervision from degradation classification while simultaneously improving image quality through reconstruction. The architecture consists of an encoder for feature extraction and two decoders for classification and reconstruction tasks, allowing for effective pre-training. Results show significant performance improvements in both CNNs and Transformers, with better generalization to new degradation types and a large dataset of 2.5 million samples for training and evaluation."
                },
                "zh": {
                    "title": "Masked Degradation Classification：提升图像恢复的创新方法",
                    "desc": "本文提出了一种名为Masked Degradation Classification Pre-Training（MaskDCPT）的方法，旨在通过对输入图像的降质类型进行分类来增强图像恢复的效果。与传统的预训练方法不同，MaskDCPT利用图像的降质类型作为极弱的监督信号，同时结合图像重建来提高性能和鲁棒性。该方法包括一个编码器和两个解码器，编码器从低质量的输入图像中提取特征，分类解码器用于识别降质类型，而重建解码器则旨在重建相应的高质量图像。通过这种设计，MaskDCPT能够有效地进行图像恢复任务，并在卷积神经网络（CNNs）和变换器（Transformers）上显著提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11062",
            "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
            "url": "https://huggingface.co/papers/2510.11062",
            "abstract": "AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.   We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
            "score": 2,
            "issue_id": 6448,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            },
            "hash": "54cb332759f2b6bc",
            "authors": [
                "Yujie Zhao",
                "Lanxiang Hu",
                "Yang Wang",
                "Minmin Hou",
                "Hao Zhang",
                "Ke Ding",
                "Jishen Zhao"
            ],
            "affiliations": [
                "Intel Corporation",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11062.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#games",
                    "#training"
                ],
                "emoji": "🤝",
                "ru": {
                    "title": "Обучение с подкреплением для команды AI-агентов",
                    "desc": "Статья представляет AT-GRPO — алгоритм обучения с подкреплением, специально разработанный для мультиагентных систем на базе LLM. Традиционные on-policy RL методы, такие как GRPO, плохо работают в мультиагентных сценариях, где промпты различаются по ролям и ходам агентов. AT-GRPO решает эту проблему через группировку по агентам и ходам, поддерживая как одно-, так и мульти-policy режимы обучения. Алгоритм показывает впечатляющие результаты: точность на задачах долгосрочного планирования выросла с 14-47% до 96-99.5%, а также значительные улучшения в математике (9-18%) и программировании (4-8%)."
                },
                "en": {
                    "title": "Boosting Multi-Agent Performance with AT-GRPO",
                    "desc": "The paper introduces AT-GRPO, a specialized reinforcement learning (RL) algorithm designed for multi-agent systems (MAS). It addresses the challenges of applying on-policy RL in MAS by implementing a grouped RL approach that considers agent roles and interaction turns. The proposed algorithm significantly improves task performance, achieving remarkable accuracy in long-horizon planning and enhancing reasoning capabilities in coding and math tasks. Overall, AT-GRPO demonstrates substantial performance gains across various applications, showcasing its effectiveness in optimizing multi-agent workflows."
                },
                "zh": {
                    "title": "AT-GRPO：多智能体系统的强化学习新突破",
                    "desc": "AT-GRPO是一种专为多智能体系统设计的强化学习算法，旨在解决在策略学习中面临的独特挑战。该算法通过角色和回合的分组方式，优化了多智能体系统中的策略学习过程。AT-GRPO在多个任务中表现出显著的性能提升，例如在长时间规划任务中，准确率从14.0%提升至96.0%至99.5%。此外，在编码和数学任务中，AT-GRPO也提高了推理性能，平均增幅分别为3.87%至7.62%和9.0%至17.93%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12831",
            "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
            "url": "https://huggingface.co/papers/2510.12831",
            "abstract": "MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit verification, and refinement, which leads to non-executable or incoherent outputs. We present MTSQL-R1, an agentic training framework for long-horizon multi-turn Text-to-SQL. We cast the task as a Markov Decision Process (MDP) in which an agent interacts with (i) a database for execution feedback and (ii) a persistent dialogue memory for coherence verification, performing an iterative propose to execute -> verify -> refine cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that MTSQL-R1 consistently outperforms strong baselines, highlighting the importance of environment-driven verification and memory-guided refinement for conversational semantic parsing. Full recipes (including code, trained models, logs, reasoning trajectories, etc.) will be released after the internal review to contribute to community research.",
            "score": 2,
            "issue_id": 6445,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 октября",
                "en": "October 12",
                "zh": "10月12日"
            },
            "hash": "54ec2109d031b622",
            "authors": [
                "Taicheng Guo",
                "Hai Wang",
                "ChaoChun Liu",
                "Mohsen Golalikhani",
                "Xin Chen",
                "Xiangliang Zhang",
                "Chandan K. Reddy"
            ],
            "affiliations": [
                "Amazon",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12831.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Итеративная генерация SQL через взаимодействие с базой данных",
                    "desc": "MTSQL-R1 — это агентный фреймворк для обучения систем преобразования текста в SQL в многошаговых диалогах. Авторы формулируют задачу как марковский процесс принятия решений, где агент итеративно генерирует SQL-запрос, выполняет его, проверяет корректность и уточняет результат. Система взаимодействует с базой данных для получения обратной связи и использует память диалога для поддержания связности. Эксперименты на бенчмарках COSQL и SPARC показывают превосходство над существующими методами благодаря циклу проверки через окружение и уточнения с учётом контекста."
                },
                "en": {
                    "title": "Transforming Multi-Turn Text-to-SQL with Iterative Learning",
                    "desc": "The paper introduces MTSQL-R1, a novel training framework designed to enhance multi-turn Text-to-SQL tasks by treating them as a Markov Decision Process (MDP). This approach allows the system to engage in iterative cycles of proposing, executing, verifying, and refining SQL queries, which improves the coherence and correctness of the generated outputs. Unlike traditional methods that generate a single query per turn without execution or verification, MTSQL-R1 leverages feedback from database interactions and maintains a dialogue memory for coherence checks. Experimental results on datasets like COSQL and SPARC show that MTSQL-R1 significantly outperforms existing models, emphasizing the value of environment-driven verification and memory-guided refinement in conversational semantic parsing."
                },
                "zh": {
                    "title": "提升多轮对话SQL转换的智能框架",
                    "desc": "MTSQL-R1是一种代理训练框架，旨在改善多轮文本到SQL的转换。它将该任务视为一个马尔可夫决策过程（MDP），通过迭代的提议-执行-验证-精炼循环来增强对话的一致性和执行能力。与传统方法不同，MTSQL-R1在每个回合中不仅生成查询，还与数据库进行交互以获取执行反馈，并利用持久的对话记忆进行一致性验证。实验结果表明，MTSQL-R1在COSQL和SPARC数据集上表现优于强基线，强调了环境驱动的验证和记忆引导的精炼在对话语义解析中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10611",
            "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
            "url": "https://huggingface.co/papers/2510.10611",
            "abstract": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) Ineffective group collaboration modeling, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) Limited task-adaptiveness in communication topology design, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose HyperAgent, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 октября",
                "en": "October 12",
                "zh": "10月12日"
            },
            "hash": "38135ea1ec6a6548",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Zijian Zhang",
                "Haochen You",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University, USA",
                "Shanghai Jiao Tong University, China",
                "South China Normal University, China",
                "University of Michigan, USA",
                "University of Pennsylvania, USA",
                "University of Science and Technology of China, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10611.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#graphs",
                    "#games"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Гиперграфы для умной коммуникации AI-агентов",
                    "desc": "Статья представляет HyperAgent — фреймворк на основе гиперграфов для оптимизации коммуникации в мультиагентных системах с LLM. В отличие от традиционных графовых подходов, использующих парные связи между агентами, HyperAgent применяет гиперрёбра для моделирования групповой коллаборации нескольких агентов одновременно. Система динамически адаптирует топологию коммуникации к сложности задачи через вариационный автоэнкодер с регуляризацией разреженности. На датасете GSM8K фреймворк достигает точности 95.07% при снижении потребления токенов на 25.33%, демонстрируя эффективность гиперграфового подхода."
                },
                "en": {
                    "title": "HyperAgent: Enhancing Multi-Agent Collaboration with Hypergraphs",
                    "desc": "HyperAgent is a new framework that uses hypergraphs to improve how multiple agents communicate and work together. Traditional methods struggle with modeling group collaboration and adapting communication for different tasks, which can lead to inefficiencies. By using hyperedges, HyperAgent can connect several agents at once, allowing for better information sharing and coordination. The framework also adjusts its communication structure based on the complexity of the task, resulting in better performance and reduced communication costs."
                },
                "zh": {
                    "title": "超图优化：提升多智能体协作的利器",
                    "desc": "HyperAgent是一个基于超图的框架，旨在优化多智能体系统中的通信拓扑，并捕捉群体协作模式，从而提高性能和效率。该框架解决了现有方法在建模群体协作时的不足，能够有效地表示多个智能体之间的关系。HyperAgent通过超边连接同一子任务中的多个智能体，并利用超图卷积层实现协作组内的一步信息聚合。实验结果表明，HyperAgent在性能和效率上均优于传统方法，展示了超图优化在多智能体通信中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10581",
            "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
            "url": "https://huggingface.co/papers/2510.10581",
            "abstract": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: (i) distinguishing symptoms from root causes in multi-agent error propagation, and (ii) tracing information dependencies beyond temporal order. To address these issues, we introduce GraphTracer, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 октября",
                "en": "October 12",
                "zh": "10月12日"
            },
            "hash": "4456157e4d3ee0bf",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Haochen You",
                "Zijian Zhang",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University",
                "Shanghai Jiao Tong University",
                "South China Normal University",
                "University of Michigan",
                "University of Pennsylvania",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10581.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#dataset",
                    "#graphs",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Графы зависимостей для поиска корневых причин ошибок в мультиагентных системах",
                    "desc": "Статья представляет GraphTracer — фреймворк для диагностики ошибок в мультиагентных системах на базе LLM. Основная проблема заключается в том, что существующие методы не могут отличить симптомы от реальных причин ошибок, когда они распространяются между агентами. GraphTracer строит графы информационных зависимостей (IDG), которые отслеживают, как агенты используют результаты работы друг друга, вместо простого анализа временной последовательности действий. Это позволяет достичь на 18% более высокой точности атрибуции ошибок по сравнению с современными моделями и улучшить производительность реальных систем на 5-14%."
                },
                "en": {
                    "title": "Revolutionizing Multi-Agent Debugging with GraphTracer",
                    "desc": "GraphTracer is a novel framework designed to improve failure attribution in multi-agent systems by utilizing Information Dependency Graphs (IDGs). It addresses the challenges of identifying root causes of errors that propagate across multiple agents and distinguishing them from mere symptoms. By analyzing information flow rather than just temporal sequences, GraphTracer enhances debugging accuracy and provides a clearer understanding of how agents interact. The framework has shown significant improvements in attribution accuracy and performance in real-world applications, making it a valuable tool for developers working with complex multi-agent systems."
                },
                "zh": {
                    "title": "GraphTracer：提升多智能体系统故障归因的准确性",
                    "desc": "GraphTracer 是一种针对多智能体系统故障归因的方法，通过构建信息依赖图来追踪信息流，从而提高调试的准确性。该方法解决了在多智能体错误传播中区分症状与根本原因的挑战，并能够超越时间顺序追踪信息依赖。GraphTracer 通过分析信息流，重新定义了故障归因，能够更有效地定位根本原因。实验结果表明，GraphTracer 在故障归因准确性上比现有模型提高了 18.18%，并在多智能体框架中实现了 4.8% 到 14.2% 的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13586",
            "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
            "url": "https://huggingface.co/papers/2510.13586",
            "abstract": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "b4af6fcdd0d708b9",
            "authors": [
                "Pasin Buakhaw",
                "Kun Kerdthaisong",
                "Phuree Phenhiran",
                "Pitikorn Khlaisamniang",
                "Supasate Vorathammathorn",
                "Piyalitt Ittichaiwong",
                "Nutchanon Yongsatianchot"
            ],
            "affiliations": [
                "Artificial Intelligence Association of Thailand",
                "Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University",
                "School of Biomedical Engineering & Imaging Sciences, Kings College London",
                "Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University",
                "Thammasat School of Engineering, Thammasat University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13586.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#games"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Умные игровые персонажи через промптинг и файн-тюнинг",
                    "desc": "Команда Tu_Character_lab разработала методы для создания диалоговых NPC персонажей в играх на основе LLM. В работе использовались два подхода: легковесный промптинг с техникой Deflanderization для API трека и файн-тюнинг модели Qwen3-14B с помощью SFT и LoRA для GPU трека. Техника Deflanderization помогает подавить излишнюю ролевую игру и улучшить выполнение задач персонажем. Решения заняли второе место в задачах 1 и 3 (API) и четвертое место в задаче 3 (GPU) на соревновании CPDC 2025."
                },
                "en": {
                    "title": "Elevating NPC Dialogue with Fine-Tuned Models and Smart Prompting",
                    "desc": "This paper discusses the participation of the Tu_Character_lab team in the CPDC 2025, focusing on creating advanced non-player characters (NPCs) for dialogue challenges. The team utilized lightweight prompting techniques and fine-tuned large language models to enhance dialogue generation and task execution. They implemented a unique Deflanderization method to maintain task fidelity while minimizing excessive role-play. Their strategies led to impressive rankings, achieving 2nd place in both Task 1 and Task 3 of the API track, and 4th place in Task 3 of the GPU track."
                },
                "zh": {
                    "title": "轻量级提示与微调模型的完美结合",
                    "desc": "本文介绍了我们在2025年常识人格对话挑战赛（CPDC）中的参与情况。我们使用轻量级提示技术和微调的大型模型，成功地在任务导向和上下文感知对话挑战中取得了高排名。具体来说，我们采用了Deflanderization提示方法来抑制过度角色扮演，并提高任务的准确性。同时，我们利用Qwen3-14B模型进行监督微调和低秩适应，提升了对话生成的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11715",
            "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
            "url": "https://huggingface.co/papers/2510.11715",
            "abstract": "Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.  \t\t\t\t\tAI-generated summary \t\t\t\t Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these \"emergent\" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.",
            "score": 1,
            "issue_id": 6449,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            },
            "hash": "25082e4025961f42",
            "authors": [
                "Ayush Shrivastava",
                "Sanyam Mehta",
                "Daniel Geng",
                "Andrew Owens"
            ],
            "affiliations": [
                "Cornell University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11715.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Трекинг точек через видео-диффузию без обучения",
                    "desc": "Исследователи обнаружили, что предобученные видео диффузионные модели могут выполнять трекинг точек в zero-shot режиме, просто визуально помечая точки на видео. Метод работает путём размещения цветного маркера на целевой точке и регенерации остальных кадров видео с промежуточного уровня шума, что распространяет маркер по траектории движения. Для сохранения видимости маркера используется исходный кадр как negative prompt, поскольку яркие маркеры нетипичны для естественных видео. Такой подход превосходит предыдущие zero-shot методы, справляется с окклюзиями и конкурирует со специализированными self-supervised моделями трекинга."
                },
                "en": {
                    "title": "Revolutionizing Point Tracking with Video Diffusion Models",
                    "desc": "This paper presents a novel approach using pretrained video diffusion models for zero-shot point tracking, which involves marking points visually and regenerating video frames. By placing a distinctively colored marker at the query point, the model can trace the point's trajectory across frames, even in the presence of occlusions. The method leverages the relationship between motion analysis and video synthesis, allowing for effective tracking without prior training on specific tracking tasks. Experimental results demonstrate that this approach outperforms existing zero-shot tracking methods and competes well with specialized models."
                },
                "zh": {
                    "title": "预训练视频扩散模型实现零-shot点跟踪",
                    "desc": "这篇论文介绍了一种预训练的视频扩散模型，可以通过视觉标记点来实现零-shot点跟踪，并且在处理遮挡时表现优于之前的方法。研究表明，跟踪器和视频生成器之间存在密切的联系，前者分析运动，后者合成运动。通过在查询点放置一个独特颜色的标记，并从中间噪声水平重新生成视频，其标记可以在帧之间传播，追踪点的轨迹。实验结果显示，这种“涌现”轨迹的性能超过了之前的零-shot方法，并且在遮挡情况下依然有效，表现与专门的自监督模型相当。"
                }
            }
        }
    ],
    "link_prev": "2025-10-15.html",
    "link_next": "2025-10-17.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10月15日"
    },
    "short_date_next": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10月17日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 12,
        "#agents": 9,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 2,
        "#video": 6,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 2,
        "#games": 8,
        "#interpretability": 3,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 22,
        "#survey": 2,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}