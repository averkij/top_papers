{
    "date": {
        "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 11",
        "zh": "11æœˆ11æ—¥"
    },
    "time_utc": "2024-11-11 08:16",
    "weekday": 0,
    "issue_id": 511,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.05288",
            "title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
            "url": "https://huggingface.co/papers/2411.05288",
            "abstract": "Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .",
            "score": 7,
            "issue_id": 509,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "19accdb712f507d9",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾ĞµĞ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Balancing Memory and Computation in Pipeline Parallelism for Language Models",
                    "desc": "This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è¯æ±‡å±‚ä»¥å¹³è¡¡è®¡ç®—ä¸å†…å­˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¯æ±‡å±‚å¯¼è‡´çš„è®¡ç®—å’Œå†…å­˜ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†è¯æ±‡å±‚å‡åŒ€åˆ†é…åˆ°ç®¡é“è®¾å¤‡ä¸Šï¼Œå¹¶å°†è®¡ç®—åˆ†ç»„åˆ°ç®¡é“ä¼ é€’ä¸­ã€‚ä¸ºäº†å‡å°‘æ¿€æ´»å†…å­˜å¼€é”€ï¼Œæˆ‘ä»¬è®¾è®¡äº†å‡ ç§ç®—æ³•æ¥é™ä½è¯æ±‡å±‚å†…çš„é€šä¿¡éšœç¢ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—å’Œå‚æ•°å†…å­˜ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œå¹¶åœ¨å¤§è¯æ±‡åœºæ™¯ä¸‹æ˜¾è‘—é™ä½äº†å³°å€¼å†…å­˜ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 7,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "b23d3650ace21f86",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "StdGEN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "StdGEN - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ StdGEN Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Semantic-aware Large Reconstruction Model (S-LRM), Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ†Ğ²ĞµÑ‚ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ StdGEN Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGENï¼šé«˜æ•ˆç”Ÿæˆå¯åˆ†è§£3Dè§’è‰²çš„åˆ›æ–°ç®¡é“",
                    "desc": "StdGENæ˜¯ä¸€ç§åˆ›æ–°çš„ç®¡é“ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒStdGENåœ¨å¯åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆç»†è‡´çš„3Dè§’è‰²ï¼Œä¸”å„ä¸ªè¯­ä¹‰ç»„ä»¶å¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘åˆ†ç¦»ã€‚å…¶æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚é€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼ŒStdGENå®ç°äº†é«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ç”Ÿæˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 3,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "38beaabd86eeaa88",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ PEFT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA, (IA)^3 Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ PEFT Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° LoRA Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šæå‡å•å…ƒæµ‹è¯•ç”Ÿæˆçš„ç»æµæ€§ä¸æœ‰æ•ˆæ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒè™½ç„¶æœ‰æ•ˆï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼ŒPEFTæ–¹æ³•é€šè¿‡åªå¾®è°ƒéƒ¨åˆ†å‚æ•°æ¥é™ä½è®¡ç®—å¼€é”€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPEFTæ–¹æ³•åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æç¤ºå¾®è°ƒåœ¨æˆæœ¬å’Œèµ„æºåˆ©ç”¨ä¸Šæœ€ä¸ºæœ‰æ•ˆã€‚è®ºæ–‡è¿˜æ¯”è¾ƒäº†ä¸åŒæ¨¡å‹æ¶æ„å’Œå¤§å°ä¸‹çš„å¤šç§PEFTæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05457",
            "title": "Improving the detection of technical debt in Java source code with an enriched dataset",
            "url": "https://huggingface.co/papers/2411.05457",
            "abstract": "Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.",
            "score": 2,
            "issue_id": 510,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "bc9c84b19f317115",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ³: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ° (Ğ¢Ğ”) Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¢Ğ”, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ… Ğº ĞºĞ¾Ğ´Ñƒ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¢Ğ”, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Bridging Comments and Code: Enhancing Technical Debt Detection",
                    "desc": "This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area."
                },
                "zh": {
                    "title": "æŠ€æœ¯å€ºåŠ¡è¯†åˆ«çš„æ–°è§†è§’",
                    "desc": "æŠ€æœ¯å€ºåŠ¡ï¼ˆTDï¼‰æ˜¯æŒ‡å¼€å‘è€…ä¸ºäº†å¿«é€Ÿè§£å†³é—®é¢˜è€Œé€‰æ‹©çš„ç®€å•æ–¹æ¡ˆæ‰€å¸¦æ¥çš„é¢å¤–å·¥ä½œå’Œæˆæœ¬ã€‚è‡ªæˆ‘æ‰¿è®¤çš„æŠ€æœ¯å€ºåŠ¡ï¼ˆSATDï¼‰æ˜¯å¼€å‘è€…é€šè¿‡æ–‡æœ¬æ³¨é‡Šä¸»åŠ¨è®°å½•å’Œæ‰¿è®¤çš„ä¸€ç§ç‰¹å®šç±»å‹çš„æŠ€æœ¯å€ºåŠ¡ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææ¥è‡ª974ä¸ªJavaé¡¹ç›®çš„æ³¨é‡Šå’Œç›¸å…³æºä»£ç ï¼Œåˆ›å»ºäº†é¦–ä¸ªç”±ä»£ç æ³¨é‡Šè¯†åˆ«çš„æŠ€æœ¯å€ºåŠ¡æ•°æ®é›†ï¼Œå¹¶å‘ç°è¿™äº›æ³¨é‡Šèƒ½æ˜¾è‘—æé«˜ç°æœ‰SATDæ£€æµ‹æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…ä¸ºæŠ€æœ¯å€ºåŠ¡çš„è¯†åˆ«æä¾›äº†æ–°çš„æ•°æ®é›†ï¼Œè¿˜æå‡ºäº†å¯ä½œä¸ºåŸºçº¿çš„åˆ†ç±»å™¨ï¼Œæ¨åŠ¨æœªæ¥ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04425",
            "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
            "url": "https://huggingface.co/papers/2411.04425",
            "abstract": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.",
            "score": 1,
            "issue_id": 509,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "397d7c5c26dfad0f",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DELIFT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). DELIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°: Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DELIFT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° 70% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimize Data, Maximize Performance with DELIFT!",
                    "desc": "This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¾®è°ƒï¼šDELIFTç®—æ³•çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDELIFTçš„æ–°ç®—æ³•ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘å†—ä½™å’Œæ— æ•ˆæ•°æ®çš„ä½¿ç”¨ã€‚DELIFTé€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œç³»ç»Ÿæ€§åœ°æ”¹è¿›äº†ä¸‰ä¸ªå…³é”®çš„å¾®è°ƒé˜¶æ®µï¼šæŒ‡ä»¤å¾®è°ƒã€ä»»åŠ¡ç‰¹å®šå¾®è°ƒå’ŒæŒç»­å¾®è°ƒã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æˆå¯¹æ•ˆç”¨åº¦é‡ï¼Œé‡åŒ–æ•°æ®æ ·æœ¬å¯¹æ¨¡å‹å“åº”å…¶ä»–æ ·æœ¬çš„æ”¹å–„ç¨‹åº¦ï¼Œä»è€Œæœ‰æ•ˆè¯„ä¼°ä¿¡æ¯ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDELIFTèƒ½å¤Ÿåœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¾®è°ƒæ•°æ®é‡å‡å°‘å¤šè¾¾70%ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½æ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡æ¨¡å‹ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…ä»‹ç»äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMï¼Œæ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œå¹¶æä¾›è¯¦ç»†çš„è®­ç»ƒæ•°æ®å’Œåè®®ã€‚é€šè¿‡è¿™ç§å¼€æ”¾æ€§ï¼Œä½œè€…å¸Œæœ›åŠ é€Ÿä»£ç AIçš„ç ”ç©¶å’Œå¯é‡å¤çš„è¿›å±•ã€‚",
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i dÃ imÇ shÄ“ngchÃ©ng, tuÄ«lÇ rÃ¨nwÃ¹ hÃ© dÃ ilÇ xÃ¬tÇ’ng zhÅng de zhÃ²ngyÃ oxÃ¬ng. SuÄ«rÃ¡n kÄifÃ ng fÇngwÃ¨n de dÃ imÇ LLMs xÃ¬ngnÃ©ng jÃ¬nkÃ¨ zhuÄnyÇ’u mÃ³xÃ­ng, dÃ n shÃ¬hÃ© yÃ¡nge kÄ“xuÃ© yÃ¡njiÅ« de gÄo zhÃ¬liÃ ng mÃ³xÃ­ng rÃ©ngrÃ¡n yÇ’uxiÃ n. WÃ¨ile tiÃ¡nbÇ” zhÃ¨ yÄ« kÃ²ngbÃ¡i, zuÃ²zhÄ› jiÃ¨shÃ o le OpenCoder, yÄ«gÃ¨ dÇngjiÄn de dÃ imÇ LLM, xÃ¬ngnÃ©ng jÃ¬mÇ lÇngxiÄn mÃ³xÃ­ng, bÃ¬ng tÃ­gÅng xiÃ¡ngxÃ¬ de xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© xiÃ©yÃ¬. TÅngguÃ² zhÃ¨ zhÇ’ng kÄifÃ ngxÃ¬ng, zuÃ²zhÄ› xÄ«wÃ ng jiÄsÃ¹ dÃ imÇ AI de yÃ¡njiÅ« hÃ© kÄ› chÃ³ngfÃ¹ de jÃ¬nbÃ¹.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"ä»£ç ç”Ÿæˆ\", \"pinyin\": \"dÃ i mÇ shÄ“ng chÃ©ng\", \"trans\": \"code generation\"},\n    {\"word\": \"æ¨ç†ä»»åŠ¡\", \"pinyin\": \"tuÄ« lÇ rÃ¨n wÃ¹\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"ä»£ç†ç³»ç»Ÿ\", \"pinyin\": \"dÃ i lÇ xÃ¬ tÇ’ng\", \"trans\": \"proxy system\"},\n    {\"word\": \"é‡è¦æ€§\", \"pinyin\": \"zhÃ²ng yÃ o xÃ¬ng\", \"trans\": \"importance\"},\n    {\"word\": \"å¼€æ”¾è®¿é—®\", \"pinyin\": \"kÄi fÃ ng fÇng wÃ¨n\", \"trans\": \"open access\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æ¥è¿‘\", \"pinyin\": \"jiÄ“ jÃ¬n\", \"trans\": \"close to\"},\n    {\"word\": \"ä¸“æœ‰æ¨¡å‹\", \"pinyin\": \"zhuÄn yÇ’u mÃ³ xÃ­ng\", \"trans\": \"proprietary model\"},\n    {\"word\": \"é€‚åˆ\", \"pinyin\": \"shÃ¬ hÃ©\", \"trans\": \"suitable\"},\n    {\"word\": \"ä¸¥æ ¼\", \"pinyin\": \"yÃ¡n gÃ©\", \"trans\": \"strict\"},\n    {\"word\": \"ç§‘å­¦ç ”ç©¶\", \"pinyin\": \"kÄ“ xuÃ© yÃ¡n jiÅ«\", \"trans\": \"scientific research\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’u xiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"å¡«è¡¥\", \"pinyin\": \"tiÃ¡n bÇ”\", \"trans\": \"fill\"},\n    {\"word\": \"ç©ºç™½\", \"pinyin\": \"kÃ²ng bÃ¡i\", \"trans\": \"gap\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"OpenCoder\", \"pinyin\": \"OpenCoder\", \"trans\": \"OpenCoder\"},\n    {\"word\": \"é¡¶å°–\", \"pinyin\": \"dÇng jiÄn\", \"trans\": \"top-notch\"},\n    {\"word\": \"åª²ç¾\", \"pinyin\": \"pÃ¬ mÄ›i\", \"trans\": \"rival\"},\n    {\"word\": \"é¢†å…ˆæ¨¡å‹\", \"pinyin\": \"lÇng xiÄn mÃ³ xÃ­ng\", \"trans\": \"leading model\"},\n    {\"word\": \"è¯¦ç»†\", \"pinyin\": \"xiÃ¡ng xÃ¬\", \"trans\": \"detailed\"},\n    {\"word\": \"è®­ç»ƒæ•°æ®\", \"pinyin\": \"xÃ¹n liÃ n shÃ¹ jÃ¹\", \"trans\": \"training data\"},\n    {\"word\": \"åè®®\", \"pinyin\": \"xiÃ© yÃ¬\", \"trans\": \"protocol\"},\n    {\"word\": \"å¼€æ”¾æ€§\", \"pinyin\": \"kÄi fÃ ng xÃ¬ng\", \"trans\": \"openness\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"},\n    {\"word\": \"å¯é‡å¤\", \"pinyin\": \"kÄ› chÃ³ng fÃ¹\", \"trans\": \"reproducible\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"}\n]",
        "trans": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While open-access code LLMs perform nearly as well as proprietary models, high-quality models suitable for rigorous scientific research remain limited. To fill this gap, the authors introduce OpenCoder, a top-tier code LLM that matches the performance of leading models and provides detailed training data and protocols. Through this openness, the authors aim to accelerate research and reproducible progress in code AI.",
        "update_ts": "2024-11-10 10:11"
    }
}