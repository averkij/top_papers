{
    "date": {
        "ru": "13 ноября",
        "en": "November 13",
        "zh": "11月13日"
    },
    "time_utc": "2024-11-13 14:09",
    "weekday": 2,
    "issue_id": 551,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 12,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "SAMPart3D: гибкая сегментация 3D-объектов без предварительного обучения",
                    "desc": "Статья представляет SAMPart3D - масштабируемый фреймворк для сегментации частей 3D-объектов без предварительного обучения. Авторы используют безтекстовые модели компьютерного зрения для извлечения признаков из 3D-данных, что позволяет обучаться на больших наборах неразмеченных 3D-объектов. Метод способен сегментировать объекты на части с разной степенью детализации, а затем присваивать семантические метки с помощью мультимодальных языковых моделей. SAMPart3D превосходит существующие методы и может применяться для редактирования и интерактивной сегментации 3D-объектов."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3D：无文本提示的3D部件分割新框架",
                    "desc": "3D部件分割是3D感知中的一项重要且具有挑战性的任务，广泛应用于机器人技术、3D生成和3D编辑等领域。本文提出了SAMPart3D框架，它能够在不依赖预定义文本提示的情况下，对任意3D对象进行多粒度的语义部件分割。该框架利用无文本依赖的视觉基础模型，从大规模未标记的3D数据集中提取丰富的3D特征，并通过条件化的部件感知特征实现灵活的分割。实验结果表明，SAMPart3D在处理复杂对象时显著优于现有的零样本3D部件分割方法，并能支持多种应用，如部件级编辑和交互式分割。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07975",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2411.07975",
            "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.",
            "score": 9,
            "issue_id": 544,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "294dc65a01cd1218",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#architecture",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Единая модель для понимания и генерации изображений",
                    "desc": "JanusFlow - это новая архитектура, объединяющая понимание и генерацию изображений в одной модели. Она интегрирует авторегрессионные языковые модели с методом rectified flow для генеративного моделирования. Ключевое преимущество - возможность обучать rectified flow в рамках больших языковых моделей без сложных модификаций архитектуры. Для улучшения производительности используется разделение энкодеров понимания и генерации, а также выравнивание их представлений при обучении."
                },
                "en": {
                    "title": "JanusFlow: Unifying Image Understanding and Generation Efficiently",
                    "desc": "JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches."
                },
                "zh": {
                    "title": "JanusFlow：图像理解与生成的统一模型",
                    "desc": "JanusFlow是一个强大的框架，将图像理解和生成统一在一个模型中。它采用了简约的架构，将自回归语言模型与修正流结合，这是生成建模中的一种先进方法。研究发现，修正流可以在大型语言模型框架内轻松训练，无需复杂的架构修改。通过解耦理解和生成编码器以及在统一训练中对齐它们的表示，JanusFlow在标准基准测试中表现出色，超越了现有的统一方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07133",
            "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
            "url": "https://huggingface.co/papers/2411.07133",
            "abstract": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.",
            "score": 6,
            "issue_id": 546,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "be2fc1cdad8aa9f3",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Парадокс больших моделей: не всегда лучшие учителя",
                    "desc": "В статье рассматривается проблема настройки больших языковых моделей (LLMs) с помощью синтетических наборов инструкций. Авторы выявили парадокс, заключающийся в том, что более крупные модели не всегда являются лучшими учителями для более мелких моделей. Для решения этой проблемы они разработали новый метрик, называемый Compatibility-Adjusted Reward (CAR), который учитывает совместимость между учителем и базовой моделью. Эксперименты показали, что CAR превосходит почти все существующие подходы."
                },
                "en": {
                    "title": "Rethinking Instruction Tuning: Size Isn't Everything!",
                    "desc": "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."
                },
                "zh": {
                    "title": "大模型不一定是好教师！",
                    "desc": "本论文探讨了指令调优在大型语言模型（LLMs）中的应用，强调了指令数据集对模型性能的重要性。我们发现，较大或较强的模型并不一定是较小模型的更好教师，这一现象被称为“大模型悖论”。此外，现有的评估指标无法准确预测响应生成器的有效性，因为它们忽略了教师模型与被调优基础模型之间的兼容性。为此，我们提出了一种新的评估指标——兼容性调整奖励（CAR），并通过实验验证了其优越性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05197",
            "title": "Hardware and Software Platform Inference",
            "url": "https://huggingface.co/papers/2411.05197",
            "abstract": "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.",
            "score": 1,
            "issue_id": 550,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "7685c8e74f6dbc6b",
            "data": {
                "categories": [
                    "#leakage",
                    "#security",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Разоблачение обмана: как определить реальное оборудование для языковой модели",
                    "desc": "Статья представляет метод HSPI (идентификация аппаратной и программной платформы), позволяющий определить архитектуру и программный стек модели машинного обучения только на основе ее поведения при вводе-выводе. Метод использует присущие различным архитектурам и компиляторам отличия для распознавания типов оборудования и программных конфигураций. Анализируя числовые паттерны в выводе модели, предложена классификационная система, способная точно идентифицировать оборудование, используемое для вывода модели. Результаты показывают возможность определения типа оборудования для черного ящика модели с точностью до 100% в белом ящике и до трех раз выше случайного угадывания в черном ящике."
                },
                "en": {
                    "title": "Verify Your Model: Uncovering the Truth Behind LLM Inference",
                    "desc": "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."
                },
                "zh": {
                    "title": "验证大型语言模型的真实性",
                    "desc": "本文介绍了一种名为硬件和软件平台推理（HSPI）的方法，用于识别机器学习模型的底层架构和软件堆栈。该方法通过分析模型的输入输出行为，利用不同架构和编译器的固有差异来区分不同类型的硬件和软件配置。研究表明，在白盒环境下，我们可以以83.9%到100%的准确率区分不同的硬件，而在黑盒环境下，准确率也能达到随机猜测的三倍以上。此方法为验证大型语言模型的真实性提供了一种有效的手段。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08017",
            "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings",
            "url": "https://huggingface.co/papers/2411.08017",
            "abstract": "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.",
            "score": 1,
            "issue_id": 547,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "0af1f4c0dc38cc5b",
            "data": {
                "categories": [
                    "#inference",
                    "#open_source",
                    "#3d",
                    "#dataset",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективная генерация 3D-моделей с помощью вейвлет-сжатия",
                    "desc": "Статья представляет новый подход к генерации 3D-моделей под названием Wavelet Latent Diffusion (WaLa). Метод использует вейвлет-кодирование для создания компактных латентных представлений 3D-форм, достигая степени сжатия 2427x. Это позволяет эффективно обучать крупномасштабные генеративные нейронные сети без увеличения времени вывода. WaLa демонстрирует улучшенное качество генерации, разнообразие и вычислительную эффективность по сравнению с существующими методами."
                },
                "en": {
                    "title": "Efficient 3D Shape Generation with Wavelet Latent Diffusion",
                    "desc": "This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public."
                },
                "zh": {
                    "title": "高效压缩，快速生成3D形状的创新方法",
                    "desc": "本论文提出了一种新的方法，称为Wavelet Latent Diffusion（WaLa），旨在提高大规模3D生成模型的效率。通过将3D形状编码为基于小波的紧凑潜在编码，WaLa实现了高达2427倍的压缩比，同时保持了细节的完整性。该方法使得训练大型生成网络变得更加高效，并且在推理时不会显著增加时间。我们的模型在多个数据集上表现出色，生成高质量的3D形状，并且开源了代码，推动了3D生成模型的发展。"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11月12日"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11月14日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。",
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "pinyin": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。\n\nPinyin transcription:\n\nZhè piān wénzhāng tǎolùn le 3D bùjiàn fēngé de zhòngyàoxìng jí qí zài jīqìrén, 3D shēngchéng hé 3D biānjí zhōng de yìngyòng. Xiànyǒu fāngfǎ lìyòng shìjué yǔyán móxíng jìnxíng 2D dào 3D de zhīshi zhēngliú, shíxiàn líng yàngběn 3D bùjiàn fēngé, dàn yīlài wénběn tíshì, xiànzhì le kuòzhǎnxìng hé chǔlǐ bùjiàn jíyì de línghuóxìng. Wénzhāng jièshào le SAMPart3D, yīgè kě kuòzhǎn de líng yàngběn 3D bùjiàn fēngé kuàngjià, bù xūyào yùdìngyì de bùjiàn biǎoqián jí. Tā shǐyòng yǔ wénběn wúguān de shìjué jīchǔ móxíng jìnxíng zhēngliú, bìng tīquǎn duō chǐdù de bùjiàn gǎnjué 3D tèzhēng. Shíyàn xiǎnshì, SAMPart3D zài dà guīmó 3D shùjùjí shàng biǎoxiàn yōuyuè, bìng chāoyué xiànyǒu fāngfǎ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"3D部件分割\", \"pinyin\": \"3D bù jiàn fēn gē\", \"trans\": \"3D part segmentation\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòng yào xìng\", \"trans\": \"importance\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"现有方法\", \"pinyin\": \"xiàn yǒu fāng fǎ\", \"trans\": \"existing methods\"},\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"知识蒸馏\", \"pinyin\": \"zhī shi zhēng liú\", \"trans\": \"knowledge distillation\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"文本提示\", \"pinyin\": \"wén běn tí shì\", \"trans\": \"textual prompts\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"扩展性\", \"pinyin\": \"kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"handle\"},\n    {\"word\": \"部件歧义\", \"pinyin\": \"bù jiàn qí yì\", \"trans\": \"part ambiguity\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"SAMPart3D\", \"pinyin\": \"SAMPart3D\", \"trans\": \"SAMPart3D\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"预定义\", \"pinyin\": \"yù dìng yì\", \"trans\": \"predefined\"},\n    {\"word\": \"标签集\", \"pinyin\": \"biāo qiān jí\", \"trans\": \"label set\"},\n    {\"word\": \"视觉基础模型\", \"pinyin\": \"shì jué jī chǔ mó xíng\", \"trans\": \"vision foundation model\"},\n    {\"word\": \"多尺度\", \"pinyin\": \"duō chǐ dù\", \"trans\": \"multi-scale\"},\n    {\"word\": \"部件感知\", \"pinyin\": \"bù jiàn gǎn zhī\", \"trans\": \"part-aware\"},\n    {\"word\": \"3D特征\", \"pinyin\": \"3D tè zhēng\", \"trans\": \"3D features\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses the importance of 3D part segmentation and its applications in robotics, 3D generation, and 3D editing. Existing methods utilize vision-language models for knowledge distillation from 2D to 3D, achieving zero-shot 3D part segmentation, but they rely on textual prompts, which limits their scalability and flexibility in handling part ambiguities. The article introduces SAMPart3D, a scalable zero-shot 3D part segmentation framework that does not require predefined part labels. It uses a text-agnostic visual foundation model for distillation and extracts multi-scale part-aware 3D features. Experiments show that SAMPart3D performs excellently on large-scale 3D datasets and outperforms existing methods.",
        "update_ts": "2024-11-13 09:10"
    }
}