{
    "date": {
        "ru": "14 мая",
        "en": "May 14",
        "zh": "5月14日"
    },
    "time_utc": "2025-05-14 07:12",
    "weekday": 2,
    "issue_id": 3750,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07591",
            "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
            "url": "https://huggingface.co/papers/2505.07591",
            "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.",
            "score": 1,
            "issue_id": 3750,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ca7c47ccc0066e55",
            "authors": [
                "Junjie Ye",
                "Caishuang Huang",
                "Zhuohan Chen",
                "Wenjie Fu",
                "Chenyuan Yang",
                "Leyi Yang",
                "Yilong Wu",
                "Peng Wang",
                "Meng Zhou",
                "Xiaolong Yang",
                "Tao Gui",
                "Qi Zhang",
                "Zhongchao Shi",
                "Jianping Fan",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Institute of Modern Languages and Linguistics, Fudan University",
                "Lenovo Research",
                "School of Computer Science, Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07591.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#alignment",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многомерная оценка следования инструкциям для больших языковых моделей",
                    "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Dimensional Constraints",
                    "desc": "This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters."
                },
                "zh": {
                    "title": "多维约束框架提升语言模型性能",
                    "desc": "本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08665",
            "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
            "url": "https://huggingface.co/papers/2505.08665",
            "abstract": "Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.",
            "score": 0,
            "issue_id": 3750,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "8cbf16dc2ec90273",
            "authors": [
                "Edoardo Bianchi",
                "Antonio Liotta"
            ],
            "affiliations": [
                "Free University of Bozen-Bolzano Bozen-Bolzano, IT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08665.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео",
                    "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами."
                },
                "en": {
                    "title": "SkillFormer: Efficient Multi-View Skill Assessment",
                    "desc": "This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation."
                },
                "zh": {
                    "title": "SkillFormer：高效的多视角技能评估架构",
                    "desc": "评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。"
                }
            }
        }
    ],
    "link_prev": "2025-05-13.html",
    "link_next": "2025-05-15.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "short_date_next": {
        "ru": "15.05",
        "en": "05/15",
        "zh": "5月15日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。",
        "title": "Seed1.5-VL Technical Report",
        "pinyin": "我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。\n\nWǒmen jièshào le Seed1.5-VL, yīgè yòngyú tuījìn tōngyòng duō móshì lǐjiě hé tuīlǐ de shìjué-yǔyán jīchǔ móxíng. Tā yóu yīgè 532M cānshù de shìjué biānmǎqì hé yīgè 20B huóyuè cānshù de hùnhé zhuānjiā (MoE) LLM zǔchéng. Jǐnguǎn jiàgòu xiāngduì jǐncuǒ, dàn tā zài 38 gè gōnggòng jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìng zài dàilǐ rènwù zhōng yōuyú OpenAI CUA hé Claude 3.7. Seed1.5-VL hái zhǎnshì le qiángdà de tuīlǐ nénglì, shìyòng yú duō móshì tuīlǐ tiǎozhàn. Wǒmen xīwàng zhèxiē nénglì néng tuīdòng gèng guǎngfàn de yìngyòng.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"Seed1.5-VL\", \"pinyin\": \"Seed1.5-VL\", \"trans\": \"Seed1.5-VL\"},\n    {\"word\": \"推进\", \"pinyin\": \"tuī jìn\", \"trans\": \"promote\"},\n    {\"word\": \"通用\", \"pinyin\": \"tōng yòng\", \"trans\": \"general\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó shuài\", \"trans\": \"multimodal\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jī chǔ mó xíng\", \"trans\": \"foundation model\"},\n    {\"word\": \"由\", \"pinyin\": \"yóu\", \"trans\": \"consist of\"},\n    {\"word\": \"参数\", \"pinyin\": \"cān shǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"视觉编码器\", \"pinyin\": \"shì jué biān mǎ qì\", \"trans\": \"visual encoder\"},\n    {\"word\": \"混合专家\", \"pinyin\": \"hùn hé zhuān jiā\", \"trans\": \"mixture of experts\"},\n    {\"word\": \"LLM\", \"pinyin\": \"LLM\", \"trans\": \"LLM\"},\n    {\"word\": \"组成\", \"pinyin\": \"zǔ chéng\", \"trans\": \"composed of\"},\n    {\"word\": \"尽管\", \"pinyin\": \"jǐn guǎn\", \"trans\": \"although\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"相对\", \"pinyin\": \"xiāng duì\", \"trans\": \"relatively\"},\n    {\"word\": \"紧凑\", \"pinyin\": \"jǐn còu\", \"trans\": \"compact\"},\n    {\"word\": \"但\", \"pinyin\": \"dàn\", \"trans\": \"but\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"公共\", \"pinyin\": \"gōng gòng\", \"trans\": \"public\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark tests\"},\n    {\"word\": \"代理任务\", \"pinyin\": \"dài lǐ rèn wù\", \"trans\": \"proxy tasks\"},\n    {\"word\": \"优于\", \"pinyin\": \"yōu yú\", \"trans\": \"superior to\"},\n    {\"word\": \"OpenAI\", \"pinyin\": \"OpenAI\", \"trans\": \"OpenAI\"},\n    {\"word\": \"CUA\", \"pinyin\": \"CUA\", \"trans\": \"CUA\"},\n    {\"word\": \"Claude\", \"pinyin\": \"Claude\", \"trans\": \"Claude\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"适用于\", \"pinyin\": \"shì yòng yú\", \"trans\": \"applicable to\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"希望\", \"pinyin\": \"xī wàng\", \"trans\": \"hope\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"drive\"},\n    {\"word\": \"广泛\", \"pinyin\": \"guǎng fàn\", \"trans\": \"widespread\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"}\n]",
        "trans": "We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications.",
        "update_ts": "2025-05-13 09:12"
    }
}