{
    "date": {
        "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 26",
        "zh": "2æœˆ26æ—¥"
    },
    "time_utc": "2025-02-26 03:17",
    "weekday": 2,
    "issue_id": 2409,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.17363",
            "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
            "url": "https://huggingface.co/papers/2502.17363",
            "abstract": "Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit",
            "score": 8,
            "issue_id": 2409,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "d1e7a717a0d2e56e",
            "authors": [
                "Tianrui Zhu",
                "Shiyi Zhang",
                "Jiawei Shao",
                "Yansong Tang"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence (TeleAI), China Telecom",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "KV-Edit: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°",
                    "desc": "KV-Edit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² DiT-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ„Ğ¾Ğ½Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ O(1), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ KV-Edit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Seamless Image Editing with KV-Edit: No Training Needed!",
                    "desc": "The paper introduces KV-Edit, a novel method for image editing that addresses the challenge of maintaining background consistency. Unlike traditional methods that require extensive training, KV-Edit utilizes a KV cache in Denoising Transformers (DiTs) to preserve background tokens, allowing for seamless integration of new content. This approach simplifies the editing process by avoiding complex mechanisms and optimizing memory usage to O(1) without sacrificing quality. Experimental results show that KV-Edit outperforms existing techniques, including those that rely on training, in both background consistency and overall image quality."
                },
                "zh": {
                    "title": "KV-Editï¼šæ— è®­ç»ƒçš„èƒŒæ™¯ä¸€è‡´æ€§å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "èƒŒæ™¯ä¸€è‡´æ€§åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿æŒä¸åŸå§‹å›¾åƒç›¸ä¼¼æ€§å’Œç”Ÿæˆç¬¦åˆç›®æ ‡å†…å®¹ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†KV-Editï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨KVç¼“å­˜æ¥ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§ï¼Œä¿ç•™èƒŒæ™¯æ ‡è®°è€Œä¸æ˜¯é‡æ–°ç”Ÿæˆï¼Œä»è€Œç®€åŒ–äº†å¤æ‚æœºåˆ¶å’Œé«˜æˆæœ¬è®­ç»ƒçš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒKV-Editåœ¨èƒŒæ™¯å’Œå›¾åƒè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18137",
            "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
            "url": "https://huggingface.co/papers/2502.18137",
            "abstract": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.",
            "score": 5,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "1029ef0dffc41bba",
            "authors": [
                "Jintao Zhang",
                "Chendong Xiang",
                "Haofeng Huang",
                "Jia Wei",
                "Haocheng Xi",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18137.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#video",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpargeAttn Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. SpargeAttn ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Accelerating Attention with SpargeAttn: Speed Meets Efficiency",
                    "desc": "This paper introduces SpargeAttn, a novel approach to implementing sparse attention in machine learning models. It addresses the challenge of quadratic time complexity in attention mechanisms by leveraging the inherent sparsity of attention maps. The proposed method employs a two-stage online filtering process to efficiently predict and optimize the attention map, allowing for the omission of unnecessary computations. Experimental results demonstrate that SpargeAttn accelerates various models across different domains, such as language and image processing, while maintaining high performance metrics."
                },
                "zh": {
                    "title": "é€šç”¨ç¨€ç–æ³¨æ„åŠ›ï¼Œæå‡æ¨¡å‹è®¡ç®—æ•ˆç‡ï¼",
                    "desc": "åœ¨å¤§å‹æ¨¡å‹ä¸­ï¼Œé«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°è‡³å…³é‡è¦ï¼Œå› ä¸ºå…¶æ—¶é—´å¤æ‚åº¦ä¸ºå¹³æ–¹çº§ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ³¨æ„åŠ›é€šå¸¸è¡¨ç°å‡ºç¨€ç–æ€§ï¼Œå³æ³¨æ„åŠ›å›¾ä¸­çš„è®¸å¤šå€¼æ¥è¿‘äºé›¶ï¼Œè¿™ä½¿å¾—å¯ä»¥çœç•¥ç›¸åº”çš„è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSpargeAttnçš„é€šç”¨ç¨€ç–å’Œé‡åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ é€Ÿå„ç§æ¨¡å‹çš„è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒç«¯åˆ°ç«¯çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µçš„åœ¨çº¿è¿‡æ»¤å™¨æ¥å®ç°ï¼Œç¬¬ä¸€é˜¶æ®µå¿«é€Ÿå‡†ç¡®åœ°é¢„æµ‹æ³¨æ„åŠ›å›¾ï¼Œç¬¬äºŒé˜¶æ®µè®¾è®¡äº†ä¸€ä¸ªåœ¨çº¿çš„softmaxæ„ŸçŸ¥è¿‡æ»¤å™¨ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18411",
            "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
            "url": "https://huggingface.co/papers/2502.18411",
            "abstract": "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.",
            "score": 4,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "a37015745aae1e1d",
            "authors": [
                "Xiangyu Zhao",
                "Shengyuan Ding",
                "Zicheng Zhang",
                "Haian Huang",
                "Maosong Cao",
                "Weiyun Wang",
                "Jiaqi Wang",
                "Xinyu Fang",
                "Wenhai Wang",
                "Guangtao Zhai",
                "Haodong Duan",
                "Hua Yang",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18411.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniAlign-V - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MM-AlignBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ MLLM Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° OmniAlign-V Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ."
                },
                "en": {
                    "title": "Aligning MLLMs with Human Preferences through OmniAlign-V",
                    "desc": "This paper presents OmniAlign-V, a new dataset containing 200,000 high-quality training samples that include diverse images and complex questions to help multi-modal large language models (MLLMs) better align with human preferences. The authors also introduce MM-AlignBench, a benchmark for evaluating how well MLLMs reflect human values. By fine-tuning MLLMs with the OmniAlign-V dataset using techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), the models show improved alignment with human preferences while maintaining their performance on standard Visual Question Answering (VQA) tasks. The resources, including datasets and benchmarks, are made publicly available to support further research in this area."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«20ä¸‡ä¸ªé«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚æ•°æ®é›†ä¸­åŒ…å«å¤šæ ·çš„å›¾åƒã€å¤æ‚çš„é—®é¢˜å’Œå¤šç§å“åº”æ ¼å¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MM-AlignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„äººç±»æ ‡æ³¨åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨OmniAlign-Vè¿›è¡Œå¾®è°ƒæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡äº†åœ¨æ ‡å‡†è§†è§‰é—®ç­”åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18364",
            "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
            "url": "https://huggingface.co/papers/2502.18364",
            "abstract": "Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.",
            "score": 1,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "23632a98b9252831",
            "authors": [
                "Yifan Pu",
                "Yiming Zhao",
                "Zhicong Tang",
                "Ruihong Yin",
                "Haoxing Ye",
                "Yuhui Yuan",
                "Dong Chen",
                "Jianmin Bao",
                "Sirui Zhang",
                "Yanbin Wang",
                "Lin Liang",
                "Lijuan Wang",
                "Ji Li",
                "Xiu Li",
                "Zhouhui Lian",
                "Gao Huang",
                "Baining Guo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18364.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ART: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Anonymous Region Transformer (ART) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ART Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Multi-Layer Image Generation with ART",
                    "desc": "This paper presents the Anonymous Region Transformer (ART), a novel approach for generating multi-layer transparent images using a global text prompt. ART allows the model to autonomously match visual tokens to text tokens through an anonymous region layout, improving upon traditional semantic layouts. The layer-wise region crop mechanism enhances efficiency by reducing attention computation costs, enabling the generation of images with many distinct layers quickly. Overall, ART introduces a new paradigm for interactive content creation, allowing for precise control and scalable image generation."
                },
                "zh": {
                    "title": "åŒ¿ååŒºåŸŸå˜æ¢å™¨ï¼šå¤šå±‚å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "å¤šå±‚å›¾åƒç”Ÿæˆæ˜¯ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿéš”ç¦»ã€é€‰æ‹©å’Œç¼–è¾‘ç‰¹å®šçš„å›¾åƒå±‚ï¼Œä»è€Œæ”¹å˜ä¸ç”Ÿæˆæ¨¡å‹çš„äº¤äº’æ–¹å¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºåŒ¿ååŒºåŸŸå˜æ¢å™¨ï¼ˆARTï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ ¹æ®å…¨å±€æ–‡æœ¬æç¤ºå’ŒåŒ¿ååŒºåŸŸå¸ƒå±€ç›´æ¥ç”Ÿæˆå¯å˜çš„å¤šå±‚é€æ˜å›¾åƒã€‚è¯¥æ–¹æ³•å…è®¸ç”Ÿæˆæ¨¡å‹è‡ªä¸»å†³å®šå“ªäº›è§†è§‰æ ‡è®°ä¸å“ªäº›æ–‡æœ¬æ ‡è®°å¯¹é½ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡ï¼Œå¹¶å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¼•å…¥é«˜è´¨é‡çš„å¤šå±‚é€æ˜å›¾åƒè‡ªç¼–ç å™¨ï¼ŒARTä¸ºäº¤äº’å¼å†…å®¹åˆ›ä½œå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18449",
            "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
            "url": "https://huggingface.co/papers/2502.18449",
            "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
            "score": 0,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "938af9b1ea2398d8",
            "authors": [
                "Yuxiang Wei",
                "Olivier Duchenne",
                "Jade Copet",
                "Quentin Carbonneaux",
                "Lingming Zhang",
                "Daniel Fried",
                "Gabriel Synnaeve",
                "Rishabh Singh",
                "Sida I. Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "FAIR at Meta",
                "GenAI at Meta",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18449.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#dataset",
                    "#math",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SWE-RL: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-RL - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», SWE-RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Llama3-SWE-RL-70B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama 3, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ LLM ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞŸĞ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Software Engineering Reasoning",
                    "desc": "This paper presents SWE-RL, a novel approach that applies reinforcement learning (RL) to enhance the reasoning abilities of large language models (LLMs) specifically for software engineering tasks. By utilizing a lightweight rule-based reward system, SWE-RL allows LLMs to learn from extensive open-source software evolution data, which includes various stages of software development. The resulting model, Llama3-SWE-RL-70B, achieves impressive performance on real-world GitHub issues, outperforming other medium-sized LLMs and even rivaling larger proprietary models. Additionally, this approach not only improves software-related reasoning but also demonstrates generalized reasoning skills across various tasks, indicating its broad applicability."
                },
                "zh": {
                    "title": "é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SWE-RLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºçœŸå®è½¯ä»¶å·¥ç¨‹çš„æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨è½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒSWE-RLèƒ½å¤Ÿä»å¤§é‡å¼€æºè½¯ä»¶æ¼”å˜æ•°æ®ä¸­å­¦ä¹ ï¼Œè‡ªåŠ¨æ¢å¤å¼€å‘è€…çš„æ¨ç†è¿‡ç¨‹å’Œè§£å†³æ–¹æ¡ˆã€‚è®­ç»ƒåçš„æ¨¡å‹Llama3-SWE-RL-70Båœ¨çœŸå®çš„GitHubé—®é¢˜ä¸Šè¾¾åˆ°äº†41.0%çš„è§£å†³ç‡ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–ä¸­å‹è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡ä»…åœ¨è½¯ä»¶æ¼”å˜æ•°æ®ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒLlama3-SWE-RLä»å±•ç°å‡ºå¹¿æ³›çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä»»åŠ¡ä¸­å–å¾—è‰¯å¥½ç»“æœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-25.html",
    "link_next": "2025-02-27.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« çš„ä¸»è¦ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªè®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®æœ‰é™çš„å¤šä»»åŠ¡é€šç”¨æ„ŸçŸ¥æ¨¡å‹ã€‚ä½œè€…ä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDICEPTION åœ¨å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…ä½¿ç”¨äº† SAM-vit-h æ¨¡å‹ 0.06% çš„æ•°æ®ã€‚DICEPTION é€šè¿‡é¢œè‰²ç¼–ç ç»Ÿä¸€äº†å„ç§æ„ŸçŸ¥ä»»åŠ¡ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯ä»¥è¢«å……åˆ†åˆ©ç”¨ã€‚åœ¨é€‚åº”å…¶ä»–ä»»åŠ¡æ—¶ï¼Œæ¨¡å‹åªéœ€åœ¨å°‘é‡å›¾åƒå’Œå‚æ•°ä¸Šè¿›è¡Œå¾®è°ƒã€‚",
        "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
        "pinyin": "è¿™ç¯‡æ–‡ç« çš„ä¸»è¦ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªè®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®æœ‰é™çš„å¤šä»»åŠ¡é€šç”¨æ„ŸçŸ¥æ¨¡å‹ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng de zhÇ”yÃ o mÃ¹biÄo shÃ¬ chuÃ ngjiÃ n yÄ«gÃ¨ jÃ¬suÃ n zÄ«yuÃ¡n hÃ© xÃ¹nliÃ n shÃ¹jÃ¹ yÇ’u xiÃ n de duÅ rÃ¨nwÃ¹ tÅngyÃ²ng gÇnjuÃ© mÃ³xÃ­ng.\n\nä½œè€…ä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚\nZuÃ²zhÄ› shÇyÃ²ngle yÃ¹ xÃ¹nliÃ n de wÃ©nbÄ›n dÃ o tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng.\n\nå®éªŒç»“æœè¡¨æ˜ï¼ŒDICEPTION åœ¨å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…ä½¿ç”¨äº† SAM-vit-h æ¨¡å‹ 0.06% çš„æ•°æ®ã€‚\nShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, DICEPTION zÃ i duÅ gÃ¨ gÇnjuÃ© rÃ¨nwÃ¹ shÃ ng biÇoxiÃ n yÅuyÃ¬, jÇn shÇyÃ²ngle SAM-vit-h mÃ³xÃ­ng 0.06% de shÃ¹jÃ¹.\n\nDICEPTION é€šè¿‡é¢œè‰²ç¼–ç ç»Ÿä¸€äº†å„ç§æ„ŸçŸ¥ä»»åŠ¡ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯ä»¥è¢«å……åˆ†åˆ©ç”¨ã€‚\nDICEPTION tÅngguÃ² yÃ¡nsÃ¨ biÄnmÇ tÇ’ngyÄ«le gÃ¨zhÇ’ng gÇnjuÃ© rÃ¨nwÃ¹, shÇdÃ© yÃ¹ xÃ¹nliÃ n de wÃ©nbÄ›n dÃ o tÃºxiÃ ng mÃ³xÃ­ng kÄ›yÇ bÃ¨i chÅngfÄ“n lÃ¬yÃ²ng.\n\nåœ¨é€‚åº”å…¶ä»–ä»»åŠ¡æ—¶ï¼Œæ¨¡å‹åªéœ€åœ¨å°‘é‡å›¾åƒå’Œå‚æ•°ä¸Šè¿›è¡Œå¾®è°ƒã€‚\nZÃ i shÃ¬yÃ¬ng qÃ­tÄ rÃ¨nwÃ¹ shÃ­, mÃ³xÃ­ng zhÇ xÅ« zÃ i shÇoliÃ ng tÃºxiÃ ng hÃ© cÄnshÃ¹ shÃ ng jÃ¬nxÃ­ng wÄ“itiÃ¡o.",
        "vocab": "[\n    {\"word\": \"åˆ›å»º\", \"pinyin\": \"chuÃ ng jiÃ n\", \"trans\": \"create\"},\n    {\"word\": \"è®¡ç®—èµ„æº\", \"pinyin\": \"jÃ¬ suÃ n zÄ« yuÃ¡n\", \"trans\": \"computational resources\"},\n    {\"word\": \"è®­ç»ƒæ•°æ®\", \"pinyin\": \"xÃ¹n liÃ n shÃ¹ jÃ¹\", \"trans\": \"training data\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’u xiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"å¤šä»»åŠ¡\", \"pinyin\": \"duÅ rÃ¨n wÃ¹\", \"trans\": \"multi-task\"},\n    {\"word\": \"é€šç”¨\", \"pinyin\": \"tÅng yÃ²ng\", \"trans\": \"general-purpose\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇn zhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-trained\"},\n    {\"word\": \"æ–‡æœ¬åˆ°å›¾åƒ\", \"pinyin\": \"wÃ©n bÄ›n dÃ o tÃº xiÃ ng\", \"trans\": \"text-to-image\"},\n    {\"word\": \"æ‰©æ•£\", \"pinyin\": \"kuÃ² sÃ n\", \"trans\": \"diffusion\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"ä»…\", \"pinyin\": \"jÇn\", \"trans\": \"only\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unify\"},\n    {\"word\": \"ç¼–ç \", \"pinyin\": \"biÄn mÇ\", \"trans\": \"encoding\"},\n    {\"word\": \"å……åˆ†åˆ©ç”¨\", \"pinyin\": \"chÅng fÃ¨n lÃ¬ yÃ²ng\", \"trans\": \"fully utilize\"},\n    {\"word\": \"é€‚åº”\", \"pinyin\": \"shÃ¬ yÃ¬ng\", \"trans\": \"adapt\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tune\"}\n]",
        "trans": "The primary objective of this article is to create a general-purpose multi-task perception model with limited computational resources and training data. The authors utilized a pre-trained text-to-image diffusion model. Experimental results demonstrate that DICEPTION performs exceptionally well on multiple perception tasks, using only 0.06% of the data from the SAM-vit-h model. DICEPTION unifies various perception tasks through color coding, enabling the full utilization of the pre-trained text-to-image model. When adapting to other tasks, the model requires only fine-tuning on a small number of images and parameters.",
        "update_ts": "2025-02-25 09:11"
    }
}