{
    "date": {
        "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 25",
        "zh": "11æœˆ25æ—¥"
    },
    "time_utc": "2024-11-25 07:11",
    "weekday": 0,
    "issue_id": 756,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 16,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Style-friendly SNR sampler, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»-ÑˆÑƒĞ¼ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ĞºĞ²Ğ°Ñ€ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹, 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼ĞµĞ¼Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "æå‡ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ç”Ÿæˆçš„ä¿¡å™ªæ¯”æ–¹æ³•",
                    "desc": "æœ€è¿‘çš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†åœ¨å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†ç‹¬ç‰¹é£æ ¼æ¨¡æ¿çš„åˆ›å»ºã€‚å¾®è°ƒå‚è€ƒå›¾åƒæ˜¯æœ€æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†é€šå¸¸ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒæ—¶çš„ç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç§¯æå°†ä¿¡å™ªæ¯”åˆ†å¸ƒå‘æ›´é«˜çš„å™ªå£°æ°´å¹³è½¬ç§»ï¼Œä»¥ä¸“æ³¨äºé£æ ¼ç‰¹å¾å‡ºç°çš„å™ªå£°æ°´å¹³ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‹¬ç‰¹é£æ ¼ï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ›´é«˜é£æ ¼å¯¹é½çš„å›¾åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15098",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "url": "https://huggingface.co/papers/2411.15098",
            "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.",
            "score": 4,
            "issue_id": 754,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "9cd668db99ed0902",
            "authors": [
                "Zhenxiong Tan",
                "Songhua Liu",
                "Xingyi Yang",
                "Qiaochu Xue",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OminiControl: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "OminiControl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Diffusion Transformer (DiT). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ DiT ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. OminiControl Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 0,1% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ UNet Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ DiT ĞºĞ°Ğº Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Image Conditioning with OminiControl",
                    "desc": "OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research."
                },
                "zh": {
                    "title": "OminiControlï¼šé«˜æ•ˆæ•´åˆå›¾åƒæ¡ä»¶çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†OminiControlï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜åº¦çµæ´»ä¸”å‚æ•°é«˜æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å›¾åƒæ¡ä»¶é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ä¸­ã€‚OminiControlåˆ©ç”¨å‚æ•°é‡ç”¨æœºåˆ¶ï¼Œä½¿DiTèƒ½å¤Ÿä½¿ç”¨è‡ªèº«ä½œä¸ºå¼ºå¤§çš„åŸºç¡€ï¼Œç¼–ç å›¾åƒæ¡ä»¶ï¼Œå¹¶é€šè¿‡çµæ´»çš„å¤šæ¨¡æ€æ³¨æ„åŠ›å¤„ç†å™¨è¿›è¡Œå¤„ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOminiControlä»…éœ€çº¦0.1%çš„é¢å¤–å‚æ•°ï¼Œå°±èƒ½æœ‰æ•ˆåœ°æ•´åˆæ³¨å…¥çš„å›¾åƒæ¡ä»¶ï¼Œå¹¶ä»¥ç»Ÿä¸€çš„æ–¹å¼å¤„ç†å¤šç§å›¾åƒæ¡ä»¶ä»»åŠ¡ã€‚é€šè¿‡åœ¨DiTè‡ªèº«ç”Ÿæˆçš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼ŒOminiControlåœ¨ä¸»é¢˜é©±åŠ¨ç”Ÿæˆå’Œç©ºé—´å¯¹é½æ¡ä»¶ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„UNetå’ŒDiTé€‚åº”æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12946",
            "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
            "url": "https://huggingface.co/papers/2411.12946",
            "abstract": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.",
            "score": 3,
            "issue_id": 756,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "de5ca9118a5cab35",
            "authors": [
                "Gabriel Chua",
                "Shing Yee Chan",
                "Shaun Khoo"
            ],
            "affiliations": [
                "Government Technology Agency Singapore",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12946.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#hallucinations",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ½ĞµÑ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM."
                },
                "en": {
                    "title": "Building Better Guardrails for Large Language Models",
                    "desc": "This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety."
                },
                "zh": {
                    "title": "æ„å»ºçµæ´»çš„é˜²æŠ¤æªæ–½ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨ä¸­å¯èƒ½å‡ºç°çš„åç¦»ä¸»é¢˜çš„è¯¯ç”¨é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æ— æ•°æ®çš„é˜²æŠ¤æªæ–½å¼€å‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„é«˜è¯¯æŠ¥ç‡å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¯¹é—®é¢˜ç©ºé—´çš„å®šæ€§å®šä¹‰ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å’Œè®­ç»ƒé˜²æŠ¤æªæ–½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€æºäº†åˆæˆæ•°æ®é›†å’Œé˜²æŠ¤æ¨¡å‹ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶å’Œå¼€å‘æä¾›äº†é‡è¦èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14762",
            "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
            "url": "https://huggingface.co/papers/2411.14762",
            "abstract": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.",
            "score": 2,
            "issue_id": 756,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "9490a40884583735",
            "authors": [
                "Huiwon Jang",
                "Sihyun Yu",
                "Jinwoo Shin",
                "Pieter Abbeel",
                "Younggyo Seo"
            ],
            "affiliations": [
                "KAIST",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14762.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "CoordTok: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "CoordTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ (x,y,t), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. CoordTok Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Video Tokenization with CoordTok",
                    "desc": "This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘æ ‡è®°åŒ–ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCoordTokçš„è§†é¢‘æ ‡è®°å™¨ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è§†é¢‘çš„æ ‡è®°åŒ–é—®é¢˜ã€‚CoordToké€šè¿‡å­¦ä¹ åæ ‡è¡¨ç¤ºä¸è¾“å…¥è§†é¢‘è¡¥ä¸ä¹‹é—´çš„æ˜ å°„ï¼Œåˆ©ç”¨äº†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCoordTokæ˜¾è‘—å‡å°‘äº†ç¼–ç é•¿è§†é¢‘æ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒCoordTokèƒ½å¤Ÿåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå°†128å¸§è§†é¢‘çš„æ ‡è®°æ•°é‡å‡å°‘åˆ°1280ä¸ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15124",
            "title": "TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training",
            "url": "https://huggingface.co/papers/2411.15124",
            "abstract": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\\"ULU 3 approach to more domains.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "44809ea81d71ef97",
            "authors": [
                "Nathan Lambert",
                "Jacob Morrison",
                "Valentina Pyatkin",
                "Shengyi Huang",
                "Hamish Ivison",
                "Faeze Brahman",
                "Lester James V. Miranda",
                "Alisa Liu",
                "Nouha Dziri",
                "Shane Lyu",
                "Yuling Gu",
                "Saumya Malik",
                "Victoria Graf",
                "Jena D. Hwang",
                "Jiangjiang Yang",
                "Ronan Le Bras",
                "Oyvind Tafjord",
                "Chris Wilhelm",
                "Luca Soldaini",
                "Noah A. Smith",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T\"ULU 3 - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT, DPO Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RLVR. T\"ULU 3 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Llama 3.1 Ğ¸ GPT-4o-mini. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Language Model Potential with T\"ULU 3",
                    "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools."
                },
                "zh": {
                    "title": "T\"ULU 3ï¼šå¼€æ”¾çš„åè®­ç»ƒæ¨¡å‹æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†T\"ULU 3ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„æœ€æ–°åè®­ç»ƒæ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’ŒæŠ€èƒ½ã€‚æˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œè®­ç»ƒé…æ–¹ï¼Œå¡«è¡¥äº†å¼€æ”¾æŠ€æœ¯ä¸ä¸“æœ‰æŠ€æœ¯ä¹‹é—´çš„é€æ˜åº¦å·®è·ã€‚T\"ULU 3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama 3.1å’ŒGPT-4o-miniç­‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¤šä»»åŠ¡è¯„ä¼°æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æŠ¥å‘Šï¼Œä»¥ä¾¿äºåœ¨æ›´å¤šé¢†åŸŸä¸­å¤ç°å’Œé€‚åº”T\"ULU 3çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14521",
            "title": "MyTimeMachine: Personalized Facial Age Transformation",
            "url": "https://huggingface.co/papers/2411.14521",
            "abstract": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.",
            "score": 1,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "d104e8a00be886bb",
            "authors": [
                "Luchao Qi",
                "Jiaye Wu",
                "Bang Gong",
                "Annie N. Wang",
                "David W. Jacobs",
                "Roni Sengupta"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14521.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ‘´",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ MyTimeMachine (MyTM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ĞµĞ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ°Ñ Ğ¡ĞµÑ‚ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ StyleGAN2. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Personalized Aging: Your Face, Your Time",
                    "desc": "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è€åŒ–ï¼ŒçœŸå®å†ç°",
                    "desc": "é¢éƒ¨è€åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œå—åˆ°æ€§åˆ«ã€ç§æ—ã€ç”Ÿæ´»æ–¹å¼ç­‰å¤šç§å› ç´ çš„å½±å“ï¼Œå› æ­¤å¾ˆéš¾å­¦ä¹ åˆ°ä¸€ä¸ªé€šç”¨çš„è€åŒ–æ¨¡å‹æ¥å‡†ç¡®é¢„æµ‹ä¸ªä½“çš„è€åŒ–æƒ…å†µã€‚ç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è€åŒ–æ•ˆæœï¼Œä½†é‡å¡‘çš„å›¾åƒå¾€å¾€ä¸ç›®æ ‡å¹´é¾„çš„å¤–è²Œä¸ç¬¦ï¼Œå› æ­¤éœ€è¦ä¸ªæ€§åŒ–å¤„ç†ã€‚æˆ‘ä»¬æå‡ºäº†MyTimeMachineï¼ˆMyTMï¼‰ï¼Œå®ƒç»“åˆäº†å…¨çƒè€åŒ–æ¨¡å‹å’Œä¸ªäººç…§ç‰‡é›†ï¼ˆä»…éœ€50å¼ å›¾åƒï¼‰æ¥å­¦ä¹ ä¸ªæ€§åŒ–çš„å¹´é¾„è½¬æ¢ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é€‚é…å™¨ç½‘ç»œå’Œä¸‰ç§æŸå¤±å‡½æ•°ï¼Œä½¿å¾—ç”Ÿæˆçš„è€åŒ–å›¾åƒèƒ½å¤Ÿä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶åœ¨è§†é¢‘ä¸­å®ç°é«˜è´¨é‡çš„è€åŒ–æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15131",
            "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
            "url": "https://huggingface.co/papers/2411.15131",
            "abstract": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "c38df92e9599db09",
            "authors": [
                "Ri-Zhao Qiu",
                "Yuchen Song",
                "Xuanbin Peng",
                "Sai Aneesh Suryadevara",
                "Ge Yang",
                "Minghuan Liu",
                "Mazeyu Ji",
                "Chengzhe Jia",
                "Ruihan Yang",
                "Xueyan Zou",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15131.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#training",
                    "#agi",
                    "#transfer_learning",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "WildLMa: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ WildLMa Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. WildLMa Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ CLIP Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑĞ²Ğ¾Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑƒĞ±Ğ¾Ñ€ĞºĞ° Ğ¼ÑƒÑĞ¾Ñ€Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Robots for Real-World Manipulation with WildLMa",
                    "desc": "This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training."
                },
                "zh": {
                    "title": "WildLMaï¼šæå‡æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­çš„æ“æ§èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWildLMaçš„ç§»åŠ¨æ“æ§æœºå™¨äººç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šé€‚åº”æ€§ä½çº§æ§åˆ¶å™¨ã€é€šç”¨è§†è§‰è¿åŠ¨æŠ€èƒ½åº“å’Œé•¿æ—¶é—´ä»»åŠ¡è§„åˆ’æ¥å£ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ŒWildLMaåœ¨æŠ“å–æˆåŠŸç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚æ¸…ç†æ ¡å›­èµ°å»Šåƒåœ¾å’Œæ“ä½œå¤æ‚ç‰©ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14208",
            "title": "Novel View Extrapolation with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2411.14208",
            "abstract": "The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "dddb7a1ecc9850f3",
            "authors": [
                "Kunhao Liu",
                "Ling Shao",
                "Shijian Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "UCAS-Terminus AI Lab, UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14208.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ”­",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ViewExtrapolator - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Stable Video Diffusion (SVD) Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ViewExtrapolator Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ SVD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ViewExtrapolator Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Novel View Extrapolation with ViewExtrapolator",
                    "desc": "This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation."
                },
                "zh": {
                    "title": "æå‡æ–°è§†å›¾å¤–æ¨çš„æ¸…æ™°åº¦ä¸çœŸå®æ„Ÿ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†å›¾åˆæˆæ–¹æ³•ï¼Œç§°ä¸ºViewExtrapolatorï¼Œæ—¨åœ¨æ”¹å–„æ–°è§†å›¾å¤–æ¨çš„æ•ˆæœã€‚ä¼ ç»Ÿçš„è¾å°„åœºæŠ€æœ¯åœ¨æ–°è§†å›¾æ’å€¼æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°è§†å›¾å¤–æ¨æ—¶æ•ˆæœè¾ƒå·®ã€‚ViewExtrapolatoråˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆå…ˆéªŒï¼Œé€šè¿‡é‡æ–°è®¾è®¡å»å™ªè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆæ–°è§†å›¾çš„æ¸…æ™°åº¦å’ŒçœŸå®æ„Ÿã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä¸åŒç±»å‹çš„3Dæ¸²æŸ“ï¼Œå¹¶ä¸”æ— éœ€å¯¹SVDè¿›è¡Œå¾®è°ƒï¼Œå…·æœ‰æ•°æ®å’Œè®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è®­ç»ƒè¿‡ç¨‹åŠå…¶åœ¨åˆ†å¸ƒåç§»ä¸Šçš„å±€é™æ€§ã€‚ä¸ºäº†æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½œè€…å¼•å…¥äº†åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰è¿‡ç¨‹ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†åå¥½æ•°æ®é›†MMPRï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ã€‚ä½œè€…å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½æ¿€å‘æ›´å¤šè¿›å±•ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è®­ç»ƒè¿‡ç¨‹åŠå…¶åœ¨åˆ†å¸ƒåç§»ä¸Šçš„å±€é™æ€§ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le xiÃ nyÇ’u kÄiyuÃ¡n duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) de xÃ¹nliÃ n guÃ²chÃ©ng jÃ­ qÃ­ zÃ i fÄ“nbÃ¹ piÄnyÃ­ shÃ ng de jÃºxÃ¬anxÃ¬ng.\n\nä¸ºäº†æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½œè€…å¼•å…¥äº†åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰è¿‡ç¨‹ã€‚\nWÃ¨ile tÃ­shÄ“ng duÅ mÃ³shÃ¬ tuÄ«lÇ nÃ©nglÃ¬, zuÃ²zhÄ› yÇnrÃ¹ le piÄnhÃ o yÅuhuÃ  (PO) guÃ²chÃ©ng.\n\nä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†åå¥½æ•°æ®é›†MMPRï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ã€‚\nTÄmen chuÃ ngjiÃ n le yÄ«gÃ¨ gÄo zhÃ¬liÃ ng de duÅ mÃ³shÃ¬ tuÄ«lÇ piÄnhÃ o shÃ¹jÃ¹jÃ­ MMPR, bÃ¬ng kÄifÄ le yÄ«zhÇ’ng hÃ¹nhÃ© piÄnhÃ o yÅuhuÃ  (MPO) fÄngfÇ.\n\nç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ã€‚\nJiÃ©gÇ”o xiÇnshÃ¬, zhÃ¨ zhÇ’ng fÄngfÇ zÃ i duÅ gÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, tÃ¨biÃ© shÃ¬ zÃ i duÅ mÃ³shÃ¬ tuÄ«lÇ rÃ¨nwÃ¹ zhÅng.\n\nä½œè€…å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½æ¿€å‘æ›´å¤šè¿›å±•ã€‚\nZuÃ²zhÄ› xÄ«wÃ ng zhÃ¨ xiÃ ng yÃ¡njiÅ« nÃ©ng jÄ«fÄ gÃ¨ng duÅ jÃ¬nzhÇn.\n\nä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚\nDÃ imÇ, shÃ¹jÃ¹ hÃ© mÃ³xÃ­ng jiÄng gÅngkÄi fÄbÃ¹.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'},\n{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open-source'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'train'},\n{'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'},\n{'word': 'å±€é™æ€§', 'pinyin': 'jÃº xiÃ n xÃ¬ng', 'trans': 'limitations'},\n{'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'enhance'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'},\n{'word': 'åå¥½', 'pinyin': 'piÄn hÃ o', 'trans': 'preference'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'},\n{'word': 'åˆ›å»º', 'pinyin': 'chuÃ ng jiÃ n', 'trans': 'create'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high-quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'develop'},\n{'word': 'æ··åˆ', 'pinyin': 'hÃ¹n hÃ©', 'trans': 'hybrid'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'},\n{'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨ biÃ©', 'trans': 'particularly'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'},\n{'word': 'æ¿€å‘', 'pinyin': 'jÄ« fÄ', 'trans': 'inspire'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},\n{'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'}]",
        "trans": "This article discusses the training process of existing open-source multimodal large language models (MLLMs) and their limitations in distribution shifts. To enhance multimodal reasoning capabilities, the authors introduce a preference optimization (PO) process. They created a high-quality multimodal reasoning preference dataset called MMPR and developed a hybrid preference optimization (MPO) method. The results show that this method performs excellently on multiple benchmark tests, particularly in multimodal reasoning tasks. The authors hope that this research will inspire further advancements. The code, data, and models will be publicly released.",
        "update_ts": "2024-11-24 09:32"
    }
}