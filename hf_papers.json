{
    "date": {
        "ru": "28 февраля",
        "en": "February 28",
        "zh": "2月28日"
    },
    "time_utc": "2025-02-28 04:13",
    "weekday": 4,
    "issue_id": 2456,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.19613",
            "title": "Self-rewarding correction for mathematical reasoning",
            "url": "https://huggingface.co/papers/2502.19613",
            "abstract": "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.",
            "score": 28,
            "issue_id": 2455,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 февраля",
                "en": "February 26",
                "zh": "2月26日"
            },
            "hash": "e2535efc8aadcc9d",
            "authors": [
                "Wei Xiong",
                "Hanning Zhang",
                "Chenlu Ye",
                "Lichang Chen",
                "Nan Jiang",
                "Tong Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19613.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самокорректирующиеся языковые модели: новый шаг к автономному ИИ",
                    "desc": "Исследователи изучают языковые модели с самовознаграждением, способные генерировать пошаговые рассуждения и оценивать их корректность без внешней обратной связи. Предложен двухэтапный алгоритмический подход для создания таких моделей, использующий только самогенерируемые данные. На первом этапе применяется последовательная выборка с отклонением для синтеза длинных цепочек рассуждений, включающих механизмы самовознаграждения и самокоррекции. Второй этап усиливает способность моделей оценивать точность ответов и улучшать выходные данные с помощью обучения с подкреплением."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Rewarding Reasoning and Self-Correction",
                    "desc": "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."
                },
                "zh": {
                    "title": "自我奖励推理：模型的独立思考与修正",
                    "desc": "我们研究了自我奖励推理的大型语言模型（LLMs），这些模型能够在推理过程中同时生成逐步推理并评估输出的正确性，而无需外部反馈。这种集成方法使得单一模型能够独立引导其推理过程，为模型部署提供了计算优势。我们特别关注自我修正的任务，模型能够自主检测响应中的错误，修正输出，并决定何时终止迭代优化循环。为此，我们提出了一种两阶段的算法框架，利用自生成的数据构建自我奖励推理模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20082",
            "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
            "url": "https://huggingface.co/papers/2502.20082",
            "abstract": "LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by \"needle-driven\" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.",
            "score": 8,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "ee15387b2b27d4c6",
            "authors": [
                "Ning Shang",
                "Li Lyna Zhang",
                "Siyuan Wang",
                "Gaokai Zhang",
                "Gilsinia Lopez",
                "Fan Yang",
                "Weizhu Chen",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Расширение контекста языковых моделей без потери качества",
                    "desc": "LongRoPE2 - это новый подход к расширению эффективного контекстного окна предобученных больших языковых моделей (LLM) до целевой длины. Метод основан на гипотезе о недостаточном обучении в высших измерениях RoPE и использует эволюционный поиск для эффективного масштабирования RoPE. LongRoPE2 применяет смешанное обучение на контекстах разной длины для адаптации весов модели. Эксперименты показали, что LongRoPE2 может расширить контекст LLaMA3-8B до 128 тысяч токенов, сохраняя производительность на коротких контекстах."
                },
                "en": {
                    "title": "Extending Context Length Without Compromise",
                    "desc": "LongRoPE2 is a new method that enhances the context length of large language models (LLMs) while maintaining their performance on shorter contexts. It introduces a hypothesis that inadequate training in higher dimensions of RoPE leads to out-of-distribution issues in existing models. The method employs a RoPE rescaling algorithm that uses evolutionary search to improve training effectiveness. Additionally, it utilizes a mixed context window training strategy to fine-tune model weights, allowing for long-context sequences without sacrificing short-context performance."
                },
                "zh": {
                    "title": "扩展上下文窗口，保持性能的创新方法",
                    "desc": "LongRoPE2是一种新方法，旨在扩展预训练大型语言模型（LLMs）的有效上下文窗口，同时保持在原始较短上下文窗口上的性能。该方法通过三个贡献实现：首先，提出了一个假设，认为在更高RoPE维度上的训练不足导致了现有方法中持续存在的分布外（OOD）问题；其次，提出了一种有效的RoPE重缩放算法，通过“针驱动”的困惑度指导的进化搜索来解决训练不足的问题；最后，采用混合上下文窗口训练方法，微调模型权重以适应长上下文序列的重缩放RoPE，同时保持短上下文的原始RoPE性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20395",
            "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
            "url": "https://huggingface.co/papers/2502.20395",
            "abstract": "In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.",
            "score": 5,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "e8862deee761c4d0",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20395.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Оптимизация маршрутизации на лету для повышения эффективности мультимодальных моделей",
                    "desc": "В статье представлен метод Re-Routing in Test-Time (R2-T2) для улучшения работы мультимодальных моделей на основе смеси экспертов (MoE). Авторы обнаружили, что маршрутизатор, обученный по принципу end-to-end, не всегда оптимально распределяет веса между экспертами для тестовых образцов. R2-T2 оптимизирует вектор весов маршрутизации во время тестирования, приближая его к векторам правильно предсказанных соседних образцов. Предложены три стратегии R2-T2 с различными целями оптимизации и пространствами поиска соседей."
                },
                "en": {
                    "title": "Optimizing Multimodal Performance with Test-Time Re-Routing",
                    "desc": "This paper addresses the performance gap in large multimodal models (LMMs) when processing non-language data compared to large language models (LLMs). The authors introduce a mixture-of-experts (MoE) approach to enhance the vision encoder, allowing for richer and more diverse representations. They identify that the router, which determines how to mix these expert representations, often fails to optimize routing weights effectively during testing. To solve this, they propose a method called Re-Routing in Test-Time (R2-T2), which fine-tunes routing weights based on nearby correctly predicted samples, significantly boosting the performance of LMMs on various challenging tasks without retraining the base model."
                },
                "zh": {
                    "title": "提升多模态模型性能的新方法",
                    "desc": "在大型多模态模型（LMMs）中，视觉表示的感知能力通常不如大型语言模型（LLMs）的推理能力，这影响了LMMs在复杂任务上的表现。最近，通过用专家混合（MoE）替换视觉编码器，缓解了这一弱点，提供了丰富且多样的表示。我们发现，端到端训练的路由器并不总能为每个测试样本生成最佳的路由权重。为了解决这个问题，我们提出了一种新颖且高效的方法“测试时重新路由（R2-T2）”，通过在测试时优化路由权重向量，显著提升了LMMs在多样化任务基准上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16645",
            "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
            "url": "https://huggingface.co/papers/2502.16645",
            "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.",
            "score": 4,
            "issue_id": 2456,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "616cf3f6f2ab1d17",
            "authors": [
                "Chenlong Wang",
                "Zhaoyang Chu",
                "Zhengxiang Cheng",
                "Xuyi Yang",
                "Kaiyue Qiu",
                "Yao Wan",
                "Zhou Zhao",
                "Xuanhua Shi",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Wuhuan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16645.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Синхронизация языковых моделей с эволюцией кода",
                    "desc": "Статья представляет CODESYNC - инструмент для выявления устаревших паттернов кода и сбора обновлений знаний о коде из сторонних библиотек Python в реальном времени. На основе CODESYNC разработан CODESYNCBENCH - комплексный бенчмарк для оценки способности моделей большого языка (LLM) синхронизироваться с эволюцией кода, охватывающий реальные обновления для 220 API из шести библиотек Python. Эксперименты на 14 современных LLM показали, что они испытывают трудности с динамической эволюцией кода даже при поддержке продвинутых методов обновления знаний. Авторы полагают, что их бенчмарк может стать основой для разработки более эффективных методов обновления знаний о коде в реальном времени."
                },
                "en": {
                    "title": "CODESYNC: Keeping Code Knowledge Fresh for LLMs",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in adapting to changes in third-party library APIs, which can lead to outdated or inefficient code. It introduces CODESYNC, a data engine designed to identify outdated code patterns and gather real-time updates from Python libraries. Additionally, the authors present CODESYNCBENCH, a benchmark for evaluating LLMs' performance in keeping up with code evolution, featuring 3,300 test cases across various tasks. The findings indicate that even advanced LLMs struggle with dynamic code changes, highlighting the need for improved methods for real-time code knowledge updating."
                },
                "zh": {
                    "title": "实时代码知识更新的基准测试",
                    "desc": "大型语言模型（LLMs）在软件工程中表现出色，但在适应不断变化的代码知识方面面临挑战，尤其是第三方库API的频繁更新。由于静态的预训练数据集，这种限制常常导致生成的代码无法执行或实现的安全性和效率不佳。为了解决这个问题，本文提出了CODESYNC，一个用于识别过时代码模式并收集来自Python第三方库的实时代码知识更新的数据引擎。基于CODESYNC，我们开发了CODESYNCBENCH，一个全面的基准测试，用于评估LLMs在代码演变中的同步能力，涵盖了来自六个Python库的220个API的真实更新。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20127",
            "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
            "url": "https://huggingface.co/papers/2502.20127",
            "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",
            "score": 2,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "ad848cf98c7468a7",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Pengfei Gao",
                "Xiangxin Meng",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "ByteDance",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20127.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rlhf",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "SoRFT: Эффективное обучение ЯМ для автоматического исправления кода",
                    "desc": "Авторы предлагают новый подход к обучению языковых моделей для решения проблем в программном коде - Subtask-oriented Reinforced Fine-Tuning (SoRFT). Метод разбивает задачу на подзадачи: локализацию файла, функции и строки кода, а также генерацию исправлений. SoRFT включает два этапа обучения: тонкую настройку с отбором данных и обучение с подкреплением на основе правил. Эксперименты показывают, что SoRFT значительно улучшает способность моделей решать проблемы в коде и обеспечивает экономичную альтернативу коммерческим моделям."
                },
                "en": {
                    "title": "Enhancing Issue Resolution with SoRFT: A Cost-Effective Approach",
                    "desc": "This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a new method designed to improve the issue-resolving capabilities of large language models (LLMs). It breaks down the issue resolution process into specific subtasks, such as file and function localization, and code editing. The training process involves two stages: first, a supervised fine-tuning phase that uses filtered data, and second, a reinforcement learning phase that applies Proximal Policy Optimization (PPO) with rewards based on ground-truth data. The results show that models trained with SoRFT outperform existing open-source models, achieving state-of-the-art results while being more cost-effective and maintaining better privacy."
                },
                "zh": {
                    "title": "提升问题解决能力的新方法",
                    "desc": "本论文提出了一种新的训练方法，称为子任务导向强化微调（SoRFT），旨在提高大型语言模型（LLMs）在问题解决方面的能力。我们将问题解决分解为结构化的子任务，包括文件定位、功能定位、行定位和代码编辑生成。SoRFT包含两个训练阶段：首先是基于拒绝采样的监督微调，其次是基于规则的强化学习，利用基于真实数据的奖励进行训练。实验结果表明，SoRFT显著提升了问题解决性能，改善了模型的泛化能力，并为商业模型提供了一种成本效益高的替代方案。"
                }
            }
        }
    ],
    "link_prev": "2025-02-27.html",
    "link_next": "2025-03-03.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2月27日"
    },
    "short_date_next": {
        "ru": "03.03",
        "en": "03/03",
        "zh": "3月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。",
        "title": "Kanana: Compute-efficient Bilingual Language Models",
        "pinyin": "Wǒmen jièshào le Kanana, yī xìliè zài hányǔ hé yīngyǔ zhōng biǎoxiàn chūsè de shuāngyǔ móxíng. Kanana de jìsuàn chéngběn xiǎnzhù dīyú lèisì guīmó de dǐngjí móxíng. Bàogào xiángxì jièshào le yùxùnliàn zhōng shǐyòng de jìshù, bāokuò gāo zhìliàng shùjù guòlǜ, fēn jiēduàn yùxùnliàn, shēndù kuòzhǎn, jiǎnzhī hé zhēngliú. Cǐwài, bàogào hái gàikuàng le Kanana móxíng zài xùnliàn hòu shǐyòng de fāngfǎ, bāokuò jiàndū wēitiáo hé piānhǎo yōuhuà, yǐ tígāo qí yǔ yònghù hùdòng de nénglì. Zùihòu, bàogào hái tǎolùn le yǔyán móxíng shìyìng tèdìng chǎngjīng de kěnéng fāngfǎ, rú qiànrù, jiǎnsuǒ zēngqiáng shēngchéng hé hánshù diàoyòng. Kanana móxíng xìliè cóng 2.1B dào 32.5B cānshù bùděng, 2.1B móxíng (jīchǔ, zhǐlìng, qiànrù) yǐ gōngkāi fābù, yǐ cùjìn hányǔ móxíng yánjiū.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'},\n{'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'},\n{'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'},\n{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'},\n{'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'},\n{'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'},\n{'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'},\n{'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'},\n{'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'},\n{'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'},\n{'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'},\n{'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'},\n{'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'},\n{'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'},\n{'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'},\n{'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'},\n{'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'},\n{'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]",
        "trans": "We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models.",
        "update_ts": "2025-02-27 09:11"
    }
}