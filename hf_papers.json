{
    "date": {
        "ru": "11 декабря",
        "en": "December 11",
        "zh": "12月11日"
    },
    "time_utc": "2024-12-11 03:28",
    "weekday": 2,
    "issue_id": 1058,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.06673",
            "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
            "url": "https://huggingface.co/papers/2412.06673",
            "abstract": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.",
            "score": 2,
            "issue_id": 1058,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "a8071141959ac48a",
            "authors": [
                "Chunwei Wang",
                "Guansong Lu",
                "Junwei Yang",
                "Runhui Huang",
                "Jianhua Han",
                "Lu Hou",
                "Wei Zhang",
                "Hang Xu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06673.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#training",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ILLUME: Единая мультимодальная ИИ-модель с улучшенной эффективностью обучения",
                    "desc": "ILLUME - это унифицированная мультимодальная большая языковая модель, объединяющая возможности понимания и генерации в рамках единой архитектуры. Модель использует эффективный визуальный токенизатор и многоэтапное обучение, что позволяет достичь высоких результатов на меньшем объеме данных. Внедрена схема самоусиливающегося мультимодального выравнивания для улучшения согласованности между текстовыми описаниями и сгенерированными изображениями. ILLUME демонстрирует конкурентоспособные результаты в различных задачах мультимодального понимания, генерации и редактирования."
                },
                "en": {
                    "title": "ILLUME: Efficient Multimodal Mastery in One Model",
                    "desc": "The paper presents ILLUME, a unified multimodal large language model (MLLM) that combines understanding and generation of both text and images. It introduces a vision tokenizer that uses semantic information to improve data efficiency, allowing for effective training with a smaller dataset of only 15 million samples. The model employs a self-enhancing multimodal alignment scheme to ensure that the generated images accurately reflect the text descriptions, reducing errors in image generation. Through extensive testing, ILLUME demonstrates competitive performance against existing models in multimodal tasks."
                },
                "zh": {
                    "title": "ILLUME：多模态理解与生成的统一模型",
                    "desc": "本文介绍了ILLUME，这是一种统一的多模态大语言模型（MLLM），它通过统一的下一个标记预测公式，将多模态理解和生成能力无缝集成在一个模型中。为了应对通常需要的大规模数据集，我们设计了一种视觉标记器，结合了语义信息，并采用渐进式多阶段训练程序，从而将预训练所需的数据集大小减少到仅1500万，远低于通常所需的数量，同时在性能上与现有的统一MLLM（如Janus）竞争或更优。我们还引入了一种新颖的自我增强多模态对齐方案，监督模型自我评估文本描述与自生成图像之间的一致性，从而提高图像理解的准确性，避免因生成图像不一致而导致的不现实和错误预测。通过广泛的实验，ILLUME在多模态理解、生成和编辑的各类基准测试中表现突出，能够与最先进的统一MLLM和专业模型竞争。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07724",
            "title": "Granite Guardian",
            "url": "https://huggingface.co/papers/2412.07724",
            "abstract": "We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian",
            "score": 1,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "9d3367b124b8b792",
            "authors": [
                "Inkit Padhi",
                "Manish Nagireddy",
                "Giandomenico Cornacchia",
                "Subhajit Chaudhury",
                "Tejaswini Pedapati",
                "Pierre Dognin",
                "Keerthiram Murugesan",
                "Erik Miehling",
                "Martín Santillán Cooper",
                "Kieran Fraser",
                "Giulio Zizzo",
                "Muhammad Zaid Hameed",
                "Mark Purcell",
                "Michael Desmond",
                "Qian Pan",
                "Inge Vejsbjerg",
                "Elizabeth M. Daly",
                "Michael Hind",
                "Werner Geyer",
                "Ambrish Rawat",
                "Kush R. Varshney",
                "Prasanna Sattigeri"
            ],
            "affiliations": [
                "IBM Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07724.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#hallucinations",
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защитник ИИ: обеспечение безопасности языковых моделей",
                    "desc": "Представлены модели Granite Guardian - набор инструментов для обнаружения рисков в запросах и ответах больших языковых моделей (LLM). Эти модели охватывают различные аспекты риска, включая социальные предубеждения, ненормативную лексику, насилие, сексуальный контент, неэтичное поведение и риски, связанные с галлюцинациями. Обученные на уникальном наборе данных, сочетающем человеческие аннотации и синтетические данные, модели Granite Guardian демонстрируют высокую производительность в обнаружении рисков. Проект выпущен с открытым исходным кодом для продвижения ответственного развития искусственного интеллекта."
                },
                "en": {
                    "title": "Granite Guardian: Safeguarding AI with Comprehensive Risk Detection",
                    "desc": "The Granite Guardian models are designed to enhance the safety of large language models (LLMs) by detecting various risks in prompts and responses. They cover a wide range of risk factors, including social bias, profanity, and hallucination-related issues, which are often missed by traditional models. These models are trained on a unique dataset that combines human annotations and synthetic data, making them effective at identifying risks like jailbreaking and retrieval-augmented generation (RAG) problems. With high AUC scores, Granite Guardian represents a significant advancement in responsible AI development and is available as open-source for community use."
                },
                "zh": {
                    "title": "Granite Guardian：安全使用大型语言模型的守护者",
                    "desc": "Granite Guardian模型是一套旨在提供风险检测的安全保障工具，适用于大型语言模型（LLM）的安全和负责任使用。这些模型覆盖多个风险维度，包括社会偏见、粗俗语言、暴力、性内容、不道德行为、越狱和幻觉相关风险。Granite Guardian模型通过结合来自多种来源的人类注释和合成数据进行训练，解决了传统风险检测模型通常忽视的风险。作为开源项目，Granite Guardian旨在促进社区内负责任的人工智能发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06845",
            "title": "Fully Open Source Moxin-7B Technical Report",
            "url": "https://huggingface.co/papers/2412.06845",
            "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
            "score": 1,
            "issue_id": 1058,
            "pub_date": "2024-12-08",
            "pub_date_card": {
                "ru": "8 декабря",
                "en": "December 8",
                "zh": "12月8日"
            },
            "hash": "410471f06d9e6883",
            "authors": [
                "Pu Zhao",
                "Xuan Shen",
                "Zhenglun Kong",
                "Yixin Shen",
                "Sung-En Chang",
                "Timothy Rupprecht",
                "Lei Lu",
                "Enfu Nan",
                "Changdi Yang",
                "Yumei He",
                "Xingchen Xu",
                "Yu Huang",
                "Wei Wang",
                "Yue Chen",
                "Yong He",
                "Yanzhi Wang"
            ],
            "affiliations": [
                "AIBAO LLC",
                "Cornell University",
                "Futurewei Technologies",
                "Harvard University",
                "Northeastern University",
                "Roboraction.ai",
                "Tulane University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06845.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#open_source",
                    "#ethics",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Moxin 7B: Открытая LLM для прозрачных инноваций в ИИ",
                    "desc": "В статье представлена модель Moxin 7B - полностью открытая большая языковая модель (LLM), разработанная в соответствии с Моделью открытости (MOF). Moxin 7B достигает высшего уровня открытости по классификации MOF благодаря полному раскрытию кода, конфигураций, наборов данных и промежуточных результатов. Эксперименты показывают, что модель превосходит популярные 7B-модели в zero-shot оценке и конкурентоспособна в few-shot оценке. Авторы подчеркивают важность открытости и прозрачности в развитии LLM для стимулирования инноваций и исследований."
                },
                "en": {
                    "title": "Moxin 7B: Leading the Way in Open-Source Language Models",
                    "desc": "This paper discusses the evolution of Large Language Models (LLMs), highlighting the contrast between proprietary models like GPT-4 and open-source alternatives such as LLaMA. It emphasizes the importance of transparency and reproducibility in AI, noting that many open-source models do not fully disclose their training processes or data. To address these issues, the authors introduce Moxin 7B, an open-source LLM that adheres to the Model Openness Framework (MOF), ensuring comprehensive access to its training code and datasets. The results demonstrate that Moxin 7B outperforms other 7B models in zero-shot tasks and remains competitive in few-shot scenarios, showcasing the potential of fully open-source LLMs."
                },
                "zh": {
                    "title": "Moxin 7B：开源语言模型的新标杆",
                    "desc": "最近，大型语言模型（LLMs）经历了显著的变革，受到了广泛关注。以GPT-4和GPT-o1为代表的专有LLMs展现了卓越的性能和多样性，同时开源LLMs如LLaMA和Mistral也因其易于定制和部署而受到青睐。尽管开源LLMs为创新和研究提供了前所未有的机会，但其商业化带来了透明性、可重复性和安全性方面的担忧。为了解决这些问题，我们推出了Moxin 7B，这是一个完全开源的LLM，遵循模型开放框架（MOF），并在透明性和开放性方面达到了最高标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07774",
            "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
            "url": "https://huggingface.co/papers/2412.07774",
            "abstract": "We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.",
            "score": 0,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "64e24bea8dffc31d",
            "authors": [
                "Xi Chen",
                "Zhifei Zhang",
                "He Zhang",
                "Yuqian Zhou",
                "Soo Ye Kim",
                "Qing Liu",
                "Yijun Li",
                "Jianming Zhang",
                "Nanxuan Zhao",
                "Yilin Wang",
                "Hui Ding",
                "Zhe Lin",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07774.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная модель для генерации и редактирования изображений на основе видеоданных",
                    "desc": "UniReal - это унифицированная модель для генерации и редактирования изображений. Она рассматривает различные задачи обработки изображений как прерывистую генерацию видео, используя входные и выходные изображения в качестве кадров. Модель обучается на масштабных видеоданных, что позволяет ей эффективно обрабатывать тени, отражения, изменения поз и взаимодействие объектов. UniReal демонстрирует универсальность и возможности применения в различных задачах компьютерного зрения."
                },
                "en": {
                    "title": "UniReal: Unifying Image Generation and Editing through Video Dynamics",
                    "desc": "UniReal is a comprehensive framework that simplifies various image generation and editing tasks by treating them as a form of video generation. It focuses on maintaining consistency between input and output images while allowing for visual variations, similar to how video frames work. By using videos as a source of universal supervision, UniReal learns complex world dynamics, enabling it to manage challenges like shadows, reflections, and object interactions effectively. This approach not only enhances traditional image tasks but also opens up new possibilities for innovative applications in image processing."
                },
                "zh": {
                    "title": "统一框架，图像生成与编辑的未来",
                    "desc": "UniReal是一个统一的框架，旨在解决各种图像生成和编辑任务。该框架通过将图像任务视为不连续的视频生成，来保持输入和输出之间的一致性，同时捕捉视觉变化。UniReal利用大规模视频作为通用监督源，学习世界动态，从而在处理阴影、反射、姿态变化和物体交互方面展现出先进的能力。该方法不仅适用于图像任务，还展现出对新应用的潜在能力。"
                }
            }
        }
    ],
    "link_prev": "2024-12-10.html",
    "link_next": "2024-12-12.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "short_date_next": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12月12日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了语言模型在解决数学问题时常常出错的问题。它介绍了ProcessBench，一个用于测量识别数学推理错误步骤能力的工具。ProcessBench包含3,400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题。每个案例都有逐步解决方案和人类专家标注的错误位置。模型需要找出最早含有错误的步骤，或者得出所有步骤都正确的结论。",
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
        "pinyin": "这篇文章讨论了语言模型在解决数学问题时常常出错的问题。\nZhè piān wénzhāng tǎolùnle yǔyán móxíng zài jiějué shùxué wèntí shí chángcháng chūcuò de wèntí.\n\n它介绍了ProcessBench，一个用于测量识别数学推理错误步骤能力的工具。\nTā jièshàole ProcessBench, yīgè yòngyú cèliáng shíbié shùxué tuīlǐ cuòwù bùzhòu nénglì de gōngjù.\n\nProcessBench包含3,400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题。\nProcessBench bāohán 3,400 gè cèshì ànlì, zhǔyào jízhōng zài jìngsài hé Àolínpǐkè jíbié de shùxué wèntí.\n\n每个案例都有逐步解决方案和人类专家标注的错误位置。\nMěi gè ànlì dōu yǒu zhúbù jiějué fāng'àn hé rénlèi zhuānjiā biāozhù de cuòwù wèizhì.\n\n模型需要找出最早含有错误的步骤，或者得出所有步骤都正确的结论。\nMóxíng xūyào zhǎochū zuìzǎo hányǒu cuòwù de bùzhòu, huòzhě déchū suǒyǒu bùzhòu dōu zhèngquè de jiélùn.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"数学\", \"pinyin\": \"shù xué\", \"trans\": \"mathematics\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"常常\", \"pinyin\": \"cháng cháng\", \"trans\": \"often\"},\n    {\"word\": \"出错\", \"pinyin\": \"chū cuò\", \"trans\": \"make a mistake\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"ProcessBench\", \"pinyin\": \"ProcessBench\", \"trans\": \"ProcessBench\"},\n    {\"word\": \"用于\", \"pinyin\": \"yòng yú\", \"trans\": \"used for\"},\n    {\"word\": \"测量\", \"pinyin\": \"cè liáng\", \"trans\": \"measure\"},\n    {\"word\": \"识别\", \"pinyin\": \"shí bié\", \"trans\": \"identify\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"错误\", \"pinyin\": \"cuò wù\", \"trans\": \"error\"},\n    {\"word\": \"步骤\", \"pinyin\": \"bù zhòu\", \"trans\": \"step\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"工具\", \"pinyin\": \"gōng jù\", \"trans\": \"tool\"},\n    {\"word\": \"包含\", \"pinyin\": \"bāo hán\", \"trans\": \"contain\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"案例\", \"pinyin\": \"àn lì\", \"trans\": \"case\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔ yào\", \"trans\": \"main\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus\"},\n    {\"word\": \"竞赛\", \"pinyin\": \"jìng sài\", \"trans\": \"competition\"},\n    {\"word\": \"奥林匹克\", \"pinyin\": \"ào lín pǐ kè\", \"trans\": \"Olympiad\"},\n    {\"word\": \"级别\", \"pinyin\": \"jí bié\", \"trans\": \"level\"},\n    {\"word\": \"逐步\", \"pinyin\": \"zhú bù\", \"trans\": \"step-by-step\"},\n    {\"word\": \"解决方案\", \"pinyin\": \"jiě jué fāng àn\", \"trans\": \"solution\"},\n    {\"word\": \"人类\", \"pinyin\": \"rén lèi\", \"trans\": \"human\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"位置\", \"pinyin\": \"wèi zhì\", \"trans\": \"position\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"找出\", \"pinyin\": \"zhǎo chū\", \"trans\": \"find out\"},\n    {\"word\": \"最早\", \"pinyin\": \"zuì zǎo\", \"trans\": \"earliest\"},\n    {\"word\": \"含有\", \"pinyin\": \"hán yǒu\", \"trans\": \"contain\"},\n    {\"word\": \"得出\", \"pinyin\": \"dé chū\", \"trans\": \"arrive at\"},\n    {\"word\": \"结论\", \"pinyin\": \"jié lùn\", \"trans\": \"conclusion\"},\n    {\"word\": \"正确\", \"pinyin\": \"zhèng què\", \"trans\": \"correct\"}\n]",
        "trans": "This article discusses the common issue of language models making errors when solving mathematical problems. It introduces ProcessBench, a tool designed to measure the ability to identify steps where mathematical reasoning errors occur. ProcessBench contains 3,400 test cases, primarily focused on competition and Olympiad-level mathematical problems. Each case includes a step-by-step solution and error locations annotated by human experts. The model is required to identify the earliest step containing an error or conclude that all steps are correct.",
        "update_ts": "2024-12-10 09:12"
    }
}