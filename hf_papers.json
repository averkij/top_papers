{
    "date": {
        "ru": "3 апреля",
        "en": "April 3",
        "zh": "4月3日"
    },
    "time_utc": "2025-04-03 08:14",
    "weekday": 3,
    "issue_id": 3046,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.00999",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "url": "https://huggingface.co/papers/2504.00999",
            "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.",
            "score": 46,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "bb6506ffd72aed19",
            "authors": [
                "Siyuan Li",
                "Luyuan Zhang",
                "Zedong Wang",
                "Juanxi Tian",
                "Cheng Tan",
                "Zicheng Liu",
                "Chang Yu",
                "Qingsong Xie",
                "Haonan Lu",
                "Haoqian Wang",
                "Zhen Lei"
            ],
            "affiliations": [
                "CAIR, HKISI-CAS",
                "MAIS CASIA",
                "OPPO AI Center",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "MergeVQ: Эффективное объединение генерации изображений и обучения представлений",
                    "desc": "Статья представляет новый метод MergeVQ, объединяющий маскированное моделирование изображений (MIM) с векторным квантованием (VQ) для улучшения генерации изображений и обучения представлений. MergeVQ использует технику слияния токенов в кодировщике для эффективного квантования и глобального выравнивания, а также кросс-внимание в декодере для восстановления деталей. Для генерации второго этапа предлагается MergeAR, выполняющий сжатие KV-кэша для эффективного предсказания в растровом порядке. Эксперименты на ImageNet показывают, что MergeVQ достигает конкурентоспособных результатов в задачах обучения визуальных представлений и генерации изображений, сохраняя высокую эффективность токенов и скорость вывода."
                },
                "en": {
                    "title": "MergeVQ: Bridging Image Generation and Representation Learning Efficiently",
                    "desc": "This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed."
                },
                "zh": {
                    "title": "MergeVQ：提升图像生成与表示学习的统一模型",
                    "desc": "本文提出了一种新的模型MergeVQ，旨在改善基于向量量化的图像生成和视觉表示学习之间的平衡。通过在编码器中引入令牌合并技术，MergeVQ能够在自注意力块后解耦潜在空间中的语义，从而提高生成质量和表示学习的效率。该模型在预训练阶段通过交叉注意力恢复细节，并在生成阶段使用KV缓存压缩来提高预测效率。实验结果表明，MergeVQ在视觉表示学习和图像生成任务中表现出色，同时保持了良好的令牌效率和推理速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00883",
            "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
            "url": "https://huggingface.co/papers/2504.00883",
            "abstract": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.",
            "score": 38,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "43fba84cc49adfad",
            "authors": [
                "Zhenyi Liao",
                "Qingsong Xie",
                "Yanhao Zhang",
                "Zijian Kong",
                "Haonan Lu",
                "Zhenyu Yang",
                "Zhijie Deng"
            ],
            "affiliations": [
                "OPPO AI Center",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00883.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#video",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в визуально-пространственном мышлении мультимодальных ИИ",
                    "desc": "Это исследование посвящено улучшению визуально-пространственного мышления мультимодальных больших языковых моделей (MLLM) с помощью обучения, подобного R1-Zero. Авторы обнаружили, что способности к визуально-пространственному мышлению у небольших и средних моделей Qwen2-VL не могут быть активированы с помощью подсказок цепочки рассуждений (CoT). Они применили обучение GRPO с использованием тщательно подобранного набора данных VSI-100k для улучшения визуально-пространственного мышления. В результате их модель vsGRPO-7B, дообученная на основе Qwen2-VL-7B, достигла производительности, сравнимой с лучшей моделью с открытым исходным кодом LLaVA-NeXT-Video-72B."
                },
                "en": {
                    "title": "Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training",
                    "desc": "This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs."
                },
                "zh": {
                    "title": "提升多模态模型的视觉空间推理能力",
                    "desc": "本研究聚焦于提升多模态大型语言模型（MLLMs）的视觉空间推理能力，特别是在视频基础的视觉空间智能（VSI）方面。我们发现小到中型的Qwen2-VL模型无法通过思维链（CoT）提示激活其视觉空间推理能力，因此引入了GRPO训练方法，并使用精心策划的VSI-100k数据集进行改进。经过120个GPU小时的训练，我们的vsGRPO-2B模型在性能上超越了基础模型12.1%，并且vsGRPO-7B模型的表现与最佳开源模型LLaVA-NeXT-Video-72B相当。我们的研究表明，保持KL惩罚在GRPO中是必要的，并且vsGRPO在与监督微调和直接偏好优化基线的比较中表现出明显的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01014",
            "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
            "url": "https://huggingface.co/papers/2504.01014",
            "abstract": "Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.",
            "score": 20,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "98efa783105c3173",
            "authors": [
                "Junhao Cheng",
                "Yuying Ge",
                "Yixiao Ge",
                "Jing Liao",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01014.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "AnimeGamer: погружение в интерактивный мир аниме через языковые инструкции",
                    "desc": "AnimeGamer - это новый подход к созданию интерактивных игр с персонажами аниме, использующий мультимодальные языковые модели (MLLM). Система генерирует динамические анимационные кадры, отображающие движения персонажей и изменения их состояний, на основе текстовых диалогов. AnimeGamer вводит новые мультимодальные представления с учетом действий, которые декодируются в видеоклипы высокого качества с помощью диффузионной модели. Метод превосходит существующие подходы по различным аспектам игрового опыта, что подтверждается автоматическими метриками и оценками людей."
                },
                "en": {
                    "title": "Transforming Anime into Interactive Gaming with Dynamic AI",
                    "desc": "This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations."
                },
                "zh": {
                    "title": "AnimeGamer：动态互动的无限动漫游戏体验",
                    "desc": "最近在图像和视频合成方面的进展为生成游戏带来了新的希望。本文提出了一种名为AnimeGamer的方法，利用多模态大语言模型生成动态动画镜头，以增强游戏的互动性和沉浸感。通过引入动作感知的多模态表示，AnimeGamer能够生成具有上下文一致性和动态变化的游戏状态。实验结果表明，AnimeGamer在游戏体验的各个方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20783",
            "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
            "url": "https://huggingface.co/papers/2503.20783",
            "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.",
            "score": 18,
            "issue_id": 3044,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "c5971e424bc52a6b",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение способностей рассуждения языковых моделей с помощью обучения с подкреплением",
                    "desc": "Исследование показало, что обучение с подкреплением (RL) в больших масштабах может напрямую улучшить способности рассуждения языковых моделей (LLM) без использования обучения с учителем. Авторы проанализировали различные базовые модели и выявили, что некоторые из них уже демонстрируют сильные способности к рассуждению без специальных шаблонов. Был обнаружен оптимизационный сдвиг в методе Group Relative Policy Optimization (GRPO), который искусственно увеличивает длину ответов. Для решения этой проблемы предложен новый метод Dr. GRPO, который улучшает эффективность токенов при сохранении производительности рассуждений."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Unbiased RL Optimization",
                    "desc": "This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model."
                },
                "zh": {
                    "title": "强化学习提升推理能力的新突破",
                    "desc": "DeepSeek-R1-Zero展示了大规模强化学习（RL）可以直接增强大型语言模型（LLMs）的推理能力，而无需监督微调。本文深入分析了R1-Zero训练的两个核心组成部分：基础模型和强化学习。我们研究了多种基础模型，包括DeepSeek-V3-Base，以了解预训练特性如何影响RL性能。我们的分析发现，DeepSeek-V3-Base已经展现出“恍然大悟”的时刻，而Qwen2.5基础模型即使在没有提示模板的情况下也表现出强大的推理能力，暗示了潜在的预训练偏差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01956",
            "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
            "url": "https://huggingface.co/papers/2504.01956",
            "abstract": "Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene",
            "score": 16,
            "issue_id": 3042,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "44f1db8ef8cc244a",
            "authors": [
                "Hanyang Wang",
                "Fangfu Liu",
                "Jiawei Chi",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01956.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VideoScene: Эффективная генерация 3D сцен из видео с помощью дистилляции диффузионных моделей",
                    "desc": "Статья представляет VideoScene - метод для эффективной генерации трехмерных сцен из разреженных видов. Авторы предлагают стратегию дистилляции 3D-aware leap flow для преодоления избыточной информации и обучения динамической политики шумоподавления. VideoScene достигает более быстрых и качественных результатов генерации 3D сцен по сравнению с предыдущими моделями диффузии видео. Этот подход открывает новые возможности для приложений преобразования видео в 3D."
                },
                "en": {
                    "title": "Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models",
                    "desc": "This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods."
                },
                "zh": {
                    "title": "高效生成3D场景的VideoScene",
                    "desc": "从稀疏视图恢复3D场景是一项具有挑战性的任务，因为它本质上是一个不适定的问题。传统方法通过几何正则化或前馈确定性模型等专门解决方案来缓解这一问题，但在输入视图重叠较少且视觉信息不足时，性能仍然下降。最近的视频生成模型显示出解决这一挑战的潜力，能够生成具有合理3D结构的视频片段。本文提出了VideoScene，通过视频扩散模型提炼生成3D场景，设计了3D感知的跃迁流蒸馏策略，以提高生成效率和效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01724",
            "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
            "url": "https://huggingface.co/papers/2504.01724",
            "abstract": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.",
            "score": 14,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "d59102a274145730",
            "authors": [
                "Yuxuan Luo",
                "Zhengkun Rong",
                "Lizhen Wang",
                "Longhao Zhang",
                "Tianshu Hu",
                "Yongming Zhu"
            ],
            "affiliations": [
                "Bytedance Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01724.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Реалистичная анимация человека с точным контролем мимики и движений",
                    "desc": "DreamActor-M1 - это новая модель на основе диффузионного трансформера для анимации человека по изображениям. Она использует гибридные сигналы управления, включающие неявные лицевые представления, 3D-сферы головы и 3D-скелеты тела для точного контроля мимики и движений. Модель применяет прогрессивное обучение на данных разного масштаба для адаптации к различным позам и ракурсам. DreamActor-M1 также интегрирует паттерны движения из последовательных кадров с визуальными ориентирами для обеспечения долгосрочной согласованности анимации."
                },
                "en": {
                    "title": "DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency",
                    "desc": "The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations."
                },
                "zh": {
                    "title": "突破动画生成的局限性，DreamActor-M1引领新潮流！",
                    "desc": "本文提出了一种基于扩散变换器的框架DreamActor-M1，旨在解决现有图像基础的人体动画方法在细粒度整体可控性、多尺度适应性和长期时间一致性方面的不足。通过混合控制信号，结合隐式面部表示、3D头部球体和3D身体骨架，实现了对面部表情和身体动作的强大控制，同时保持动画的表现力和身份一致性。为了适应不同的身体姿势和图像尺度，采用了渐进训练策略，使用不同分辨率和尺度的数据进行训练。实验结果表明，该方法在肖像、上半身和全身生成方面优于现有的最先进技术，具有强大的长期一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00824",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
            "url": "https://huggingface.co/papers/2504.00824",
            "abstract": "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.",
            "score": 13,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "b135f3f003dcaaff",
            "authors": [
                "Yubo Wang",
                "Xueguang Ma",
                "Ping Nie",
                "Huaye Zeng",
                "Zhiheng Lyu",
                "Yuxuan Zhang",
                "Benjamin Schneider",
                "Yi Lu",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#rag",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "ScholarCopilot: ИИ-помощник для профессионального академического письма",
                    "desc": "ScholarCopilot - это унифицированная система, улучшающая существующие большие языковые модели для генерации профессиональных академических статей с точными и контекстуально релевантными цитатами. Она динамически определяет, когда извлекать научные ссылки, генерируя токен [RET], и использует его представление для поиска релевантных цитат в базе данных. ScholarCopilot совместно оптимизирует задачи генерации и цитирования в рамках единой системы для повышения эффективности. Обученная на 500 тысячах статей из arXiv, модель достигает точности извлечения top-1 в 40.1% на оценочном наборе данных, превосходя базовые модели."
                },
                "en": {
                    "title": "Enhancing Academic Writing with ScholarCopilot",
                    "desc": "This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience."
                },
                "zh": {
                    "title": "ScholarCopilot：提升学术写作的智能助手",
                    "desc": "本研究提出了ScholarCopilot，一个统一框架，旨在提升大型语言模型在生成专业学术文章时的准确性和相关性。该系统通过生成检索标记[RET]，动态决定何时检索学术参考文献，并利用其表示从数据库中查找相关引用。ScholarCopilot在生成和引用任务上进行联合优化，以提高效率。经过在500K篇arXiv论文上的训练，该模型在评估数据集上实现了40.1%的顶级检索准确率，且在学术写作样本的生成质量上超越了参数更多的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01934",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
            "url": "https://huggingface.co/papers/2504.01934",
            "abstract": "We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.",
            "score": 11,
            "issue_id": 3042,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "a50e19f04d94405f",
            "authors": [
                "Runhui Huang",
                "Chunwei Wang",
                "Junwei Yang",
                "Guansong Lu",
                "Yunlong Yuan",
                "Jianhua Han",
                "Lu Hou",
                "Wei Zhang",
                "Lanqing Hong",
                "Hengshuang Zhao",
                "Hang Xu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01934.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ILLUME+: Унифицированная мультимодальная модель нового поколения",
                    "desc": "ILLUME+ - это новая мультимодальная языковая модель, использующая двойную визуальную токенизацию и диффузионный декодер. Она улучшает глубокое семантическое понимание и высококачественную генерацию изображений. ILLUME+ вводит унифицированный двойной визуальный токенизатор DualViTok, сохраняющий как мелкие текстуры, так и семантику, согласованную с текстом. Модель демонстрирует конкурентоспособную производительность в задачах мультимодального понимания, генерации и редактирования по сравнению с существующими унифицированными MLLM и специализированными моделями."
                },
                "en": {
                    "title": "ILLUME+: Unifying Understanding, Generation, and Editing in One Model",
                    "desc": "ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications."
                },
                "zh": {
                    "title": "ILLUME+: 多模态理解与生成的新突破",
                    "desc": "ILLUME+ 是一种新型的多模态学习模型，结合了双重视觉标记和扩散解码器，旨在提升深层语义理解和高保真图像生成能力。与现有的统一模型相比，ILLUME+ 能够更好地处理理解、生成和编辑这三种基本能力。通过引入 DualViTok，ILLUME+ 保留了细致的纹理和文本对齐的语义，同时采用粗到细的图像表示策略，增强了多模态理解和生成的能力。此外，ILLUME+ 在图像生成质量和超分辨率方面表现出色，展现了与现有模型的竞争力，为未来的多模态应用奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01848",
            "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
            "url": "https://huggingface.co/papers/2504.01848",
            "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.",
            "score": 11,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "60923777325e85cc",
            "authors": [
                "Giulio Starace",
                "Oliver Jaffe",
                "Dane Sherburn",
                "James Aung",
                "Jun Shern Chan",
                "Leon Maksin",
                "Rachel Dias",
                "Evan Mays",
                "Benjamin Kinsella",
                "Wyatt Thompson",
                "Johannes Heidecke",
                "Amelia Glaese",
                "Tejal Patwardhan"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01848.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PaperBench: измеряем способность ИИ воспроизводить передовые исследования",
                    "desc": "PaperBench - это новый бенчмарк для оценки способности ИИ-агентов воспроизводить современные исследования в области искусственного интеллекта. Он включает в себя 20 статей из ICML 2024, которые агенты должны воспроизвести с нуля, включая понимание вклада статьи, разработку кодовой базы и успешное выполнение экспериментов. Для объективной оценки разработаны рубрики, которые иерархически разбивают каждую задачу на подзадачи с четкими критериями оценки. Лучший протестированный агент, Claude 3.5 Sonnet (New) с открытым исходным кодом, достиг среднего балла воспроизведения 21.0%."
                },
                "en": {
                    "title": "Evaluating AI's Research Replication Skills with PaperBench",
                    "desc": "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."
                },
                "zh": {
                    "title": "PaperBench：评估AI复制研究能力的基准",
                    "desc": "我们介绍了PaperBench，这是一个评估人工智能代理复制最先进AI研究能力的基准。代理需要从头开始复制20篇ICML 2024的亮点和口头论文，包括理解论文贡献、开发代码库和成功执行实验。为了进行客观评估，我们开发了分层的评分标准，将每个复制任务分解为更小的子任务，并设定明确的评分标准。我们的评估还包括使用基于大型语言模型的评审者自动评分，并与顶尖的机器学习博士进行比较，发现目前的模型尚未超越人类基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01308",
            "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
            "url": "https://huggingface.co/papers/2504.01308",
            "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
            "score": 10,
            "issue_id": 3040,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "315938d70f25095e",
            "authors": [
                "Jiawei Wang",
                "Yushen Zuo",
                "Yuanjun Chai",
                "Zhendong Liu",
                "Yichen Fu",
                "Yichun Feng",
                "Kin-man Lam"
            ],
            "affiliations": [
                "Nanjing University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China",
                "University of Washington",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01308.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита визуально-языковых моделей от атак с помощью шумовой аугментации",
                    "desc": "Исследователи обнаружили уязвимость визуально-языковых моделей (VLM) к атакам с использованием зашумленных или искаженных изображений. Для решения этой проблемы они предложили метод Robust-VLGuard, включающий набор данных для обучения и дообучение с аугментацией шумом. Также был разработан метод DiffPure-VLM, использующий диффузионные модели для преобразования состязательных возмущений в гауссовский шум. Эксперименты показали, что предложенные методы значительно повышают устойчивость VLM к различным типам атак."
                },
                "en": {
                    "title": "Strengthening Vision-Language Models Against Noise Attacks",
                    "desc": "This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality."
                },
                "zh": {
                    "title": "增强视觉语言模型的安全性",
                    "desc": "视觉语言模型（VLMs）通过结合视觉信息扩展了大型语言模型（LLMs）的能力，但在处理噪声或损坏的图像时仍然容易受到攻击。现有的VLMs在训练过程中采取了安全措施以减轻这些攻击，但对噪声增强视觉输入的脆弱性却未给予足够重视。我们提出了Robust-VLGuard，这是一个多模态安全数据集，结合了对齐和不对齐的图像-文本对，并通过噪声增强的微调来降低攻击成功率，同时保持VLM的功能。实验结果表明，扩散模型的分布转移特性与我们微调的VLMs很好地对齐，显著减轻了不同强度的对抗扰动。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01204",
            "title": "Articulated Kinematics Distillation from Video Diffusion Models",
            "url": "https://huggingface.co/papers/2504.01204",
            "abstract": "We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/",
            "score": 9,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "c8630e7ca691cef3",
            "authors": [
                "Xuan Li",
                "Qianli Ma",
                "Tsung-Yi Lin",
                "Yongxin Chen",
                "Chenfanfu Jiang",
                "Ming-Yu Liu",
                "Donglai Xiang"
            ],
            "affiliations": [
                "NVIDIA",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01204.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Революция в анимации персонажей: от скелета к реалистичному движению",
                    "desc": "Articulated Kinematics Distillation (AKD) - это новый подход к генерации высококачественной анимации персонажей, объединяющий скелетную анимацию и современные генеративные модели. AKD использует скелетное представление для 3D-моделей, значительно уменьшая количество степеней свободы за счет фокуса на управлении суставами. Метод применяет Score Distillation Sampling с предобученными видео-диффузионными моделями для синтеза сложных движений, сохраняя структурную целостность. AKD показывает превосходные результаты по сравнению с существующими методами генерации текста в 4D анимацию."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with AKD",
                    "desc": "Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible."
                },
                "zh": {
                    "title": "关节运动蒸馏：高保真动画的新方法",
                    "desc": "本文提出了一种名为关节运动蒸馏（AKD）的框架，用于生成高保真角色动画。AKD通过使用基于骨骼的表示，显著减少了自由度，专注于关节级控制，从而实现高效且一致的运动合成。通过与预训练的视频扩散模型结合的得分蒸馏采样（SDS），AKD能够蒸馏复杂的关节运动，同时保持结构完整性，克服了4D神经变形场在保持形状一致性方面的挑战。实验表明，AKD在3D一致性和运动质量上优于现有的文本到4D生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2405.20216",
            "title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback",
            "url": "https://huggingface.co/papers/2405.20216",
            "abstract": "The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.",
            "score": 8,
            "issue_id": 3046,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "8ffe2bddf2c0ee58",
            "authors": [
                "Sanghyeon Na",
                "Yonggyu Kim",
                "Hyunjoon Lee"
            ],
            "affiliations": [
                "Kakao"
            ],
            "pdf_title_img": "assets/pdf/title_img/2405.20216.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🧑‍🎨",
                "ru": {
                    "title": "Революция в генерации изображений людей с помощью DPO",
                    "desc": "Эта статья представляет новый подход к генерации изображений людей с использованием метода Direct Preference Optimization (DPO). Авторы предлагают эффективный способ создания специализированного набора данных DPO для обучения моделей без необходимости дорогостоящей обратной связи от людей. Они также вводят модифицированную функцию потерь, которая улучшает процесс обучения DPO, минимизируя артефакты и повышая точность изображений. Результаты показывают значительное улучшение в генерации изображений людей с естественной анатомией, позами и соответствием текстовым запросам."
                },
                "en": {
                    "title": "Revolutionizing Human Image Generation with Direct Preference Optimization",
                    "desc": "This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts."
                },
                "zh": {
                    "title": "优化人类图像生成的新方法",
                    "desc": "本文探讨了通过文本到图像（T2I）方法生成高质量人类图像的挑战。与一般图像生成不同，人类图像合成需要满足严格的人体姿势、解剖结构和与文本提示对齐的标准。我们提出了一种新颖的方法，利用直接偏好优化（DPO）专门针对人类图像生成，构建高效的DPO数据集以训练模型，减少对昂贵人类反馈的依赖。通过修改损失函数，我们的训练过程能够减少伪影并提高图像的真实感，显著提升人类图像生成的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23135",
            "title": "LSNet: See Large, Focus Small",
            "url": "https://huggingface.co/papers/2503.23135",
            "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.",
            "score": 3,
            "issue_id": 3040,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 марта",
                "en": "March 29",
                "zh": "3月29日"
            },
            "hash": "d2ac65a2356c89c3",
            "authors": [
                "Ao Wang",
                "Hui Chen",
                "Zijia Lin",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist, Tsinghua University",
                "Department of Automation, Tsinghua University",
                "School of Software, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23135.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "LSNet: Эффективное компьютерное зрение по принципу 'Смотри широко, фокусируйся узко'",
                    "desc": "Статья представляет новый подход к проектированию легковесных нейронных сетей для компьютерного зрения, вдохновленный человеческой системой зрения. Авторы предлагают стратегию 'Смотри широко, фокусируйся узко' и вводят LS-свертку, сочетающую восприятие с большим ядром и агрегацию с малым ядром. На основе LS-свертки разработано семейство моделей LSNet, которое эффективно обрабатывает визуальную информацию. Эксперименты показывают, что LSNet превосходит существующие легковесные сети по производительности и эффективности в различных задачах компьютерного зрения."
                },
                "en": {
                    "title": "See Large, Focus Small: Efficient Vision Networks",
                    "desc": "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."
                },
                "zh": {
                    "title": "轻量级视觉网络的新策略：大视野，小聚焦",
                    "desc": "本文提出了一种新的轻量级视觉网络设计策略，称为“See Large, Focus Small”。该策略结合了大核感知和小核聚合的LS卷积，能够高效捕捉广泛的感知信息并实现精确的特征聚合。通过这种方法，LSNet在多种视觉任务中展现出优越的性能和效率，克服了现有轻量级模型在计算预算有限情况下的局限性。实验结果表明，LSNet在实时应用中表现出色，适合实际部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00406",
            "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
            "url": "https://huggingface.co/papers/2504.00406",
            "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
            "score": 1,
            "issue_id": 3046,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "d4ef7ad5ea6d5aad",
            "authors": [
                "Jiuzhou Han",
                "Wray Buntine",
                "Ehsan Shareghi"
            ],
            "affiliations": [
                "College of Engineering and Computer Science, VinUniversity",
                "Department of Data Science & AI, Monash University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00406.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#agents",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "VerifiAgent: умный верификатор для надежных ответов языковых моделей",
                    "desc": "Статья представляет VerifiAgent - унифицированного агента для верификации ответов больших языковых моделей. VerifiAgent использует двухуровневый подход: мета-верификацию для оценки полноты и согласованности ответов, и адаптивную верификацию на основе инструментов для различных типов рассуждений. Экспериментальные результаты показывают превосходство VerifiAgent над базовыми методами верификации во всех задачах рассуждения. Агент также демонстрирует эффективность в масштабировании вывода, достигая лучших результатов с меньшими затратами по сравнению с существующими моделями вознаграждения процесса в области математических рассуждений."
                },
                "en": {
                    "title": "VerifiAgent: Enhancing Reliability in Language Model Reasoning",
                    "desc": "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."
                },
                "zh": {
                    "title": "VerifiAgent：智能验证，提升推理准确性",
                    "desc": "大型语言模型展现了出色的推理能力，但常常产生不可靠或错误的回答。现有的验证方法通常是针对特定模型或领域，计算资源消耗大，且在不同推理任务中缺乏可扩展性。为了解决这些问题，我们提出了VerifiAgent，一个统一的验证代理，集成了两级验证：元验证评估模型回答的完整性和一致性，工具自适应验证则根据推理类型自动选择合适的验证工具。实验结果表明，VerifiAgent在所有推理任务中优于基线验证方法，并能通过利用验证结果的反馈进一步提高推理准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18817",
            "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations",
            "url": "https://huggingface.co/papers/2503.18817",
            "abstract": "Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.",
            "score": 1,
            "issue_id": 3046,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "fa652fd57c6312f8",
            "authors": [
                "Jeonghyeon Kim",
                "Sangheum Hwang"
            ],
            "affiliations": [
                "Seoul National University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18817.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение обнаружения выбросов через мультимодальную тонкую настройку",
                    "desc": "Статья посвящена обнаружению выбросов (OoDD) в мультимодальных моделях машинного обучения. Авторы предлагают метод мультимодальной тонкой настройки (MMFT) для улучшения производительности OoDD. Они вводят регуляризацию для усиления кросс-модального выравнивания, сближая семантически похожие изображения и тексты в гиперсферическом пространстве представлений. Экспериментальные результаты на наборах данных ImageNet-1k OoD показывают, что предложенный метод в сочетании с пост-хок подходами OoDD достигает наилучших результатов в обнаружении выбросов."
                },
                "en": {
                    "title": "Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning",
                    "desc": "This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na\"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets."
                },
                "zh": {
                    "title": "多模态微调提升分布外检测性能",
                    "desc": "本论文探讨了多模态微调（MMFT）在分布外检测（OoDD）中的应用。以往的研究主要集中在单一模态模型上，而我们提出的方法通过增强图像和文本嵌入之间的跨模态对齐，显著提升了OoDD性能。我们分析了传统微调方法的局限性，并提出了一种新的训练目标，以更好地利用预训练的知识。通过在ImageNet-1k OoD基准数据集上的实验，我们的方法在OoDD性能和ID准确率上均达到了最先进的水平。"
                }
            }
        }
    ],
    "link_prev": "2025-04-02.html",
    "link_next": "2025-04-04.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "02.04",
        "en": "04/02",
        "zh": "4月2日"
    },
    "short_date_next": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4月4日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为Any2Caption的新框架，旨在解决当前视频生成社区中准确理解用户意图的瓶颈问题。该框架的核心思想是将各种条件解释步骤与视频合成步骤分离。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频和特定提示（如区域、运动和摄像头姿态）转换为密集、结构化的字幕，从而为视频生成提供更好的指导。文章还引入了一个大规模数据集Any2CapIns，包含337K个实例和407K个条件，用于任意条件到字幕的指令调优。综合评估表明，该系统在可控性和视频质量方面显著优于现有的视频生成模型。",
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
        "pinyin": "这篇文章介绍了一种名为Any2Caption的新框架，旨在解决当前视频生成社区中准确理解用户意图的瓶颈问题。该框架的核心思想是将各种条件解释步骤与视频合成步骤分离。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频和特定提示（如区域、运动和摄像头姿态）转换为密集、结构化的字幕，从而为视频生成提供更好的指导。文章还引入了一个大规模数据集Any2CapIns，包含337K个实例和407K个条件，用于任意条件到字幕的指令调优。综合评估表明，该系统在可控性和视频质量方面显著优于现有的视频生成模型。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wèi Any2Caption de xīn kuàng jià, zhǐ yú jiě jué dāng qián shì pǐn shēng chéng shè qū zhōng zhǔn què lǐ jiě yòng hù de píng yǐ wèn ti. gǎi kuàng jià de hé xīn sī xiǎng shì jiāng gè zhǒng tiáo jiàn jiě shì bù zhòu yǔ shì pǐn hé chéng bù zhòu fēn lí. tōng guò lì yòng xiàn dài duō mó shì dà yǔ yán mó xíng (MLLMs), Any2Caption néng gòu jiāng wén běn, tú xiàng, shì pǐn hé tè dìng tí shì (rú qū yù, yùn dòng hé shè xiàng tóu zī tài) zhuǎn huàn wèi mì jī, jié gòu huà de zì mù, cóng ér wéi shì pǐn shēng chéng tí gōng gěng hǎo de zhǐ dǎo. wén zhāng hái yǐn rù le yī gè dà guī mó shù jù Any2CapIns, bāo hán 337K gè shí lì hé 407K gè tiáo jiàn, yòng yú rèn yì tiáo jiàn dào zì mù de zhǐ lìng tiáo yōu. zòng hé píng gǔ biǎo míng, gǎi xì tǒng zài kè kòng xìng hé shì pǐn zhì liàng fāng miàn xiǎn zhù yōu xiàn yǒu de shì pǐn shēng chéng mó xíng.",
        "vocab": "[{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '瓶颈', 'pinyin': 'píng jǐng', 'trans': 'bottleneck'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '核心', 'pinyin': 'hé xīn', 'trans': 'core'},\n{'word': '思想', 'pinyin': 'sī xiǎng', 'trans': 'idea'},\n{'word': '解释', 'pinyin': 'jiě shì', 'trans': 'interpretation'},\n{'word': '步骤', 'pinyin': 'bù zhòu', 'trans': 'step'},\n{'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesis'},\n{'word': '分离', 'pinyin': 'fēn lí', 'trans': 'separation'},\n{'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'},\n{'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '转换', 'pinyin': 'zhuǎn huàn', 'trans': 'convert'},\n{'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'},\n{'word': '结构化', 'pinyin': 'jié gòu huà', 'trans': 'structured'},\n{'word': '字幕', 'pinyin': 'zì mù', 'trans': 'subtitle'},\n{'word': '指导', 'pinyin': 'zhǐ dǎo', 'trans': 'guidance'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '实例', 'pinyin': 'shí lì', 'trans': 'instance'},\n{'word': '调优', 'pinyin': 'tiáo yōu', 'trans': 'tuning'},\n{'word': '综合', 'pinyin': 'zòng hé', 'trans': 'comprehensive'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'},\n{'word': '可控性', 'pinyin': 'kě kòng xìng', 'trans': 'controllability'},\n{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}]",
        "trans": "This article introduces a new framework called Any2Caption, aimed at addressing the bottleneck issue of accurately understanding user intent in the current video generation community. The core idea of the framework is to separate various condition interpretation steps from video synthesis steps. By leveraging modern multimodal large language models (MLLMs), Any2Caption can convert text, images, videos, and specific prompts (such as regions, motion, and camera poses) into dense, structured captions, providing better guidance for video generation. The article also introduces a large-scale dataset, Any2CapIns, containing 337K instances and 407K conditions, for instruction tuning from any condition to captions. Comprehensive evaluations indicate that the system significantly outperforms existing video generation models in terms of controllability and video quality.",
        "update_ts": "2025-04-02 09:11"
    }
}