{
    "date": {
        "ru": "4 ноября",
        "en": "November 4",
        "zh": "11月4日"
    },
    "time_utc": "2025-11-04 06:18",
    "weekday": 1,
    "issue_id": 6771,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.22115",
            "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
            "url": "https://huggingface.co/papers/2510.22115",
            "abstract": "We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.",
            "score": 27,
            "issue_id": 6771,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "d8f4a5a5ca5b930e",
            "authors": [
                "Ling-Team",
                "Ang Li",
                "Ben Liu",
                "Binbin Hu",
                "Bing Li",
                "Bingwei Zeng",
                "Borui Ye",
                "Caizhi Tang",
                "Changxin Tian",
                "Chao Huang",
                "Chao Zhang",
                "Chen Qian",
                "Chenchen Ju",
                "Chenchen Li",
                "Chengfu Tang",
                "Chili Fu",
                "Chunshao Ren",
                "Chunwei Wu",
                "Cong Zhang",
                "Cunyin Peng",
                "Dafeng Xu",
                "Daixin Wang",
                "Dalong Zhang",
                "Dingnan Jin",
                "Dingyuan Zhu",
                "Dongke Hu",
                "Fangzheng Zhao",
                "Feifan Wu",
                "Feng Zhu",
                "Gangshan Wang",
                "Haitao Zhang",
                "Hailin Zhao",
                "Hanxiao Zhang",
                "Hanzi Wang",
                "Hao Qian",
                "Haoyi Yu",
                "Heng Zhang",
                "Hongliang Zhang",
                "Hongzhi Luan",
                "Huirong Dong",
                "Huizhong Li",
                "Jia Li",
                "Jia Liu",
                "Jialong Zhu",
                "Jian Sha",
                "Jianping Wei",
                "Jiaolong Yang",
                "Jieyue Ma",
                "Jiewei Wu",
                "Jinjing Huang",
                "Jingyun Tian",
                "Jingyuan Zhang",
                "Jinquan Sun",
                "Juanhui Tu",
                "Jun Liu",
                "Jun Xu",
                "Jun Zhou",
                "Junjie Ou",
                "Junpeng Fang",
                "Kaihong Zhang",
                "Kaiqin Hu",
                "Ke Shi",
                "Kun Tang",
                "Kunlong Chen",
                "Lanyin Mei",
                "Lei Liang",
                "Lei Xu",
                "Libo Zhang",
                "Lin Ju",
                "Lin Yuan",
                "Ling Zhong",
                "Lintao Ma",
                "Lu Liu",
                "Lu Yu",
                "Lun Cai",
                "Meiqi Zhu",
                "Mengying Li",
                "Min Chen",
                "Minghao Xue",
                "Minghong Cai",
                "Mingming Yin",
                "Peijie Jiang",
                "Peilong Zhao",
                "Pingping Liu",
                "Qian Zhao",
                "Qing Cui",
                "Qingxiang Huang",
                "Qingyuan Yang",
                "Quankun Yu",
                "Shaowei Wei",
                "Shijie Lian",
                "Shoujian Zheng",
                "Shun Song",
                "Shungen Zhang",
                "Shuo Zhang",
                "Siyuan Li",
                "Song Liu",
                "Ting Guo",
                "Tong Zhao",
                "Wanli Gu",
                "Weichang Wu",
                "Weiguang Han",
                "Wenjing Fang",
                "Wubin Wang",
                "Xiang Shu",
                "Xiao Shi",
                "Xiaoshun Lan",
                "Xiaolu Zhang",
                "Xiaqing Sun",
                "Xin Zhao",
                "Xingyu Lu",
                "Xiong Xu",
                "Xudong Wang",
                "Xudong Wang",
                "Xuemin Yang",
                "Yajie Yang",
                "Yang Xiang",
                "Yanzhe Li",
                "Yi Zhang",
                "Yilong Wang",
                "Yingxue Li",
                "Yongzhen Guo",
                "Yuzhuo Fu",
                "Yuanyuan Wang",
                "Yue Yang",
                "Yue Yu",
                "Yufeng Deng",
                "Yun Zhang",
                "Yunfei Xu",
                "Yuqi Zhang",
                "Yuxiao He",
                "Zengke Gui",
                "Zhaoxin Huan",
                "Zhaoyang Wang",
                "Zhibo Zhu",
                "Zhihao Wang",
                "Zhiqiang Zhang",
                "Zhoufei Wang",
                "Zihang Zeng",
                "Ziqi Liu",
                "Zitao Xuan",
                "Zuoli Tang"
            ],
            "affiliations": [
                "Inclusion AI",
                "Open Language Foundation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22115.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24788",
            "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
            "url": "https://huggingface.co/papers/2510.24788",
            "abstract": "Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.",
            "score": 23,
            "issue_id": 6771,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "5e11d715260f761b",
            "pdf_title_img": "assets/pdf/title_img/2510.24788.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00086",
            "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
            "url": "https://huggingface.co/papers/2511.00086",
            "abstract": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.",
            "score": 17,
            "issue_id": 6771,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "5da45ea159eaeff7",
            "pdf_title_img": "assets/pdf/title_img/2511.00086.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01163",
            "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
            "url": "https://huggingface.co/papers/2511.01163",
            "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.",
            "score": 13,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "b7660e491d21d85e",
            "pdf_title_img": "assets/pdf/title_img/2511.01163.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.27571",
            "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
            "url": "https://huggingface.co/papers/2510.27571",
            "abstract": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.",
            "score": 8,
            "issue_id": 6771,
            "pub_date": "2025-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "cde2718fd05fefd7",
            "pdf_title_img": "assets/pdf/title_img/2510.27571.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.26491",
            "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
            "url": "https://huggingface.co/papers/2510.26491",
            "abstract": "Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop Curriculum RL with Off-Policy Influence guidance (CROPI), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR.",
            "score": 4,
            "issue_id": 6771,
            "pub_date": "2025-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "90ede1ba680b5efe",
            "pdf_title_img": "assets/pdf/title_img/2510.26491.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01833",
            "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
            "url": "https://huggingface.co/papers/2511.01833",
            "abstract": "The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-with-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-with-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce TIR-Bench, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.",
            "score": 3,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "db7aa80001628cea",
            "pdf_title_img": "assets/pdf/title_img/2511.01833.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01857",
            "title": "Trove: A Flexible Toolkit for Dense Retrieval",
            "url": "https://huggingface.co/papers/2511.01857",
            "abstract": "We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "447d2c76748bcbc1",
            "pdf_title_img": "assets/pdf/title_img/2511.01857.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01846",
            "title": "Towards Robust Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2511.01846",
            "abstract": "Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "027047e8d79aea9a",
            "pdf_title_img": "assets/pdf/title_img/2511.01846.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01775",
            "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
            "url": "https://huggingface.co/papers/2511.01775",
            "abstract": "Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "fe53da4979233ebe",
            "pdf_title_img": "assets/pdf/title_img/2511.01775.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01295",
            "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
            "url": "https://huggingface.co/papers/2511.01295",
            "abstract": "Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "4d5c971763d4c43e",
            "pdf_title_img": "assets/pdf/title_img/2511.01295.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00405",
            "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
            "url": "https://huggingface.co/papers/2511.00405",
            "abstract": "The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "7866a0534b62a36f",
            "pdf_title_img": "assets/pdf/title_img/2511.00405.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00279",
            "title": "LongCat-Flash-Omni Technical Report",
            "url": "https://huggingface.co/papers/2511.00279",
            "abstract": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.",
            "score": 2,
            "issue_id": 6771,
            "pub_date": "2025-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "e14acc53d087c71b",
            "pdf_title_img": "assets/pdf/title_img/2511.00279.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01718",
            "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
            "url": "https://huggingface.co/papers/2511.01718",
            "abstract": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4times faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.",
            "score": 1,
            "issue_id": 6771,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 ноября",
                "en": "November 3",
                "zh": "11月3日"
            },
            "hash": "0eaaf37e2b1fa4bf",
            "pdf_title_img": "assets/pdf/title_img/2511.01718.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00062",
            "title": "World Simulation with Video Foundation Models for Physical AI",
            "url": "https://huggingface.co/papers/2511.00062",
            "abstract": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5times smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.",
            "score": 1,
            "issue_id": 6771,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "7762bcd0ac3fdf9d",
            "pdf_title_img": "assets/pdf/title_img/2511.00062.jpg",
            "data": {
                "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
            }
        }
    ],
    "link_prev": "2025-11-03.html",
    "link_next": "2025-11-05.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "03.11",
        "en": "11/03",
        "zh": "11月3日"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11月5日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}