{
    "date": {
        "ru": "10 декабря",
        "en": "December 10",
        "zh": "12月10日"
    },
    "time_utc": "2024-12-10 09:12",
    "weekday": 1,
    "issue_id": 1041,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.06559",
            "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2412.06559",
            "abstract": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.",
            "score": 24,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "02f9abef0bc10297",
            "authors": [
                "Chujie Zheng",
                "Zhenru Zhang",
                "Beichen Zhang",
                "Runji Lin",
                "Keming Lu",
                "Bowen Yu",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06559.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "ProcessBench: новый бенчмарк для оценки выявления ошибок в математических рассуждениях",
                    "desc": "Статья представляет ProcessBench - набор данных для оценки способности моделей машинного обучения идентифицировать ошибки в математических рассуждениях. ProcessBench содержит 3400 тестовых примеров, в основном олимпиадного уровня, с пошаговыми решениями и аннотациями ошибок от экспертов. Авторы провели обширное тестирование различных моделей, включая Process Reward Models (PRM) и критические модели на основе больших языковых моделей. Результаты показывают, что существующие PRM плохо обобщаются на сложные задачи, а лучшая открытая модель QwQ-32B-Preview демонстрирует конкурентоспособность с проприетарной GPT-4."
                },
                "en": {
                    "title": "Enhancing Error Detection in Math Reasoning with ProcessBench",
                    "desc": "This paper presents ProcessBench, a benchmark designed to evaluate how well language models can identify errors in mathematical reasoning. It includes 3,400 test cases that focus on advanced math problems, with each case providing a detailed solution and expert-annotated error locations. The study compares the performance of process reward models (PRMs) and critic models, revealing that existing PRMs struggle with complex problems while critic models, particularly those based on general language models, perform better. The findings suggest that ProcessBench can enhance research on assessing reasoning processes in language models, contributing to their effective oversight."
                },
                "zh": {
                    "title": "提升语言模型数学推理的错误识别能力",
                    "desc": "本文介绍了一个名为ProcessBench的工具，用于评估语言模型在数学推理中识别错误步骤的能力。该工具包含3400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题上。每个案例都提供了逐步解决方案，并由人类专家标注了错误位置。研究发现，现有的过程奖励模型在更具挑战性的数学问题上表现不佳，而经过微调的PRM在识别错误方面优于一般语言模型的批评能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06699",
            "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
            "url": "https://huggingface.co/papers/2412.06699",
            "abstract": "Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data -- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Please refer to our project page at: https://vision.baai.ac.cn/see3d",
            "score": 3,
            "issue_id": 1041,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "1d5d0c1aa060a03f",
            "authors": [
                "Baorui Ma",
                "Huachen Gao",
                "Haoge Deng",
                "Zhengxiong Luo",
                "Tiejun Huang",
                "Lulu Tang",
                "Xinlong Wang"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence (BAAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06699.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#3d",
                    "#diffusion",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Вы видите - вы понимаете 3D",
                    "desc": "См3D - это модель мультиракурсной диффузии, обученная на масштабных интернет-видео для создания 3D-контента. Она получает знания о 3D, анализируя только визуальное содержание огромных объемов видеоданных. Для обучения был создан высококачественный набор данных WebVi3D, содержащий 320 млн кадров из 16 млн видеоклипов. См3D использует новаторский визуальный условный сигнал и интегрируется в конвейер на основе деформации для высококачественной 3D-генерации."
                },
                "en": {
                    "title": "You See It, You Got It: 3D Generation from Video Data",
                    "desc": "This paper introduces See3D, a novel multi-view diffusion model designed for generating 3D content from large-scale Internet videos. Unlike traditional methods that depend on limited 3D labels or 2D priors, See3D leverages a vast dataset of multi-view images, called WebVi3D, which is curated from 320 million frames across 16 million video clips. The model innovatively uses a visual-condition approach that eliminates the need for explicit 3D geometry or camera pose annotations, allowing it to learn generic 3D priors effectively. The results demonstrate that See3D excels in zero-shot and open-world generation tasks, outperforming existing models that rely on expensive 3D datasets."
                },
                "zh": {
                    "title": "通过视觉内容实现开放世界3D生成",
                    "desc": "本文介绍了一种名为See3D的视觉条件多视角扩散模型，旨在通过大规模互联网视频进行开放世界的3D内容生成。该模型通过自动过滤多视角不一致性和不足观察，构建了一个包含320M帧的高质量多视角图像数据集WebVi3D。为了消除对相机姿态的依赖，See3D引入了一种创新的视觉条件，通过在掩蔽视频数据中添加时间相关噪声生成纯2D诱导视觉信号。最终，See3D在单一和稀疏重建基准测试中表现出显著的零样本和开放世界生成能力，超越了基于昂贵3D数据集训练的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03123",
            "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
            "url": "https://huggingface.co/papers/2412.03123",
            "abstract": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.",
            "score": 2,
            "issue_id": 1040,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "bfad93a82eaec473",
            "authors": [
                "Xiaojun Xu",
                "Jinghan Jia",
                "Yuanshun Yao",
                "Yang Liu",
                "Hang Li"
            ],
            "affiliations": [
                "ByteDance Research",
                "Michigan State University",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03123.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#open_source",
                    "#small_models"
                ],
                "emoji": "💧",
                "ru": {
                    "title": "Невидимые водяные знаки в тексте с помощью ИИ",
                    "desc": "Исследователи предлагают метод встраивания незаметного многобитного водяного знака в текст с помощью перефразирования, используя большие языковые модели (LLM). Они обучают пару LLM-парафразеров, которые ведут себя по-разному, чтобы их различия в перефразировании могли быть идентифицированы специальным декодером. Для встраивания водяного знака используются два парафразера, которые кодируют заданный бинарный код на уровне предложений. Эксперименты показывают высокую эффективность метода, его устойчивость к искажениям и хорошую обобщаемость на новые данные."
                },
                "en": {
                    "title": "Stealthy Multi-Bit Watermarking via Paraphrasing with LLMs",
                    "desc": "This paper presents a method for embedding a multi-bit watermark in text using paraphrasing techniques with large language models (LLMs). The authors fine-tune two distinct LLM paraphrasers that create different paraphrases, allowing a trained decoder to identify the semantic differences and extract the embedded watermark. The watermark is encoded at the sentence level by alternating between the two paraphrasers, achieving high detection accuracy while preserving the original text's meaning. The proposed method demonstrates robustness against various text perturbations and maintains effectiveness even with out-of-distribution data."
                },
                "zh": {
                    "title": "隐形水印，语义保留！",
                    "desc": "本文提出了一种通过大语言模型（LLMs）进行的不可察觉的多比特文本水印嵌入方法。我们微调了一对表现不同的LLM改写器，以便通过文本语义的差异来识别其改写结果。为了嵌入多比特水印，我们交替使用两个改写器在句子级别编码预定义的二进制代码，并使用文本分类器作为解码器来解码每一位水印。实验结果表明，我们的水印在保持原句语义信息的同时，检测AUC超过99.99%，并且在词语替换和句子改写扰动下表现出良好的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06769",
            "title": "Training Large Language Models to Reason in a Continuous Latent Space",
            "url": "https://huggingface.co/papers/2412.06769",
            "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
            "score": 2,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "0ab7afee5f208244",
            "authors": [
                "Shibo Hao",
                "Sainbayar Sukhbaatar",
                "DiJia Su",
                "Xian Li",
                "Zhiting Hu",
                "Jason Weston",
                "Yuandong Tian"
            ],
            "affiliations": [
                "FAIR at Meta",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#rl",
                    "#training"
                ],
                "emoji": "🥥",
                "ru": {
                    "title": "Coconut: непрерывные рассуждения в скрытом пространстве для больших языковых моделей",
                    "desc": "Статья представляет новую парадигму рассуждений для больших языковых моделей под названием Coconut (Chain of Continuous Thought). В отличие от традиционного подхода цепочки размышлений (CoT), Coconut использует скрытое состояние модели как непрерывное представление хода рассуждений. Эксперименты показывают, что Coconut может эффективно усиливать способности языковых моделей в задачах рассуждения, позволяя им выполнять поиск в ширину вместо детерминированного пути. Результаты демонстрируют преимущества Coconut над CoT в некоторых задачах логического вывода, требующих значительного бэктрекинга при планировании."
                },
                "en": {
                    "title": "Unlocking Reasoning Potential with Continuous Thought",
                    "desc": "This paper introduces a new reasoning approach for large language models (LLMs) called Coconut, which operates in a continuous latent space rather than the traditional language space. The authors argue that the language space can limit reasoning capabilities, as many tokens are not essential for reasoning tasks. By using the last hidden state of the LLM as a representation of reasoning, Coconut allows for more flexible exploration of reasoning paths, enabling the model to consider multiple alternatives simultaneously. Experimental results show that Coconut outperforms the conventional chain-of-thought method in logical reasoning tasks that require backtracking, demonstrating the effectiveness of this novel paradigm."
                },
                "zh": {
                    "title": "Coconut：超越语言空间的推理新范式",
                    "desc": "大型语言模型（LLMs）通常在“语言空间”中进行推理，使用链式思维（CoT）来解决复杂问题。然而，语言空间并不总是最优的推理方式，因为许多词汇主要用于文本连贯性，而非推理本身。本文提出了一种新范式Coconut（连续思维链），利用LLM的最后隐藏状态作为推理状态的表示，并直接在连续空间中进行输入嵌入。实验表明，Coconut在多个推理任务中有效增强了LLM的表现，尤其在需要大量回溯的逻辑推理任务中表现优于传统的CoT。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06782",
            "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
            "url": "https://huggingface.co/papers/2412.06782",
            "abstract": "In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks.",
            "score": 2,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "584dec780be05e2d",
            "authors": [
                "Zhefei Gong",
                "Pengxiang Ding",
                "Shangke Lyu",
                "Siteng Huang",
                "Mingyang Sun",
                "Wei Zhao",
                "Zhaoxin Fan",
                "Donglin Wang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06782.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#agents",
                    "#training",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "CARP: эффективное и точное обучение роботов через поэтапное уточнение действий",
                    "desc": "В этой статье представлен новый подход к обучению визуомоторной политики роботов, называемый CARP. Он использует двухэтапный процесс: сначала автоэнкодер действий обучается многомасштабным представлениям последовательности действий, а затем трансформер в стиле GPT уточняет предсказание последовательности через поэтапный авторегрессивный процесс. CARP показывает высокую точность и плавность действий, сравнимую или превосходящую диффузионные модели, при этом сохраняя эффективность авторегрессивных подходов. Метод демонстрирует конкурентоспособные показатели успешности и в 10 раз более быстрый вывод по сравнению с современными методами."
                },
                "en": {
                    "title": "Efficient and Accurate Action Generation with CARP",
                    "desc": "This paper presents the Coarse-to-Fine AutoRegressive Policy (CARP), a new method for robotic visuomotor policy learning that enhances action trajectory generation. CARP improves upon traditional autoregressive models by breaking down the action generation into two stages: first, it uses an action autoencoder to create multi-scale representations, and then a GPT-style transformer refines these predictions. This approach not only increases the accuracy and smoothness of actions but also maintains efficiency, achieving up to 10x faster inference than existing methods. Extensive evaluations show that CARP outperforms diffusion-based models and achieves competitive success rates in various robotic tasks."
                },
                "zh": {
                    "title": "高效灵活的机器人动作生成新范式",
                    "desc": "在机器人视觉运动策略学习中，基于扩散模型的技术在动作轨迹生成的准确性上取得了显著成功，但由于多次去噪步骤和复杂约束，效率较低。本文提出了一种新颖的粗到细自回归策略（CARP），将自回归动作生成过程重新定义为粗到细的下一尺度方法。CARP将动作生成分为两个阶段：首先，动作自编码器学习整个动作序列的多尺度表示；然后，GPT风格的变换器通过粗到细的自回归过程精炼序列预测。该方法在效率上与自回归策略相当，同时在准确性和流畅性上与基于扩散的策略相匹配或超越，展示了高性能、高效和灵活的机器人任务动作生成新范式。"
                }
            }
        }
    ],
    "link_prev": "2024-12-09.html",
    "link_next": "2024-12-11.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "09.12",
        "en": "12/09",
        "zh": "12月9日"
    },
    "short_date_next": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12月11日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了语言模型在解决数学问题时常常出错的问题。它介绍了ProcessBench，一个用于测量识别数学推理错误步骤能力的工具。ProcessBench包含3,400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题。每个案例都有逐步解决方案和人类专家标注的错误位置。模型需要找出最早含有错误的步骤，或者得出所有步骤都正确的结论。",
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
        "pinyin": "这篇文章讨论了语言模型在解决数学问题时常常出错的问题。\nZhè piān wénzhāng tǎolùnle yǔyán móxíng zài jiějué shùxué wèntí shí chángcháng chūcuò de wèntí.\n\n它介绍了ProcessBench，一个用于测量识别数学推理错误步骤能力的工具。\nTā jièshàole ProcessBench, yīgè yòngyú cèliáng shíbié shùxué tuīlǐ cuòwù bùzhòu nénglì de gōngjù.\n\nProcessBench包含3,400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题。\nProcessBench bāohán 3,400 gè cèshì ànlì, zhǔyào jízhōng zài jìngsài hé Àolínpǐkè jíbié de shùxué wèntí.\n\n每个案例都有逐步解决方案和人类专家标注的错误位置。\nMěi gè ànlì dōu yǒu zhúbù jiějué fāng'àn hé rénlèi zhuānjiā biāozhù de cuòwù wèizhì.\n\n模型需要找出最早含有错误的步骤，或者得出所有步骤都正确的结论。\nMóxíng xūyào zhǎochū zuìzǎo hányǒu cuòwù de bùzhòu, huòzhě déchū suǒyǒu bùzhòu dōu zhèngquè de jiélùn.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"数学\", \"pinyin\": \"shù xué\", \"trans\": \"mathematics\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"常常\", \"pinyin\": \"cháng cháng\", \"trans\": \"often\"},\n    {\"word\": \"出错\", \"pinyin\": \"chū cuò\", \"trans\": \"make a mistake\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"ProcessBench\", \"pinyin\": \"ProcessBench\", \"trans\": \"ProcessBench\"},\n    {\"word\": \"用于\", \"pinyin\": \"yòng yú\", \"trans\": \"used for\"},\n    {\"word\": \"测量\", \"pinyin\": \"cè liáng\", \"trans\": \"measure\"},\n    {\"word\": \"识别\", \"pinyin\": \"shí bié\", \"trans\": \"identify\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"错误\", \"pinyin\": \"cuò wù\", \"trans\": \"error\"},\n    {\"word\": \"步骤\", \"pinyin\": \"bù zhòu\", \"trans\": \"step\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"工具\", \"pinyin\": \"gōng jù\", \"trans\": \"tool\"},\n    {\"word\": \"包含\", \"pinyin\": \"bāo hán\", \"trans\": \"contain\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"案例\", \"pinyin\": \"àn lì\", \"trans\": \"case\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔ yào\", \"trans\": \"main\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus\"},\n    {\"word\": \"竞赛\", \"pinyin\": \"jìng sài\", \"trans\": \"competition\"},\n    {\"word\": \"奥林匹克\", \"pinyin\": \"ào lín pǐ kè\", \"trans\": \"Olympiad\"},\n    {\"word\": \"级别\", \"pinyin\": \"jí bié\", \"trans\": \"level\"},\n    {\"word\": \"逐步\", \"pinyin\": \"zhú bù\", \"trans\": \"step-by-step\"},\n    {\"word\": \"解决方案\", \"pinyin\": \"jiě jué fāng àn\", \"trans\": \"solution\"},\n    {\"word\": \"人类\", \"pinyin\": \"rén lèi\", \"trans\": \"human\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"位置\", \"pinyin\": \"wèi zhì\", \"trans\": \"position\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"找出\", \"pinyin\": \"zhǎo chū\", \"trans\": \"find out\"},\n    {\"word\": \"最早\", \"pinyin\": \"zuì zǎo\", \"trans\": \"earliest\"},\n    {\"word\": \"含有\", \"pinyin\": \"hán yǒu\", \"trans\": \"contain\"},\n    {\"word\": \"得出\", \"pinyin\": \"dé chū\", \"trans\": \"arrive at\"},\n    {\"word\": \"结论\", \"pinyin\": \"jié lùn\", \"trans\": \"conclusion\"},\n    {\"word\": \"正确\", \"pinyin\": \"zhèng què\", \"trans\": \"correct\"}\n]",
        "trans": "This article discusses the common issue of language models making errors when solving mathematical problems. It introduces ProcessBench, a tool designed to measure the ability to identify steps where mathematical reasoning errors occur. ProcessBench contains 3,400 test cases, primarily focused on competition and Olympiad-level mathematical problems. Each case includes a step-by-step solution and error locations annotated by human experts. The model is required to identify the earliest step containing an error or conclude that all steps are correct.",
        "update_ts": "2024-12-10 09:12"
    }
}