{
    "date": {
        "ru": "16 декабря",
        "en": "December 16",
        "zh": "12月16日"
    },
    "time_utc": "2024-12-16 18:14",
    "weekday": 0,
    "issue_id": 1150,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.10360",
            "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
            "url": "https://huggingface.co/papers/2412.10360",
            "abstract": "Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.   We begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.   Guided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME.",
            "score": 68,
            "issue_id": 1137,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "780ae1aa1dc1af24",
            "authors": [
                "Orr Zohar",
                "Xiaohan Wang",
                "Yann Dubois",
                "Nikhil Mehta",
                "Tong Xiao",
                "Philippe Hansen-Estruch",
                "Licheng Yu",
                "Xiaofang Wang",
                "Felix Juefei-Xu",
                "Ning Zhang",
                "Serena Yeung-Levy",
                "Xide Xia"
            ],
            "affiliations": [
                "Meta",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10360.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#transfer_learning",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Раскрывая секреты понимания видео в больших мультимодальных моделях",
                    "desc": "Исследование раскрывает механизмы понимания видео в крупных мультимодальных моделях (LMM). Авторы обнаружили принцип масштабируемой согласованности, позволяющий переносить решения с маленьких моделей на большие. Они изучили различные аспекты видео-LMM, включая выборку кадров, архитектуры и состав данных. На основе полученных выводов представлено семейство моделей Apollo, демонстрирующее передовые результаты для разных размеров моделей."
                },
                "en": {
                    "title": "Unlocking Video Understanding in Large Multimodal Models with Apollo",
                    "desc": "This paper investigates the mechanisms behind video understanding in Large Multimodal Models (LMMs), which are complex AI systems that process both video and text. The authors identify a principle called Scaling Consistency, which shows that insights from smaller models can be applied to larger ones, helping to reduce computational costs. They also explore various aspects of video-LMMs, such as video sampling methods and architecture choices, to improve performance. The result of their research is Apollo, a new family of LMMs that significantly outperforms existing models in video perception tasks."
                },
                "zh": {
                    "title": "揭示视频理解的关键机制",
                    "desc": "本论文探讨了大型多模态模型（LMMs）在视频理解方面的机制，指出目前对其理解仍然不足。研究发现，缩放一致性是影响视频-LMM研究计算需求的主要因素，较小模型和数据集的设计和训练决策可以有效转移到更大模型上。通过对视频特定方面的深入研究，提出了Apollo系列模型，这些模型在不同规模下表现优异，能够高效处理长达一小时的视频。我们的实验结果显示，Apollo-3B在LongVideoBench上超越了大多数现有的7B模型，Apollo-7B在多个基准测试中也表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09624",
            "title": "GenEx: Generating an Explorable World",
            "url": "https://huggingface.co/papers/2412.09624",
            "abstract": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
            "score": 51,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "c4524ac73801b5cd",
            "authors": [
                "Taiming Lu",
                "Tianmin Shu",
                "Junfei Xiao",
                "Luoxin Ye",
                "Jiahao Wang",
                "Cheng Peng",
                "Chen Wei",
                "Daniel Khashabi",
                "Rama Chellappa",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.09624.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#agi",
                    "#3d"
                ],
                "emoji": "🌎",
                "ru": {
                    "title": "Генеративное воображение для исследования 3D-мира",
                    "desc": "GenEx - это система, способная планировать сложное исследование физического мира с помощью генеративного воображения. Она создает целостную трехмерную среду на основе всего одного RGB-изображения, генерируя панорамные видеопотоки. Модель обучена на данных из Unreal Engine и демонстрирует высокое качество генерации мира, согласованность при длительных траекториях и возможности активного 3D-картирования. Агенты на основе GPT используют GenEx для выполнения сложных задач в воображаемом пространстве, что открывает перспективы для исследования реального мира."
                },
                "en": {
                    "title": "GenEx: Empowering AI Exploration with Generative Imagination",
                    "desc": "This paper presents GenEx, a novel system designed to enhance artificial intelligence's ability to explore and navigate 3D environments. GenEx utilizes generative imagination to create realistic 3D environments from a single RGB image, enabling AI agents to visualize and interact with these spaces through panoramic video. The system is built on extensive 3D data from Unreal Engine, ensuring high-quality world generation and robust consistency during exploration. By leveraging predictive expectations, GPT-assisted agents can perform complex tasks, improving their decision-making in both exploratory and navigational contexts."
                },
                "zh": {
                    "title": "GenEx：开启AI探索3D世界的新篇章",
                    "desc": "本研究介绍了GenEx系统，它能够通过生成想象来规划复杂的3D世界探索。GenEx从单张RGB图像生成一致的3D环境，并通过全景视频流将其呈现出来。该系统利用来自虚幻引擎的可扩展3D世界数据，捕捉360度的环境，为AI代理提供了广阔的探索空间。GenEx展示了高质量的世界生成和强大的3D能力，使得AI代理能够执行复杂的任务，包括无目标探索和目标驱动导航。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09604",
            "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
            "url": "https://huggingface.co/papers/2412.09604",
            "abstract": "The remarkable success of Large Language Models (LLMs) has extended to the multimodal domain, achieving outstanding performance in image understanding and generation. Recent efforts to develop unified Multimodal Large Language Models (MLLMs) that integrate these capabilities have shown promising results. However, existing approaches often involve complex designs in model architecture or training pipeline, increasing the difficulty of model training and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful encoder-free MLLM capable of both image understanding and generation. To address challenges identified in existing encoder-free unified MLLMs, we introduce the token folding mechanism and the vision-expert-based progressive alignment pretraining strategy, which effectively support high-resolution image understanding while reducing training complexity. After being trained on large-scale mixed image-text data with a unified next-token prediction objective, SynerGen-VL achieves or surpasses the performance of existing encoder-free unified MLLMs with comparable or smaller parameter sizes, and narrows the gap with task-specific state-of-the-art models, highlighting a promising path toward future unified MLLMs. Our code and models shall be released.",
            "score": 24,
            "issue_id": 1142,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "1c0941190b24e85f",
            "authors": [
                "Hao Li",
                "Changyao Tian",
                "Jie Shao",
                "Xizhou Zhu",
                "Zhaokai Wang",
                "Jinguo Zhu",
                "Wenhan Dou",
                "Xiaogang Wang",
                "Hongsheng Li",
                "Lewei Lu",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Beijing National Research Center for Information Science and Technology",
                "MMLab, The Chinese University of Hong Kong",
                "Nanjing University",
                "OpenGVLab, Shanghai AI Laboratory",
                "SenseTime Research",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09604.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#multimodal",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Простая, но мощная мультимодальная ИИ-модель для понимания и генерации изображений",
                    "desc": "Статья представляет SynerGen-VL - новую мультимодальную языковую модель без энкодера для понимания и генерации изображений. Авторы вводят механизм сворачивания токенов и стратегию предобучения с прогрессивным выравниванием на основе экспертов по зрению для эффективной работы с изображениями высокого разрешения. SynerGen-VL обучается на масштабных мультимодальных данных с единой целевой функцией предсказания следующего токена. Модель демонстрирует результаты на уровне или выше существующих аналогов при сравнимом или меньшем размере."
                },
                "en": {
                    "title": "Simplifying Multimodal Learning with SynerGen-VL",
                    "desc": "This paper presents SynerGen-VL, a new type of Multimodal Large Language Model (MLLM) that simplifies the process of image understanding and generation without the need for complex encoders. It introduces innovative techniques like token folding and a vision-expert-based pretraining strategy to enhance performance while minimizing training difficulties. By training on a large dataset of mixed image and text, SynerGen-VL achieves competitive results compared to existing models, even with fewer parameters. This work suggests a more efficient approach to developing unified MLLMs, paving the way for future advancements in the field."
                },
                "zh": {
                    "title": "SynerGen-VL：简化的多模态大型语言模型",
                    "desc": "本论文介绍了一种名为SynerGen-VL的多模态大型语言模型（MLLM），它能够同时进行图像理解和生成。我们提出了一种简单而强大的无编码器设计，旨在降低模型训练的复杂性。通过引入令牌折叠机制和基于视觉专家的渐进对齐预训练策略，SynerGen-VL在高分辨率图像理解方面表现出色。经过大规模混合图像-文本数据的训练，SynerGen-VL在性能上与现有模型相当或更优，展示了未来统一多模态大型语言模型的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07769",
            "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
            "url": "https://huggingface.co/papers/2412.07769",
            "abstract": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-of-the-art models in medical LLM evaluation benchmarks. Our model also sets a new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2.",
            "score": 20,
            "issue_id": 1140,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "046676af13bd3252",
            "authors": [
                "Sahal Shaji Mullappilly",
                "Mohammed Irfan Kurpath",
                "Sara Pieri",
                "Saeed Yahya Alseiari",
                "Shanavas Cholakkal",
                "Khaled Aldahmani",
                "Fahad Khan",
                "Rao Anwer",
                "Salman Khan",
                "Timothy Baldwin",
                "Hisham Cholakkal"
            ],
            "affiliations": [
                "Govt Medical College Kozhikode",
                "Linkoping University",
                "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)",
                "Shaikh Tahnoon bin Mohammed Medical City (STMC)",
                "Sheikh Shakhbout Medical City (SSMC)",
                "Tawam Hospital"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07769.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#science",
                    "#healthcare",
                    "#machine_translation",
                    "#multimodal"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Революция в медицинском ИИ: двуязычная мультимодальная модель BiMediX2",
                    "desc": "BiMediX2 - это двуязычная (арабско-английская) большая мультимодальная модель для биомедицинской области, объединяющая текстовые и визуальные модальности. Модель основана на архитектуре Llama3.1 и обучена на обширном двуязычном наборе данных из 1,6 млн медицинских взаимодействий. BiMediX2 достигает передовых результатов в различных медицинских задачах, включая анализ изображений и генерацию отчетов. Авторы также предлагают новый двуязычный бенчмарк BiMed-MBench для оценки медицинских мультимодальных моделей."
                },
                "en": {
                    "title": "BiMediX2: Bridging Bilingual Medical Understanding with Multimodal Intelligence",
                    "desc": "This paper presents BiMediX2, a bilingual large multimodal model designed for biomedical applications, capable of processing both text and images in Arabic and English. It utilizes the Llama3.1 architecture to enhance interactions in medical contexts, allowing for complex conversations that involve medical imagery. The model is trained on a substantial bilingual dataset of 1.6 million samples, achieving superior performance on various medical benchmarks, particularly in multimodal evaluations. BiMediX2 not only surpasses existing models like GPT-4 in accuracy but also sets new standards for bilingual medical language models."
                },
                "zh": {
                    "title": "双语生物医学模型的创新突破",
                    "desc": "本文介绍了BiMediX2，这是一个双语（阿拉伯语-英语）生物医学专家大型多模态模型（LMM），具有统一架构，能够整合文本和视觉模态，从而实现高级图像理解和医疗应用。BiMediX2利用Llama3.1架构，支持阿拉伯语和英语的无缝交互，能够处理基于文本的输入和涉及医疗图像的多轮对话。该模型在一个包含160万样本的双语医疗数据集上进行训练，涵盖了多种医疗交互，文本和图像模态混合。BiMediX2在多个医疗基准测试中表现出色，尤其在英语和阿拉伯语的多模态医疗评估中，分别提高了超过9%和20%的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10047",
            "title": "Large Action Models: From Inception to Implementation",
            "url": "https://huggingface.co/papers/2412.10047",
            "abstract": "As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence.   In this paper, we present a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. We begin with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, we provide a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. We conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications.   The code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/.",
            "score": 17,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "be65080464153291",
            "authors": [
                "Lu Wang",
                "Fangkai Yang",
                "Chaoyun Zhang",
                "Junting Lu",
                "Jiaxu Qian",
                "Shilin He",
                "Pu Zhao",
                "Bo Qiao",
                "Ray Huang",
                "Si Qin",
                "Qisheng Su",
                "Jiayi Ye",
                "Yudi Zhang",
                "Jian-Guang Lou",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Eindhoven University of Technology",
                "Microsoft",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#training",
                    "#open_source",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "От слов к делу: новый этап в развитии искусственного интеллекта",
                    "desc": "Статья представляет концепцию Крупномасштабных Моделей Действий (LAM), которые призваны перевести ИИ от пассивного понимания языка к активному выполнению задач. Авторы предлагают комплексную структуру для разработки LAM, включая сбор данных, обучение модели и интеграцию с окружением. На примере агента для Windows OS демонстрируется процесс создания LAM. Статья также обсуждает текущие ограничения LAM и направления будущих исследований."
                },
                "en": {
                    "title": "From Language to Action: Advancing AI with Large Action Models",
                    "desc": "This paper discusses the shift from Large Language Models (LLMs) to Large Action Models (LAMs), which are designed to perform actions in real-world environments rather than just generating text. LAMs are enabled by agent systems and represent a step towards achieving artificial general intelligence by allowing AI to complete tasks actively. The authors present a framework for developing LAMs, detailing the stages from data collection to model training and evaluation, using a Windows OS-based agent as an example. They also address the limitations of current LAMs and suggest future research directions to enhance their capabilities in practical applications."
                },
                "zh": {
                    "title": "从语言理解到行动执行的智能转型",
                    "desc": "随着人工智能的不断进步，市场对能够执行实际操作的智能代理系统的需求日益增加。本文提出了一种大型行动模型（LAMs）的综合框架，旨在从传统的大型语言模型（LLMs）转变为能够在动态环境中生成和执行行动的模型。我们通过Windows操作系统的代理作为案例，详细介绍了LAM开发的关键阶段，包括数据收集、模型训练、环境集成和评估。最后，我们讨论了LAMs的当前局限性以及未来研究和工业应用的方向，强调了实现LAMs在实际应用中潜力的挑战与机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09283",
            "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
            "url": "https://huggingface.co/papers/2412.09283",
            "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering remarkable results. Training typically relies on video-caption paired data, which plays a crucial role in enhancing generation performance. However, current video captions often suffer from insufficient details, hallucinations and imprecise motion depiction, affecting the fidelity and consistency of generated videos. In this work, we propose a novel instance-aware structured caption framework, termed InstanceCap, to achieve instance-level and fine-grained video caption for the first time. Based on this scheme, we design an auxiliary models cluster to convert original video into instances to enhance instance fidelity. Video instances are further used to refine dense prompts into structured phrases, achieving concise yet precise descriptions. Furthermore, a 22K InstanceVid dataset is curated for training, and an enhancement pipeline that tailored to InstanceCap structure is proposed for inference. Experimental results demonstrate that our proposed InstanceCap significantly outperform previous models, ensuring high fidelity between captions and videos while reducing hallucinations.",
            "score": 16,
            "issue_id": 1137,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "8a8c6d346689077b",
            "authors": [
                "Tiehan Fan",
                "Kepan Nan",
                "Rui Xie",
                "Penghao Zhou",
                "Zhenheng Yang",
                "Chaoyou Fu",
                "Xiang Li",
                "Jian Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09283.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#data",
                    "#video",
                    "#dataset",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "InstanceCap: точная генерация видео на основе детальных описаний объектов",
                    "desc": "Исследователи представили новый подход к генерации видео на основе текста под названием InstanceCap. Эта технология использует структурированные описания на уровне отдельных объектов для повышения точности и детализации генерируемого видео. InstanceCap включает в себя вспомогательные модели для выделения объектов из видео и уточнения подробных описаний. Авторы также создали датасет InstanceVid из 22 тысяч примеров для обучения модели и разработали специальный конвейер для улучшения результатов во время вывода."
                },
                "en": {
                    "title": "Enhancing Video Generation with Instance-Level Captions",
                    "desc": "This paper introduces InstanceCap, a new framework for generating detailed video captions that improve the quality of text-to-video generation. It addresses common issues in existing video captions, such as lack of detail and inaccuracies in motion representation. By focusing on instance-level descriptions, InstanceCap enhances the fidelity of video generation through a structured captioning approach. The authors also present a new dataset, InstanceVid, and an enhancement pipeline that together improve the alignment between video content and generated captions, leading to better overall performance."
                },
                "zh": {
                    "title": "实例感知，提升视频生成质量",
                    "desc": "近年来，文本到视频生成技术迅速发展，取得了显著成果。训练通常依赖于视频和字幕配对的数据，这对提高生成性能至关重要。然而，现有的视频字幕往往缺乏细节，存在幻觉和不精确的运动描绘，影响生成视频的真实感和一致性。为了解决这些问题，我们提出了一种新颖的实例感知结构化字幕框架，称为InstanceCap，首次实现了实例级和细粒度的视频字幕。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09626",
            "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
            "url": "https://huggingface.co/papers/2412.09626",
            "abstract": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with the previous best-performing method, FreeScale unlocks the generation of 8k-resolution images for the first time.",
            "score": 11,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "1551e9966255aa0a",
            "authors": [
                "Haonan Qiu",
                "Shiwei Zhang",
                "Yujie Wei",
                "Ruihang Chu",
                "Hangjie Yuan",
                "Xiang Wang",
                "Yingya Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09626.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#video",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "FreeScale: Прорыв в генерации визуального контента сверхвысокого разрешения",
                    "desc": "Статья представляет FreeScale - новый метод генерации высококачественных изображений и видео высокого разрешения без дополнительного обучения моделей. Авторы решают проблему появления повторяющихся паттернов при увеличении разрешения, используя слияние информации с разных масштабов и извлечение нужных частотных компонентов. Эксперименты показывают превосходство FreeScale над существующими методами для моделей изображений и видео. Метод позволяет впервые генерировать изображения с разрешением 8K."
                },
                "en": {
                    "title": "Unlocking High-Resolution Visuals with FreeScale",
                    "desc": "This paper introduces FreeScale, a new method for generating high-resolution images and videos using visual diffusion models. Traditional models struggle with high-resolution outputs due to limited training data and computational resources, often resulting in low-quality visuals with repetitive patterns. FreeScale addresses this by employing a tuning-free inference approach that fuses information from various scales, allowing the model to better handle high-frequency details. Experimental results demonstrate that FreeScale significantly enhances the quality of generated visuals, achieving 8k-resolution outputs for the first time."
                },
                "zh": {
                    "title": "FreeScale：无调优的高分辨率视觉生成新范式",
                    "desc": "视觉扩散模型在生成高保真图像或视频时面临分辨率限制的问题，主要是由于缺乏高分辨率数据和计算资源。最近的研究尝试了无调优策略，以展示预训练模型在高分辨率视觉生成方面的潜力，但仍然容易产生低质量的视觉内容和重复模式。我们提出了FreeScale，这是一种无调优推理范式，通过尺度融合实现更高分辨率的视觉生成。实验结果表明，FreeScale在图像和视频模型的高分辨率生成能力上优于以往的方法，首次实现了8k分辨率图像的生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08645",
            "title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
            "url": "https://huggingface.co/papers/2412.08645",
            "abstract": "This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composited image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning.",
            "score": 8,
            "issue_id": 1142,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "3a06f3f96c756398",
            "authors": [
                "Daniel Winter",
                "Asaf Shul",
                "Matan Cohen",
                "Dana Berman",
                "Yael Pritch",
                "Alex Rav-Acha",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "Google",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08645.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Фотореалистичная вставка объектов без дополнительной настройки",
                    "desc": "Статья представляет метод без дополнительной настройки для вставки объектов и генерации изображений по заданному объекту. Метод ObjectMate использует большой набор данных с различными видами одних и тех же объектов для обучения модели диффузии. Это позволяет достичь лучшего сохранения идентичности объекта и более фотореалистичной композиции по сравнению с существующими методами. ObjectMate не требует долгой настройки во время вывода, что отличает его от многих других методов с использованием нескольких опорных изображений."
                },
                "en": {
                    "title": "ObjectMate: Seamless Object Insertion Without Tuning",
                    "desc": "This paper presents ObjectMate, a novel method for object insertion and subject-driven image generation without the need for tuning. It addresses the challenges of seamlessly integrating objects into scenes while maintaining their identity and achieving photorealistic results. The authors leverage large unlabeled datasets to create a diverse set of views for mass-produced objects, which serves as a powerful source of supervision. By employing a text-to-image diffusion architecture, ObjectMate outperforms existing methods in both identity preservation and composition quality, all without requiring time-consuming adjustments during testing."
                },
                "zh": {
                    "title": "无调优的物体插入与生成方法",
                    "desc": "本文介绍了一种无需调优的方法，用于物体插入和基于主题的生成。该任务涉及将多个视角的物体合成到由图像或文本指定的场景中。现有方法在实现无缝合成物体、保持真实的姿态和光照方面面临挑战。我们提出的ObjectMate方法通过利用大规模未标记数据集中重复出现的物体视角，创建了强大的配对数据集，从而实现了更好的身份保留和更逼真的合成效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07517",
            "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
            "url": "https://huggingface.co/papers/2412.07517",
            "abstract": "Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved. This paper introduces FireFlow, a simple yet effective zero-shot approach that inherits the startling capacity of ReFlow-based models (such as FLUX) in generation while extending its capabilities to accurate inversion and editing in 8 steps. We first demonstrate that a carefully designed numerical solver is pivotal for ReFlow inversion, enabling accurate inversion and reconstruction with the precision of a second-order solver while maintaining the practical efficiency of a first-order Euler method. This solver achieves a 3times runtime speedup compared to state-of-the-art ReFlow inversion and editing techniques, while delivering smaller reconstruction errors and superior editing results in a training-free mode. The code is available at https://github.com/HolmesShuan/FireFlow{this URL}.",
            "score": 7,
            "issue_id": 1135,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "70f97a4533ea4ebb",
            "authors": [
                "Yingying Deng",
                "Xiangyu He",
                "Changwang Mei",
                "Peisong Wang",
                "Fan Tang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔥",
                "ru": {
                    "title": "FireFlow: Молниеносная инверсия и редактирование изображений в ReFlow моделях",
                    "desc": "Статья представляет FireFlow - новый подход к быстрой инверсии и редактированию изображений в моделях на основе Rectified Flows (ReFlows). Авторы разработали специальный численный решатель, который позволяет точно инвертировать и реконструировать изображения за 8 шагов. Метод FireFlow работает в 3 раза быстрее существующих техник инверсии ReFlow, обеспечивая при этом меньшие ошибки реконструкции и лучшие результаты редактирования. Важно отметить, что FireFlow не требует дополнительного обучения и может применяться к уже обученным моделям."
                },
                "en": {
                    "title": "FireFlow: Fast and Accurate Image Inversion and Editing",
                    "desc": "This paper presents FireFlow, a new method that improves the inversion and editing capabilities of Rectified Flows (ReFlows) in machine learning. FireFlow uses a specially designed numerical solver that combines the efficiency of first-order methods with the accuracy of second-order methods, allowing for faster and more precise image reconstruction. The approach achieves a threefold increase in speed compared to existing ReFlow techniques while reducing errors and enhancing editing quality without requiring additional training. Overall, FireFlow enhances the usability of ReFlow models for generating and manipulating images effectively."
                },
                "zh": {
                    "title": "FireFlow：高效的图像反演与编辑新方法",
                    "desc": "本文介绍了一种名为FireFlow的新方法，它在快速采样的基础上，解决了图像反演和编辑的问题。FireFlow继承了基于ReFlow模型的强大生成能力，并在8个步骤内实现了准确的反演和编辑。我们设计了一种数值求解器，使得ReFlow的反演过程更加精确，同时保持了高效性。与现有的ReFlow反演和编辑技术相比，该求解器在运行速度上提高了3倍，并且在训练无关的模式下，重建误差更小，编辑效果更佳。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09428",
            "title": "Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation",
            "url": "https://huggingface.co/papers/2412.09428",
            "abstract": "Multimodal music generation aims to produce music from diverse input modalities, including text, videos, and images. Existing methods use a common embedding space for multimodal fusion. Despite their effectiveness in other modalities, their application in multimodal music generation faces challenges of data scarcity, weak cross-modal alignment, and limited controllability. This paper addresses these issues by using explicit bridges of text and music for multimodal alignment. We introduce a novel method named Visuals Music Bridge (VMB). Specifically, a Multimodal Music Description Model converts visual inputs into detailed textual descriptions to provide the text bridge; a Dual-track Music Retrieval module that combines broad and targeted retrieval strategies to provide the music bridge and enable user control. Finally, we design an Explicitly Conditioned Music Generation framework to generate music based on the two bridges. We conduct experiments on video-to-music, image-to-music, text-to-music, and controllable music generation tasks, along with experiments on controllability. The results demonstrate that VMB significantly enhances music quality, modality, and customization alignment compared to previous methods. VMB sets a new standard for interpretable and expressive multimodal music generation with applications in various multimedia fields. Demos and code are available at https://github.com/wbs2788/VMB.",
            "score": 5,
            "issue_id": 1139,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "3887e6d6a5eecb26",
            "authors": [
                "Baisen Wang",
                "Le Zhuo",
                "Zhaokai Wang",
                "Chenxi Bao",
                "Wu Chengjing",
                "Xuecheng Nie",
                "Jiao Dai",
                "Jizhong Han",
                "Yue Liao",
                "Si Liu"
            ],
            "affiliations": [
                "Beihang University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "MT Lab, Meitu Inc.",
                "School of Cyberspace Security, University of Chinese Academy of Sciences",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09428.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "VMB: Новый стандарт интерпретируемой и выразительной мультимодальной генерации музыки",
                    "desc": "Статья представляет новый метод мультимодальной генерации музыки под названием Visuals Music Bridge (VMB). VMB использует явные мосты между текстом и музыкой для улучшения межмодального выравнивания. Метод включает в себя модель описания музыки, двухтрековый модуль поиска музыки и фреймворк генерации музыки с явными условиями. Эксперименты показывают, что VMB значительно улучшает качество музыки, модальность и настраиваемость по сравнению с предыдущими методами."
                },
                "en": {
                    "title": "Bridging Modalities for Enhanced Music Generation",
                    "desc": "This paper presents a new approach to multimodal music generation, which creates music from different types of inputs like text, images, and videos. The authors introduce the Visuals Music Bridge (VMB) method, which improves the alignment between these modalities by using explicit connections between text and music. They develop a Multimodal Music Description Model to transform visual inputs into text descriptions and a Dual-track Music Retrieval module to enhance user control over the music generation process. Experimental results show that VMB significantly improves the quality and customization of generated music compared to existing methods, making it a valuable tool for multimedia applications."
                },
                "zh": {
                    "title": "视觉音乐桥：多模态音乐生成的新标准",
                    "desc": "多模态音乐生成旨在从多种输入模态（如文本、视频和图像）中生成音乐。现有方法使用共同的嵌入空间进行多模态融合，但在多模态音乐生成中面临数据稀缺、跨模态对齐弱和可控性有限等挑战。本文提出了一种新方法，称为视觉音乐桥（VMB），通过文本和音乐的显式桥梁来解决这些问题。实验结果表明，VMB在音乐质量、模态和定制对齐方面显著优于之前的方法，设定了可解释和富有表现力的多模态音乐生成的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09611",
            "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
            "url": "https://huggingface.co/papers/2412.09611",
            "abstract": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities.",
            "score": 4,
            "issue_id": 1143,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "a5d955bf540c4f06",
            "authors": [
                "Yusuf Dalva",
                "Kavana Venkatesh",
                "Pinar Yanardag"
            ],
            "affiliations": [
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09611.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "FluxSpace: семантическое редактирование для моделей ректифицированного потока",
                    "desc": "Статья представляет FluxSpace - новый метод редактирования изображений для моделей ректифицированного потока. Эта техника позволяет контролировать семантику генерируемых изображений, используя представления, полученные из трансформерных блоков. FluxSpace обеспечивает широкий спектр задач редактирования - от точной настройки до художественного творчества. Метод отличается масштабируемостью и эффективностью, а также способностью к разделению признаков."
                },
                "en": {
                    "title": "FluxSpace: Precision Editing in Image Generation",
                    "desc": "This paper presents FluxSpace, a new method for image editing that improves upon rectified flow models, which are known for generating high-quality images. While these models excel at creating images, they often fail to allow for specific edits without altering other unrelated features. FluxSpace addresses this issue by utilizing a representation space that enables precise control over the semantics of the images. The method leverages the learned representations from transformer blocks in rectified flow models, facilitating a variety of editing tasks, from detailed adjustments to creative artistic modifications."
                },
                "zh": {
                    "title": "FluxSpace：解耦图像编辑的新方法",
                    "desc": "本文介绍了一种名为FluxSpace的图像编辑方法，旨在解决现有的修正流模型在图像编辑中的局限性。尽管修正流模型在高质量图像合成方面表现出色，但它们在进行图像的解耦编辑时常常遇到困难。FluxSpace利用修正流变换器中学习到的表示，提供了一种语义可解释的表示空间，使得用户能够进行精确的属性特定修改。该方法不仅支持细粒度的图像编辑，还能实现艺术创作，展现了良好的可扩展性和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09856",
            "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
            "url": "https://huggingface.co/papers/2412.09856",
            "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15times (11.5times) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.",
            "score": 3,
            "issue_id": 1149,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "d981bcdb0d035c11",
            "authors": [
                "Hongjie Wang",
                "Chih-Yao Ma",
                "Yen-Cheng Liu",
                "Ji Hou",
                "Tao Xu",
                "Jialiang Wang",
                "Felix Juefei-Xu",
                "Yaqiao Luo",
                "Peizhao Zhang",
                "Tingbo Hou",
                "Peter Vajda",
                "Niraj K. Jha",
                "Xiaoliang Dai"
            ],
            "affiliations": [
                "Meta",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09856.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LinGen: революция в генерации длинных видео с линейной сложностью",
                    "desc": "Исследователи предложили новый метод генерации видео по тексту под названием LinGen. Этот подход позволяет создавать высококачественные видео длиной до минуты на одном GPU, значительно снижая вычислительные затраты по сравнению с существующими моделями. LinGen использует блок MATE вместо самовнимания, что обеспечивает линейную сложность вычислений относительно количества пикселей. Экспериментальные результаты показывают, что LinGen превосходит другие модели по качеству видео при значительном сокращении вычислительных ресурсов."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Linear Complexity",
                    "desc": "This paper introduces LinGen, a new framework for text-to-video generation that significantly reduces computational costs. Unlike traditional methods that scale quadratically with pixel count, LinGen operates with linear complexity, allowing for the generation of high-resolution videos up to a minute long on a single GPU. The framework utilizes a novel MATE block, which combines two branches to effectively capture both short and long-range correlations in video data. Experimental results show that LinGen not only reduces latency but also maintains high video quality, outperforming existing models in various evaluations."
                },
                "zh": {
                    "title": "线性复杂度，分钟级视频生成新突破！",
                    "desc": "本文提出了一种线性复杂度的文本到视频生成框架LinGen，旨在降低生成视频的计算成本。传统的扩散变换器在生成视频时，计算成本随着像素数量的增加而呈平方增长，限制了视频长度。LinGen通过引入MATE模块，替代了计算密集型的自注意力机制，使得生成高分辨率的分钟级视频成为可能。实验结果表明，LinGen在视频质量上优于现有模型，并显著减少了计算延迟，推动了长时间视频生成的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10319",
            "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
            "url": "https://huggingface.co/papers/2412.10319",
            "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.",
            "score": 3,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "a6269882457435d4",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Chengruidong Zhang",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10319.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SCBench: новый взгляд на оценку длинноконтекстных языковых моделей через призму KV-кэша",
                    "desc": "Статья представляет новый бенчмарк SCBench для оценки методов работы с длинным контекстом в языковых моделях с акцентом на KV-кэш. SCBench оценивает генерацию, сжатие, извлечение и загрузку KV-кэша на 12 задачах с общим контекстом. Авторы проанализировали 8 категорий решений для длинного контекста на 8 языковых моделях. Результаты показывают, что методы с субквадратичной памятью уступают в многоходовых сценариях, а динамическое прореживание дает более выразительные KV-кэши."
                },
                "en": {
                    "title": "Optimizing Long-Context LLMs with SCBench: A KV Cache Revolution",
                    "desc": "This paper introduces SCBench, a new benchmark designed to evaluate long-context methods in large language models (LLMs) with a focus on the key-value (KV) cache. It addresses the limitations of existing benchmarks by considering the entire lifecycle of the KV cache, including its generation, compression, retrieval, and loading. The study analyzes various long-context solutions, revealing that memory-efficient methods can struggle in multi-turn scenarios, while certain sparse encoding techniques perform well. The findings also highlight the importance of dynamic sparsity and layer-level sparsity in optimizing memory usage and performance in LLMs."
                },
                "zh": {
                    "title": "优化长上下文的KV缓存评估",
                    "desc": "本文介绍了SCBench（SharedContextBench），一个针对长上下文方法的基准测试，重点关注KV缓存的生命周期。研究表明，现有的基准测试往往只关注单次请求，而忽视了KV缓存的重用，这在实际应用中至关重要。SCBench涵盖了KV缓存的生成、压缩、检索和加载等四个方面，并通过12个任务的共享上下文进行评估。我们的研究发现，动态稀疏性在KV缓存中表现更好，而混合架构中的层级稀疏性则有效降低了内存使用，同时保持了强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08347",
            "title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs",
            "url": "https://huggingface.co/papers/2412.08347",
            "abstract": "We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's Tulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (Delta11%), and mathematical reasoning with 51.6% on GSM8K (Delta3.4%), with an alternate version achieving scoring 57.1% on ARC (Delta5.4%). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models.",
            "score": 2,
            "issue_id": 1141,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "a7238338dc7e3853",
            "authors": [
                "Sultan Alrashed"
            ],
            "affiliations": [
                "Saudi Data & Artificial Intelligence Authority"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08347.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#open_source",
                    "#science",
                    "#architecture",
                    "#small_models",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация обучения малых языковых моделей для повышения их эффективности",
                    "desc": "В статье представлена модель SmolTulu-1.7b-Instruct, созданная путем адаптации pipeline Tulu 3 от AllenAI для улучшения базовой модели SmolLM2-1.7B от Huggingface. Исследование показало, что соотношение скорости обучения и размера батча значительно влияет на производительность модели в зависимости от задачи. Для задач рассуждения оптимальны более высокие соотношения, а для распознавания паттернов - более низкие. Результатом стала модель SmolTulu, достигшая лучших показателей среди моделей до 2 млрд параметров в задачах следования инструкциям и математических рассуждений."
                },
                "en": {
                    "title": "Optimizing Learning Dynamics for Enhanced Language Model Performance",
                    "desc": "The paper introduces SmolTulu-1.7b-Instruct, an instruction-tuned language model that enhances the SmolLM2-1.7B base model using AllenAI's Tulu 3 post-training pipeline. It highlights the importance of the learning rate and batch size relationship, showing that different tasks require different optimization strategies for optimal performance. For reasoning tasks, a higher learning rate to batch size ratio is beneficial, while pattern recognition tasks perform better with lower ratios. The model achieves state-of-the-art results among sub-2B parameter models, providing valuable insights and resources for future research in model alignment and optimization."
                },
                "zh": {
                    "title": "优化小模型，提升大能力",
                    "desc": "我们介绍了SmolTulu-1.7b-Instruct，这是一个经过指令调优的语言模型，旨在提升Huggingface的SmolLM2-1.7B基础模型的性能。通过对135M参数模型的全面实证分析，我们发现学习率与批量大小之间的关系对模型性能有显著影响，且这种影响因任务而异。推理任务如ARC和GSM8K在较高的学习率与批量大小比率下表现更好，而模式识别任务如HellaSwag和IFEval则在较低的比率下达到最佳性能。我们的研究成果为SmolTulu的发展提供了指导，使其在指令跟随和数学推理任务中在小于2B参数模型中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10345",
            "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
            "url": "https://huggingface.co/papers/2412.10345",
            "abstract": "Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models' spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.",
            "score": 1,
            "issue_id": 1148,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "cf176777ca0c3426",
            "authors": [
                "Ruijie Zheng",
                "Yongyuan Liang",
                "Shuaiyi Huang",
                "Jianfeng Gao",
                "Hal Daumé III",
                "Andrey Kolobov",
                "Furong Huang",
                "Jianwei Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10345.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#robotics",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Визуальные траектории для улучшения пространственно-временного восприятия роботов",
                    "desc": "В этой работе представлен метод визуального трассировочного промптинга для улучшения пространственно-временного восприятия моделей vision-language-action (VLA) в робототехнике. Авторы разработали модель TraceVLA, дообучив OpenVLA на собственном наборе данных из 150 тысяч траекторий манипуляций роботов. TraceVLA превзошла OpenVLA на 10% в симуляторе SimplerEnv и в 3,5 раза на реальном роботе WidowX. Кроме того, была создана компактная VLA-модель на основе Phi-3-Vision, которая сравнима по производительности с OpenVLA, но значительно эффективнее при инференсе."
                },
                "en": {
                    "title": "Enhancing Robotic Action Prediction with Visual Trace Prompting",
                    "desc": "This paper addresses the limitations of large vision-language-action (VLA) models in understanding spatial-temporal dynamics for robotic manipulation tasks. The authors propose a novel technique called visual trace prompting, which enhances the models' ability to predict actions by visually encoding state-action trajectories. They introduce the TraceVLA model, which is fine-tuned on a dataset of 150,000 robot manipulation trajectories, leading to significant performance improvements over the baseline OpenVLA model. The results show that TraceVLA achieves state-of-the-art performance in various environments and tasks, demonstrating its effectiveness and generalization capabilities in real-world robotic applications."
                },
                "zh": {
                    "title": "提升机器人操作的时空感知能力",
                    "desc": "本文提出了一种新的方法，称为视觉轨迹提示，旨在提高大型视觉-语言-动作（VLA）模型在机器人交互中的时空感知能力。通过对状态-动作轨迹进行视觉编码，我们开发了新的TraceVLA模型，并在150K机器人操作轨迹的数据集上进行了微调。实验结果表明，TraceVLA在多个配置和任务中表现出色，超越了OpenVLA模型，尤其在真实机器人任务中表现出3.5倍的提升。我们的紧凑型VLA模型在推理效率上也有显著改善，证明了该方法的有效性和通用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09910",
            "title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images",
            "url": "https://huggingface.co/papers/2412.09910",
            "abstract": "Deep neural networks (DNNs) offer significant promise for improving breast cancer diagnosis in medical imaging. However, these models are highly susceptible to adversarial attacks--small, imperceptible changes that can mislead classifiers--raising critical concerns about their reliability and security. Traditional attacks rely on fixed-norm perturbations, misaligning with human perception. In contrast, diffusion-based attacks require pre-trained models, demanding substantial data when these models are unavailable, limiting practical use in data-scarce scenarios. In medical imaging, however, this is often unfeasible due to the limited availability of datasets. Building on recent advancements in learnable prompts, we propose Prompt2Perturb (P2P), a novel language-guided attack method capable of generating meaningful attack examples driven by text instructions. During the prompt learning phase, our approach leverages learnable prompts within the text encoder to create subtle, yet impactful, perturbations that remain imperceptible while guiding the model towards targeted outcomes. In contrast to current prompt learning-based approaches, our P2P stands out by directly updating text embeddings, avoiding the need for retraining diffusion models. Further, we leverage the finding that optimizing only the early reverse diffusion steps boosts efficiency while ensuring that the generated adversarial examples incorporate subtle noise, thus preserving ultrasound image quality without introducing noticeable artifacts. We show that our method outperforms state-of-the-art attack techniques across three breast ultrasound datasets in FID and LPIPS. Moreover, the generated images are both more natural in appearance and more effective compared to existing adversarial attacks. Our code will be publicly available https://github.com/yasamin-med/P2P.",
            "score": 1,
            "issue_id": 1140,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "bda49360304ea17e",
            "authors": [
                "Yasamin Medghalchi",
                "Moein Heidari",
                "Clayton Allard",
                "Leonid Sigal",
                "Ilker Hacihaliloglu"
            ],
            "affiliations": [
                "University of British Columbia, Vancouver, BC, Canada",
                "Vector Institute for AI, Toronto, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09910.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#security",
                    "#healthcare",
                    "#diffusion"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "Текстовые подсказки как оружие против ИИ в медицинской диагностике",
                    "desc": "Данная статья представляет новый метод атаки на глубокие нейронные сети в области диагностики рака груди по медицинским изображениям. Авторы предлагают подход Prompt2Perturb (P2P), использующий текстовые инструкции для создания незаметных изменений в изображениях, которые вводят модель в заблуждение. В отличие от существующих методов, P2P не требует переобучения диффузионных моделей и эффективно работает на ограниченных наборах данных. Эксперименты показывают превосходство P2P над современными методами атак на трех наборах данных ультразвуковых изображений груди."
                },
                "en": {
                    "title": "Enhancing Breast Cancer Diagnosis with Smart Adversarial Attacks",
                    "desc": "This paper introduces Prompt2Perturb (P2P), a novel method for generating adversarial attacks on deep neural networks used in breast cancer diagnosis. Unlike traditional methods that rely on fixed perturbations, P2P utilizes learnable prompts to create subtle changes in medical images based on text instructions. This approach allows for the generation of effective adversarial examples without the need for extensive datasets or retraining of models. The results demonstrate that P2P outperforms existing techniques, producing more natural-looking images while maintaining the quality of ultrasound diagnostics."
                },
                "zh": {
                    "title": "利用语言指导的对抗攻击提升医学影像安全性",
                    "desc": "深度神经网络（DNN）在医学影像中的乳腺癌诊断中具有很大潜力，但它们容易受到对抗攻击的影响，这些攻击通过微小的、不可察觉的变化来误导分类器。传统的对抗攻击依赖于固定范数的扰动，这与人类的感知不一致。我们提出了一种新方法Prompt2Perturb（P2P），它利用可学习的提示生成有意义的攻击示例，避免了对抗攻击中常见的重训练需求。我们的实验表明，P2P在乳腺超声数据集上优于现有的对抗攻击技术，生成的图像更自然且更有效。"
                }
            }
        }
    ],
    "link_prev": "2024-12-13.html",
    "link_next": "2024-12-17.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12月13日"
    },
    "short_date_next": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12月17日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 9,
        "#robotics": 1,
        "#agi": 3,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了GenEx系统，它能通过单张RGB图像生成3D一致的想象环境。该系统利用虚幻引擎的3D数据，帮助AI代理在复杂任务中进行探索和导航。GenEx展示了高质量的世界生成和强大的3D能力，如3D映射和循环一致性。文章总结说，GenEx为提升想象空间中的 embodied AI 提供了变革性平台，并有潜力扩展到现实世界探索。",
        "title": "GenEx: Generating an Explorable World",
        "pinyin": "这篇文章介绍了GenEx系统，它能通过单张RGB图像生成3D一致的想象环境。该系统利用虚幻引擎的3D数据，帮助AI代理在复杂任务中进行探索和导航。GenEx展示了高质量的世界生成和强大的3D能力，如3D映射和循环一致性。文章总结说，GenEx为提升想象空间中的 embodied AI 提供了变革性平台，并有潜力扩展到现实世界探索。\n\nzhè piān wén zhāng jiè shào le GenEx xì tǒng, tā néng tōng guò dān zhāng RGB tú xíng shēng chéng 3D yī zhì de xiǎng xiàng huán jìng. gǎi xì tǒng lì yòng xū huàn yǐn qíng de 3D shù jù, bāng zhù AI dài lǐ zài fú zà rèn wù zhōng jìn xíng tàn suǒ hé dǎo háng. GenEx zhǎn shì le gāo zhì liàng de shì jiè shēng chéng hé qiáng dà de 3D néng lì, rú 3D yǐng shè hé xún huán yī zhì xìng. wén zhāng zǒng jiè shuō, GenEx wèi tí shēng xiǎng xiàng kōng zhōng de embodied AI tí gōng le biàn gé xìng píng tái, bìng yǒu qián lì kuò zhǎn dào xiàn shí shì jiè tàn suǒ.",
        "vocab": "[{'word': 'GenEx', 'pinyin': '', 'trans': 'GenEx'}, {'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'}, {'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through'}, {'word': '单张', 'pinyin': 'dān zhāng', 'trans': 'single'}, {'word': 'RGB', 'pinyin': '', 'trans': 'RGB'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'}, {'word': '想象', 'pinyin': 'xiǎngxiàng', 'trans': 'imaginary'}, {'word': '环境', 'pinyin': 'huánjìng', 'trans': 'environment'}, {'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'}, {'word': '虚幻', 'pinyin': 'xūhuàn', 'trans': 'virtual'}, {'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'}, {'word': '数据', 'pinyin': 'shùjù', 'trans': 'data'}, {'word': '帮助', 'pinyin': 'bāngzhù', 'trans': 'help'}, {'word': 'AI', 'pinyin': '', 'trans': 'AI'}, {'word': '代理', 'pinyin': 'dàilǐ', 'trans': 'agent'}, {'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '进行', 'pinyin': 'jìnxíng', 'trans': 'conduct'}, {'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'explore'}, {'word': '导航', 'pinyin': 'dǎoháng', 'trans': 'navigate'}, {'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'}, {'word': '世界', 'pinyin': 'shìjiè', 'trans': 'world'}, {'word': '强大', 'pinyin': 'qiángdà', 'trans': 'powerful'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '映射', 'pinyin': 'yìngshè', 'trans': 'mapping'}, {'word': '循环', 'pinyin': 'xúnhuán', 'trans': 'cyclic'}, {'word': '一致性', 'pinyin': 'yīzhìxìng', 'trans': 'consistency'}, {'word': '总结', 'pinyin': 'zǒngjié', 'trans': 'summarize'}, {'word': '说', 'pinyin': 'shuō', 'trans': 'say'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'}, {'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'}, {'word': 'embodied', 'pinyin': '', 'trans': 'embodied'}, {'word': '变革性', 'pinyin': 'biàngéxìng', 'trans': 'transformative'}, {'word': '平台', 'pinyin': 'píngtái', 'trans': 'platform'}, {'word': '潜力', 'pinyin': 'qiánlì', 'trans': 'potential'}, {'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'}, {'word': '现实', 'pinyin': 'xiànshí', 'trans': 'real'}, {'word': '世界', 'pinyin': 'shìjiè', 'trans': 'world'}, {'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'explore'}]",
        "trans": "This article introduces the GenEx system, which can generate a 3D-consistent imagined environment from a single RGB image. The system leverages 3D data from the Unreal Engine to assist AI agents in exploration and navigation in complex tasks. GenEx demonstrates high-quality world generation and powerful 3D capabilities, such as 3D mapping and cyclic consistency. The article concludes that GenEx provides a transformative platform for enhancing embodied AI in imagined spaces and has the potential to extend to real-world exploration.",
        "update_ts": "2024-12-16 09:12"
    }
}