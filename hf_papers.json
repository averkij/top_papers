{
    "date": {
        "ru": "20 Ğ¸ÑĞ½Ñ",
        "en": "June 20",
        "zh": "6æœˆ20æ—¥"
    },
    "time_utc": "2025-06-20 07:11",
    "weekday": 4,
    "issue_id": 4398,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.15154",
            "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
            "url": "https://huggingface.co/papers/2506.15154",
            "abstract": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  \t\t\t\t\tAI-generated summary \t\t\t\t Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.",
            "score": 2,
            "issue_id": 4397,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "f2b380f0491c0add",
            "authors": [
                "Anuradha Chopra",
                "Abhinaba Roy",
                "Dorien Herremans"
            ],
            "affiliations": [
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15154.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#games",
                    "#multimodal",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "SonicVerse: ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "SonicVerse - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MusicBench Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MIRFLEX. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Music Descriptions with SonicVerse",
                    "desc": "SonicVerse is a multi-task music captioning model that improves the quality of music descriptions by integrating audio feature detection. It captures both low-level acoustic details and high-level musical attributes through a projection-based architecture. This model generates detailed captions for short music pieces and can create time-informed descriptions for longer compositions by using a large-language model. By enhancing the training dataset with music features, SonicVerse demonstrates improved caption quality and detail in its outputs."
                },
                "zh": {
                    "title": "SonicVerseï¼šæå‡éŸ³ä¹å­—å¹•è´¨é‡çš„å¤šä»»åŠ¡æ¨¡å‹",
                    "desc": "SonicVerseæ˜¯ä¸€ç§å¤šä»»åŠ¡éŸ³ä¹å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†éŸ³é¢‘ç‰¹å¾æ£€æµ‹ä»¥æé«˜å­—å¹•è´¨é‡ã€‚è¯¥æ¨¡å‹é€šè¿‡å…³é”®éŸ³è°ƒæ£€æµ‹ã€å£°ä¹æ£€æµ‹ç­‰è¾…åŠ©ä»»åŠ¡ï¼Œæ•æ‰éŸ³ä¹çš„ä½çº§å£°å­¦ç»†èŠ‚å’Œé«˜çº§éŸ³ä¹å±æ€§ã€‚å…¶å…³é”®è´¡çŒ®åœ¨äºé‡‡ç”¨åŸºäºæŠ•å½±çš„æ¶æ„ï¼Œå°†éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºè¯­è¨€æ ‡è®°ï¼ŒåŒæ—¶é€šè¿‡ä¸“ç”¨è¾…åŠ©å¤´æ£€æµ‹éŸ³ä¹ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç‰¹å¾çš„ç»“åˆæ˜¾è‘—æå‡äº†ç”Ÿæˆå­—å¹•çš„è´¨é‡å’Œç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14837",
            "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
            "url": "https://huggingface.co/papers/2506.14837",
            "abstract": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.",
            "score": 0,
            "issue_id": 4396,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ½Ñ",
                "en": "June 15",
                "zh": "6æœˆ15æ—¥"
            },
            "hash": "3172095671c65e03",
            "authors": [
                "Chengzhi Xu",
                "Yuyang Wang",
                "Lai Wei",
                "Lichao Sun",
                "Weiran Huang"
            ],
            "affiliations": [
                "Lehigh University",
                "MIFA Lab, Shanghai Jiao Tong University",
                "Shanghai Innovation Institute",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14837.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ",
                    "desc": "ChartIR - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ² ĞºĞ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ². ChartIR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ChartIR Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2-VL Ğ¸ GPT-4."
                },
                "en": {
                    "title": "ChartIR: Refining Code Generation from Charts with Structured Instructions",
                    "desc": "ChartIR is a novel approach that enhances the performance of multimodal large language models (MLLMs) in generating code from charts by separating the tasks of visual understanding and code translation. It employs structured instructions to guide the model in accurately interpreting visual elements and translating them into executable code. The method involves two main stages: initial code generation followed by iterative refinement, which allows for progressive improvements in the output. Experimental results demonstrate that ChartIR significantly outperforms existing methods on both open-source and closed-source models."
                },
                "zh": {
                    "title": "ChartIRï¼šæå‡å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆçš„æ™ºèƒ½æ–¹æ³•",
                    "desc": "ChartIRæ˜¯ä¸€ç§é€šè¿‡ç»“æ„åŒ–æŒ‡ä»¤å’Œè¿­ä»£ä¼˜åŒ–æ¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†è§†è§‰ç†è§£å’Œä»£ç ç¿»è¯‘ä»»åŠ¡åˆ†å¼€ï¼Œé¦–å…ˆé€šè¿‡æè¿°å’Œå·®å¼‚ä¸¤ç§ç»“æ„åŒ–æŒ‡ä»¤æ¥æ•æ‰å›¾è¡¨çš„è§†è§‰å…ƒç´ ã€‚æ¥ç€ï¼ŒChartIRå°†æ•´ä½“å›¾è¡¨ç”Ÿæˆæµç¨‹åˆ†ä¸ºåˆå§‹ä»£ç ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ä¸¤ä¸ªé˜¶æ®µï¼Œä»è€Œé€æ­¥æå‡æœ€ç»ˆè¾“å‡ºçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒChartIRåœ¨å¼€æºæ¨¡å‹Qwen2-VLå’Œé—­æºæ¨¡å‹GPT-4oä¸Šå‡è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-19.html",
    "link_next": "2025-06-23.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}