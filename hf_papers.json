{
    "date": {
        "ru": "26 февраля",
        "en": "February 26",
        "zh": "2月26日"
    },
    "time_utc": "2025-02-26 05:10",
    "weekday": 2,
    "issue_id": 2411,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.18411",
            "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
            "url": "https://huggingface.co/papers/2502.18411",
            "abstract": "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.",
            "score": 34,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "a37015745aae1e1d",
            "authors": [
                "Xiangyu Zhao",
                "Shengyuan Ding",
                "Zicheng Zhang",
                "Haian Huang",
                "Maosong Cao",
                "Weiyun Wang",
                "Jiaqi Wang",
                "Xinyu Fang",
                "Wenhai Wang",
                "Guangtao Zhai",
                "Haodong Duan",
                "Hua Yang",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18411.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Приведение мультимодальных ИИ-моделей в соответствие с человеческими ценностями",
                    "desc": "Статья представляет OmniAlign-V - набор данных для улучшения мультимодальных языковых моделей (MLLM) в соответствии с человеческими предпочтениями. Авторы также разработали MM-AlignBench - бенчмарк для оценки соответствия MLLM человеческим ценностям. Эксперименты показывают, что дообучение на OmniAlign-V значительно улучшает согласованность моделей с предпочтениями людей. Датасеты, бенчмарк и код доступны в открытом доступе."
                },
                "en": {
                    "title": "Aligning MLLMs with Human Preferences through OmniAlign-V",
                    "desc": "This paper presents OmniAlign-V, a new dataset containing 200,000 high-quality training samples that include diverse images and complex questions to help multi-modal large language models (MLLMs) better align with human preferences. The authors also introduce MM-AlignBench, a benchmark for evaluating how well MLLMs reflect human values. By fine-tuning MLLMs with the OmniAlign-V dataset using techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), the models show improved alignment with human preferences while maintaining their performance on standard Visual Question Answering (VQA) tasks. The resources, including datasets and benchmarks, are made publicly available to support further research in this area."
                },
                "zh": {
                    "title": "提升多模态模型与人类偏好的对齐",
                    "desc": "本文介绍了OmniAlign-V，这是一个包含20万个高质量训练样本的综合数据集，旨在提高多模态大型语言模型（MLLMs）与人类偏好的对齐。数据集中包含多样的图像、复杂的问题和多种响应格式，以增强模型的适应性。我们还提出了MM-AlignBench，这是一个专门设计的人类标注基准，用于评估MLLMs与人类价值观的对齐程度。实验结果表明，使用OmniAlign-V进行微调显著提高了模型的人类偏好对齐，同时保持或提升了在标准视觉问答基准上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18137",
            "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
            "url": "https://huggingface.co/papers/2502.18137",
            "abstract": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.",
            "score": 23,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "1029ef0dffc41bba",
            "authors": [
                "Jintao Zhang",
                "Chendong Xiang",
                "Haofeng Huang",
                "Jia Wei",
                "Haocheng Xi",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18137.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#video",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Универсальное разреженное внимание для ускорения нейросетей",
                    "desc": "В статье представлен новый метод SpargeAttn для оптимизации внимания в нейронных сетях. Он использует двухэтапный онлайн-фильтр для предсказания карты внимания и пропуска некоторых матричных умножений. SpargeAttn универсален и применим к различным моделям, включая генерацию текста, изображений и видео. Эксперименты показывают значительное ускорение работы моделей без потери качества."
                },
                "en": {
                    "title": "Accelerating Attention with SpargeAttn: Speed Meets Efficiency",
                    "desc": "This paper introduces SpargeAttn, a novel approach to implementing sparse attention in machine learning models. It addresses the challenge of quadratic time complexity in attention mechanisms by leveraging the inherent sparsity of attention maps. The proposed method employs a two-stage online filtering process to efficiently predict and optimize the attention map, allowing for the omission of unnecessary computations. Experimental results demonstrate that SpargeAttn accelerates various models across different domains, such as language and image processing, while maintaining high performance metrics."
                },
                "zh": {
                    "title": "通用稀疏注意力，提升模型计算效率！",
                    "desc": "在大型模型中，高效的注意力实现至关重要，因为其时间复杂度为平方级。幸运的是，注意力通常表现出稀疏性，即注意力图中的许多值接近于零，这使得可以省略相应的计算。本文提出了一种名为SpargeAttn的通用稀疏和量化注意力机制，能够加速各种模型的计算，同时保持端到端的性能。我们的方法通过两阶段的在线过滤器来实现，第一阶段快速准确地预测注意力图，第二阶段设计了一个在线的softmax感知过滤器，进一步提高了计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17363",
            "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
            "url": "https://huggingface.co/papers/2502.17363",
            "abstract": "Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit",
            "score": 14,
            "issue_id": 2409,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "d1e7a717a0d2e56e",
            "authors": [
                "Tianrui Zhu",
                "Shiyi Zhang",
                "Jiawei Shao",
                "Yansong Tang"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence (TeleAI), China Telecom",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "KV-Edit: Революция в редактировании изображений с сохранением согласованности фона",
                    "desc": "KV-Edit - это новый подход к редактированию изображений, основанный на использовании KV-кэша в DiT-моделях для сохранения согласованности фона. Метод позволяет генерировать новый контент, который органично интегрируется с фоном в заданных пользователем областях, без необходимости сложных механизмов или дорогостоящего обучения. Авторы оптимизировали пространственную сложность до O(1), используя метод без инверсии. Эксперименты показали, что KV-Edit значительно превосходит существующие подходы по качеству фона и изображения в целом."
                },
                "en": {
                    "title": "Seamless Image Editing with KV-Edit: No Training Needed!",
                    "desc": "The paper introduces KV-Edit, a novel method for image editing that addresses the challenge of maintaining background consistency. Unlike traditional methods that require extensive training, KV-Edit utilizes a KV cache in Denoising Transformers (DiTs) to preserve background tokens, allowing for seamless integration of new content. This approach simplifies the editing process by avoiding complex mechanisms and optimizing memory usage to O(1) without sacrificing quality. Experimental results show that KV-Edit outperforms existing techniques, including those that rely on training, in both background consistency and overall image quality."
                },
                "zh": {
                    "title": "KV-Edit：无训练的背景一致性图像编辑新方法",
                    "desc": "背景一致性在图像编辑任务中仍然是一个重要挑战。现有方法在保持与原始图像相似性和生成符合目标内容之间存在权衡。我们提出了KV-Edit，这是一种无训练的方法，利用KV缓存来保持背景一致性，保留背景标记而不是重新生成，从而简化了复杂机制和高成本训练的需求。实验表明，KV-Edit在背景和图像质量方面显著优于现有方法，甚至超越了基于训练的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17262",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "url": "https://huggingface.co/papers/2502.17262",
            "abstract": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the \"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.",
            "score": 10,
            "issue_id": 2410,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "246c4bb39b0f6996",
            "authors": [
                "Chengyin Xu",
                "Kaiyuan Chen",
                "Xiao Li",
                "Ke Shen",
                "Chenggang Li"
            ],
            "affiliations": [
                "Seed-LLM, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17262.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Предсказание будущего больших языковых моделей",
                    "desc": "Статья представляет новый метод прогнозирования производительности больших языковых моделей (LLM) на различных задачах. Авторы предлагают подход Clustering-On-Difficulty (COD), который группирует задачи по сложности и выбирает предсказуемое подмножество для оценки. COD использует ансамбль небольших моделей для точного прогнозирования результатов 70-миллиардной LLM на восьми важных бенчмарках. Этот метод помогает эффективнее распределять вычислительные ресурсы при обучении LLM и контролировать процесс обучения."
                },
                "en": {
                    "title": "Predicting LLM Performance with Clustering-On-Difficulty",
                    "desc": "This paper addresses the challenges of predicting the performance of Large Language Models (LLMs) before they are fully trained. It introduces a new framework called Clustering-On-Difficulty (COD), which clusters tasks based on their difficulty to create a predictable subset of tasks. By focusing on this subset, the framework allows for more accurate predictions of how well the LLM will perform on a broader set of tasks. The method has shown significant accuracy improvements, achieving a mean deviation of just 1.36% when predicting the performance of a 70 billion parameter LLM."
                },
                "zh": {
                    "title": "基于难度聚类的下游性能预测",
                    "desc": "这篇论文讨论了如何在训练大型语言模型（LLM）之前准确预测其在下游任务上的表现。由于“涌现现象”和任务难度分布不均，现有的性能预测方法往往不够准确。为了解决这些问题，作者提出了一种基于难度聚类的下游性能预测框架（COD），通过聚类任务并排除不适合的集群来构建可预测的支持子集。该方法在70B LLM的性能预测中表现出色，提供了有效的资源分配建议。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18449",
            "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
            "url": "https://huggingface.co/papers/2502.18449",
            "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
            "score": 8,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "938af9b1ea2398d8",
            "authors": [
                "Yuxiang Wei",
                "Olivier Duchenne",
                "Jade Copet",
                "Quentin Carbonneaux",
                "Lingming Zhang",
                "Daniel Fried",
                "Gabriel Synnaeve",
                "Rishabh Singh",
                "Sida I. Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "FAIR at Meta",
                "GenAI at Meta",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18449.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#dataset",
                    "#math",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SWE-RL: Революция в обучении языковых моделей для задач разработки ПО",
                    "desc": "Данная статья представляет SWE-RL - первый подход к масштабированию обучения с подкреплением (RL) для улучшения рассуждений больших языковых моделей (LLM) в реальных задачах разработки программного обеспечения. Используя легковесную систему вознаграждений на основе правил, SWE-RL позволяет LLM автономно восстанавливать процессы рассуждений разработчиков, обучаясь на обширных данных эволюции открытого программного обеспечения. Модель Llama3-SWE-RL-70B, обученная на основе Llama 3, достигает впечатляющих результатов на бенчмарке SWE-bench Verified, превосходя другие LLM среднего размера. Интересно, что несмотря на обучение только на данных эволюции ПО, модель демонстрирует улучшенные навыки обобщенных рассуждений в различных областях."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Software Engineering Reasoning",
                    "desc": "This paper presents SWE-RL, a novel approach that applies reinforcement learning (RL) to enhance the reasoning abilities of large language models (LLMs) specifically for software engineering tasks. By utilizing a lightweight rule-based reward system, SWE-RL allows LLMs to learn from extensive open-source software evolution data, which includes various stages of software development. The resulting model, Llama3-SWE-RL-70B, achieves impressive performance on real-world GitHub issues, outperforming other medium-sized LLMs and even rivaling larger proprietary models. Additionally, this approach not only improves software-related reasoning but also demonstrates generalized reasoning skills across various tasks, indicating its broad applicability."
                },
                "zh": {
                    "title": "通过强化学习提升大语言模型的推理能力",
                    "desc": "这篇论文介绍了SWE-RL，这是第一个将强化学习应用于真实软件工程的模型。通过使用轻量级的基于规则的奖励机制，SWE-RL能够从大量开源软件演变数据中学习，自动恢复开发者的推理过程和解决方案。训练后的模型Llama3-SWE-RL-70B在真实的GitHub问题上达到了41.0%的解决率，表现优于其他中型语言模型。尽管仅在软件演变数据上进行强化学习，Llama3-SWE-RL仍展现出广泛的推理能力，能够在多个领域任务中取得良好结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15499",
            "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
            "url": "https://huggingface.co/papers/2502.15499",
            "abstract": "Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing gradient explosion and dissipation. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
            "score": 7,
            "issue_id": 2410,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "3fb4636aad6d75b4",
            "authors": [
                "Ya Wang",
                "Zhijian Zhuo",
                "Yutao Zeng",
                "Xun Zhou",
                "Jian Yang",
                "Xiaoqing Li"
            ],
            "affiliations": [
                "Capital University of Economics and Business",
                "School of Mathematical Sciences, Peking University",
                "Seed-Foundation-Model, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15499.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Стабильное обучение языковых моделей через разделение масштаба и распределения",
                    "desc": "Статья представляет новый метод Scale-Distribution Decoupling (SDD) для стабилизации обучения больших языковых моделей (LLM). SDD разделяет масштаб и распределение весовой матрицы в полносвязных слоях, применяя нормализацию и обучаемый вектор масштабирования. Это предотвращает взрыв и затухание градиентов, особенно в глубоких сетях. Эксперименты показывают, что метод стабилизирует обучение различных архитектур LLM и превосходит существующие техники."
                },
                "en": {
                    "title": "Stabilizing Large Language Model Training with SDD",
                    "desc": "This paper addresses the issue of training stability in large language models, especially those using Post-Norm Transformers, which often face problems like gradient explosion and dissipation. The authors introduce a new method called Scale-Distribution Decoupling (SDD), which separates the scale and distribution of weight matrices in fully-connected layers to enhance training stability. By implementing a normalization mechanism and a learnable scaling vector, SDD ensures well-conditioned gradients and improves optimization efficiency in deep networks. Experimental results show that SDD not only stabilizes training across various architectures but also outperforms existing normalization techniques while being lightweight and compatible with current frameworks."
                },
                "zh": {
                    "title": "规模-分布解耦：稳定大型语言模型训练的创新方法",
                    "desc": "在大型语言模型（LLMs）的预训练中，训练稳定性一直是一个挑战，尤其是对于后归一化变换器架构，容易出现梯度爆炸和消散。本文提出了一种新方法，称为规模-分布解耦（SDD），通过明确解耦全连接层中权重矩阵的规模和分布来稳定训练。SDD采用归一化机制来调节激活值，并使用可学习的缩放向量来保持良好的梯度条件，有效防止梯度爆炸和消散。实验结果表明，我们的方法在各种LLM架构中稳定了训练，并在不同的归一化配置中优于现有技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16794",
            "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
            "url": "https://huggingface.co/papers/2502.16794",
            "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",
            "score": 3,
            "issue_id": 2410,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "680b3da301440d2b",
            "authors": [
                "Xilin Jiang",
                "Sukru Samet Dindar",
                "Vishal Choudhari",
                "Stephan Bickel",
                "Ashesh Mehta",
                "Guy M McKhann",
                "Adeen Flinker",
                "Daniel Friedman",
                "Nima Mesgarani"
            ],
            "affiliations": [
                "Department of Electrical Engineering, Columbia University, USA",
                "Department of Neurological Surgery, Columbia University, USA",
                "Hofstra Northwell School of Medicine, USA",
                "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, USA",
                "Neurology Department, New York University, USA",
                "The Feinstein Institutes for Medical Research, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16794.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#alignment",
                    "#audio"
                ],
                "emoji": "👂",
                "ru": {
                    "title": "Слушать как человек: ИИ с избирательным вниманием",
                    "desc": "Статья представляет новый подход к обработке аудиоданных с использованием больших языковых моделей (LLM), учитывающий избирательное внимание человека. Авторы предлагают систему AAD-LLM, которая интегрирует сигналы мозга для определения, на какого говорящего слушатель обращает внимание. Модель сначала предсказывает объект внимания на основе нейронной активности, а затем генерирует ответы с учетом этого состояния внимания. Система показала улучшенное соответствие намерениям слушателя в задачах описания говорящих, транскрипции речи и ответов на вопросы в ситуациях с несколькими говорящими."
                },
                "en": {
                    "title": "Listening with Intention: Enhancing Machine Hearing through Attention",
                    "desc": "This paper introduces a new approach to auditory processing in machine learning called Intention-Informed Auditory Scene Understanding (II-ASU). It presents a prototype system, Auditory Attention-Driven LLM (AAD-LLM), which uses brain signals to determine which speaker a listener is focusing on in complex sound environments. By integrating intracranial electroencephalography (iEEG) data, the model can tailor its responses based on the listener's attention, enhancing the relevance of its outputs. The evaluation shows that AAD-LLM significantly improves performance in tasks like speaker description and question answering, aligning better with human auditory perception."
                },
                "zh": {
                    "title": "意图驱动的听觉理解，提升机器听觉能力",
                    "desc": "这篇论文介绍了一种新的听觉场景理解模型，称为意图驱动的听觉场景理解（II-ASU）。该模型通过分析脑电图信号，识别听众关注的特定说话者，从而生成更符合听众意图的响应。与传统的听觉大语言模型不同，AAD-LLM能够根据听众的注意力状态调整其输出。研究表明，该模型在多说话者场景中的表现优于现有模型，能够更好地理解和响应听众的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18364",
            "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
            "url": "https://huggingface.co/papers/2502.18364",
            "abstract": "Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.",
            "score": 3,
            "issue_id": 2409,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "23632a98b9252831",
            "authors": [
                "Yifan Pu",
                "Yiming Zhao",
                "Zhicong Tang",
                "Ruihong Yin",
                "Haoxing Ye",
                "Yuhui Yuan",
                "Dong Chen",
                "Jianmin Bao",
                "Sirui Zhang",
                "Yanbin Wang",
                "Lin Liang",
                "Lijuan Wang",
                "Ji Li",
                "Xiu Li",
                "Zhouhui Lian",
                "Gao Huang",
                "Baining Guo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18364.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "ART: революция в генерации многослойных изображений",
                    "desc": "В статье представлен метод Anonymous Region Transformer (ART) для генерации многослойных прозрачных изображений на основе текстового запроса и анонимной разметки областей. ART позволяет модели автономно определять соответствие между визуальными и текстовыми токенами, что отличает его от семантической разметки. Механизм послойной обрезки регионов значительно снижает вычислительные затраты и позволяет эффективно генерировать изображения с множеством слоев. Авторы также предлагают автоэнкодер для кодирования и декодирования прозрачности в многослойных изображениях."
                },
                "en": {
                    "title": "Revolutionizing Multi-Layer Image Generation with ART",
                    "desc": "This paper presents the Anonymous Region Transformer (ART), a novel approach for generating multi-layer transparent images using a global text prompt. ART allows the model to autonomously match visual tokens to text tokens through an anonymous region layout, improving upon traditional semantic layouts. The layer-wise region crop mechanism enhances efficiency by reducing attention computation costs, enabling the generation of images with many distinct layers quickly. Overall, ART introduces a new paradigm for interactive content creation, allowing for precise control and scalable image generation."
                },
                "zh": {
                    "title": "匿名区域变换器：多层图像生成的新方法",
                    "desc": "多层图像生成是一个重要任务，使用户能够隔离、选择和编辑特定的图像层，从而改变与生成模型的交互方式。本文介绍了一种名为匿名区域变换器（ART）的新方法，它可以根据全局文本提示和匿名区域布局直接生成可变的多层透明图像。该方法允许生成模型自主决定哪些视觉标记与哪些文本标记对齐，显著提高了生成效率，并减少了计算成本。通过引入高质量的多层透明图像自编码器，ART为交互式内容创作建立了新的范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18356",
            "title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
            "url": "https://huggingface.co/papers/2502.18356",
            "abstract": "We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents.",
            "score": 2,
            "issue_id": 2410,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "54dae5eb2e25ce92",
            "authors": [
                "George Thomas",
                "Alex J. Chan",
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Fraser Greenlee",
                "Andy Toulis",
                "Marvin Purtorab"
            ],
            "affiliations": [
                "Clusterfudge Ltd.",
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18356.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#games",
                    "#agents"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "WebGames: тестирование AI-агентов в реальных веб-сценариях",
                    "desc": "WebGames - это комплексный набор тестов для оценки AI-агентов общего назначения, предназначенных для веб-браузинга. Он включает более 50 интерактивных задач, разработанных для тестирования ограничений современных систем искусственного интеллекта в различных аспектах взаимодействия с веб-браузером. Результаты оценки ведущих моделей vision-language, таких как GPT-4o, Claude Computer-Use, Gemini-1.5-Pro и Qwen2-VL, показывают значительный разрыв в возможностях по сравнению с человеческой производительностью. Бенчмарк WebGames доступен публично и предоставляет надежную основу для измерения прогресса в разработке более способных веб-агентов."
                },
                "en": {
                    "title": "Bridging the AI Performance Gap in Web Browsing",
                    "desc": "WebGames is a new benchmark suite designed to test web-browsing AI agents with over 50 interactive challenges. These challenges are easy for humans but are meant to expose the weaknesses of AI in tasks like browser interactions and cognitive processing. The testing environment is self-contained, allowing for consistent evaluations with clear correct answers. When tested, top AI models showed a significant performance gap compared to humans, achieving only 43.1% success versus 95.7% for humans, indicating that current AI struggles with intuitive web tasks."
                },
                "zh": {
                    "title": "WebGames：评估网页浏览AI的全新基准",
                    "desc": "我们介绍了WebGames，这是一个全面的基准测试套件，旨在通过50多个互动挑战评估通用网页浏览AI代理。这些挑战设计得对人类来说简单明了，但系统地测试当前AI系统在基本浏览器交互、先进输入处理、认知任务、工作流自动化和互动娱乐等方面的局限性。我们的框架通过一个封闭的测试环境消除了外部依赖，确保可重复的评估和可验证的真实解决方案。评估结果显示，最佳AI系统的成功率仅为43.1%，而人类的表现为95.7%，突显了当前AI系统在处理人类直观的常见网页交互模式方面的基本局限性。"
                }
            }
        }
    ],
    "link_prev": "2025-02-25.html",
    "link_next": "2025-02-27.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2月25日"
    },
    "short_date_next": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2月27日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章的主要目标是创建一个计算资源和训练数据有限的多任务通用感知模型。作者使用了预训练的文本到图像扩散模型。实验结果表明，DICEPTION 在多个感知任务上表现优异，仅使用了 SAM-vit-h 模型 0.06% 的数据。DICEPTION 通过颜色编码统一了各种感知任务，使得预训练的文本到图像模型可以被充分利用。在适应其他任务时，模型只需在少量图像和参数上进行微调。",
        "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
        "pinyin": "这篇文章的主要目标是创建一个计算资源和训练数据有限的多任务通用感知模型。\nZhè piān wénzhāng de zhǔyào mùbiāo shì chuàngjiàn yīgè jìsuàn zīyuán hé xùnliàn shùjù yǒu xiàn de duō rènwù tōngyòng gǎnjué móxíng.\n\n作者使用了预训练的文本到图像扩散模型。\nZuòzhě shǐyòngle yù xùnliàn de wénběn dào túxiàng kuòsàn móxíng.\n\n实验结果表明，DICEPTION 在多个感知任务上表现优异，仅使用了 SAM-vit-h 模型 0.06% 的数据。\nShíyàn jiéguǒ biǎomíng, DICEPTION zài duō gè gǎnjué rènwù shàng biǎoxiàn yōuyì, jǐn shǐyòngle SAM-vit-h móxíng 0.06% de shùjù.\n\nDICEPTION 通过颜色编码统一了各种感知任务，使得预训练的文本到图像模型可以被充分利用。\nDICEPTION tōngguò yánsè biānmǎ tǒngyīle gèzhǒng gǎnjué rènwù, shǐdé yù xùnliàn de wénběn dào túxiàng móxíng kěyǐ bèi chōngfēn lìyòng.\n\n在适应其他任务时，模型只需在少量图像和参数上进行微调。\nZài shìyìng qítā rènwù shí, móxíng zhǐ xū zài shǎoliàng túxiàng hé cānshù shàng jìnxíng wēitiáo.",
        "vocab": "[\n    {\"word\": \"创建\", \"pinyin\": \"chuàng jiàn\", \"trans\": \"create\"},\n    {\"word\": \"计算资源\", \"pinyin\": \"jì suàn zī yuán\", \"trans\": \"computational resources\"},\n    {\"word\": \"训练数据\", \"pinyin\": \"xùn liàn shù jù\", \"trans\": \"training data\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒu xiàn\", \"trans\": \"limited\"},\n    {\"word\": \"多任务\", \"pinyin\": \"duō rèn wù\", \"trans\": \"multi-task\"},\n    {\"word\": \"通用\", \"pinyin\": \"tōng yòng\", \"trans\": \"general-purpose\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎn zhī\", \"trans\": \"perception\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"文本到图像\", \"pinyin\": \"wén běn dào tú xiàng\", \"trans\": \"text-to-image\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"仅\", \"pinyin\": \"jǐn\", \"trans\": \"only\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒng yī\", \"trans\": \"unify\"},\n    {\"word\": \"编码\", \"pinyin\": \"biān mǎ\", \"trans\": \"encoding\"},\n    {\"word\": \"充分利用\", \"pinyin\": \"chōng fèn lì yòng\", \"trans\": \"fully utilize\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"}\n]",
        "trans": "The primary objective of this article is to create a general-purpose multi-task perception model with limited computational resources and training data. The authors utilized a pre-trained text-to-image diffusion model. Experimental results demonstrate that DICEPTION performs exceptionally well on multiple perception tasks, using only 0.06% of the data from the SAM-vit-h model. DICEPTION unifies various perception tasks through color coding, enabling the full utilization of the pre-trained text-to-image model. When adapting to other tasks, the model requires only fine-tuning on a small number of images and parameters.",
        "update_ts": "2025-02-25 09:11"
    }
}