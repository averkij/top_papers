{
    "date": {
        "ru": "20 мая",
        "en": "May 20",
        "zh": "5月20日"
    },
    "time_utc": "2025-05-20 07:12",
    "weekday": 1,
    "issue_id": 3850,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.11820",
            "title": "Chain-of-Model Learning for Language Model",
            "url": "https://huggingface.co/papers/2505.11820",
            "abstract": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.",
            "score": 38,
            "issue_id": 3848,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "2e8115f0fe78856b",
            "authors": [
                "Kaitao Song",
                "Xiaohua Wang",
                "Xu Tan",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Yongliang Shen",
                "Cen LU",
                "Zihao Li",
                "Zifan Song",
                "Caihua Shan",
                "Yansen Wang",
                "Kan Ren",
                "Xiaoqing Zheng",
                "Tao Qin",
                "Yuqing Yang",
                "Dongsheng Li",
                "Lili Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research",
                "ShanghaiTech University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11820.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#inference",
                    "#agi",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Цепная революция в языковых моделях: гибкость и эффективность",
                    "desc": "В этой статье представлена новая парадигма обучения под названием Chain-of-Model (CoM), которая внедряет причинно-следственные связи в скрытые состояния каждого слоя модели в виде цепочки. Авторы вводят концепцию Chain-of-Representation (CoR), формулирующую скрытые состояния на каждом уровне как комбинацию нескольких под-представлений на уровне скрытых измерений. На основе этого принципа разработана архитектура Chain-of-Language-Model (CoLM), которая внедряет идею CoM в каждый слой Transformer. Экспериментальные результаты показывают, что семейство моделей CoLM достигает сопоставимой производительности со стандартным Transformer, одновременно обеспечивая большую гибкость в масштабировании и развертывании."
                },
                "en": {
                    "title": "Scaling Language Models with Chain-of-Model Efficiency",
                    "desc": "This paper introduces a new learning approach called Chain-of-Model (CoM), which enhances model training efficiency by incorporating causal relationships into the hidden states of each layer. It presents the Chain-of-Representation (CoR) concept, where hidden states are formed from multiple sub-representations, allowing each layer to only access its preceding chains. The CoM framework enables models to scale up by adding more chains, providing flexibility in deploying various sub-models of different sizes. The Chain-of-Language-Model (CoLM) and its variant CoLM-Air further optimize Transformer architectures by sharing key-value pairs across chains, resulting in improved performance and adaptability for language models."
                },
                "zh": {
                    "title": "链式模型：灵活高效的语言模型新范式",
                    "desc": "本文提出了一种新颖的学习范式，称为链式模型（CoM），它将因果关系融入每一层的隐藏状态，以链式结构提高模型训练的效率和推理的灵活性。我们引入了链式表示（CoR）的概念，将每一层的隐藏状态表示为多个子表示的组合（即链）。在每一层中，输出表示的每个链只能查看输入表示中所有前面的链，从而使得基于CoM框架构建的模型能够通过增加链的数量逐步扩大模型规模，并提供不同大小的子模型以实现灵活推理。基于这一原理，我们设计了链式语言模型（CoLM），并进一步引入了CoLM-Air，通过引入键值共享机制，计算第一个链中的所有键和值，然后在所有链之间共享，从而展示了额外的可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11896",
            "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.11896",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.",
            "score": 33,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "bdc79864df7cbd51",
            "authors": [
                "Chenwei Lou",
                "Zewei Sun",
                "Xinnian Liang",
                "Meng Qu",
                "Wei Shen",
                "Wenqi Wang",
                "Yuntao Li",
                "Qingping Yang",
                "Shuangzhi Wu"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11896.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AdaCoT: Умное рассуждение для языковых моделей",
                    "desc": "AdaCoT - это новый фреймворк, позволяющий крупным языковым моделям (LLM) адаптивно решать, когда использовать метод цепочки рассуждений (Chain-of-Thought, CoT). Используя обучение с подкреплением, в частности Proximal Policy Optimization (PPO), AdaCoT оптимизирует баланс между производительностью модели и вычислительными затратами, связанными с применением CoT. Ключевым техническим вкладом является метод Selective Loss Masking (SLM), предотвращающий коллапс границы принятия решений во время многоэтапного обучения с подкреплением. Эксперименты показывают, что AdaCoT значительно снижает использование CoT для запросов, не требующих сложных рассуждений, сохраняя при этом высокую производительность на сложных задачах."
                },
                "en": {
                    "title": "Adaptive Reasoning for Efficient Language Models",
                    "desc": "This paper presents AdaCoT, a new framework that improves the efficiency of Large Language Models (LLMs) by adaptively deciding when to use Chain-of-Thought (CoT) prompting. Traditional CoT prompting can be computationally expensive, especially for simpler queries, but AdaCoT optimizes this by framing the decision to use CoT as a Pareto optimization problem. The authors employ reinforcement learning, specifically Proximal Policy Optimization (PPO), to dynamically adjust when CoT is triggered based on the complexity of the input. Their approach includes a technique called Selective Loss Masking (SLM) to ensure stable training, resulting in significant reductions in CoT usage while maintaining high performance on complex tasks."
                },
                "zh": {
                    "title": "自适应链式推理，提升效率与性能",
                    "desc": "大型语言模型（LLMs）在处理复杂推理任务时表现出色，但在某些情况下面临挑战。为了解决这一问题，本文提出了AdaCoT（自适应链式推理），它允许模型根据输入的复杂性自适应地决定是否使用链式推理。我们将自适应推理视为一个帕累托优化问题，旨在平衡模型性能与链式推理的计算成本。实验结果表明，AdaCoT在不需要复杂推理的查询中显著减少了链式推理的使用，提升了效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11254",
            "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction",
            "url": "https://huggingface.co/papers/2505.11254",
            "abstract": "The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.",
            "score": 29,
            "issue_id": 3847,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "2aba31b686859e82",
            "authors": [
                "Jeffrey Willette",
                "Heejun Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11254.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Коррекция распределения для эффективного разреженного внимания",
                    "desc": "Статья предлагает новый метод для повышения эффективности разреженного внимания в трансформерах. Авторы обнаружили, что разреженное вычисление вызывает сдвиг распределения в выходных данных механизма внимания, что приводит к снижению производительности. Предложенная процедура корректирует этот сдвиг, приближая распределение выходных данных разреженного внимания к квадратичному. Метод может применяться поверх любого алгоритма разреженного внимания, значительно повышая точность при сохранении высокой разреженности и скорости обработки."
                },
                "en": {
                    "title": "Boosting Sparse Attention: Aligning Outputs for Enhanced Performance",
                    "desc": "This paper addresses the inefficiencies of the attention mechanism in transformers, which typically has a quadratic complexity that increases inference costs for long sequences. It highlights that while sparse attention methods can reduce computation, they often lead to performance degradation due to a distributional shift in attention outputs. The authors propose a novel procedure to correct this shift, aligning sparse attention outputs more closely with those of traditional quadratic attention. Their method significantly improves performance, achieving an average increase of 36 percentage points while maintaining high sparsity and speed, making it much faster than existing methods."
                },
                "zh": {
                    "title": "稀疏注意力的分布修正，提升性能与效率",
                    "desc": "本文探讨了变换器的注意力机制在处理长序列时的计算复杂度问题，导致推理成本高和延迟大。尽管注意力矩阵通常是稀疏的，但稀疏注意力推理方法在减少计算负担的同时，可能会导致性能下降。我们发现，性能下降的一个原因是稀疏计算引起了注意力输出的分布偏移，这使得解码时的查询与预填阶段的键对齐不佳。为了解决这个问题，我们提出了一种简单而有效的修正方法，使稀疏注意力输出的分布更接近于二次注意力，从而显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13417",
            "title": "AdaptThink: Reasoning Models Can Learn When to Think",
            "url": "https://huggingface.co/papers/2505.13417",
            "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.",
            "score": 27,
            "issue_id": 3845,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "edd33223d8d833a7",
            "authors": [
                "Jiajie Zhang",
                "Nianyi Lin",
                "Lei Hou",
                "Ling Feng",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13417.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное мышление для оптимизации рассуждений ИИ",
                    "desc": "Исследователи представили новый алгоритм AdaptThink, который учит модели рассуждения адаптивно выбирать оптимальный режим мышления в зависимости от сложности задачи. Алгоритм использует метод обучения с подкреплением и включает в себя ограниченную целевую функцию оптимизации и стратегию выборки по важности. Эксперименты показали, что AdaptThink значительно сокращает вычислительные затраты при одновременном повышении производительности моделей. На трех наборах математических данных алгоритм сократил среднюю длину ответа модели DeepSeek-R1-Distill-Qwen-1.5B на 53% и повысил ее точность на 2.4%."
                },
                "en": {
                    "title": "Optimize Reasoning with Adaptive Thinking Modes!",
                    "desc": "This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection."
                },
                "zh": {
                    "title": "自适应思考模式选择，提升推理效率与质量",
                    "desc": "最近，大型推理模型在各种任务上表现出色，但其冗长的思考过程显著增加了推理开销，导致效率成为瓶颈。本文提出了一种名为NoThinking的方法，鼓励推理模型跳过思考，直接生成最终解决方案，适用于相对简单的任务。基于此，我们提出了AdaptThink，这是一种新颖的强化学习算法，旨在根据问题难度自适应选择最佳思考模式。实验表明，AdaptThink显著降低了推理成本，同时提高了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13227",
            "title": "Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis",
            "url": "https://huggingface.co/papers/2505.13227",
            "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.",
            "score": 23,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "fe01e0daca57b031",
            "authors": [
                "Tianbao Xie",
                "Jiaqi Deng",
                "Xiaochuan Li",
                "Junlin Yang",
                "Haoyuan Wu",
                "Jixuan Chen",
                "Wenjing Hu",
                "Xinyuan Wang",
                "Yuhui Xu",
                "Zekun Wang",
                "Yiheng Xu",
                "Junli Wang",
                "Doyen Sahoo",
                "Tao Yu",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13227.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#graphs",
                    "#agents",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Революция в обучении ИИ работе с компьютерными интерфейсами",
                    "desc": "Статья представляет новый бенчмарк OSWorld-G для оценки способности моделей машинного обучения к интерпретации естественного языка в контексте графических интерфейсов. Авторы также создали крупнейший датасет Jedi, содержащий 4 миллиона примеров для обучения моделей взаимодействию с компьютерными интерфейсами. Модели, обученные на Jedi, превзошли существующие подходы на нескольких бенчмарках, включая OSWorld-G. Исследование показало, что улучшенное понимание интерфейсов значительно повышает способности крупных языковых моделей выполнять сложные компьютерные задачи."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Comprehensive Datasets and Models",
                    "desc": "This paper addresses the challenge of GUI grounding, which is the process of translating natural language commands into actions on graphical user interfaces. Current benchmarks are limited as they only focus on simple tasks, neglecting the complexities of real-world interactions that require understanding of software context and layout. The authors introduce OSWorld-G, a new benchmark with 564 detailed samples and the Jedi dataset, which contains 4 million examples to improve grounding tasks. Their findings show that using the Jedi dataset significantly enhances the performance of multi-scale models in executing complex computer tasks, demonstrating the importance of specialized data for effective grounding."
                },
                "zh": {
                    "title": "提升计算机使用代理的基础能力",
                    "desc": "本论文探讨了图形用户界面（GUI）基础的自然语言指令映射问题，指出现有基准测试过于简化，无法反映真实世界的复杂交互。为了解决这一问题，作者提出了OSWorld-G基准，包含564个精细注释的样本，涵盖文本匹配、元素识别、布局理解和精确操作等多种任务类型。此外，作者合成并发布了最大的计算机使用基础数据集Jedi，包含400万个示例，展示了其在复杂计算机任务中的有效性。通过详细的消融研究，论文还识别了影响基础性能的关键因素，并验证了不同界面元素的专门数据结合能够实现对新界面的组合泛化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13427",
            "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
            "url": "https://huggingface.co/papers/2505.13427",
            "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.",
            "score": 17,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "22362149c9b7b5ae",
            "authors": [
                "Lingxiao Du",
                "Fanqing Meng",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Ping Luo",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13427.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#math",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Автоматизированное обучение мультимодальных моделей пошаговым рассуждениям",
                    "desc": "В статье представлена модель MM-PRM, обучающаяся оценивать промежуточные шаги рассуждений в мультимодальных задачах. Авторы создали датасет MM-K12 с 10 000 мультимодальных математических задач и использовали метод Монте-Карло для генерации более 700 тысяч аннотаций шагов решения без участия человека. Применение MM-PRM в схеме вывода Best-of-N значительно улучшило результаты как на тестовом наборе MM-K12, так и на внешних бенчмарках. Исследование показывает эффективность использования мягких меток, меньших скоростей обучения и разнообразия путей для оптимизации производительности модели."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Process Supervision",
                    "desc": "This paper introduces MM-PRM, a novel process reward model designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) in solving complex math problems. The authors highlight that MLLMs often struggle with multi-step reasoning due to insufficient supervision of intermediate steps. To overcome this, they create a strong multimodal model called MM-Policy and a new dataset, MM-K12, containing 10,000 multimodal math problems. By employing a Monte Carlo Tree Search method, they generate over 700,000 annotations to train the PRM, which significantly enhances the logical consistency of reasoning in various benchmarks."
                },
                "zh": {
                    "title": "过程监督提升多模态推理的逻辑稳健性",
                    "desc": "这篇论文介绍了一种新的多模态过程奖励模型（MM-PRM），旨在提高多模态大语言模型在复杂多步骤推理中的表现。研究发现，现有模型在推理过程中缺乏细粒度的监督，导致逻辑不一致或部分正确的结果。为了解决这个问题，作者构建了一个强大的多模态模型MM-Policy，并创建了一个包含10,000个可验证答案的多模态数学问题数据集MM-K12。通过无人工标注的方式生成超过70万条步骤级注释，MM-PRM在推理路径评分中表现出显著的改进，证明了过程监督在增强多模态推理系统逻辑稳健性方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13379",
            "title": "Thinkless: LLM Learns When to Think",
            "url": "https://huggingface.co/papers/2505.13379",
            "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless",
            "score": 17,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "d41117eabc11e5c3",
            "authors": [
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13379.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное переключение между кратким и развернутым мышлением в языковых моделях",
                    "desc": "Статья представляет Thinkless - обучаемую систему, позволяющую языковым моделям адаптивно выбирать между кратким и развернутым рассуждением в зависимости от сложности задачи. Система использует обучение с подкреплением и два управляющих токена: <short> для кратких ответов и <think> для детального рассуждения. В основе метода лежит алгоритм DeGRPO, который разделяет цель обучения на выбор режима рассуждения и улучшение точности ответов. Эмпирические результаты показывают, что Thinkless способен сократить использование длинных цепочек рассуждений на 50-90%, значительно повышая эффективность моделей."
                },
                "en": {
                    "title": "Thinkless: Smart Reasoning for Efficient Language Models",
                    "desc": "This paper introduces Thinkless, a framework designed to enhance the efficiency of Reasoning Language Models (RLMs) by enabling them to choose between short-form and long-form reasoning based on task complexity. The framework utilizes reinforcement learning and two control tokens, <short> for brief answers and <think> for detailed reasoning, to guide the model's response strategy. A novel algorithm called Decoupled Group Relative Policy Optimization (DeGRPO) is employed to separate the learning objectives, allowing for better control over reasoning mode selection and response accuracy. The results show that Thinkless can significantly reduce the reliance on long-chain reasoning, improving computational efficiency while maintaining performance on various benchmarks."
                },
                "zh": {
                    "title": "让模型学会何时思考",
                    "desc": "本文提出了一种名为Thinkless的可学习框架，旨在提高大型语言模型（LLM）在推理任务中的效率。该框架通过强化学习训练，使模型能够根据任务复杂性和自身能力自适应选择短期或长期推理。Thinkless使用两个控制标记<short>和<think>来分别表示简洁回答和详细推理。实验结果表明，Thinkless能够将长期推理的使用减少50%至90%，显著提升推理语言模型的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12805",
            "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA",
            "url": "https://huggingface.co/papers/2505.12805",
            "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the B matrix and transmits it to the server. The server aggregates the B matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields a new adaptive A composed of the orthonormal right singular vectors of BA, and an updated B containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing A to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of A bounds the gradient norms of B and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.",
            "score": 16,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "41ef4fd84db0c7fb",
            "authors": [
                "Seanie Lee",
                "Sangwoo Park",
                "Dong Bok Lee",
                "Dominik Wagner",
                "Haebin Seong",
                "Tobias Bocklet",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Technische Hochschule Nürnberg Georg Simon Ohm"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12805.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#benchmark",
                    "#security",
                    "#optimization"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "FedSVD: Защищенное федеративное обучение языковых моделей с сохранением эффективности",
                    "desc": "Статья представляет новый метод FedSVD для эффективного федеративного обучения языковых моделей с дифференциальной приватностью. FedSVD решает проблему усиления шума в методе Low-Rank Adaptation (LoRA) при использовании DP-SGD. Метод использует сингулярное разложение (SVD) для глобальной репараметризации, что позволяет избежать квадратичного усиления шума. FedSVD показывает улучшенную стабильность и производительность по сравнению с базовыми методами в различных настройках приватности."
                },
                "en": {
                    "title": "Enhancing Federated Learning with FedSVD: A Noise-Resilient Approach",
                    "desc": "This paper presents FedSVD, a novel method that enhances the fine-tuning of language models in federated learning while addressing the challenges posed by noise amplification in differentially private stochastic gradient descent (DP-SGD). By utilizing singular value decomposition (SVD), FedSVD allows clients to optimize only one matrix (B) and send it to a central server, which then aggregates these updates to improve model adaptation. This approach mitigates the noise amplification that occurs when combining LoRA with DP-SGD, ensuring that the model remains expressive without compromising privacy. The results demonstrate that FedSVD improves both stability and performance across various privacy settings, outperforming existing methods."
                },
                "zh": {
                    "title": "FedSVD：优化联邦学习中的低秩适应",
                    "desc": "本文提出了一种名为FedSVD的方法，旨在解决低秩适应（LoRA）在联邦学习中与差分隐私随机梯度下降（DP-SGD）结合时的噪声放大问题。通过引入基于奇异值分解（SVD）的全局重参数化，FedSVD允许每个客户端仅优化B矩阵并将其传输到服务器。服务器聚合B矩阵，计算BA的乘积，并通过SVD重新因式分解，从而生成新的适应性A和更新后的B。该方法有效减少了噪声放大，同时提高了模型的稳定性和性能，尤其在隐私设置下表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12504",
            "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models",
            "url": "https://huggingface.co/papers/2505.12504",
            "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.",
            "score": 16,
            "issue_id": 3847,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "f8b07da7e5e43f1e",
            "authors": [
                "Zongkai Liu",
                "Fanqing Meng",
                "Lingxiao Du",
                "Zhixiang Zhou",
                "Chao Yu",
                "Wenqi Shao",
                "Qiaosheng Zhang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12504.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Стабильное обучение с подкреплением для языковых моделей",
                    "desc": "Статья представляет новый алгоритм CPGD для стабилизации обучения с подкреплением языковых моделей. CPGD вводит ограничение на дрейф политики на основе KL-дивергенции и использует механизм отсечения для предотвращения чрезмерных обновлений политики. Авторы теоретически обосновывают CPGD и эмпирически демонстрируют, что он уменьшает нестабильность, наблюдаемую в предыдущих подходах. Результаты показывают значительное улучшение производительности при сохранении стабильности обучения."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning with CPGD",
                    "desc": "This paper introduces Clipped Policy Gradient Optimization with Policy Drift (CPGD), a new algorithm aimed at improving the stability of reinforcement learning in language models. Traditional methods often face issues like training collapse due to large policy updates and improper clipping. CPGD addresses these challenges by using a KL divergence-based policy drift constraint to regulate updates and a clipping mechanism to limit excessive changes. The authors provide theoretical support for CPGD and demonstrate its effectiveness in enhancing performance while ensuring stable training."
                },
                "zh": {
                    "title": "稳定强化学习，提升语言模型性能",
                    "desc": "最近，基于规则的强化学习（RL）在语言模型（LM）的推理能力上取得了显著进展，但现有的RL方法如GRPO、REINFORCE++和RLOO常常面临训练不稳定的问题。为了解决这个问题，我们提出了一种新算法——带有策略漂移的剪切策略梯度优化（CPGD），旨在稳定语言模型中的策略学习。CPGD通过基于KL散度的策略漂移约束动态地规范策略更新，并利用对数比率的剪切机制防止过大的策略更新。我们的理论分析和实证结果表明，CPGD不仅缓解了之前方法的训练不稳定性，还显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13215",
            "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
            "url": "https://huggingface.co/papers/2505.13215",
            "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
            "score": 15,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "0ab00a261298ad44",
            "authors": [
                "Seungjun Oh",
                "Younggeun Lee",
                "Hyejin Jeon",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Sungkyunkwan University",
                "Department of Artificial Intelligence, Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13215.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Гибридный 3D-4D подход для эффективной реконструкции динамических сцен",
                    "desc": "Статья представляет новый метод 3D-4DGS для реконструкции динамических 3D-сцен. Он комбинирует 3D гауссианы для статичных областей и 4D гауссианы для динамических элементов. Это позволяет значительно сократить вычислительные затраты и память по сравнению с полностью 4D подходом. Метод демонстрирует более быстрое обучение при сохранении или улучшении визуального качества."
                },
                "en": {
                    "title": "Efficient 3D-4D Scene Reconstruction with Hybrid Gaussian Splatting",
                    "desc": "This paper presents a new method called hybrid 3D-4D Gaussian Splatting (3D-4DGS) for dynamic 3D scene reconstruction. It combines 3D and 4D Gaussian representations to efficiently model static and dynamic elements in a scene. By converting static regions to 3D Gaussians, the method reduces computational load and memory usage while preserving the quality of dynamic elements with 4D Gaussians. The results show that 3D-4DGS achieves faster training times and improved visual quality compared to traditional 4D Gaussian Splatting techniques."
                },
                "zh": {
                    "title": "高效的动态3D场景重建新方法",
                    "desc": "最近动态3D场景重建的进展显示出良好的效果，能够实现高保真度的3D新视图合成，并提高时间一致性。在这些方法中，4D高斯点云（4DGS）因其能够建模高保真的空间和时间变化而受到关注。然而，现有方法在静态区域冗余分配4D高斯时，导致了显著的计算和内存开销，并可能降低图像质量。我们提出了一种混合3D-4D高斯点云（3D-4DGS）框架，能够自适应地用3D高斯表示静态区域，同时为动态元素保留4D高斯，从而显著提高计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12992",
            "title": "Fractured Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.12992",
            "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.",
            "score": 11,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "464c0b3fac842217",
            "authors": [
                "Baohao Liao",
                "Hanze Dong",
                "Yuhui Xu",
                "Doyen Sahoo",
                "Christof Monz",
                "Junnan Li",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12992.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное масштабирование рассуждений языковых моделей без переобучения",
                    "desc": "Эта статья представляет новый метод под названием Fractured Sampling для улучшения рассуждений больших языковых моделей (LLM) во время вывода. Метод основан на идее усечения цепочки рассуждений (Chain-of-Thought, CoT) и позволяет балансировать между полным CoT и генерацией только ответа по трем осям: количество траекторий рассуждений, количество финальных решений на траекторию и глубина усечения. Авторы показывают, что Fractured Sampling достигает лучшего соотношения точности и вычислительных затрат на пяти эталонных наборах данных для задач рассуждения. Результаты демонстрируют значительное улучшение масштабирования производительности LLM при рассуждениях без необходимости переобучения модели."
                },
                "en": {
                    "title": "Efficient Reasoning with Fractured Sampling",
                    "desc": "This paper presents a new method called Fractured Sampling that enhances the reasoning abilities of large language models (LLMs) during inference without the need for retraining. It builds on the concept of Chain-of-Thought (CoT) prompting, which improves accuracy by generating intermediate reasoning steps, but often at a high token cost. The authors demonstrate that a truncated version of CoT can achieve similar results with significantly fewer tokens. By exploring different ways to balance reasoning depth and the number of solutions, Fractured Sampling offers a more efficient approach that improves accuracy while reducing computational costs."
                },
                "zh": {
                    "title": "高效推理：Fractured Sampling的创新之路",
                    "desc": "本文探讨了一种新的推理时间缩放技术，称为Fractured Sampling，旨在提高大型语言模型（LLMs）的推理能力。通过截断链式思维（CoT），该方法在生成最终答案时减少了所需的token数量，同时保持了与完整CoT相似的准确性。Fractured Sampling在推理轨迹数量、每条轨迹的最终解决方案数量和推理深度等三个维度上进行插值，从而优化了准确性与成本的平衡。实验结果表明，该方法在多个推理基准上表现出色，能够实现更高效和可扩展的LLM推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12081",
            "title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12081",
            "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).",
            "score": 11,
            "issue_id": 3845,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "9b7953f88ae7653d",
            "authors": [
                "Yuqi Liu",
                "Tianyuan Qu",
                "Zhisheng Zhong",
                "Bohao Peng",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12081.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единая модель для многозадачного визуального восприятия",
                    "desc": "В статье представлен VisionReasoner - унифицированная модель для решения различных задач визуального восприятия. Модель использует новые стратегии когнитивного обучения с несколькими объектами и систематическое переформулирование задач для улучшения способностей к рассуждению. VisionReasoner генерирует структурированный процесс рассуждений перед выдачей ответов на запросы пользователей. Экспериментальные результаты показывают превосходство VisionReasoner над Qwen2.5VL в задачах обнаружения, сегментации и подсчета объектов."
                },
                "en": {
                    "title": "VisionReasoner: Unifying Visual Perception with Advanced Reasoning",
                    "desc": "This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception."
                },
                "zh": {
                    "title": "统一视觉感知的推理能力",
                    "desc": "本文介绍了一种名为VisionReasoner的统一框架，能够在共享模型中处理多种视觉感知任务。通过设计新颖的多对象认知学习策略和系统的任务重构，VisionReasoner增强了其推理能力，以分析视觉输入并解决多样的感知任务。该模型在生成所需输出之前，会先进行结构化的推理过程，以响应用户查询。实验结果表明，VisionReasoner在检测、分割和计数等三个关键领域的十个任务上表现优异，超越了Qwen2.5VL。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11932",
            "title": "Neuro-Symbolic Query Compiler",
            "url": "https://huggingface.co/papers/2505.11932",
            "abstract": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.",
            "score": 10,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "9445be4eff7e4edc",
            "authors": [
                "Yuyao Zhang",
                "Zhicheng Dou",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11932.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Компиляция запросов для точного поиска в RAG-системах",
                    "desc": "QCompiler - это нейро-символическая система для улучшения понимания сложных запросов в RAG-системах. Она использует минимальную грамматику BNF для формализации запросов и компилирует их в абстрактные синтаксические деревья. Это позволяет более точно извлекать документы и генерировать ответы на сложные запросы с вложенными структурами. QCompiler включает в себя переводчик выражений запросов, лексический синтаксический анализатор и рекурсивный процессор."
                },
                "en": {
                    "title": "Enhancing Query Understanding in RAG Systems with QCompiler",
                    "desc": "This paper introduces QCompiler, a neuro-symbolic framework designed to enhance the understanding of complex search queries in Retrieval-Augmented Generation (RAG) systems. It utilizes a specially designed Backus-Naur Form (BNF) grammar to formalize these queries, ensuring that they are both complete and free of unnecessary complexity. QCompiler operates through a series of components, including a Query Expression Translator and a Lexical Syntax Parser, which work together to convert queries into Abstract Syntax Trees (ASTs). By focusing on the atomicity of sub-queries, QCompiler improves the accuracy of document retrieval and response generation for intricate queries."
                },
                "zh": {
                    "title": "提升RAG系统的复杂查询识别能力",
                    "desc": "本文提出了一种名为QCompiler的神经符号框架，旨在提高检索增强生成（RAG）系统对复杂查询的识别能力。QCompiler基于语言语法规则和编译器设计，设计了一种最小但足够的巴科斯-诺尔形式（BNF）语法G[q]，以形式化复杂查询。与以往方法不同，这种语法在保持完整性的同时，减少了冗余。通过将查询编译成抽象语法树（AST），QCompiler能够更精确地检索文档并生成响应，从而显著提升RAG系统处理复杂查询的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12849",
            "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
            "url": "https://huggingface.co/papers/2505.12849",
            "abstract": "Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow's sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow",
            "score": 7,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "191f5a409cc6b32e",
            "authors": [
                "Ben Liu",
                "Zhen Qin"
            ],
            "affiliations": [
                "TapTap, Shanghai, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#cv",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение генерации изображений в TarFlow с помощью оптимизированных итераций",
                    "desc": "Данная статья представляет метод ускорения процесса сэмплирования в модели TarFlow для генерации изображений. Авторы применяют итерационный метод Гаусса-Зейделя-Якоби и вводят две метрики: Convergence Ranking Metric (CRM) и Initial Guessing Metric (IGM). CRM используется для определения сложности блоков TarFlow, а IGM оценивает качество начальных значений для итераций. Эксперименты показали значительное ускорение сэмплирования (до 5.32 раз) без ухудшения качества генерируемых изображений."
                },
                "en": {
                    "title": "Accelerating TarFlow: Faster Sampling without Quality Loss",
                    "desc": "This paper presents an optimization strategy for the TarFlow model, which combines transformer architecture with Normalizing Flow for image generation. The authors introduce the Gauss-Seidel-Jacobi (GS-Jacobi) iteration method to accelerate the slow sampling process of TarFlow. They identify that certain blocks within the model are more critical for image generation and propose metrics to evaluate their performance: the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM). Experimental results show that the GS-Jacobi method significantly improves sampling speed while preserving image quality, achieving notable speed-ups across various benchmarks."
                },
                "zh": {
                    "title": "加速TarFlow采样，提升图像生成效率",
                    "desc": "图像生成模型在多个应用中取得了显著进展。TarFlow模型结合了变换器架构和归一化流模型，在多个基准测试中达到了最先进的结果。然而，由于因果注意力的顺序计算，TarFlow的采样过程非常缓慢。本文通过优化策略，利用高斯-赛德尔-雅可比迭代方法显著加速了TarFlow的采样过程，同时保持生成图像的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11855",
            "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research",
            "url": "https://huggingface.co/papers/2505.11855",
            "abstract": "Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.",
            "score": 6,
            "issue_id": 3849,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "8432e529923dacc5",
            "authors": [
                "Guijin Son",
                "Jiwoo Hong",
                "Honglu Fan",
                "Heejeong Nam",
                "Hyunwoo Ko",
                "Seungwon Lim",
                "Jinyeop Song",
                "Jinha Choi",
                "Gonçalo Paulo",
                "Youngjae Yu",
                "Stella Biderman"
            ],
            "affiliations": [
                "Boeing Korea",
                "EleutherAI",
                "KAIST",
                "MIT",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11855.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Большие языковые модели пока не готовы быть научными рецензентами",
                    "desc": "Статья представляет SPOT - набор данных из 83 опубликованных научных работ с 91 значительной ошибкой, приведшей к опечаткам или отзыву. Авторы оценивают способность современных больших языковых моделей (LLM) обнаруживать эти ошибки в качестве автоматических верификаторов научных рукописей. Результаты показывают, что даже лучшие модели достигают лишь 21.1% полноты и 6.1% точности, демонстрируя ненадежность и непоследовательность. Качественный анализ выявляет, что ошибки моделей напоминают студенческие заблуждения, указывая на значительный разрыв между текущими возможностями LLM и требованиями к надежной ИИ-assisted верификации научных работ."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs as Verifiers in Scientific Discovery",
                    "desc": "This paper investigates the use of large language models (LLMs) as tools for verifying scientific manuscripts, rather than just generating content. The authors introduce a dataset called SPOT, which includes published papers with significant errors that could lead to errata or retraction. They evaluate various state-of-the-art LLMs on this dataset and find that their performance is lacking, with low recall and precision rates. The study concludes that current LLMs are not yet reliable enough for academic verification, as they often make errors similar to those of novice students."
                },
                "zh": {
                    "title": "大型语言模型在学术验证中的挑战",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在学术验证中的应用，提出了一个名为SPOT的数据集，包含83篇已发表论文和91个显著错误。研究发现，当前最先进的LLMs在识别错误方面的表现不佳，最高召回率仅为21.1%，精确率为6.1%。此外，模型的置信度普遍较低，且在多次独立测试中，模型很少能重新发现相同的错误。通过与领域专家的定性分析，发现即使是最强的模型也会犯类似学生级别的误解错误，显示出当前LLMs在可靠的学术验证中存在显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13444",
            "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.13444",
            "abstract": "Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.",
            "score": 3,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "7bd36c8068fb640c",
            "authors": [
                "Liyan Tang",
                "Grace Kim",
                "Xinyu Zhao",
                "Thom Lake",
                "Wenxuan Ding",
                "Fangcong Yin",
                "Prasann Singhal",
                "Manya Wadhwa",
                "Zeyu Leo Liu",
                "Zayne Sprague",
                "Ramya Namuduri",
                "Bodun Hu",
                "Juan Diego Rodriguez",
                "Puyuan Peng",
                "Greg Durrett"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13444.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#cv",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Раскрывая пробелы в визуальном мышлении ИИ при анализе диаграмм",
                    "desc": "Статья представляет новый тестовый набор данных ChartMuseum для оценки понимания диаграмм моделями компьютерного зрения и обработки естественного языка. Исследование показывает, что современные мультимодальные модели значительно уступают людям в задачах, требующих сложного визуального анализа диаграмм. Авторы обнаружили существенное снижение производительности моделей при увеличении визуальной сложности задач. ChartMuseum эффективно выявляет разрыв между возможностями моделей и людей в понимании диаграмм, особенно в задачах, требующих преимущественно визуального рассуждения."
                },
                "en": {
                    "title": "Bridging the Gap in Chart Understanding: Visual vs. Textual Reasoning",
                    "desc": "This paper addresses the challenges faced by large vision-language models (LVLMs) in understanding charts, highlighting their struggle with visual reasoning compared to textual reasoning. The authors present a case study using a synthetic dataset that shows a significant drop in model performance as visual complexity increases, while human performance remains stable. They introduce ChartMuseum, a new benchmark for Chart Question Answering (QA) that includes 1,162 expert-annotated questions designed to test both visual and textual reasoning. The results reveal a substantial performance gap between humans and models, with the best model achieving only 63% accuracy compared to 93% for humans, particularly struggling with questions that require visual reasoning."
                },
                "zh": {
                    "title": "图表理解：人类与模型的差距",
                    "desc": "图表理解对大型视觉语言模型（LVLMs）提出了独特的挑战，因为它需要复杂的文本和视觉推理能力的结合。当前的LVLM在这些技能之间存在显著的不平衡，尤其是在视觉推理方面表现不佳。我们通过一个合成数据集进行案例研究，发现随着视觉复杂性的增加，模型性能显著下降，而人类的表现则保持稳定。我们引入了ChartMuseum，这是一个新的图表问答基准，包含1162个专家注释的问题，旨在评估复杂的视觉和文本推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13437",
            "title": "FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance",
            "url": "https://huggingface.co/papers/2505.13437",
            "abstract": "Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as \"switch leap with 0.5 turn\" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.",
            "score": 2,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "28a08dbfb09c6639",
            "authors": [
                "Dian Shao",
                "Mingfei Shi",
                "Shengda Xu",
                "Haodong Chen",
                "Yongle Huang",
                "Binglu Wang"
            ],
            "affiliations": [
                "School of Astronautics, Northwestern Polytechnical University, Xian, China",
                "School of Automation, Northwestern Polytechnical University, Xian, China",
                "School of Software, Northwestern Polytechnical University, Xian, China",
                "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13437.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🤸",
                "ru": {
                    "title": "Точная генерация движений человека с помощью физического моделирования",
                    "desc": "Статья представляет FinePhys - фреймворк для генерации точных движений человека с использованием физических моделей. Система сначала оценивает 2D позы, затем преобразует их в 3D с помощью обучения в контексте. Далее применяется физическое моделирование на основе уравнений Эйлера-Лагранжа для улучшения 3D поз. Полученные физически корректные позы комбинируются с данными для создания многомасштабных 2D тепловых карт, используемых в процессе диффузии."
                },
                "en": {
                    "title": "Bridging Physics and Data for Realistic Human Action Generation",
                    "desc": "This paper presents FinePhys, a framework designed to improve the generation of fine-grained human actions in videos by integrating physics-based modeling. It addresses the challenges of synthesizing complex movements, such as gymnastics routines, by first estimating 2D poses and then converting them to 3D using in-context learning. To enhance the stability and interpretability of the generated poses, FinePhys employs a motion re-estimation module based on Euler-Lagrange equations, which calculates joint accelerations through bidirectional temporal updates. The combination of physics-based predictions with data-driven approaches results in more realistic and coherent human actions, as demonstrated by superior performance on benchmark datasets."
                },
                "zh": {
                    "title": "FinePhys：物理驱动的细粒度人类动作生成框架",
                    "desc": "尽管视频生成技术取得了显著进展，但合成物理上合理的人类动作仍然是一个持续的挑战，尤其是在建模细粒度语义和复杂时间动态方面。本文提出了FinePhys，一个细粒度人类动作生成框架，结合物理学以获得有效的骨骼指导。FinePhys首先以在线方式估计2D姿势，然后通过上下文学习进行2D到3D的维度提升。通过引入基于物理的运动重新估计模块，FinePhys能够生成更自然和合理的细粒度人类动作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13389",
            "title": "Faster Video Diffusion with Trainable Sparse Attention",
            "url": "https://huggingface.co/papers/2505.13389",
            "abstract": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53times with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6times and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.",
            "score": 2,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "33a48c202961951b",
            "authors": [
                "Peiyuan Zhang",
                "Haofeng Huang",
                "Yongqi Chen",
                "Will Lin",
                "Zhengzhong Liu",
                "Ion Stoica",
                "Eric P. Xing",
                "Hao Zhang"
            ],
            "affiliations": [
                "MBZUAI",
                "UC Berkeley",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#open_source",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективное разреженное внимание для масштабирования видео-диффузионных моделей",
                    "desc": "Статья представляет новый метод разреженного внимания (VSA) для масштабирования видео-диффузионных трансформеров. VSA использует двухэтапный подход: грубый этап для выявления критических токенов и тонкий этап для вычисления внимания только внутри важных областей. Метод обучается от начала до конца, не требует пост-обработки и сохраняет 85% эффективности по сравнению с полным вниманием. Эксперименты показывают, что VSA сокращает вычислительные затраты в 2,53 раза без потери качества, а также ускоряет генерацию в 6 раз для существующих моделей."
                },
                "en": {
                    "title": "Efficient Attention for Scalable Video Diffusion",
                    "desc": "This paper introduces VSA, a novel trainable sparse attention mechanism designed to improve the efficiency of video diffusion transformers (DiTs). By focusing on a small subset of critical tokens, VSA reduces the computational burden associated with traditional quadratic attention. The method consists of a coarse stage that pools tokens and identifies important ones, followed by a fine stage that computes attention only within these selected tokens. The results demonstrate that VSA significantly decreases training FLOPS while maintaining performance, making it a viable alternative for scaling video diffusion models."
                },
                "zh": {
                    "title": "可训练稀疏注意力：视频扩散模型的新选择",
                    "desc": "这篇论文提出了一种名为VSA的可训练稀疏注意力机制，旨在解决视频扩散变换器（DiTs）在处理3D注意力时的计算限制。VSA通过将注意力计算分为粗略阶段和精细阶段，显著提高了计算效率，同时保持了高效的训练性能。实验结果表明，VSA在不降低扩散损失的情况下，将训练的FLOPS减少了2.53倍，并且在开源模型Wan-2.1上实现了6倍的注意力计算加速。该研究表明，可训练的稀疏注意力是全注意力的有效替代方案，并为视频扩散模型的进一步扩展提供了关键支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12996",
            "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12996",
            "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.",
            "score": 1,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "56ff8af5ab05144f",
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12996.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#rl",
                    "#low_resource",
                    "#translation"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Революция в машинном переводе: от одноязычного к многоязычному совершенству",
                    "desc": "Статья описывает новый метод моделирования вознаграждения для обучения с подкреплением в нейронном машинном переводе. Авторы используют сравнение результатов перевода с сильной моделью рассуждений (LRM) для формирования вознаграждений. Эксперименты показывают превосходство этого метода, достигая нового уровня качества в литературном переводе. Метод успешно расширен на многоязычный перевод для 11 языков, демонстрируя впечатляющие результаты в 90 направлениях перевода."
                },
                "en": {
                    "title": "Revolutionizing Multilingual Translation with Advanced Reward Modeling",
                    "desc": "This paper discusses advancements in large reasoning models (LRMs) for neural machine translation (MT), particularly focusing on improving translation capabilities through reinforcement learning (RL). The authors introduce a novel reward modeling method that evaluates translation quality by comparing outputs from a policy MT model against a powerful LRM, DeepSeek-R1-671B. Their approach not only enhances performance in literary translation but also extends to 11 languages, achieving state-of-the-art results and surpassing existing LRMs. The lightweight reward modeling allows for effective transfer of translation skills across multiple languages, demonstrating significant improvements in multilingual MT performance."
                },
                "zh": {
                    "title": "强化学习助力多语言翻译新突破",
                    "desc": "近年来，大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在复杂问题上展现了出色的能力，尤其是在数学和编程方面。一些开创性研究尝试将LRMs的成功应用于神经机器翻译（MT），并通过强化学习（RL）构建具有深度推理能力的MT模型。尽管取得了一些进展，但这些研究主要集中在高资源语言上，如英语和中文，其他语言的表现仍不明确。此外，我们设计了一种新的奖励建模方法，通过与强大的LRM（如DeepSeek-R1-671B）比较翻译结果，为MT模型提供奖励，从而充分发挥强化学习的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12257",
            "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas",
            "url": "https://huggingface.co/papers/2505.12257",
            "abstract": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",
            "score": 0,
            "issue_id": 3849,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "74901d316cc1d6cb",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12257.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности LLM в обнаружении ошибок через структурированную настройку контекста",
                    "desc": "Это исследование изучает структурированный подход к настройке контекста больших языковых моделей (LLM) для улучшения их способности выявлять технические ошибки в сложных научных документах. Методология основана на принципах устойчивого рабочего процесса запросов (PWP) и направлена на повышение надежности общедоступных LLM для задач точной валидации. Эксперименты показали, что адаптированный PWP-подход улучшил идентификацию текстовых ошибок и даже позволил модели Gemini 2.5 Pro обнаружить скрытую ошибку в формуле на изображении. Результаты указывают на потенциал этого метода для разработки более надежных аналитических рабочих процессов на основе LLM, особенно для задач, требующих тщательного обнаружения ошибок в научно-технических документах."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy in Scientific Error Detection with PWP",
                    "desc": "This paper explores how to improve Large Language Models (LLMs) in identifying subtle errors in complex scientific documents, especially those with images and formulas. It introduces a method called Persistent Workflow Prompting (PWP) to better condition LLMs during their analysis, enhancing their ability to detect inaccuracies. The study specifically tests this approach on Gemini 2.5 Pro and ChatGPT Plus o3, showing that PWP can significantly improve error detection compared to basic prompting strategies. The findings suggest that this method could lead to more reliable LLMs for validating technical content, although further research is needed to confirm its effectiveness across different contexts."
                },
                "zh": {
                    "title": "提升LLM在科学文档中的错误识别能力",
                    "desc": "本研究探讨了如何利用结构化的上下文条件来改善大型语言模型（LLMs）在复杂科学和技术文档中的错误识别能力。研究采用了持久工作提示（PWP）原则，旨在提高LLMs在推理时的表现，尤其是在验证化学公式时。通过对不同提示策略的评估，发现适应PWP结构的提示能够有效提高文本错误的识别率，并成功发现了之前未被手动审查识别的图像公式错误。该方法为开发更强大的LLM驱动分析工作流提供了有希望的技术，尤其是在需要细致错误检测的任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11988",
            "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text",
            "url": "https://huggingface.co/papers/2505.11988",
            "abstract": "Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.",
            "score": 0,
            "issue_id": 3850,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "95b404534e69c826",
            "authors": [
                "Ahmed Lekssays",
                "Utsav Shukla",
                "Husrev Taha Sencar",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Independent Researcher",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11988.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#data",
                    "#hallucinations",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Точное распознавание техник злоумышленников с минимумом данных",
                    "desc": "TechniqueRAG - это новая система для идентификации техник противников в текстах по кибербезопасности. Она использует извлечение информации и генеративные языковые модели, обученные на небольшом количестве примеров. TechniqueRAG применяет переранжирование с помощью LLM для улучшения релевантности извлеченной информации. Эксперименты показывают, что система достигает наилучших результатов без необходимости в больших размеченных датасетах."
                },
                "en": {
                    "title": "Enhancing Cyber Defense with TechniqueRAG: Precision without Excessive Resources",
                    "desc": "This paper introduces TechniqueRAG, a novel framework designed to improve the identification of adversarial techniques in cybersecurity texts. It combines retrieval-augmented generation (RAG) with instruction-tuned large language models (LLMs) to enhance domain specificity while minimizing the need for extensive labeled datasets. By fine-tuning only the generation component and employing zero-shot LLM re-ranking, TechniqueRAG improves the quality of retrieved candidates, ensuring they are more relevant to the specific domain of adversarial techniques. The results show that TechniqueRAG outperforms existing methods, achieving high accuracy without the heavy resource demands typically associated with task-specific optimizations."
                },
                "zh": {
                    "title": "提升安全文本对抗技术识别的创新框架",
                    "desc": "本文提出了一种名为TechniqueRAG的框架，旨在提高对安全文本中对抗性技术的识别能力。该框架结合了现成的检索器、经过指令调优的大型语言模型（LLM）和最小的文本-技术对，解决了数据稀缺的问题。通过仅对生成组件进行微调，TechniqueRAG避免了对资源密集型检索训练的依赖，同时通过零-shot LLM重排序提高了检索质量和领域特异性。实验结果表明，TechniqueRAG在多个安全基准上实现了最先进的性能，无需大量特定任务的优化或标记数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03332",
            "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
            "url": "https://huggingface.co/papers/2505.03332",
            "abstract": "Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",
            "score": 0,
            "issue_id": 3849,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "9c52936c2b9a7443",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03332.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "PWP: Новый подход к критическому анализу научных работ с помощью LLM",
                    "desc": "Статья представляет новый метод инженерии промптов под названием Persistent Workflow Prompting (PWP) для улучшения способности больших языковых моделей (LLM) проводить критический анализ научных рукописей. PWP использует иерархическую модульную архитектуру для определения детальных рабочих процессов анализа, которые сохраняются в течение сессии. Метод был разработан с помощью итеративного применения мета-промптинга и мета-рассуждений для систематической кодификации экспертных процессов рецензирования. Демонстрации показывают, что PWP-управляемая LLM способна выявлять серьезные методологические недостатки и выполнять сложные задачи анализа."
                },
                "en": {
                    "title": "Empowering LLMs for Expert Scientific Review with Persistent Workflow Prompting",
                    "desc": "This paper addresses the challenges faced by Large Language Models (LLMs) in performing critical peer reviews of scientific manuscripts, particularly due to data limitations and the intricacies of expert reasoning. It introduces a new methodology called Persistent Workflow Prompting (PWP), which allows users to create structured prompts that guide LLMs through detailed analysis workflows without needing coding skills. The PWP framework is designed to help LLMs systematically evaluate scientific content by integrating various forms of data, such as text and images, to identify flaws and assess claims. The authors demonstrate the effectiveness of PWP in analyzing experimental chemistry manuscripts, showcasing its ability to enhance LLM performance in complex scientific evaluations."
                },
                "zh": {
                    "title": "持久工作流提示：提升科学评审的智能化",
                    "desc": "这篇论文介绍了一种新的提示工程方法，称为持久工作流提示（PWP），旨在帮助大型语言模型（LLMs）进行科学手稿的批判性同行评审。PWP通过标准的LLM聊天界面，使用分层模块化的架构，定义详细的分析工作流程，能够系统化地编码专家评审的工作流程。通过迭代的元提示技术和元推理，PWP能够引导LLM进行多模态评估，识别实验化学手稿中的主要方法论缺陷。该方法不仅提供了具体应用的示例，还展示了如何利用现有的LLM进行复杂科学任务的深入分析。"
                }
            }
        }
    ],
    "link_prev": "2025-05-19.html",
    "link_next": "2025-05-21.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "19.05",
        "en": "05/19",
        "zh": "5月19日"
    },
    "short_date_next": {
        "ru": "21.05",
        "en": "05/21",
        "zh": "5月21日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 6,
        "#benchmark": 10,
        "#agents": 1,
        "#cv": 3,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 5,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 11,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 13,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1,
        "#translation": 1
    },
    "zh": {
        "text": "这篇文章介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包括多种大型语言模型，旨在提高性能、效率和多语言能力。它包含密集和混合专家架构，参数规模从0.6到2350亿不等。Qwen3的创新之处在于将思考模式和非思考模式结合在一个框架中，消除了切换模型的需要。它还引入了思考预算机制，允许用户根据任务复杂性动态分配计算资源。",
        "title": "Qwen3 Technical Report",
        "pinyin": "这篇文章介绍了Qwen3，这是Qwen模型系列的最新版本。\nZhè piān wénzhāng jièshào le Qwen3, zhè shì Qwen móxíng xìliè de zuìxīn bǎnběn.\n\nQwen3包括多种大型语言模型，旨在提高性能、效率和多语言能力。\nQwen3 bāokuò duōzhǒng dàxíng yǔyán móxíng, zhǐ zài tígāo xìngnéng, xiàolǜ hé duōyǔyán nénglì.\n\n它包含密集和混合专家架构，参数规模从0.6到2350亿不等。\nTā bāohán mìjí hé hùnhé zhuānjiā jiàgòu, cānshù guīmó cóng 0.6 dào 2350 yì bùděng.\n\nQwen3的创新之处在于将思考模式和非思考模式结合在一个框架中，消除了切换模型的需要。\nQwen3 de chuàngxīn zhī chù zài yú jiāng sīkǎo móshì hé fēi sīkǎo móshì jiéhé zài yīgè kuàngjià zhōng, xiāochú le qiēhuàn móxíng de xūyào.\n\n它还引入了思考预算机制，允许用户根据任务复杂性动态分配计算资源。\nTā hái yǐnrù le sīkǎo yùsuàn jīzhì, yǔnxǔ yònghù gēnjù rènwù fùzáxìng dòngtài fēnpèi jìsuàn zīyuán.",
        "vocab": "[\n    {\"word\": \"系列\", \"pinyin\": \"xìliè\", \"trans\": \"series\"},\n    {\"word\": \"版本\", \"pinyin\": \"bǎnběn\", \"trans\": \"version\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duōyǔyán\", \"trans\": \"multilingual\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"包含\", \"pinyin\": \"bāohán\", \"trans\": \"contain\"},\n    {\"word\": \"密集\", \"pinyin\": \"mìjí\", \"trans\": \"dense\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùnhé\", \"trans\": \"hybrid\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuānjiā\", \"trans\": \"expert\"},\n    {\"word\": \"架构\", \"pinyin\": \"jiàgòu\", \"trans\": \"architecture\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameter\"},\n    {\"word\": \"规模\", \"pinyin\": \"guīmó\", \"trans\": \"scale\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovation\"},\n    {\"word\": \"之处\", \"pinyin\": \"zhīchù\", \"trans\": \"place\"},\n    {\"word\": \"思考\", \"pinyin\": \"sīkǎo\", \"trans\": \"think\"},\n    {\"word\": \"模式\", \"pinyin\": \"móshì\", \"trans\": \"mode\"},\n    {\"word\": \"结合\", \"pinyin\": \"jiéhé\", \"trans\": \"combine\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"消除\", \"pinyin\": \"xiāochú\", \"trans\": \"eliminate\"},\n    {\"word\": \"切换\", \"pinyin\": \"qiēhuàn\", \"trans\": \"switch\"},\n    {\"word\": \"需要\", \"pinyin\": \"xūyào\", \"trans\": \"need\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"预算\", \"pinyin\": \"yùsuàn\", \"trans\": \"budget\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"允许\", \"pinyin\": \"yǔnxǔ\", \"trans\": \"allow\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēnjù\", \"trans\": \"according to\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"复杂性\", \"pinyin\": \"fùzáxìng\", \"trans\": \"complexity\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamic\"},\n    {\"word\": \"分配\", \"pinyin\": \"fēnpèi\", \"trans\": \"allocate\"},\n    {\"word\": \"计算\", \"pinyin\": \"jìsuàn\", \"trans\": \"compute\"},\n    {\"word\": \"资源\", \"pinyin\": \"zīyuán\", \"trans\": \"resources\"}\n]",
        "trans": "This article introduces Qwen3, the latest version in the Qwen model series. Qwen3 includes a variety of large language models aimed at enhancing performance, efficiency, and multilingual capabilities. It features dense and mixture-of-experts architectures, with parameter sizes ranging from 0.6 to 2350 billion. The innovation of Qwen3 lies in combining thinking and non-thinking modes within a single framework, eliminating the need to switch models. It also introduces a thinking budget mechanism, allowing users to dynamically allocate computational resources based on the complexity of the task.",
        "update_ts": "2025-05-19 09:13"
    }
}