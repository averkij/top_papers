{
    "date": {
        "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 9",
        "zh": "10æœˆ9æ—¥"
    },
    "time_utc": "2025-10-09 16:15",
    "weekday": 3,
    "issue_id": 6335,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.06590",
            "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
            "url": "https://huggingface.co/papers/2510.06590",
            "abstract": "MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.",
            "score": 54,
            "issue_id": 6328,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "c05ff2a1615b401f",
            "authors": [
                "Ziyuan Huang",
                "DanDan Zheng",
                "Cheng Zou",
                "Rui Liu",
                "Xiaolong Wang",
                "Kaixiang Ji",
                "Weilong Chai",
                "Jianxin Sun",
                "Libin Wang",
                "Yongjie Lv",
                "Taozhi Huang",
                "Jiajia Liu",
                "Qingpei Guo",
                "Ming Yang",
                "Jingdong Chen",
                "Jun Zhou"
            ],
            "affiliations": [
                "Ant Group",
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06590.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MingTok: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "MingTok â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‚ĞµÑ€ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, MingTok Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, MingTok Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MingTok: Unifying Vision and Language with Continuous Tokenization",
                    "desc": "MingTok is a new visual tokenizer that uses a continuous latent space to improve how machines understand and generate images and text together. Traditional methods use discrete tokenizers, which can create errors that limit how well machines can understand visual information. MingTok introduces a three-stage process that first encodes low-level features, then expands them semantically, and finally reconstructs the visuals, allowing for better performance in both understanding and generating tasks. This approach enables a unified framework for various vision-language tasks, achieving state-of-the-art results by treating both understanding and generation as next-token predictions in a shared space."
                },
                "zh": {
                    "title": "MingTokï¼šç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ ‡è®°å™¨",
                    "desc": "MingTokæ˜¯ä¸€ç§è¿ç»­æ½œåœ¨ç©ºé—´çš„è§†è§‰æ ‡è®°å™¨ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚å®ƒé€šè¿‡è‡ªå›å½’æ¡†æ¶å®ç°äº†åœ¨è¿™ä¸¤ä¸ªé¢†åŸŸçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚MingToké‡‡ç”¨ä¸‰é˜¶æ®µçš„æ¶æ„ï¼Œåˆ†åˆ«è¿›è¡Œä½çº§ç¼–ç ã€è¯­ä¹‰æ‰©å±•å’Œè§†è§‰é‡å»ºï¼Œä»¥æ»¡è¶³ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„ä¸åŒéœ€æ±‚ã€‚é€šè¿‡åœ¨å…±äº«çš„è¿ç»­ç©ºé—´ä¸­å°†ç†è§£å’Œç”Ÿæˆä»»åŠ¡éƒ½è§†ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ŒMingTokæ”¯æŒå¤šè½®ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå¦‚è¿­ä»£ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03215",
            "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.03215",
            "abstract": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.",
            "score": 53,
            "issue_id": 6321,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "3453eb2a78f90630",
            "authors": [
                "Tianyu Fu",
                "Zihan Min",
                "Hanling Zhang",
                "Jichao Yan",
                "Guohao Dai",
                "Wanli Ouyang",
                "Yu Wang"
            ],
            "affiliations": [
                "Infinigence AI",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03215.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#agi"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±Ñ‰ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ²: Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑÑˆ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Cache-to-Cache (C2C) Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· KV-Cache Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºÑÑˆĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8.5-10.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ° 3.0-5.0% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ C2C Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Direct Semantic Communication for Enhanced LLM Collaboration",
                    "desc": "Cache-to-Cache (C2C) introduces a novel method for Large Language Models (LLMs) to communicate directly using their internal KV-caches instead of relying on text. This approach enhances the semantic richness of the communication, allowing models to share information more effectively and efficiently. By employing a neural network to project and merge the KV-caches, C2C minimizes the loss of information and reduces latency associated with text-based exchanges. Experimental results demonstrate that C2C improves accuracy by 8.5-10.5% and speeds up communication by approximately 2.0x compared to traditional methods."
                },
                "zh": {
                    "title": "ç›´æ¥è¯­ä¹‰é€šä¿¡ï¼Œæå‡æ¨¡å‹æ•ˆç‡",
                    "desc": "Cache-to-Cache (C2C) æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´ç›´æ¥è¿›è¡Œè¯­ä¹‰é€šä¿¡ã€‚é€šè¿‡ç¥ç»ç½‘ç»œæŠ•å½±ï¼ŒC2C å¯ä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘å»¶è¿Ÿï¼Œé¿å…äº†ä¼ ç»Ÿæ–‡æœ¬é€šä¿¡ä¸­ä¿¡æ¯æŸå¤±å’Œé€å­—ç”Ÿæˆçš„å»¶è¿Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆæºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„KVç¼“å­˜ï¼Œç›´æ¥ä¼ é€’è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹é—´äº¤æµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC2C åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ–‡æœ¬é€šä¿¡æ–¹å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06308",
            "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
            "url": "https://huggingface.co/papers/2510.06308",
            "abstract": "Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
            "score": 35,
            "issue_id": 6322,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "7ea946e09493c01b",
            "authors": [
                "Yi Xin",
                "Qi Qin",
                "Siqi Luo",
                "Kaiwen Zhu",
                "Juncheng Yan",
                "Yan Tai",
                "Jiayi Lei",
                "Yuewen Cao",
                "Keqi Wang",
                "Yibin Wang",
                "Jinbin Bai",
                "Qian Yu",
                "Dengyang Jiang",
                "Yuandong Pu",
                "Haoxing Chen",
                "Le Zhuo",
                "Junjun He",
                "Gen Luo",
                "Tianbin Li",
                "Ming Hu",
                "Jin Ye",
                "Shenglong Ye",
                "Bo Zhang",
                "Chang Xu",
                "Wenhai Wang",
                "Hongsheng Li",
                "Guangtao Zhai",
                "Tianfan Xue",
                "Bin Fu",
                "Xiaohong Liu",
                "Yu Qiao",
                "Yihao Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Sydney",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06308.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Lumina-DiMOO - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ foundational Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… unified Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ autoregressive Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multi-Modal Generation with Lumina-DiMOO",
                    "desc": "Lumina-DiMOO is an open-source foundational model designed for efficient multi-modal generation and understanding. It employs fully discrete diffusion modeling, which distinguishes it from traditional autoregressive and hybrid models. This innovative technique enhances sampling efficiency and supports a wide range of tasks, such as text-to-image generation and image editing. By achieving state-of-the-art results on various benchmarks, Lumina-DiMOO aims to advance research in multi-modal and discrete diffusion models."
                },
                "zh": {
                    "title": "Lumina-DiMOOï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆä¸ç†è§£æ¨¡å‹",
                    "desc": "Lumina-DiMOOæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨å®Œå…¨ç¦»æ•£çš„æ‰©æ•£å»ºæ¨¡æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚ä¸ä¹‹å‰çš„ç»Ÿä¸€æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å¤„ç†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºæ—¶è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ç­‰å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°äº†æ›´é«˜çš„é‡‡æ ·æ•ˆç‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ¨åŠ¨å¤šæ¨¡æ€å’Œç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶è¿›å±•ï¼Œæˆ‘ä»¬å°†ä»£ç å’Œæ£€æŸ¥ç‚¹å‘å¸ƒç»™ç¤¾åŒºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06917",
            "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
            "url": "https://huggingface.co/papers/2510.06917",
            "abstract": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  \t\t\t\t\tAI-generated summary \t\t\t\t Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
            "score": 29,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "8587217f59423924",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06917.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#inference",
                    "#long_context"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¿Ğ¾ĞºĞ° ÑĞ»ÑƒÑˆĞ°ĞµÑˆÑŒ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SHANKS â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ spoken language models (SLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° Ğ½Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SHANKS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ° 37.1% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ 56.9% Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "SHANKS: Real-Time Reasoning for Smarter Conversations",
                    "desc": "The paper introduces SHANKS, a novel inference framework designed for spoken language models (SLMs) that allows them to engage in unspoken reasoning while listening to user input. This approach addresses the limitation of current large language models, which only process information after the user has finished speaking, leading to delays in interaction. By streaming input in chunks, SHANKS enables real-time reasoning and decision-making, allowing the model to interrupt users when necessary and complete tasks more efficiently. The results show significant improvements in interruption accuracy and tool call completion, demonstrating the potential for more dynamic and responsive conversational AI."
                },
                "zh": {
                    "title": "å®æ—¶æ€è€ƒï¼Œæå‡äº¤äº’æ•ˆç‡",
                    "desc": "SHANKSæ˜¯ä¸€ä¸ªé€šç”¨æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·è¾“å…¥æ—¶çš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨ç”¨æˆ·è¯´è¯æ—¶ç”Ÿæˆæœªè¯´å‡ºçš„æ¨ç†ï¼Œä»è€Œå‡å°‘å“åº”å»¶è¿Ÿã€‚SHANKSé€šè¿‡å°†è¾“å…¥è¯­éŸ³åˆ†æˆå›ºå®šæ—¶é•¿çš„å—è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿåœ¨æ¥æ”¶æ¯ä¸ªå—æ—¶è¿›è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒSHANKSåœ¨ç”¨æˆ·è§£å†³æ•°å­¦é—®é¢˜æ—¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¸­æ–­ç”¨æˆ·ï¼Œå¹¶åœ¨å¯¹è¯ä¸­æå‰å®Œæˆå·¥å…·è°ƒç”¨ï¼Œæ˜¾è‘—æé«˜äº†äº¤äº’æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06710",
            "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
            "url": "https://huggingface.co/papers/2510.06710",
            "abstract": "RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.",
            "score": 27,
            "issue_id": 6327,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "61d9506100fc9423",
            "authors": [
                "Hongzhi Zang",
                "Mingjie Wei",
                "Si Xu",
                "Yongji Wu",
                "Zhen Guo",
                "Yuanqing Wang",
                "Hao Lin",
                "Liangzhi Shi",
                "Yuqing Xie",
                "Zhexuan Xu",
                "Zhihao Liu",
                "Kang Chen",
                "Wenhao Tang",
                "Quanlu Zhang",
                "Weinan Zhang",
                "Chao Yu",
                "Yu Wang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Infinigence AI",
                "Institute of Automation, Chinese Academy of Sciences",
                "Peking University",
                "Tsinghua University",
                "UC Berkeley",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06710.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ: RL Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ supervised learning",
                    "desc": "RLinf-VLA â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ supervised fine-tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, RL Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ RL (PPO, GRPO) Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 98.11% Ğ½Ğ° 130 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… LIBERO Ğ¸ 97.66% Ğ½Ğ° 25 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ManiSkill. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Franka Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ supervised fine-tuning."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language-Action Training",
                    "desc": "RLinf-VLA is a new framework designed to enhance the training of vision-language-action (VLA) models using reinforcement learning (RL). Unlike traditional supervised fine-tuning, which can struggle with generalization, RLinf-VLA optimizes model performance through direct interaction with tasks. The framework supports various VLA architectures and RL algorithms, allowing for efficient training and faster processing times. Initial results show that models trained with RLinf-VLA outperform those trained with supervised methods, demonstrating better adaptability in real-world applications."
                },
                "zh": {
                    "title": "RLinf-VLAï¼šåŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
                    "desc": "RLinf-VLAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç›¸æ¯”äºç›‘ç£å¾®è°ƒï¼Œå®ƒæä¾›äº†æ›´å¥½çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨RL+VLAè®­ç»ƒä¸­æ•´åˆæ¸²æŸ“ã€è®­ç»ƒå’Œæ¨ç†çš„æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡çµæ´»çš„èµ„æºåˆ†é…è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒã€‚RLinf-VLAæ”¯æŒå¤šç§VLAæ¶æ„å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡æ‹Ÿä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ€»ç»“äº†ä¸€ç³»åˆ—æœ€ä½³å®è·µï¼Œå¸®åŠ©å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºVLAè®­ç»ƒï¼Œå¹¶å±•ç¤ºäº†åœ¨çœŸå®æœºå™¨äººä¸Šçš„åˆæ­¥éƒ¨ç½²æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07310",
            "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
            "url": "https://huggingface.co/papers/2510.07310",
            "abstract": "MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
            "score": 26,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "1dfefa942d4dfe75",
            "authors": [
                "Siyoon Jin",
                "Seongchan Kim",
                "Dahyun Chung",
                "Jaeho Lee",
                "Hyunwook Choi",
                "Jisu Nam",
                "Jiyoung Kim",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07310.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#video",
                    "#interpretability"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¼Ğ°ÑĞºĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MATRIX-11K Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞºĞ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, ĞºĞ°Ğº video DiT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² semantic grounding (ÑĞ²ÑĞ·ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸) Ğ¸ semantic propagation (ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MATRIX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ hallucination Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Generation with MATRIX Regularization",
                    "desc": "This paper introduces the MATRIX-11K dataset, which includes interaction-aware captions and multi-instance mask tracks to improve video generation models known as video DiTs. The authors analyze how these models represent interactions through two main perspectives: semantic grounding and semantic propagation, focusing on how well they capture and maintain relationships between objects over time. They propose a new regularization technique called MATRIX that aligns the attention mechanisms in specific layers of video DiTs with the multi-instance mask tracks from their dataset. The results show that MATRIX enhances interaction fidelity and semantic alignment, leading to better video generation outcomes while minimizing issues like drift and hallucination."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„äº¤äº’ä¿çœŸåº¦ä¸è¯­ä¹‰å¯¹é½",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†MATRIX-11Kæ•°æ®é›†å’ŒMATRIXæ­£åˆ™åŒ–å¦‚ä½•æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆDiTsï¼‰åœ¨äº¤äº’ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šå®ä¾‹æˆ–ä¸»ä½“-å¯¹è±¡äº¤äº’æ—¶å­˜åœ¨å›°éš¾ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«äº¤äº’æ„è¯†å­—å¹•å’Œå¤šå®ä¾‹æ©ç è½¨è¿¹çš„æ•°æ®é›†ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰åŸºç¡€å’Œè¯­ä¹‰ä¼ æ’­ä¸¤ä¸ªè§†è§’ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘åˆ°æ–‡æœ¬å’Œè§†é¢‘åˆ°è§†é¢‘çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æœ€ç»ˆï¼ŒMATRIXæ­£åˆ™åŒ–é€šè¿‡å¯¹é½ç‰¹å®šå±‚çš„æ³¨æ„åŠ›ä¸å¤šå®ä¾‹æ©ç è½¨è¿¹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07315",
            "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
            "url": "https://huggingface.co/papers/2510.07315",
            "abstract": "Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
            "score": 25,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "664f235019a59971",
            "authors": [
                "Ming Zhong",
                "Xiang Zhou",
                "Ting-Yun Chang",
                "Qingze Wang",
                "Nan Xu",
                "Xiance Si",
                "Dan Garrette",
                "Shyam Upadhyay",
                "Jeremiah Liu",
                "Jiawei Han",
                "Benoit Schillings",
                "Jiao Sun"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "University of Illinois Urbana-Champaign (UIUC)",
                "University of Southern California (USC)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07315.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Vibe Check: ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ´ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Vibe Checker â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VeriCode â€” Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸Ğ· 30 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 31 Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´ Â«Ğ¾Ñ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Â» â€” Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¼Ñ‹ÑĞµĞ»."
                },
                "en": {
                    "title": "Aligning AI with Human Coding Preferences",
                    "desc": "This paper introduces Vibe Checker, a new method for evaluating Large Language Models (LLMs) that combines functional correctness with instruction following to align better with human coding preferences. It highlights the importance of not just passing functional tests but also ensuring that code feels right and meets user expectations. The authors present VeriCode, a taxonomy of 30 verifiable code instructions, to measure how well models follow these instructions. Their findings show that a composite score of functional correctness and instruction following is a better predictor of human preference in coding tasks, revealing that instruction following is crucial for improving LLM performance in real-world applications."
                },
                "zh": {
                    "title": "Vibe Checkerï¼šæ›´è´´è¿‘äººç±»ç¼–ç åå¥½çš„è¯„ä¼°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVibe Checkerçš„è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚Vibe Checkerç»“åˆäº†åŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œæ—¨åœ¨æ›´å¥½åœ°ç¬¦åˆäººç±»çš„ç¼–ç åå¥½ã€‚æˆ‘ä»¬å¼•å…¥äº†VeriCodeï¼Œä¸€ä¸ªåŒ…å«30ä¸ªå¯éªŒè¯ä»£ç æŒ‡ä»¤çš„åˆ†ç±»æ³•ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„ç»¼åˆå¾—åˆ†ä¸äººç±»åå¥½é«˜åº¦ç›¸å…³ï¼Œåè€…åœ¨å®é™…ç¼–ç¨‹ä»»åŠ¡ä¸­æˆä¸ºä¸»è¦çš„åŒºåˆ†å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04678",
            "title": "Multi-Agent Tool-Integrated Policy Optimization",
            "url": "https://huggingface.co/papers/2510.04678",
            "abstract": "MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
            "score": 16,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "a8d8251d93e429c4",
            "authors": [
                "Zhanfeng Mo",
                "Xingxuan Li",
                "Yuntao Chen",
                "Lidong Bing"
            ],
            "affiliations": [
                "MiroMind AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04678.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ñ†ĞµĞ»Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MATPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ»Ğ¸Ğ±Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. MATPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18,38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Multi-Agent Roles in LLMs for Enhanced Performance",
                    "desc": "MATPO is a novel reinforcement learning approach that enhances the performance of large language models (LLMs) by integrating multiple agent roles within a single model. It introduces a planner-worker framework, where the planner strategizes and the worker executes tasks, allowing for better management of context and tool responses. This method leverages a credit assignment mechanism to optimize the training of these roles, avoiding the need for multiple LLMs and thus saving memory. Experimental results demonstrate that MATPO significantly improves performance and robustness compared to traditional single-agent systems, making it a promising solution for complex reasoning tasks."
                },
                "zh": {
                    "title": "å¤šä»£ç†å·¥å…·é›†æˆä¼˜åŒ–ï¼Œæå‡LLMæ€§èƒ½ä¸é²æ£’æ€§",
                    "desc": "MATPOæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å•ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤šä»£ç†è§’è‰²ï¼Œæå‡å…¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è§’è‰²ç‰¹å®šçš„æç¤ºï¼Œå…è®¸è§„åˆ’è€…å’Œå·¥ä½œè€…åœ¨åŒä¸€LLMå®ä¾‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†ç°æœ‰å•ä»£ç†ç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡é•¿åº¦å’Œå·¥å…·å“åº”å™ªå£°æ–¹é¢çš„å±€é™ã€‚MATPOé‡‡ç”¨äº†ä¸€ç§åŸåˆ™æ€§çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œé¿å…äº†éƒ¨ç½²å¤šä¸ªLLMæ‰€éœ€çš„é«˜å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸“ä¸šåŒ–çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMATPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºå•ä»£ç†åŸºçº¿å¹³å‡æé«˜äº†18.38%çš„æ€§èƒ½ï¼Œå¹¶å¯¹å™ªå£°å·¥å…·è¾“å‡ºè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04204",
            "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization\n  Modeling",
            "url": "https://huggingface.co/papers/2510.04204",
            "abstract": "CALM framework uses expert interventions to refine LRM reasoning for optimization tasks, achieving high accuracy with fewer modifications compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional non-reflective datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose CALM (Corrective Adaptation with Lightweight Modification), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop STORM (Smart Thinking Optimization Reasoning Model), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
            "score": 16,
            "issue_id": 6332,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "b7f79f325ea1e236",
            "authors": [
                "Zhengyang Tang",
                "Zihan Ye",
                "Chenyu Huang",
                "Xuhan Huang",
                "Chengpeng Li",
                "Sihang Li",
                "Guanhua Chen",
                "Ming Yan",
                "Zizhuo Wang",
                "Hongyuan Zha",
                "Dayiheng Liu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc.",
                "Shanghai University of Finance and Economics",
                "Shenzhen Loop Area Institute (SLAI)",
                "Southern University of Science and Technology",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04204.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CALM â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… reasoning-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LRM) Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 2.6% ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CALM ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ STORM Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 68.9% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑ€Ğ°Ğ²Ğ½ÑĞ²ÑˆĞ¸ÑÑŒ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² 671 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Refining Reasoning with Expert Hints for Optimization Success",
                    "desc": "The CALM framework enhances Large Reasoning Models (LRMs) by using expert interventions to correct reasoning errors during optimization tasks. It allows LRMs to maintain their advanced reasoning capabilities while making minimal modifications to their outputs. By incorporating concise corrective hints from experts, CALM enables the model to generate high-quality data for further training through supervised fine-tuning. This approach leads to significant improvements in accuracy, demonstrating a more effective method for adapting LRMs to complex optimization challenges."
                },
                "zh": {
                    "title": "CALMæ¡†æ¶ï¼šä¼˜åŒ–æ¨ç†çš„æ™ºèƒ½å¹²é¢„",
                    "desc": "CALMæ¡†æ¶é€šè¿‡ä¸“å®¶å¹²é¢„æ¥ä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¼˜åŒ–ä»»åŠ¡ä¸­å®ç°é«˜å‡†ç¡®ç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCALMåœ¨ä¿®æ”¹æ–¹é¢çš„éœ€æ±‚æ›´å°‘ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨LRMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯†åˆ«æ¨ç†ç¼ºé™·å¹¶æä¾›ç®€æ´çš„çº æ­£æç¤ºï¼Œå¸®åŠ©LRMç”Ÿæˆæ›´ä¼˜è´¨çš„æ¨ç†è½¨è¿¹ã€‚æœ€ç»ˆï¼ŒåŸºäºCALMçš„STORMæ¨¡å‹åœ¨å¤šä¸ªä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€é«˜å¹³å‡å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†åŠ¨æ€æç¤ºæ•°æ®åˆæˆçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04212",
            "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
            "url": "https://huggingface.co/papers/2510.04212",
            "abstract": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
            "score": 14,
            "issue_id": 6321,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "e0a5e1e23247359f",
            "authors": [
                "Haiquan Qiu",
                "Quanming Yao"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04212.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¥",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ²: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¸, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ flash attention Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ²ÑƒÑ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… low-rank Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºÑ€ÑƒĞ³ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºĞ°Ğ¶Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ flash attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Stabilizing Low-Precision Training in Transformers",
                    "desc": "This paper addresses the challenges of training transformer models using low-precision formats, which often lead to significant training instabilities. It identifies the root cause of catastrophic loss explosions during low-precision training with flash attention, linking it to low-rank representations and biased rounding errors. The authors explain how these issues create a cycle of error accumulation that disrupts weight updates and training dynamics. To resolve this, they propose a minimal modification to the flash attention mechanism that reduces rounding bias, stabilizing the training process effectively."
                },
                "zh": {
                    "title": "ä½ç²¾åº¦è®­ç»ƒä¸­çš„é—ªå­˜æ³¨æ„åŠ›ç¨³å®šæ€§è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨ä½ç²¾åº¦è®­ç»ƒå˜æ¢å™¨æ¨¡å‹æ—¶ï¼Œä½¿ç”¨é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶æ‰€é¢ä¸´çš„ç¾éš¾æ€§æŸå¤±çˆ†ç‚¸é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§é—®é¢˜å¹¶éå¶ç„¶ï¼Œè€Œæ˜¯ç”±äºæ³¨æ„åŠ›æœºåˆ¶ä¸­å‡ºç°çš„ç›¸ä¼¼ä½ç§©è¡¨ç¤ºå’Œä½ç²¾åº¦ç®—æœ¯ä¸­å›ºæœ‰çš„åå·®èˆå…¥è¯¯å·®ç›¸äº’ä½œç”¨æ‰€å¯¼è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹é—ªå­˜æ³¨æ„åŠ›æœºåˆ¶çš„æœ€å°ä¿®æ”¹ï¼Œèƒ½å¤Ÿå‡è½»èˆå…¥è¯¯å·®çš„åå·®ï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€ç®€å•çš„æ”¹åŠ¨ï¼Œæˆ‘ä»¬éªŒè¯äº†åˆ†æç»“æœï¼Œå¹¶ä¸ºè¿™ä¸€é•¿æœŸå­˜åœ¨çš„é—®é¢˜æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06751",
            "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
            "url": "https://huggingface.co/papers/2510.06751",
            "abstract": "OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
            "score": 13,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "719c136fdd131e9e",
            "authors": [
                "Junhan Zhu",
                "Hesong Wang",
                "Mingluo Su",
                "Zefang Wang",
                "Huan Wang"
            ],
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06751.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´",
                    "desc": "OBS-Diff â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (pruning) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Optimal Brain Surgeon Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ñ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ²ĞµÑ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Pruning for Powerful Diffusion Models",
                    "desc": "OBS-Diff is a new framework designed to efficiently prune large-scale text-to-image diffusion models, which are typically expensive to run. Traditional one-shot pruning methods struggle with these models due to their complex iterative denoising processes. This paper introduces a modified version of the Optimal Brain Surgeon technique, allowing for various levels of pruning while maintaining model performance. By focusing on the error accumulation during the diffusion process and implementing a smart pruning strategy, OBS-Diff significantly speeds up inference with little loss in image quality."
                },
                "zh": {
                    "title": "OBS-Diffï¼šé«˜æ•ˆå‹ç¼©æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "OBS-Diffæ˜¯ä¸€ç§æ–°é¢–çš„ä¸€æ¬¡æ€§å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨å‹ç¼©å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„è´¨é‡æŸå¤±å’Œæ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚ç°æœ‰çš„ä¸€æ¬¡æ€§ç½‘ç»œå‰ªææ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å…·æœ‰è¿­ä»£å»å™ªçš„ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒOBS-Diffå¯¹ç»å…¸çš„æœ€ä½³è„‘å¤–ç§‘åŒ»ç”Ÿï¼ˆOBSï¼‰è¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿å…¶é€‚åº”ç°ä»£æ‰©æ•£æ¨¡å‹çš„å¤æ‚æ¶æ„ï¼Œå¹¶æ”¯æŒå¤šç§å‰ªæç²’åº¦ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æ­¥é•¿æ„ŸçŸ¥çš„Hessianæ„é€ å’Œé«˜æ•ˆçš„åˆ†ç»„é¡ºåºå‰ªæç­–ç•¥ï¼ŒOBS-Diffåœ¨è§†è§‰è´¨é‡æŸå¤±æœ€å°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ‰©æ•£æ¨¡å‹çš„æœ€å…ˆè¿›çš„ä¸€æ¬¡æ€§å‰ªæã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04230",
            "title": "Pushing on Multilingual Reasoning Models with Language-Mixed\n  Chain-of-Thought",
            "url": "https://huggingface.co/papers/2510.04230",
            "abstract": "A language-mixed chain-of-thought reasoning approach improves performance in Korean-specific tasks by switching between English and Korean, achieving state-of-the-art results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
            "score": 13,
            "issue_id": 6332,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "26e466401e9dcc0f",
            "authors": [
                "Guijin Son",
                "Donghun Yang",
                "Hitesh Laxmichand Patel",
                "Amit Agarwal",
                "Hyunwoo Ko",
                "Chanuk Lim",
                "Srikant Panda",
                "Minhyuk Kim",
                "Nikunj Drolia",
                "Dasol Choi",
                "Kyong-Ha Lee",
                "Youngjae Yu"
            ],
            "affiliations": [
                "KISTI",
                "Korea University",
                "Modulabs",
                "OneLineAI",
                "Oracle AI",
                "Seoul National University",
                "University College Dublin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04230.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#low_resource",
                    "#benchmark",
                    "#multilingual",
                    "#dataset",
                    "#long_context",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ‡°ğŸ‡·",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ reasoning: Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ĞºĞ°Ğº ÑĞºĞ¾Ñ€ÑŒ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Language-Mixed CoT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ (ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ğ¼) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ reasoning ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Yi-Sang Ñ 5.79M ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ 3.7M Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… reasoning traces, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen3-32B. Ğ˜Ñ… Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ KO-REAson-35B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ°Ğ½ÑĞ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ° 5 Ğ¸Ğ· 9 Ñ‚ĞµÑÑ‚Ğ¾Ğ² ÑĞ¾ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ +18.6 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ CoT Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ cross-lingual Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging Languages for Better Reasoning in Korean Tasks",
                    "desc": "This paper introduces a novel reasoning approach called Language-Mixed Chain-of-Thought (CoT) that enhances performance on Korean-specific tasks by alternating between English and Korean. By using English as a reference point, the method minimizes translation errors and improves reasoning capabilities. The authors present a comprehensive dataset, Yi-Sang, consisting of millions of Korean prompts and reasoning traces, which supports the training of various models. The best-performing model, KO-REAson-35B, achieves state-of-the-art results across multiple benchmarks, demonstrating significant improvements for both large and smaller models."
                },
                "zh": {
                    "title": "è¯­è¨€æ··åˆæ¨ç†ï¼Œæå‡éŸ©è¯­ä»»åŠ¡è¡¨ç°ï¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è¯­è¨€æ··åˆçš„æ€ç»´é“¾æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜éŸ©è¯­ç‰¹å®šä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨è‹±è¯­å’ŒéŸ©è¯­ä¹‹é—´åˆ‡æ¢ï¼Œåˆ©ç”¨è‹±è¯­ä½œä¸ºé”šç‚¹æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘ç¿»è¯‘å¸¦æ¥çš„å¹²æ‰°ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†åä¸ºYi-Sangçš„éŸ©è¯­æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†å¤šä¸ªæ¨¡å‹ï¼Œå…¶ä¸­æœ€å¥½çš„æ¨¡å‹KO-REAson-35Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆç»©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­è¨€æ··åˆçš„æ€ç»´é“¾æ¨ç†æ–¹æ³•æ¯”å•è¯­æ¨ç†æ–¹æ³•æ›´æœ‰æ•ˆï¼Œä¸”å¯¹ä¸­å°å‹æ¨¡å‹ä¹Ÿæœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07318",
            "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
            "url": "https://huggingface.co/papers/2510.07318",
            "abstract": "A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
            "score": 12,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "95c7f8990db3ab94",
            "authors": [
                "Yunhao Fang",
                "Weihao Yu",
                "Shu Zhong",
                "Qinghao Ye",
                "Xuehan Xiong",
                "Lai Wei"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07318.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ¿Ğ¿Ğ¾ĞºĞ°Ğ¼Ğ¿ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ KV-ĞºĞµÑˆĞ° Transformer) Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Artificial Hippocampus Network). AHN Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RNN-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Mamba2, DeltaNet Ğ¸ Gated DeltaNet. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ AHN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹ Ñ full-attention Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 40.5% Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 74%."
                },
                "en": {
                    "title": "Enhancing Long-Sequence Modeling with Memory Integration",
                    "desc": "This paper presents a new memory framework for neural networks that combines short-term and long-term memory to enhance the modeling of long sequences. The framework uses a sliding window for short-term memory, which retains recent information, while an Artificial Hippocampus Network (AHN) compresses older data into a fixed-size long-term memory. By integrating this approach into existing RNN-like architectures, the models show improved efficiency and performance on long-context tasks. Experiments reveal that these models not only outperform traditional methods but also significantly reduce computational costs and memory usage."
                },
                "zh": {
                    "title": "æå‡é•¿åºåˆ—å»ºæ¨¡æ•ˆç‡çš„è®°å¿†æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„ç¥ç»ç½‘ç»œè®°å¿†æ¡†æ¶ï¼Œä»¥æé«˜é•¿åºåˆ—å»ºæ¨¡çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†è®¤çŸ¥ç§‘å­¦ä¸­çš„å¤šå­˜å‚¨æ¨¡å‹ï¼Œä½¿ç”¨å˜æ¢å™¨çš„KVç¼“å­˜ä½œä¸ºæ— æŸçŸ­æœŸè®°å¿†ï¼ŒåŒæ—¶é€šè¿‡äººå·¥æµ·é©¬ä½“ç½‘ç»œï¼ˆAHNï¼‰å°†è¶…å‡ºçª—å£çš„ä¿¡æ¯å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„é•¿æœŸè®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨AHNçš„æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5-3B-Instructæ¨¡å‹åœ¨å¼•å…¥AHNåï¼Œæ¨ç†è®¡ç®—é‡å‡å°‘äº†40.5%ï¼Œå†…å­˜ç¼“å­˜å‡å°‘äº†74.0%ï¼ŒåŒæ—¶åœ¨LV-Evalä¸Šçš„å¹³å‡å¾—åˆ†ä»4.41æé«˜åˆ°5.88ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07019",
            "title": "Native Hybrid Attention for Efficient Sequence Modeling",
            "url": "https://huggingface.co/papers/2510.07019",
            "abstract": "Native Hybrid Attention (NHA) combines linear and full attention mechanisms to maintain long-term context while improving efficiency, outperforming Transformers in recall-intensive tasks and offering efficiency gains in pretrained LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single softmax attention operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.",
            "score": 12,
            "issue_id": 6328,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "d6cafe9c8dd0bad4",
            "authors": [
                "Jusen Du",
                "Jiaxi Hu",
                "Tao Zhang",
                "Weigao Sun",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07019.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Transformer",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Native Hybrid Attention (NHA) â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼. NHA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ linear RNN Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² key-value ÑĞ»Ğ¾Ñ‚Ğ°Ñ… Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸Ğ· ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Transformer Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ LLM Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ â€” Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ°."
                },
                "en": {
                    "title": "Efficient Recall with Native Hybrid Attention",
                    "desc": "Native Hybrid Attention (NHA) is a new approach that combines linear and full attention mechanisms to enhance efficiency while preserving long-term context in machine learning models. Unlike traditional Transformers, which struggle with quadratic complexity, NHA integrates both types of attention in a single layer, allowing for better recall in tasks that require remembering information over long sequences. It uses a linear RNN to manage key-value slots and incorporates short-term tokens through a sliding window, optimizing the attention process with a single softmax operation. Experimental results demonstrate that NHA outperforms existing models in recall-intensive tasks and can be effectively integrated into pretrained large language models for improved performance and efficiency."
                },
                "zh": {
                    "title": "æ··åˆæ³¨æ„åŠ›ï¼Œæå‡æ•ˆç‡ä¸å¬å›ç‡ï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºNative Hybrid Attentionï¼ˆNHAï¼‰ï¼Œå®ƒç»“åˆäº†çº¿æ€§å’Œå…¨æ³¨æ„åŠ›æœºåˆ¶ã€‚NHAèƒ½å¤Ÿåœ¨ä¿æŒé•¿æœŸä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œæé«˜è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜å¬å›ç‡çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„Transformeræ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¿æ€§RNNæ›´æ–°å…³é”®å€¼æ§½ï¼Œå¹¶åˆ©ç”¨æ»‘åŠ¨çª—å£ä¸­çš„çŸ­æœŸæ ‡è®°æ¥å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNHAåœ¨å¬å›å¯†é›†å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†Transformerå’Œå…¶ä»–æ··åˆåŸºçº¿ï¼ŒåŒæ—¶åœ¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¹Ÿå®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05862",
            "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
            "url": "https://huggingface.co/papers/2510.05862",
            "abstract": "Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
            "score": 12,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "efa8b7d057b81865",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Juntao Li",
                "Lijun Wu",
                "Haijia Gui",
                "Min Zhang"
            ],
            "affiliations": [
                "LCM Laboratory",
                "Shanghai Artificial Intelligence Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context Denoising Training (CDT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼), Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Integrated Gradient Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ CDT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ GPT-4o."
                },
                "en": {
                    "title": "Enhancing Long-Context Models with Context Denoising Training",
                    "desc": "Context Denoising Training (CDT) is a novel approach designed to enhance the performance of long-context models (LCMs) by reducing the impact of contextual noise. This noise, which consists of irrelevant tokens, can distract the model from focusing on important information needed for accurate predictions. The paper introduces the Integrated Gradient (IG) score as a metric to identify and measure this noise, allowing for targeted mitigation strategies. By implementing CDT, the model's attention on critical tokens is improved, leading to better overall performance in various tasks, even achieving results comparable to advanced models like GPT-4o."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡å»å™ªï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "ä¸Šä¸‹æ–‡å»å™ªè®­ç»ƒï¼ˆCDTï¼‰é€šè¿‡å‡å°‘ä¸Šä¸‹æ–‡å™ªå£°ï¼Œæå‡äº†é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½ã€‚é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆLCMsï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“å—åˆ°æ— å…³æ ‡è®°çš„å¹²æ‰°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åº¦é‡æ ‡å‡†â€”â€”ç§¯åˆ†æ¢¯åº¦ï¼ˆIGï¼‰åˆ†æ•°ï¼Œç”¨äºæ£€æµ‹å’Œé‡åŒ–ä¸Šä¸‹æ–‡ä¸­çš„å™ªå£°ä¿¡æ¯ã€‚é€šè¿‡ç®€å•çš„å™ªå£°ç¼“è§£æ–¹æ³•ï¼ŒCDTæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å…³é”®æ ‡è®°çš„å…³æ³¨ï¼Œä»è€Œæ”¹å–„äº†åç»­çš„é¢„æµ‹æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07143",
            "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
            "url": "https://huggingface.co/papers/2510.07143",
            "abstract": "VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench.",
            "score": 10,
            "issue_id": 6325,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "8dd7873ac575e1d1",
            "authors": [
                "Chenfei Liao",
                "Wensong Wang",
                "Zichen Wen",
                "Xu Zheng",
                "Yiyu Wang",
                "Haocong He",
                "Yuanhuiyi Lyu",
                "Lutao Jiang",
                "Xin Zou",
                "Yuqian Fu",
                "Bin Ren",
                "Linfeng Zhang",
                "Xuming Hu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "University of Pisa",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07143.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "VTC-Bench: Ñ‡ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VTC-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑˆÑƒĞ¼ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸, Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. VTC-Bench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "VTC-Bench: Fair Evaluation for Visual Token Compression",
                    "desc": "VTC-Bench is a new evaluation framework designed to improve the assessment of visual token compression methods in Multimodal Large Language Models (MLLMs). It addresses the issue of noisy benchmarks that were not originally intended for evaluating compression techniques, leading to inaccurate performance comparisons. The framework incorporates a data filtering mechanism that helps to denoise these benchmarks, allowing for a more reliable evaluation of compression methods. Our findings show that simple image downsampling can outperform complex compression techniques, highlighting the need for a better evaluation approach."
                },
                "zh": {
                    "title": "VTC-Benchï¼šå…¬å¹³è¯„ä¼°è§†è§‰ä»¤ç‰Œå‹ç¼©çš„æ–°æ¡†æ¶",
                    "desc": "VTC-Benchæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºè§†è§‰ä»¤ç‰Œå‹ç¼©æä¾›å…¬å¹³çš„è¯„ä¼°ã€‚å®ƒé€šè¿‡å¼•å…¥æ•°æ®è¿‡æ»¤æœºåˆ¶æ¥å»å™ªç°æœ‰åŸºå‡†ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•çš„å›¾åƒä¸‹é‡‡æ ·åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºè®¸å¤šå…ˆè¿›çš„å‹ç¼©æ–¹æ³•ã€‚VTC-Benchçš„ç›®æ ‡æ˜¯è§£å†³å½“å‰åŸºå‡†æµ‹è¯•çš„å™ªå£°é—®é¢˜ï¼Œä½¿è§†è§‰ä»¤ç‰Œå‹ç¼©æ–¹æ³•çš„è¯„ä¼°æ›´åŠ å…¬æ­£å’Œå‡†ç¡®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06557",
            "title": "The Markovian Thinker",
            "url": "https://huggingface.co/papers/2510.06557",
            "abstract": "Markovian Thinking, implemented in Delethink, enables efficient and scalable reinforcement learning for long-chain-of-thought reasoning in LLMs by decoupling thinking length from context size, resulting in linear compute and constant memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
            "score": 10,
            "issue_id": 6330,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "24d3191793e49f1b",
            "authors": [
                "Milad Aghajohari",
                "Kamran Chitsaz",
                "Amirhossein Kazemnejad",
                "Sarath Chandar",
                "Alessandro Sordoni",
                "Aaron Courville",
                "Siva Reddy"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Chandar Research Lab",
                "McGill University",
                "Microsoft Research",
                "Mila",
                "Polytechnique MontrÃ©al",
                "ServiceNow Research",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06557.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rlhf",
                    "#long_context",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Markovian Thinking Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Delethink â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LongCoT) Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ (Ñ‡Ğ°Ğ½ĞºĞ¸), Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 24K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ°Ñ… Ğ¿Ğ¾ 8K, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Efficient Long-Chain Reasoning with Markovian Thinking",
                    "desc": "This paper introduces Markovian Thinking, a new approach for reinforcement learning (RL) that enhances long-chain-of-thought reasoning in large language models (LLMs). By decoupling the length of reasoning from the context size, it allows for linear computational costs and constant memory usage, making it more efficient. The authors present Delethink, an RL environment that organizes reasoning into fixed-size chunks, enabling the model to maintain coherence across resets. The results demonstrate that this method significantly reduces resource consumption while improving reasoning capabilities compared to traditional RL methods."
                },
                "zh": {
                    "title": "é©¬å°”å¯å¤«æ€ç»´ï¼šé«˜æ•ˆæ¨ç†çš„æ–°è·¯å¾„",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé©¬å°”å¯å¤«æ€ç»´çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿é“¾æ¨ç†ä¸­çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡å°†æ€ç»´é•¿åº¦ä¸ä¸Šä¸‹æ–‡å¤§å°è§£è€¦ï¼Œé©¬å°”å¯å¤«æ€ç»´ä½¿å¾—è®¡ç®—å¤æ‚åº¦å‘ˆçº¿æ€§å¢é•¿ï¼ŒåŒæ—¶å†…å­˜ä½¿ç”¨ä¿æŒä¸å˜ã€‚æˆ‘ä»¬é€šè¿‡Delethinkç¯å¢ƒå®ç°äº†è¿™ä¸€ç†å¿µï¼Œè¯¥ç¯å¢ƒå°†æ¨ç†ç»“æ„åŒ–ä¸ºå›ºå®šå¤§å°çš„å—ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ¯ä¸ªå—å†…è¿›è¡Œå¸¸è§„æ€è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Delethinkçš„æ¨¡å‹åœ¨æ¨ç†é•¿åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05057",
            "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
            "url": "https://huggingface.co/papers/2510.05057",
            "abstract": "An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  \t\t\t\t\tAI-generated summary \t\t\t\t A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
            "score": 10,
            "issue_id": 6326,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "1e9f45f6911e2d61",
            "authors": [
                "Mingyu Liu",
                "Jiuhe Shu",
                "Hui Chen",
                "Zeju Li",
                "Canyu Zhao",
                "Jiange Yang",
                "Shenyuan Gao",
                "Hao Chen",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Nanjing University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05057.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ StaMo Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ²ÑĞµĞ³Ğ¾ Ğ² Ğ´Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Diffusion Transformer Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ°: Ñ€Ğ°Ğ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ supervised Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 14.3% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO Ğ¸ Ğ½Ğ° 30% Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹, ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ egocentric Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»ÑĞ´ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficient State Representation for Enhanced Robotic Motion",
                    "desc": "This paper presents an unsupervised method called StaMo that learns a compact state representation for robotic applications using a lightweight encoder and a Diffusion Transformer decoder. The method addresses the challenge of creating efficient and expressive state representations, which are crucial for decision-making in robotics. By utilizing a two-token representation, StaMo improves performance significantly on various benchmarks while enabling the decoding of latent actions from static images. This approach not only enhances interpretability and efficiency but also demonstrates the ability to capture structured dynamics without requiring explicit supervision."
                },
                "zh": {
                    "title": "ç´§å‡‘çŠ¶æ€è¡¨ç¤ºï¼Œæå‡æœºå™¨äººæ™ºèƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§ç¼–ç å™¨å’Œæ‰©æ•£å˜æ¢å™¨è§£ç å™¨å­¦ä¹ ç´§å‡‘çš„çŠ¶æ€è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä»é™æ€å›¾åƒä¸­æå–å‡ºé«˜æ•ˆä¸”å¯è§£é‡Šçš„çŠ¶æ€è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººåœ¨çœŸå®ä»»åŠ¡ä¸­çš„æˆåŠŸç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨LIBEROæ•°æ®é›†ä¸Šæé«˜äº†14.3%çš„æ€§èƒ½ï¼Œå¹¶åœ¨å®é™…ä»»åŠ¡ä¸­æå‡äº†30%ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ½œåœ¨æ’å€¼è·å¾—çš„çŠ¶æ€å·®å¼‚å¯ä»¥è‡ªç„¶åœ°ä½œä¸ºæœ‰æ•ˆçš„æ½œåœ¨åŠ¨ä½œï¼Œè¿›ä¸€æ­¥è§£ç ä¸ºå¯æ‰§è¡Œçš„æœºå™¨äººåŠ¨ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07238",
            "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
            "url": "https://huggingface.co/papers/2510.07238",
            "abstract": "Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.",
            "score": 9,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "1477f170bed5f8aa",
            "authors": [
                "Xunyi Jiang",
                "Dingyi Chang",
                "Julian McAuley",
                "Xin Xu"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07238.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑÑ‚Ğ°Ñ€ĞµÑÑ‚: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°ÑÑ‚ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ¾ÑĞµĞ¼ÑŒ LLM, Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑÑ‚Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ñ… ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Outdated Benchmarks, Unreliable Evaluations: Rethinking LLM Factuality",
                    "desc": "This research paper examines how the aging of factuality benchmarks affects the evaluation of large language models (LLMs). It highlights that many commonly used benchmarks are outdated, which can lead to unreliable assessments of how factual these models are. The authors conducted a systematic study of five popular benchmarks and eight LLMs, using a new fact retrieval pipeline and metrics to measure the impact of benchmark aging. The findings suggest that relying on these old benchmarks can mislead evaluations, emphasizing the need for updated standards in assessing LLM factuality."
                },
                "zh": {
                    "title": "åŸºå‡†è€åŒ–å½±å“LLMäº‹å®æ€§è¯„ä¼°çš„å¯é æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†äº‹å®åŸºå‡†çš„è€åŒ–åŠå…¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº‹å®æ€§è¯„ä¼°çš„å½±å“ï¼Œå‘ç°ç”±äºæ ·æœ¬è¿‡æ—¶ï¼Œè¯„ä¼°ç»“æœå­˜åœ¨æ˜¾è‘—çš„ä¸å¯é æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°å®ä¸–ç•Œçš„å¿«é€Ÿå‘å±•ï¼Œå¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°åŸºå‡†çš„é™æ€ç‰¹æ€§å¼•å‘äº†å¯¹å…¶å¯é æ€§çš„æ‹…å¿§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†äº”ä¸ªæµè¡Œçš„äº‹å®åŸºå‡†å’Œå…«ä¸ªä¸åŒå¹´ä»½å‘å¸ƒçš„LLMï¼Œæå‡ºäº†ä¸€ä¸ªæ›´æ–°çš„äº‹å®æ£€ç´¢ç®¡é“å’Œä¸‰ç§æŒ‡æ ‡æ¥é‡åŒ–åŸºå‡†è€åŒ–åŠå…¶å¯¹LLMäº‹å®æ€§è¯„ä¼°çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¤šæµè¡Œäº‹å®åŸºå‡†ä¸­çš„æ ·æœ¬å·²ç»è¿‡æ—¶ï¼Œå¯¼è‡´å¯¹LLMäº‹å®æ€§çš„è¯„ä¼°ä¸å¯é ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01954",
            "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
            "url": "https://huggingface.co/papers/2510.01954",
            "abstract": "PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
            "score": 8,
            "issue_id": 6327,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "4b2f2c5648bbe9c2",
            "authors": [
                "Yongyi Su",
                "Haojie Zhang",
                "Shijie Li",
                "Nanqing Liu",
                "Jingyi Liao",
                "Junyi Pan",
                "Yuan Liu",
                "Xiaofen Xing",
                "Chong Sun",
                "Chen Li",
                "Nancy F. Chen",
                "Shuicheng Yan",
                "Xulei Yang",
                "Xun Xu"
            ],
            "affiliations": [
                "Foshan University",
                "Institute for Infocomm Research (I2R), A*STAR",
                "Nanyang Technological University",
                "National University of Singapore",
                "South China University of Technology",
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01954.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#training",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸÑ€ÑĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ğ¿Ğ°Ñ‚Ñ‡Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ PaDT â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Visual Reference Tokens (VRT), Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ LLM Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ grounding Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ MLLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "PaDT: Directly Bridging Text and Vision for Superior Performance",
                    "desc": "The paper introduces PaDT, a new approach for multimodal large language models (MLLMs) that generates both text and visual outputs directly. This method addresses limitations of previous models that used indirect representations, which hindered performance in tasks like segmentation. PaDT utilizes Visual Reference Tokens (VRTs) that are integrated with textual tokens, allowing for better detection and localization of objects. The results show that PaDT outperforms existing models in various visual perception tasks, demonstrating its effectiveness and efficiency."
                },
                "zh": {
                    "title": "PaDTï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„ç»Ÿä¸€èŒƒå¼",
                    "desc": "PaDTæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èŒƒå¼ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è¾“å‡ºï¼Œæå‡äº†è§†è§‰æ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPaDTä½¿ç”¨è§†è§‰å‚è€ƒæ ‡è®°ï¼ˆVRTsï¼‰ï¼Œè¿™äº›æ ‡è®°ç›´æ¥ä»å›¾åƒçš„è§†è§‰è¡¥ä¸åµŒå…¥ä¸­æå–ï¼Œå¹¶ä¸æ–‡æœ¬è¾“å‡ºæ— ç¼ç»“åˆã€‚é€šè¿‡è½»é‡çº§è§£ç å™¨ï¼ŒPaDTèƒ½å¤Ÿå°†è¾“å‡ºè½¬åŒ–ä¸ºæ£€æµ‹ã€åˆ†å‰²å’Œå®šä½é¢„æµ‹ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒPaDTåœ¨å››ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06783",
            "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
            "url": "https://huggingface.co/papers/2510.06783",
            "abstract": "TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.",
            "score": 6,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "54afe116302f9e7c",
            "authors": [
                "Akshit Singh",
                "Shyam Marjit",
                "Wei Lin",
                "Paul Gavrikov",
                "Serena Yeung-Levy",
                "Hilde Kuehne",
                "Rogerio Feris",
                "Sivan Doveh",
                "James Glass",
                "M. Jehanzeb Mirza"
            ],
            "affiliations": [
                "IISc Bangalore",
                "Independent Researcher",
                "JKU Linz",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Stanford",
                "TÃ¼bingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06783.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ TTRV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğµ GRPO, Ğ³Ğ´Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: Ğ´Ğ¾ 52.4% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ 29.8% Ğ² visual question answering Ğ½Ğ° 16 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ TTRV Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ InternVL 8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 2.3%, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Enhancing Vision Language Understanding with Test-Time Reinforcement Learning",
                    "desc": "The paper introduces TTRV, a novel approach that enhances vision language understanding using test-time reinforcement learning (RL) without requiring labeled data. It improves the Group Relative Policy Optimization (GRPO) framework by creating rewards based on the frequency of outputs from the base model, allowing for dynamic adaptation during inference. The method also encourages diversity in outputs by rewarding lower entropy in the empirical distribution of predictions. TTRV shows significant performance improvements in object recognition and visual question answering (VQA), outperforming existing models like GPT-4o in certain benchmarks, even in scenarios with limited data."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€ç†è§£",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTTRVçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºè§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œè€Œæ— éœ€æ ‡è®°æ•°æ®ã€‚è¯¥æ–¹æ³•æ”¹è¿›äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºé¢‘ç‡è®¾è®¡å¥–åŠ±ä¿¡å·ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚TTRVåœ¨ç‰©ä½“è¯†åˆ«å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«æé«˜äº†52.4%å’Œ29.8%ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨æ•°æ®æå…¶æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒTTRVä¹Ÿèƒ½åœ¨è¯†åˆ«ä»»åŠ¡ä¸­å®ç°é«˜è¾¾5.5%çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05644",
            "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
            "url": "https://huggingface.co/papers/2510.05644",
            "abstract": "The African Languages Lab addresses the underserved status of African languages in NLP by creating a large dataset and demonstrating improved model performance through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.",
            "score": 6,
            "issue_id": 6327,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "9dc4aa03a2c96864",
            "authors": [
                "Sheriff Issaka",
                "Keyi Wang",
                "Yinka Ajibola",
                "Oluwatumininu Samuel-Ipaye",
                "Zhaoyi Zhang",
                "Nicte Aguillon Jimenez",
                "Evans Kofi Agyei",
                "Abraham Lin",
                "Rohan Ramachandran",
                "Sadick Abdul Mumin",
                "Faith Nchifor",
                "Mohammed Shuraim",
                "Lieqi Liu",
                "Erick Rosas Gonzalez",
                "Sylvester Kpei",
                "Jemimah Osei",
                "Carlene Ajeneza",
                "Persis Boateng",
                "Prisca Adwoa Dufie Yeboah",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "Carleton University",
                "Columbia University",
                "Cornell University",
                "Georgia Institute of Technology",
                "Northwestern University in Qatar",
                "Soka University of America",
                "Stetson University",
                "University of California, Los Angeles",
                "University of Cape Coast",
                "University of Wisconsin - Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05644.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¸Ğ· Ñ‚ĞµĞ½Ğ¸: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ NLP",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ African Languages Lab Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² NLP Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ…. ĞĞ½Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 40 Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ 19 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ 12,628 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ +23.69 ChrF++, +0.33 COMET Ğ¸ +15.34 BLEU Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñƒ Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğµ."
                },
                "en": {
                    "title": "Empowering African Languages in NLP",
                    "desc": "The African Languages Lab (All Lab) aims to improve the representation of African languages in natural language processing (NLP) by creating a large, high-quality dataset. This dataset includes 19 billion tokens of text and over 12,000 hours of speech data across 40 languages, addressing the significant underrepresentation of these languages in computational linguistics. The paper demonstrates that fine-tuning models with this dataset leads to notable performance improvements, with average increases in evaluation metrics like BLEU and ChrF++. Additionally, the initiative supports local researchers, fostering sustainable development in the field of NLP for African languages."
                },
                "zh": {
                    "title": "æå‡éæ´²è¯­è¨€å¤„ç†èƒ½åŠ›çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "éæ´²è¯­è¨€å®éªŒå®¤æ—¨åœ¨è§£å†³éæ´²è¯­è¨€åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸è¶³ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªå¤§å‹æ•°æ®é›†å¹¶å±•ç¤ºæ¨¡å‹æ€§èƒ½çš„æå‡ã€‚è¯¥å®éªŒå®¤æ”¶é›†äº†40ç§è¯­è¨€çš„å¤šæ¨¡æ€è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®ï¼ŒåŒ…å«190äº¿ä¸ªå•è¯­æ–‡æœ¬å’Œ12628å°æ—¶çš„å¯¹é½è¯­éŸ³æ•°æ®ã€‚é€šè¿‡å¯¹æ•°æ®é›†çš„ç²¾ç»†è°ƒä¼˜ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨31ç§è¯­è¨€ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹³å‡æé«˜äº†23.69 ChrF++ã€0.33 COMETå’Œ15.34 BLEUåˆ†æ•°ã€‚æ­¤å¤–ï¼Œå®éªŒå®¤è¿˜æˆåŠŸåŸ¹å…»äº†15åæ—©æœŸèŒä¸šç ”ç©¶äººå‘˜ï¼Œå»ºç«‹äº†å¯æŒç»­çš„æœ¬åœ°èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07313",
            "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
            "url": "https://huggingface.co/papers/2510.07313",
            "abstract": "WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
            "score": 5,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "72337b6fb9bb399b",
            "authors": [
                "Zezhong Qian",
                "Xiaowei Chi",
                "Yuming Li",
                "Shizun Wang",
                "Zhiyuan Qin",
                "Xiaozhu Ju",
                "Sirui Han",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beijing Innovation Center of Humanoid Robotics",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07313.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "WristWorld â€” ÑÑ‚Ğ¾ 4D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ 4D ÑÑ†ĞµĞ½Ñƒ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞºĞ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ´ĞºĞ¾ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLA Ğ½Ğ° 3.81% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Generating Wrist-View Videos from Anchor Views",
                    "desc": "WristWorld is a novel 4D world model designed to generate wrist-view videos from anchor views, addressing the challenge of limited wrist-view data in video generation. It enhances video generation consistency and improves Visual Language Action (VLA) performance by capturing detailed hand-object interactions. The model operates in two stages: first, it reconstructs wrist-view poses and 4D point clouds using a Spatial Projection Consistency (SPC) Loss, and second, it synthesizes coherent wrist-view videos from these reconstructions. Experiments show that WristWorld achieves state-of-the-art results in video generation, significantly improving task completion rates and bridging the gap between anchor and wrist views."
                },
                "zh": {
                    "title": "WristWorldï¼šä»é”šç‚¹è§†å›¾ç”Ÿæˆæ‰‹è…•è§†è§’è§†é¢‘çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "WristWorldæ˜¯ä¸€ä¸ª4Dä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿä»é”šç‚¹è§†å›¾ç”Ÿæˆæ‰‹è…•è§†è§’çš„è§†é¢‘ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§å’ŒVLAæ€§èƒ½ã€‚æ‰‹è…•è§†è§’çš„è§‚å¯Ÿå¯¹äºVLAæ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ•æ‰åˆ°ç»†è‡´çš„æ‰‹ä¸ç‰©ä½“çš„äº¤äº’ï¼Œç›´æ¥å¢å¼ºäº†æ“ä½œæ€§èƒ½ã€‚ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹æ— æ³•å¼¥è¡¥é”šç‚¹è§†å›¾ä¸æ‰‹è…•è§†å›¾ä¹‹é—´çš„å·®è·ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦æ‰‹è…•è§†è§’çš„ç¬¬ä¸€å¸§ã€‚WristWorldé€šè¿‡é‡å»ºå’Œç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨ç©ºé—´æŠ•å½±ä¸€è‡´æ€§æŸå¤±å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒæˆåŠŸåˆæˆäº†æ—¶é—´ä¸Šè¿è´¯çš„æ‰‹è…•è§†è§’è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01982",
            "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
            "url": "https://huggingface.co/papers/2510.01982",
            "abstract": "A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO (G^2RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our G^2RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.",
            "score": 5,
            "issue_id": 6324,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "76076ef0cec86cef",
            "authors": [
                "Yujie Zhou",
                "Pengyang Ling",
                "Jiazi Bu",
                "Yibin Wang",
                "Yuhang Zang",
                "Jiaqi Wang",
                "Li Niu",
                "Guangtao Zhai"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01982.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#alignment",
                    "#rlhf",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Granular-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ flow-based Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ·ĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Singular Stochastic Sampling Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Multi-Granularity Advantage Integration Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ G^2RPO Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Reward Assessment in Reinforcement Learning with Granular-GRPO",
                    "desc": "The Granular-GRPO (G^2RPO) framework enhances reinforcement learning (RL) in diffusion and flow models by improving how rewards are assessed and reducing bias during the denoising process. It utilizes Stochastic Differential Equations (SDE) for stochastic sampling, allowing for diverse exploration of denoising directions. The framework introduces a Singular Stochastic Sampling strategy to ensure that rewards are closely aligned with the noise introduced, leading to better reward signals. Additionally, a Multi-Granularity Advantage Integration module aggregates advantages from different diffusion scales, resulting in a more accurate evaluation of sampling directions and improved performance over existing methods."
                },
                "zh": {
                    "title": "æå‡å¼ºåŒ–å­¦ä¹ çš„Granular-GRPOæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Granular-GRPOæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£å’Œæµæ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¹è¿›å¥–åŠ±è¯„ä¼°å’Œå‡å°‘å»å™ªä¸­çš„åå·®ï¼Œæå‡äº†ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬å¼•å…¥äº†å•ä¸€éšæœºé‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¯æŒé€æ­¥éšæœºæ¢ç´¢ï¼Œå¹¶ç¡®ä¿å¥–åŠ±ä¸æ³¨å…¥å™ªå£°ä¹‹é—´çš„é«˜ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG^2RPOåœ¨å¤šç§å¥–åŠ±æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºæµçš„GRPOåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07307",
            "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
            "url": "https://huggingface.co/papers/2510.07307",
            "abstract": "MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. We introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. We apply MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality.",
            "score": 4,
            "issue_id": 6322,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "3cffdc5a57565890",
            "authors": [
                "Rushi Qiang",
                "Yuchen Zhuang",
                "Anikait Singh",
                "Percy Liang",
                "Chao Zhang",
                "Sherry Yang",
                "Bo Dai"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07307.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#agents"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ° ML-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MLE-Smith â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ-Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MLE-Smith Ğº 224 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ 606 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Automating Diverse MLE Tasks with MLE-Smith",
                    "desc": "MLE-Smith is a novel automated system designed to create diverse and high-quality machine learning engineering (MLE) tasks from raw datasets. It utilizes a multi-agent pipeline that follows a generate-verify-execute approach, ensuring that the tasks produced are scalable and maintain rigorous quality standards. The system incorporates a hybrid verification mechanism to uphold structural integrity and semantic accuracy, while also confirming the tasks' practical applicability through interactive execution. By applying MLE-Smith to numerous real-world datasets, the study demonstrates its capability to generate a wide variety of tasks that align well with the performance of existing language models."
                },
                "zh": {
                    "title": "MLE-Smithï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æœºå™¨å­¦ä¹ ä»»åŠ¡çš„åˆ©å™¨",
                    "desc": "MLE-Smith æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“ç®¡é“ï¼Œèƒ½å¤Ÿä»åŸå§‹æ•°æ®é›†ä¸­åˆ›å»ºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ï¼ˆMLEï¼‰ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç”Ÿæˆ-éªŒè¯-æ‰§è¡Œçš„é«˜æ•ˆèŒƒå¼ï¼Œæå‡äº†ä»»åŠ¡çš„å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ç¡®ä¿äº†ä»»åŠ¡çš„è´¨é‡å’Œå®é™…åº”ç”¨æ€§ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç»“æ„åŒ–ä»»åŠ¡è®¾è®¡å’Œæ ‡å‡†åŒ–é‡æ„ï¼Œå¹¶ç»“åˆæ··åˆéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡çš„ç»“æ„è§„åˆ™å’Œè¯­ä¹‰åˆç†æ€§ã€‚é€šè¿‡åœ¨224ä¸ªçœŸå®æ•°æ®é›†ä¸Šåº”ç”¨MLE-Smithï¼Œç”Ÿæˆäº†606ä¸ªè·¨å¤šä¸ªç±»åˆ«å’Œç›®æ ‡çš„ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06953",
            "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces",
            "url": "https://huggingface.co/papers/2510.06953",
            "abstract": "Step-level uniformity in information density, measured using entropy-based metrics, improves reasoning accuracy in large language models across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
            "score": 4,
            "issue_id": 6331,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "f4a84fa12f0087d2",
            "authors": [
                "Minju Gwak",
                "Guijin Son",
                "Jaehyung Kim"
            ],
            "affiliations": [
                "OneLine AI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06953.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ â€” ĞºĞ»ÑÑ‡ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (UID) Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ²ĞµĞ»Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 10-32% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ÑÑ‚ Ñ€ĞµĞ·ĞºĞ¸Ñ… ÑĞºĞ°Ñ‡ĞºĞ¾Ğ² Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Boosting Reasoning Accuracy with Uniform Information Density",
                    "desc": "This paper explores the Uniform Information Density (UID) hypothesis, which posits that effective communication has a consistent flow of information. The authors introduce a new metric based on entropy to measure step-level information density in large language models (LLMs). They find that maintaining uniformity in information density significantly enhances reasoning accuracy across various benchmarks, with improvements of 10-32% over existing methods. The study concludes that uniform information density is a valuable indicator for assessing and improving the reasoning capabilities of LLMs."
                },
                "zh": {
                    "title": "ä¿¡æ¯å¯†åº¦å‡åŒ€æ€§æå‡æ¨ç†å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¿¡æ¯å¯†åº¦çš„å‡åŒ€æ€§å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å‡†ç¡®æ€§çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†µçš„é€æ­¥ä¿¡æ¯å¯†åº¦åº¦é‡ï¼Œå¹¶å¼•å…¥äº†å±€éƒ¨å’Œå…¨å±€å‡åŒ€æ€§è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€æ­¥å‡åŒ€æ€§ä¸ä»…åœ¨ç†è®ºä¸Šå…·æœ‰é‡è¦æ„ä¹‰ï¼Œè¿˜èƒ½åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œæ­£ç¡®çš„æ¨ç†è½¨è¿¹é¿å…äº†ä¿¡æ¯å¯†åº¦çš„å‰§çƒˆæ³¢åŠ¨ï¼Œè€Œé”™è¯¯çš„è½¨è¿¹åˆ™è¡¨ç°å‡ºä¸è§„åˆ™çš„ä¿¡æ¯çˆ†å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06855",
            "title": "Online Generic Event Boundary Detection",
            "url": "https://huggingface.co/papers/2510.06855",
            "abstract": "A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets.",
            "score": 3,
            "issue_id": 6325,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "fe5f12618eba61f3",
            "authors": [
                "Hyungrok Jung",
                "Daneul Kim",
                "Seunggyun Lim",
                "Jeany Son",
                "Jonghyun Choi"
            ],
            "affiliations": [
                "GIST",
                "POSTECH",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06855.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#video",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Online Generic Event Boundary Detection (On-GEBD) â€” Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Estimator Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Kinetics-GEBD Ğ¸ TAPOS."
                },
                "en": {
                    "title": "Real-Time Event Detection: Bridging Human Perception and Machine Learning",
                    "desc": "This paper presents a new approach for detecting event boundaries in streaming videos, called Online Generic Event Boundary Detection (On-GEBD). Unlike traditional methods that analyze complete video frames, this framework processes video data in real-time, mimicking human perception. The proposed On-GEBD framework, named Estimator, utilizes two main components: the Consistent Event Anticipator (CEA) for predicting future frames and the Online Boundary Discriminator (OBD) for measuring prediction errors. Experimental results show that Estimator significantly outperforms existing models and achieves results comparable to offline methods, highlighting its effectiveness in identifying subtle event changes in dynamic video streams."
                },
                "zh": {
                    "title": "å®æ—¶äº‹ä»¶è¾¹ç•Œæ£€æµ‹çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å®æ—¶äº‹ä»¶è¾¹ç•Œæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†æµåª’ä½“è§†é¢‘ä¸­çš„ç»†å¾®äº‹ä»¶å˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„é€šç”¨äº‹ä»¶è¾¹ç•Œæ£€æµ‹æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨çº¿å®æ—¶è¯†åˆ«äº‹ä»¶è¾¹ç•Œï¼Œè€Œæ— éœ€è®¿é—®æœªæ¥å¸§ã€‚æ¡†æ¶çš„æ ¸å¿ƒåŒ…æ‹¬ä¸€è‡´äº‹ä»¶é¢„æµ‹å™¨ï¼ˆCEAï¼‰å’Œåœ¨çº¿è¾¹ç•Œåˆ¤åˆ«å™¨ï¼ˆOBDï¼‰ï¼Œå‰è€…åŸºäºè¿‡å»å¸§é¢„æµ‹æœªæ¥å¸§ï¼Œåè€…åˆ™é€šè¿‡æµ‹é‡é¢„æµ‹è¯¯å·®æ¥è°ƒæ•´é˜ˆå€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨Kinetics-GEBDå’ŒTAPOSæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„åœ¨çº¿è§†é¢‘ç†è§£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04999",
            "title": "Bridging Text and Video Generation: A Survey",
            "url": "https://huggingface.co/papers/2510.04999",
            "abstract": "A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
            "score": 3,
            "issue_id": 6321,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "2ff1fd3dd4354c0a",
            "authors": [
                "Nilay Kumar",
                "Priyansh Bhandari",
                "G. Maragatham"
            ],
            "affiliations": [
                "Department of Computational Intelligence SRM Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#video",
                    "#diffusion",
                    "#survey"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ GAN Ğº Diffusion: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° (text-to-video), Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¾Ñ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… GAN Ğ¸ VAE Ğ´Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Diffusion-Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ’ Ğ·Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Transforming Text into Video: A Journey Through Generative Models",
                    "desc": "This paper surveys the evolution of text-to-video (T2V) generative models, highlighting the transition from Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) to advanced Diffusion-Transformer architectures. It discusses the potential applications of T2V technology in various fields and the improvements in output quality and coherence achieved through these newer models. The paper also addresses ongoing challenges such as alignment, long-range coherence, and computational efficiency, while providing insights into training configurations and evaluation metrics used in the field. Finally, it outlines future research directions and open challenges to guide further advancements in T2V generation."
                },
                "zh": {
                    "title": "æ–‡æœ¬ç”Ÿæˆè§†é¢‘æŠ€æœ¯çš„æœªæ¥æ¢ç´¢",
                    "desc": "æœ¬æ–‡å¯¹æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åˆ°æ··åˆæ‰©æ•£-å˜æ¢å™¨æ¶æ„çš„å‘å±•å†ç¨‹ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¯¹é½ã€é•¿æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è¿™äº›æ¨¡å‹çš„å·¥ä½œåŸç†ã€è§£å†³çš„å±€é™æ€§ä»¥åŠä¸ºä½•éœ€è¦å‘æ–°æ¶æ„èŒƒå¼è½¬å˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å½“å‰çš„å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨T2VæŠ€æœ¯çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06261",
            "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
            "url": "https://huggingface.co/papers/2510.06261",
            "abstract": "AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
            "score": 3,
            "issue_id": 6322,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "d4056c4e12881117",
            "authors": [
                "Zhanke Zhou",
                "Chentao Cao",
                "Xiao Feng",
                "Xuan Li",
                "Zongze Li",
                "Xiangyu Lu",
                "Jiangchao Yao",
                "Weikai Huang",
                "Linrui Xu",
                "Tian Cheng",
                "Guanyu Jiang",
                "Yiming Zheng",
                "Brando Miranda",
                "Tongliang Liu",
                "Sanmi Koyejo",
                "Masashi Sugiyama",
                "Bo Han"
            ],
            "affiliations": [
                "Cooperative Medianet Innovation Center, Shanghai Jiao Tong University",
                "RIKEN AIP",
                "Stanford University",
                "Sydney AI Centre, The University of Sydney",
                "TMLR Group, Department of Computer Science, Hong Kong Baptist University",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06261.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: AlphaApollo Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ğ»Ğ¾Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM",
                    "desc": "AlphaApollo â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ foundation models: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ â€” Python Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ retrieval Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ â€” Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. AlphaApollo Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIME 2024/2025 ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: +5.15% Average@32 Ğ¸ +23.34% Pass@32 Ğ´Ğ»Ñ Qwen2.5-14B-Instruct, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾."
                },
                "en": {
                    "title": "Elevating Foundation Models with Self-Evolving Reasoning",
                    "desc": "AlphaApollo is a self-evolving reasoning system designed to improve the performance of foundation models (FMs) by integrating various tools and refining its processes iteratively. It addresses limitations in the intrinsic reasoning capacity of models and enhances reliability during testing by using a combination of computational and retrieval tools. The system allows for multi-round solution evolution, maintaining a shared state map that tracks candidates and feedback for continuous improvement. In evaluations, AlphaApollo demonstrated significant performance gains across multiple models, showcasing its ability to effectively utilize tools and surpass traditional baselines."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„æ¨ç†ç³»ç»Ÿï¼Œæå‡åŸºç¡€æ¨¡å‹æ€§èƒ½",
                    "desc": "AlphaApollo æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¨ç†ç³»ç»Ÿï¼Œé€šè¿‡å·¥å…·é›†æˆå’Œè¿­ä»£ä¼˜åŒ–æ¥æå‡åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒè§£å†³äº†åŸºç¡€æ¨¡å‹æ¨ç†ä¸­çš„ä¸¤ä¸ªç“¶é¢ˆï¼šæ¨¡å‹å†…åœ¨èƒ½åŠ›æœ‰é™å’Œæµ‹è¯•æ—¶è¿­ä»£ä¸å¯é ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è®¡ç®—å·¥å…·ï¼ˆå¦‚ Python åŠå…¶æ•°å€¼å’Œç¬¦å·åº“ï¼‰å’Œæ£€ç´¢å·¥å…·ï¼ˆç›¸å…³å¤–éƒ¨ä¿¡æ¯ï¼‰ï¼Œä»¥æ‰§è¡Œç²¾ç¡®è®¡ç®—å’Œåšå‡ºåŸºäºè¯æ®çš„å†³ç­–ã€‚é€šè¿‡å…±äº«çŠ¶æ€å›¾è®°å½•å€™é€‰é¡¹ã€å¯æ‰§è¡Œæ£€æŸ¥å’Œåé¦ˆï¼ŒAlphaApollo æ”¯æŒå¤šè½®ã€å¤šæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆæ¼”åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œé€šè¿‡ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07041",
            "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
            "url": "https://huggingface.co/papers/2510.07041",
            "abstract": "U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
            "score": 2,
            "issue_id": 6321,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "382dc490ec7534fd",
            "authors": [
                "Fenghe Tang",
                "Chengqi Dong",
                "Wenxin Ma",
                "Zikang Xu",
                "Heqin Zhu",
                "Zihang Jiang",
                "Rongsheng Wang",
                "Yuhao Wang",
                "Chenxu Wu",
                "Shaohua Kevin Zhou"
            ],
            "affiliations": [
                "HCNS",
                "MIRACLE Center",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07041.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "U-Bench: Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "U-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 100 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² U-Net Ğ½Ğ° 28 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ 10 Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° U-Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¾Ğ²ĞµÑ‚Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "U-Bench: A New Standard for Evaluating U-Net Models in Medical Imaging",
                    "desc": "U-Bench is a new benchmark designed to evaluate 100 different U-Net models across 28 datasets and 10 types of imaging. It focuses on three main areas: how robust the models are statistically, how well they can generalize to new data without prior training (zero-shot), and how efficient they are in terms of computation. The benchmark introduces a unique metric called U-Score, which helps to balance performance and efficiency, making it easier to choose the right model for specific tasks. By providing comprehensive evaluations and public access to resources, U-Bench aims to improve the way U-Net models are assessed and utilized in medical image segmentation."
                },
                "zh": {
                    "title": "U-Benchï¼šU-Netå˜ä½“çš„å…¨é¢è¯„ä¼°åŸºå‡†",
                    "desc": "U-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°100ç§U-Netå˜ä½“åœ¨28ä¸ªæ•°æ®é›†å’Œ10ç§æˆåƒæ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨ç»Ÿè®¡ç¨³å¥æ€§ã€é›¶æ ·æœ¬æ³›åŒ–å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥åŸºå‡†æµ‹è¯•å¡«è¡¥äº†ä»¥å¾€ç¼ºä¹ç³»ç»Ÿè¯„ä¼°çš„ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†U-Scoreï¼Œå¸®åŠ©ç ”ç©¶è€…ç†è§£æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡ç³»ç»Ÿåˆ†ææ•°æ®é›†ç‰¹å¾å’Œæ¨¡å‹æ¶æ„å¯¹æ€§èƒ½çš„å½±å“ï¼ŒU-Benchä¸ºç ”ç©¶è€…æä¾›äº†æ¨¡å‹é€‰æ‹©çš„æŒ‡å¯¼ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œåè®®å‡å·²å…¬å¼€ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„å†ç°æ€§å’Œæœªæ¥æ–¹æ³•çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07037",
            "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
            "url": "https://huggingface.co/papers/2510.07037",
            "abstract": "This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.  \t\t\t\t\tAI-generated summary \t\t\t\t Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing unique_references studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
            "score": 2,
            "issue_id": 6325,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "dcf8b5cd9f394b07",
            "authors": [
                "Rajvee Sheth",
                "Samridhi Raj Sinha",
                "Mahavir Patil",
                "Himanshu Beniwal",
                "Mayank Singh"
            ],
            "affiliations": [
                "IIT Gandhinagar",
                "LINGO Research Group",
                "NMIMS Mumbai",
                "SVNIT Surat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07037.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multilingual",
                    "#architecture",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#low_resource"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ¾Ğ² â€” ÑĞ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ LLM, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 12 NLP-Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ±Ğ¾Ğ»ĞµĞµ 30 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 80 ÑĞ·Ñ‹ĞºĞ¾Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ Ğ·Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Navigating Code-Switching: Challenges and Advances in Multilingual NLP",
                    "desc": "This survey examines the progress and obstacles faced by large language models (LLMs) in handling code-switching, which is the mixing of languages in communication. It highlights that despite advancements, many LLMs still have difficulties with mixed-language inputs due to a lack of diverse datasets and evaluation methods. The paper reviews a wide range of studies across various NLP tasks and languages, categorizing recent improvements in model architecture and training strategies. It emphasizes the importance of developing inclusive datasets and fair evaluation practices to enhance multilingual capabilities in AI."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šè¯­è¨€æ™ºèƒ½çš„ä»£ç åˆ‡æ¢ç ”ç©¶",
                    "desc": "è¿™ç¯‡è°ƒæŸ¥è®ºæ–‡åˆ†æäº†å¯¹ä»£ç åˆ‡æ¢ï¼ˆCSWï¼‰æ•æ„Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç°çŠ¶ï¼Œå¼ºè°ƒäº†å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿›å±•å’ŒæŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿…é€Ÿå‘å±•ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†æ··åˆè¯­è¨€è¾“å…¥ã€æœ‰é™çš„CSWæ•°æ®é›†å’Œè¯„ä¼°åè§æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚è®ºæ–‡æä¾›äº†å¯¹CSWæ•æ„ŸLLMç ”ç©¶çš„é¦–æ¬¡å…¨é¢åˆ†æï¼Œæ¶µç›–äº†äº”ä¸ªç ”ç©¶é¢†åŸŸã€12ä¸ªNLPä»»åŠ¡ã€30å¤šä¸ªæ•°æ®é›†å’Œ80å¤šç§è¯­è¨€ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªè·¯çº¿å›¾ï¼Œå¼ºè°ƒéœ€è¦åŒ…å®¹æ€§æ•°æ®é›†ã€å…¬å¹³è¯„ä¼°å’ŒåŸºäºè¯­è¨€å­¦çš„æ¨¡å‹ï¼Œä»¥å®ç°çœŸæ­£çš„å¤šè¯­è¨€æ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06673",
            "title": "Heptapod: Language Modeling on Visual Signals",
            "url": "https://huggingface.co/papers/2510.06673",
            "abstract": "Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.",
            "score": 2,
            "issue_id": 6326,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "19f2468fce496409",
            "authors": [
                "Yongxin Zhu",
                "Jiawei Chen",
                "Yuanzhe Chen",
                "Zhuo Chen",
                "Dongya Jia",
                "Jian Cong",
                "Xiaobin Zhuang",
                "Yuping Wang",
                "Yuxuan Wang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06673.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ™",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ 2D Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Heptapod â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ causal attention Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ 2D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾regÑ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ self-supervised Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ masked autoencoding, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ FID 2.70, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ causal Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Heptapod: Redefining Image Generation with Causal Attention",
                    "desc": "Heptapod is an advanced image autoregressive model that utilizes causal attention to enhance image generation. It innovatively predicts the next 2D distribution of images, allowing it to learn from the entire spatial grid at each step. By integrating sequential modeling with holistic self-supervised learning, Heptapod effectively captures complex image semantics. Its performance on the ImageNet benchmark, achieving an FID of 2.70, demonstrates its superiority over earlier causal autoregressive models."
                },
                "zh": {
                    "title": "Heptapodï¼šå›¾åƒç”Ÿæˆçš„æ–°æ€è·¯",
                    "desc": "Heptapodæ˜¯ä¸€ç§å›¾åƒè‡ªå›å½’æ¨¡å‹ï¼Œé‡‡ç”¨å› æœæ³¨æ„åŠ›å’Œä¸‹ä¸€æ­¥äºŒç»´åˆ†å¸ƒé¢„æµ‹çš„æ–¹æ³•ã€‚å®ƒç»“åˆäº†é¡ºåºå»ºæ¨¡å’Œæ•´ä½“è‡ªç›‘ç£å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡é‡å»ºä¸ºä¸­å¿ƒçš„è§†è§‰æ ‡è®°å™¨ï¼Œå­¦ä¹ åœ¨æ¯ä¸ªæ—¶é—´æ­¥é¢„æµ‹æ•´ä¸ªäºŒç»´ç©ºé—´ç½‘æ ¼çš„åˆ†å¸ƒã€‚Heptapodçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å­¦ä¹ ç›®æ ‡ï¼Œç»Ÿä¸€äº†è‡ªå›å½’æ¡†æ¶çš„é¡ºåºå»ºæ¨¡ä¸æ©ç è‡ªç¼–ç çš„è‡ªç›‘ç£å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05491",
            "title": "NorMuon: Making Muon more efficient and scalable",
            "url": "https://huggingface.co/papers/2510.05491",
            "abstract": "NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.",
            "score": 2,
            "issue_id": 6326,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "51527f2ec5914706",
            "authors": [
                "Zichong Li",
                "Liming Liu",
                "Chen Liang",
                "Weizhu Chen",
                "Tuo Zhao"
            ],
            "affiliations": [
                "Georgia Tech",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05491.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NorMuon â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ learning rates Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ½ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ğ¸Ğ·-Ğ·Ğ° Ñ‡ĞµĞ³Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NorMuon Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 21.74% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Adam Ğ¸ Ğ½Ğ° 11.31% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Muon Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "NorMuon: Balancing Efficiency and Utilization in Large Language Model Training",
                    "desc": "NorMuon is a new optimizer designed to improve the training of large language models by combining orthogonalization with neuron-level adaptive learning rates. This approach enhances training efficiency and ensures that all parameters are utilized effectively, preventing any single neuron from dominating the optimization process. By maintaining second-order momentum statistics and applying normalization, NorMuon balances the updates across neurons while benefiting from improved optimization geometry. Experiments show that NorMuon outperforms traditional optimizers like Adam and Muon, achieving significant gains in training efficiency without increasing memory usage."
                },
                "zh": {
                    "title": "NorMuonï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„æ–°ä¼˜åŒ–å™¨",
                    "desc": "NorMuonæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–å™¨ï¼Œå®ƒç»“åˆäº†æ­£äº¤åŒ–å’Œç¥ç»å…ƒçº§è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿æŒæ¯ä¸ªç¥ç»å…ƒçš„äºŒé˜¶åŠ¨é‡ç»Ÿè®¡ï¼Œå¹¶åœ¨æ­£äº¤åŒ–åè¿›è¡Œè¡Œå½’ä¸€åŒ–ï¼Œè§£å†³äº†å‚æ•°åˆ©ç”¨ä¸å‡è¡¡çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNorMuonåœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šå‡ä¼˜äºAdamå’ŒMuonï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†21.74%ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œæ­£äº¤åŒ–å’Œè‡ªé€‚åº”å­¦ä¹ ç‡æ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥ä¸ºå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–å™¨è®¾è®¡å¼€è¾Ÿæ–°çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21842",
            "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
            "url": "https://huggingface.co/papers/2509.21842",
            "abstract": "DeepTravel is an end-to-end reinforcement learning framework for autonomous travel planning that uses a hierarchical reward system and reply-augmented learning to improve performance over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
            "score": 2,
            "issue_id": 6328,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "400daa394145aff0",
            "authors": [
                "Yansong Ning",
                "Rui Liu",
                "Jun Wang",
                "Kai Chen",
                "Wei Li",
                "Jun Fang",
                "Kan Zheng",
                "Naiqiang Tan",
                "Hao Liu"
            ],
            "affiliations": [
                "Didichuxing Co. Ltd",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21842.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ frontier Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "DeepTravel â€” ÑÑ‚Ğ¾ end-to-end Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ³ĞµĞ½Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² sandbox-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ replay-augmented learning Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen3 32B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ frontier LLMs Ğ²Ñ€Ğ¾Ğ´Ğµ OpenAI o1, o3 Ğ¸ DeepSeek R1 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Travel Planning with DeepTravel's Smart Learning!",
                    "desc": "DeepTravel is a novel reinforcement learning framework designed for autonomous travel planning. It utilizes a hierarchical reward system to enhance the agent's ability to plan and execute travel itineraries while reflecting on tool responses for continuous improvement. The framework operates in a controlled sandbox environment, allowing for effective training without the limitations of real-world data inconsistencies. By implementing a reply-augmented learning approach, DeepTravel significantly boosts the performance of smaller language models in travel planning tasks compared to larger models."
                },
                "zh": {
                    "title": "DeepTravelï¼šè‡ªä¸»æ—…è¡Œè§„åˆ’çš„æ–°çºªå…ƒ",
                    "desc": "DeepTravelæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“æ³¨äºè‡ªä¸»æ—…è¡Œè§„åˆ’ã€‚å®ƒé‡‡ç”¨åˆ†å±‚å¥–åŠ±ç³»ç»Ÿå’Œå›å¤å¢å¼ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œå·¥å…·ï¼Œå¹¶æ ¹æ®å·¥å…·çš„åé¦ˆè¿›è¡Œåæ€å’Œä¼˜åŒ–ã€‚é€šè¿‡æ„å»ºå¼ºå¤§çš„æ²™ç›’ç¯å¢ƒå’Œæœ‰æ•ˆçš„å¥–åŠ±å»ºæ¨¡ï¼ŒDeepTravelåœ¨æ—…è¡Œè§„åˆ’ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å‰æ²¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05891",
            "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
            "url": "https://huggingface.co/papers/2510.05891",
            "abstract": "A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D^3QE across different AR models, with robustness to real-world perturbations. Code is available at https://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
            "score": 1,
            "issue_id": 6321,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "b5fce1a59c0d659b",
            "authors": [
                "Yanran Zhang",
                "Bingyao Yu",
                "Yu Zheng",
                "Wenzhao Zheng",
                "Yueqi Duan",
                "Lei Chen",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University, China",
                "Department of Electronic Engineering, Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05891.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DÂ³QE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ² codebook Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ codebook Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ARForensics Ğ¸Ğ· 7 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Detecting AI-Generated Images with D^3QE",
                    "desc": "This paper presents a new method called Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for detecting images created by visual autoregressive models. It focuses on analyzing the frequency statistics of codebooks and the quantization errors that arise during image generation. By using a transformer that incorporates these frequency statistics into its attention mechanism, the method effectively distinguishes between real and synthetic images. The proposed approach shows high accuracy and generalization across various autoregressive models, making it robust against real-world image variations."
                },
                "zh": {
                    "title": "åˆ©ç”¨D^3QEæ£€æµ‹è‡ªå›å½’ç”Ÿæˆå›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£åˆ†å¸ƒå·®å¼‚æ„ŸçŸ¥é‡åŒ–è¯¯å·®ï¼ˆD^3QEï¼‰æ¥æ£€æµ‹ç”±è§†è§‰è‡ªå›å½’æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æä»£ç æœ¬é¢‘ç‡ç»Ÿè®¡å’Œé‡åŒ–è¯¯å·®ï¼Œè¯†åˆ«çœŸå®ä¸ä¼ªé€ å›¾åƒä¹‹é—´çš„ç‹¬ç‰¹æ¨¡å¼å’Œé¢‘ç‡åˆ†å¸ƒåå·®ã€‚ä¸ä¼ ç»Ÿçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè‡ªå›å½’æ¨¡å‹é€šè¿‡ç¦»æ•£æ ‡è®°é¢„æµ‹ç”Ÿæˆå›¾åƒï¼Œå±•ç°å‡ºæ›´é«˜çš„åˆæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD^3QEåœ¨ä¸åŒè‡ªå›å½’æ¨¡å‹ä¸­å…·æœ‰ä¼˜è¶Šçš„æ£€æµ‹å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06475",
            "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning\n  with Puzzles",
            "url": "https://huggingface.co/papers/2510.06475",
            "abstract": "PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.",
            "score": 0,
            "issue_id": 6335,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "f3109cfc5f50d36a",
            "authors": [
                "Yitao Long",
                "Yuru Jiang",
                "Hongjun Liu",
                "Yilun Zhao",
                "Jingchen Sun",
                "Yiqiu Shen",
                "Chen Zhao",
                "Arman Cohan",
                "Dennis Shasha"
            ],
            "affiliations": [
                "NYU Grossman School of Medicine",
                "New York University",
                "University at Buffalo, SUNY",
                "Yale University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06475.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ AI: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PuzzlePlex â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ foundation models Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 15 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹, Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ reasoning models Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. PuzzlePlex Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "PuzzlePlex: Benchmarking Reasoning and Planning in Foundation Models",
                    "desc": "The paper introduces PuzzlePlex, a benchmark designed to evaluate the reasoning and planning abilities of foundation models through a variety of puzzles. It includes 15 different types of puzzles, ranging from deterministic to stochastic games, and accommodates both single-player and two-player scenarios. The framework allows for the generation of increasingly complex puzzles as models improve, and it provides metrics for assessing model performance in instruction-based and code-based settings. The results indicate that while reasoning models excel in instruction-based tasks, code-based tasks present more challenges but are scalable and efficient alternatives."
                },
                "zh": {
                    "title": "PuzzlePlexï¼šè¯„ä¼°æ¨ç†ä¸è§„åˆ’èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "PuzzlePlexåŸºå‡†æµ‹è¯•è¯„ä¼°åŸºç¡€æ¨¡å‹çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½¿ç”¨å¤šæ ·åŒ–çš„éš¾é¢˜æä¾›æ€§èƒ½å’Œå¯æ‰©å±•æ€§çš„æŒ‡æ ‡å’Œè§è§£ã€‚è¯¥åŸºå‡†åŒ…å«15ç§ç±»å‹çš„éš¾é¢˜ï¼ŒåŒ…æ‹¬ç¡®å®šæ€§å’Œéšæœºæ€§æ¸¸æˆï¼Œéš¾åº¦å„å¼‚ï¼Œé€‚ç”¨äºå•äººå’ŒåŒäººåœºæ™¯ã€‚PuzzlePlexæ¡†æ¶ä¸ºæ¯ä¸ªæ¸¸æˆæä¾›å…¨é¢çš„ç¯å¢ƒï¼Œå¹¶æ”¯æŒç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„å®ä¾‹ï¼Œä»¥é€‚åº”åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ç»†è‡´çš„æŒ‡æ ‡æ¥æµ‹é‡æ€§èƒ½ï¼Œå¹¶å¯¹å‰æ²¿åŸºç¡€æ¨¡å‹åœ¨æŒ‡ä»¤åŸºç¡€å’Œä»£ç åŸºç¡€çš„ä¸¤ç§è®¾ç½®ä¸‹è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06426",
            "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial\n  Long-Form Question Answering",
            "url": "https://huggingface.co/papers/2510.06426",
            "abstract": "FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.",
            "score": 0,
            "issue_id": 6335,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "70ed711d0398d91b",
            "authors": [
                "Yitao Long",
                "Tiansheng Hu",
                "Yilun Zhao",
                "Arman Cohan",
                "Chen Zhao"
            ],
            "affiliations": [
                "NYU Shanghai",
                "New York University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06426.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#hallucinations",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° LLM Ğ½Ğ° Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinLFQA â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° end-to-end Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾ Ñ post-hoc Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Financial Question Answering with Reliable Attributions",
                    "desc": "FinLFQA is a benchmark that assesses how well Large Language Models (LLMs) can answer complex financial questions while providing reliable attributions. It highlights the issue of LLMs generating plausible but incorrect answers, known as hallucinations, and emphasizes the need for nuanced attribution beyond simple reference retrieval. The evaluation focuses on three key aspects: extracting supporting evidence from financial reports, demonstrating intermediate numerical reasoning, and applying domain-specific financial knowledge. The study shows that fine-grained metrics are crucial for evaluating model performance and that iterative refinement is beneficial when guided by external feedback."
                },
                "zh": {
                    "title": "è¯„ä¼°é‡‘èé—®ç­”ä¸­çš„å½’å› èƒ½åŠ›",
                    "desc": "FinLFQAæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ç¯‡é‡‘èé—®ç­”ä¸­æä¾›å¯é å’Œç»†è‡´å½’å› èƒ½åŠ›çš„åŸºå‡†ã€‚ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨ç®€å•çš„å¼•ç”¨æ£€ç´¢ï¼Œè€Œæˆ‘ä»¬è®¤ä¸ºåœ¨é‡‘èåº”ç”¨ä¸­ï¼Œå½’å› åº”è¶…è¶Šè¿™ä¸€ç‚¹ã€‚FinLFQAé€šè¿‡äººç±»æ³¨é‡Šè¯„ä¼°ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šä»è´¢åŠ¡æŠ¥å‘Šä¸­æå–çš„æ”¯æŒè¯æ®ã€ä¸­é—´æ•°å€¼æ¨ç†æ­¥éª¤ï¼Œä»¥åŠå½±å“æ¨ç†è¿‡ç¨‹çš„é¢†åŸŸç‰¹å®šé‡‘èçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–ç­”æ¡ˆè´¨é‡å’Œå½’å› è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04910",
            "title": "Glocal Information Bottleneck for Time Series Imputation",
            "url": "https://huggingface.co/papers/2510.04910",
            "abstract": "A new training paradigm, Glocal Information Bottleneck, improves time series imputation by aligning latent representations to retain global structure and local details under high missingness.  \t\t\t\t\tAI-generated summary \t\t\t\t Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
            "score": 0,
            "issue_id": 6334,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "cd332a35c362b2d3",
            "authors": [
                "Jie Yang",
                "Kexin Zhang",
                "Guibin Zhang",
                "Philip S. Yu",
                "Kaize Ding"
            ],
            "affiliations": [
                "National University of Singapore",
                "Northwestern University",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04910.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Glocal Information Bottleneck Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ°Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğµ Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Information Bottleneck framework, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Global Alignment loss, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Glocal-IB: Bridging Global Structure and Local Details in Time Series Imputation",
                    "desc": "The paper introduces a new training method called Glocal Information Bottleneck (Glocal-IB) to enhance time series imputation, which is the process of filling in missing values in time-dependent data. Traditional models often focus on recovering individual values but struggle with high rates of missing data, leading to poor performance during inference. Glocal-IB addresses this by aligning latent representations to maintain both global structure and local details, thus reducing the impact of noise from missing values. Experiments show that this approach significantly improves the quality of imputations and the alignment of latent representations across various datasets."
                },
                "zh": {
                    "title": "å…¨å±€ä¸å±€éƒ¨ä¿¡æ¯çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºGlocal Information Bottleneckï¼ˆGlocal-IBï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ—¶é—´åºåˆ—ç¼ºå¤±å€¼å¡«è¡¥ã€‚ä¼ ç»Ÿæ¨¡å‹é€šå¸¸åªå…³æ³¨å±€éƒ¨ä¿¡æ¯ï¼Œä¼˜åŒ–ç‚¹å¯¹ç‚¹é‡å»ºæŸå¤±ï¼Œå¯¼è‡´åœ¨é«˜ç¼ºå¤±ç‡ä¸‹è¡¨ç°ä¸ä½³ã€‚Glocal-IBé€šè¿‡å¼•å…¥å…¨å±€å¯¹é½æŸå¤±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ä¿ç•™å…¨å±€ç»“æ„çš„åŒæ—¶ï¼ŒæŠ‘åˆ¶ç”±ç¼ºå¤±å€¼å¼•èµ·çš„å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGlocal-IBåœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡å’Œå¯¹é½çš„æ½œåœ¨è¡¨ç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05152",
            "title": "A Single Character can Make or Break Your LLM Evals",
            "url": "https://huggingface.co/papers/2510.05152",
            "abstract": "The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.",
            "score": 0,
            "issue_id": 6335,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "acabea177e30672c",
            "authors": [
                "Jingtong Su",
                "Jianyu Zhang",
                "Karen Ullrich",
                "LÃ©on Bottou",
                "Mark Ibrahim"
            ],
            "affiliations": [
                "FAIR at Meta",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05152.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ» Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²ÑÑ‘: ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ (Ğ·Ğ°Ğ¿ÑÑ‚Ğ°Ñ, Ñ‚Ğ¾Ñ‡ĞºĞ° Ñ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°) Ğ´Ñ€Ğ°Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ñ‚ĞµÑÑ‚Ğµ MMLU Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ½Ğ° Â±23% Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°-Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼ĞµĞ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ¾Ñ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ». ĞĞ½Ğ°Ğ»Ğ¸Ğ· attention heads Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Delimiter Decisions Matter: Boosting LLM Performance!",
                    "desc": "This paper investigates how the choice of delimiter in formatting in-context examples affects the performance of large language models (LLMs). It reveals that even minor formatting decisions, such as using a comma or a new line, can lead to significant variations in model performance, with differences up to 23% on tasks like MMLU. The study shows that this sensitivity is consistent across various model families and does not improve with larger models. Additionally, the authors propose methods to enhance LLM robustness by specifying delimiters in prompts and provide recommendations for optimal delimiter choices."
                },
                "zh": {
                    "title": "åˆ†éš”ç¬¦é€‰æ‹©å½±å“æ¨¡å‹è¡¨ç°çš„å…³é”®",
                    "desc": "åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­é€‰æ‹©åˆ†éš”ç¬¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„å½±å“ã€‚å°½ç®¡ç¤ºä¾‹çš„æ•°é‡å·²ç»è¢«ç ”ç©¶å’Œæ ‡å‡†åŒ–ï¼Œä½†åˆ†éš”ç¬¦çš„é€‰æ‹©å´é²œæœ‰æ¢è®¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåˆ†éš”ç¬¦çš„ä¸åŒé€‰æ‹©å¯ä»¥å¯¼è‡´æ¨¡å‹å“åº”è´¨é‡çš„æ˜¾è‘—å˜åŒ–ï¼Œç”šè‡³å½±å“æ¨¡å‹çš„æ’åã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›æ–¹æ³•æ¥æé«˜LLMå¯¹åˆ†éš”ç¬¦é€‰æ‹©çš„é²æ£’æ€§ï¼Œå»ºè®®åœ¨æç¤ºä¸­æ˜ç¡®æŒ‡å®šåˆ†éš”ç¬¦ä»¥æå‡æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-08.html",
    "link_next": "2025-10-10.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "08.10",
        "en": "10/08",
        "zh": "10æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.10",
        "en": "10/10",
        "zh": "10æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 4,
        "#benchmark": 18,
        "#agents": 6,
        "#cv": 7,
        "#rl": 7,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 1,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 11,
        "#healthcare": 0,
        "#training": 25,
        "#robotics": 2,
        "#agi": 1,
        "#games": 6,
        "#interpretability": 6,
        "#reasoning": 12,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 25,
        "#survey": 4,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 8,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 3
    }
}