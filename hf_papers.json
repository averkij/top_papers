{
    "date": {
        "ru": "1 июля",
        "en": "July 1",
        "zh": "7月1日"
    },
    "time_utc": "2025-07-01 06:18",
    "weekday": 1,
    "issue_id": 4573,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.23044",
            "title": "Ovis-U1 Technical Report",
            "url": "https://huggingface.co/papers/2506.23044",
            "abstract": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.",
            "score": 22,
            "issue_id": 4573,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "caa82e446dde84a7",
            "authors": [
                "Guo-Hua Wang",
                "Shanshan Zhao",
                "Xinjie Zhang",
                "Liangfu Cao",
                "Pengxin Zhan",
                "Lunhao Duan",
                "Shiyin Lu",
                "Minghao Fu",
                "Xiaohao Chen",
                "Jianshan Zhao",
                "Yang Li",
                "Qing-Guo Chen"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23044.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#architecture",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Ovis-U1: Новый уровень мультимодального ИИ",
                    "desc": "Ovis-U1 - это мультимодальная языковая модель с 3 миллиардами параметров, объединяющая понимание разных типов данных, генерацию изображений по тексту и редактирование изображений. Модель использует диффузионный визуальный декодер и двунаправленный уточнитель токенов для задач генерации изображений. Ovis-U1 применяет новый унифицированный подход к обучению, начиная с языковой модели, что позволяет достичь лучших результатов по сравнению с обучением только на задачах понимания или генерации. Модель демонстрирует высокие результаты в различных бенчмарках, превосходя современные аналоги в мультимодальном понимании, генерации и редактировании изображений."
                },
                "en": {
                    "title": "Ovis-U1: Unifying Text and Image Mastery",
                    "desc": "Ovis-U1 is a powerful machine learning model with 3 billion parameters that excels in understanding and generating images from text, as well as editing images. It uses a unique training method that combines language understanding and image generation, leading to improved performance over models that focus on just one of these tasks. The model features a diffusion-based visual decoder and a bidirectional token refiner, which enhance its capabilities in generating high-quality images. Ovis-U1 has achieved impressive scores on various benchmarks, outperforming other state-of-the-art models in multimodal tasks."
                },
                "zh": {
                    "title": "Ovis-U1：多模态统一模型的突破",
                    "desc": "Ovis-U1是一个拥有30亿参数的统一模型，结合了多模态理解、文本到图像生成和图像编辑的能力。它采用基于扩散的视觉解码器和双向令牌精炼器，使得图像生成任务的表现与领先模型如GPT-4o相当。与一些使用固定多语言模型进行生成任务的模型不同，Ovis-U1采用了一种新的统一训练方法，从语言模型开始训练。通过将理解和生成任务结合，Ovis-U1在多个基准测试中表现出色，推动了多模态理解、生成和编辑的边界。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23858",
            "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
            "url": "https://huggingface.co/papers/2506.23858",
            "abstract": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.",
            "score": 20,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "684efafe36e7bbc8",
            "authors": [
                "Jianzong Wu",
                "Liang Hou",
                "Haotian Yang",
                "Xin Tao",
                "Ye Tian",
                "Pengfei Wan",
                "Di Zhang",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23858.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VMoBA: Эффективное внимание для генерации видео",
                    "desc": "VMoBA - это новый механизм разреженного внимания для видео-диффузионных моделей (VDM). Он решает проблему квадратичной сложности полных механизмов внимания, ускоряя обучение и инференс. VMoBA использует схему разбиения на блоки 1D-2D-3D, глобальный выбор блоков и выбор на основе порога для адаптации к пространственно-временным паттернам внимания. Эксперименты показывают значительное ускорение при сохранении или улучшении качества генерации видео."
                },
                "en": {
                    "title": "Accelerating Video Generation with Sparse Attention",
                    "desc": "VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality."
                },
                "zh": {
                    "title": "VMoBA：加速视频生成的新稀疏注意力机制",
                    "desc": "VMoBA是一种新颖的稀疏注意力机制，专为视频扩散模型（VDMs）设计，旨在解决全注意力机制的平方复杂度问题，从而加速训练和推理。通过对预训练视频变换器的注意力模式进行深入分析，VMoBA能够有效捕捉视频数据的时空特性。该机制通过三项关键改进提升了原有的MoBA框架，包括动态适应时空注意力模式的分层递归块划分方案、优先选择最显著的查询-键块交互的全局块选择，以及基于阈值的块选择来动态确定关注块的数量。实验结果表明，VMoBA在长序列训练中显著加速VDMs的训练，同时在生成质量上与全注意力机制相当或更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24123",
            "title": "Calligrapher: Freestyle Text Image Customization",
            "url": "https://huggingface.co/papers/2506.24123",
            "abstract": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.",
            "score": 14,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "ee3cd26fec757450",
            "authors": [
                "Yue Ma",
                "Qingyan Bai",
                "Hao Ouyang",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Hongyu Liu",
                "Zichen Liu",
                "Haofan Wang",
                "Jingye Chen",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group, China",
                "Hong Kong University of Science and Technology, China",
                "InstantX, Independent Research Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24123.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "✒️",
                "ru": {
                    "title": "Искусственный каллиграф: новый уровень цифровой типографики",
                    "desc": "Статья представляет Calligrapher - новую систему для генерации цифровой типографики на основе диффузионных моделей. Ключевые особенности включают самодистилляцию для создания стилистического эталона и локализованное внедрение стиля через обучаемый энкодер. Система использует предобученную генеративную модель текст-изображение и большую языковую модель для автоматического конструирования типографических образцов. Calligrapher превосходит традиционные модели в точности воспроизведения стилистических деталей и позиционирования глифов."
                },
                "en": {
                    "title": "Empowering Digital Typography with Style Consistency",
                    "desc": "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."
                },
                "zh": {
                    "title": "Calligrapher：自动化高质量数字排版的创新框架",
                    "desc": "Calligrapher 是一个基于扩散的框架，结合了自蒸馏和局部风格注入技术，旨在生成高质量且风格一致的数字排版。该框架解决了排版定制中风格控制和数据依赖的挑战，提出了三项关键技术贡献。首先，开发了一种自蒸馏机制，利用预训练的文本到图像生成模型和大型语言模型，自动构建以风格为中心的排版基准。其次，通过可训练的风格编码器引入局部风格注入框架，提取参考图像中的强健风格特征，从而提升目标风格的对齐精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24119",
            "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.24119",
            "abstract": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
            "score": 7,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "fbb4b5b047892d14",
            "authors": [
                "Bo Liu",
                "Leon Guertler",
                "Simon Yu",
                "Zichen Liu",
                "Penghui Qi",
                "Daniel Balcells",
                "Mickel Liu",
                "Cheston Tan",
                "Weiyan Shi",
                "Min Lin",
                "Wee Sun Lee",
                "Natasha Jaques"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), A*STAR",
                "National University of Singapore",
                "Northeastern University",
                "Plastic Labs",
                "Sea AI Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24119.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#games",
                    "#transfer_learning",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосовершенствование ИИ через игры с нулевой суммой",
                    "desc": "Статья представляет SPIRAL - фреймворк для самообучения языковых моделей через игру в игры с нулевой суммой против улучшающихся версий самих себя. Этот подход устраняет необходимость в человеческом кураторстве, создавая бесконечный курс прогрессивно усложняющихся задач. Обучение на одной только игре Кун-покер улучшило результаты модели Qwen3-4B-Base на 8.6% в математике и 8.4% в общих рассуждениях. Анализ показал, что перенос навыков происходит через три когнитивных паттерна: систематическую декомпозицию, расчет ожидаемой ценности и анализ по отдельным случаям."
                },
                "en": {
                    "title": "Empowering AI Reasoning through Self-Play in Zero-Sum Games",
                    "desc": "The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models."
                },
                "zh": {
                    "title": "自我对弈：提升语言模型推理能力的新方法",
                    "desc": "本文介绍了一种名为SPIRAL的自我对弈框架，旨在通过零和游戏提升语言模型的推理能力。该方法通过模型与自身不断改进的版本进行多轮对弈，消除了对人工监督的需求。SPIRAL能够生成无限的逐步挑战问题，使模型必须适应更强的对手，从而实现自我提升。实验结果表明，使用SPIRAL进行训练的模型在数学和一般推理方面均有显著提升，展示了零和游戏在自主推理发展中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23542",
            "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
            "url": "https://huggingface.co/papers/2506.23542",
            "abstract": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
            "score": 6,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "9c9286ea4d796818",
            "authors": [
                "Weida Wang",
                "Changyong He",
                "Jin Zeng",
                "Di Qiu"
            ],
            "affiliations": [
                "Google",
                "School of Computer Science and Technology, Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23542.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#graphs",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Революционное шумоподавление ToF: стабильность и четкость через графовое слияние",
                    "desc": "Предложена новая нейронная сеть для шумоподавления в датчиках глубины Time-of-Flight (ToF), использующая инвариантное к движению слияние графов и адаптивные фильтры. Сеть одновременно улучшает временную стабильность и пространственную четкость изображений глубины. Метод основан на самоподобии графовых структур во времени, что позволяет применять межкадровое геометрическое внимание для слияния графов. Экспериментальные результаты показывают, что предложенный подход достигает наилучших показателей точности и согласованности на синтетических и реальных наборах данных."
                },
                "en": {
                    "title": "Enhancing ToF Depth Images with Motion-Invariant Graph Fusion",
                    "desc": "This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets."
                },
                "zh": {
                    "title": "提升ToF深度图像的去噪效果",
                    "desc": "本文提出了一种新颖的时间飞行（ToF）深度去噪网络，利用运动不变图融合和自适应滤波器来提高时间稳定性和空间清晰度。以往的研究主要集中在单帧处理或多帧处理，但未考虑不同帧中对应像素的深度变化，导致时间不一致和空间模糊。我们的方法通过图结构的时间自相似性，实现跨帧几何注意力的图融合，并结合图的平滑性先验和ToF噪声分布的数据信度项，构建最大后验去噪问题。实验结果表明，该方案在合成DVToF数据集上实现了最先进的性能，并在真实Kinectv2数据集上展现了良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17930",
            "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
            "url": "https://huggingface.co/papers/2506.17930",
            "abstract": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent \"gibberish\" can remarkably improve performance across diverse tasks. Notably, the \"gibberish\" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",
            "score": 6,
            "issue_id": 4570,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 июня",
                "en": "June 22",
                "zh": "6月22日"
            },
            "hash": "346a389b3fbfb2bd",
            "authors": [
                "Jianyu Wang",
                "Zhiqiang Hu",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "MiroMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17930.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Бессмыслица в промптах улучшает работу языковых моделей",
                    "desc": "В статье представлена новая парадигма проектирования промптов под названием PromptQuine. Исследование показывает, что обрезание случайных демонстраций до 'бессмыслицы' может улучшить производительность больших языковых моделей (LLM) в различных задачах. Этот метод превосходит современные методы автоматической оптимизации промптов. PromptQuine использует эволюционный поиск для автоматического обнаружения стратегии обрезки, используя только небольшие наборы данных."
                },
                "en": {
                    "title": "Unlocking LLM Potential with 'Gibberish' Prompts!",
                    "desc": "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."
                },
                "zh": {
                    "title": "胡言乱语，提升模型表现的秘密",
                    "desc": "本文提出了一种新的提示设计范式，称为PromptQuine，挑战了传统的大型语言模型（LLM）提示方法。研究表明，将随机示例修剪成看似无意义的“胡言乱语”可以显著提高模型在多种任务上的表现，甚至超越了现有的最佳方法。尽管发现有效的修剪策略并不简单，但PromptQuine框架通过自我发现的方式，自动搜索修剪策略，利用低数据环境进行优化。我们的研究结果希望能为在上下文学习中的机制研究提供指导，并推动更开放的搜索算法的发展，以实现更有效的LLM提示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17417",
            "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
            "url": "https://huggingface.co/papers/2506.17417",
            "abstract": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.",
            "score": 6,
            "issue_id": 4570,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "57576e8287f4515e",
            "authors": [
                "Mingyuan Wu",
                "Meitang Li",
                "Jingcheng Yang",
                "Jize Jiang",
                "Kaizhuo Yan",
                "Zhaoheng Li",
                "Minjia Zhang",
                "Klara Nahrstedt"
            ],
            "affiliations": [
                "University of Illinois Urbana Champaign",
                "University of Michigan Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17417.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений в визуально-языковых моделях: победа генерации над верификацией",
                    "desc": "Исследование показывает, что методы вывода, такие как масштабирование во время декодирования и самоуточнение, улучшают способности рассуждения в визуально-языковых моделях. Генеративные методы, например мажоритарное голосование, оказались эффективнее методов верификации. Интересно, что модели, обученные с помощью обучения с подкреплением, не продемонстрировали ожидаемых преимуществ в самокоррекции. Основной причиной этого является отсутствие у таких моделей надежных возможностей самопроверки в визуальной и текстовой модальностях."
                },
                "en": {
                    "title": "Boosting VLM Reasoning with Inference-Time Techniques",
                    "desc": "This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities."
                },
                "zh": {
                    "title": "推理时技术提升视觉-语言模型的推理能力",
                    "desc": "本文探讨了推理时技术如何提升视觉-语言模型（VLM）的推理能力。研究发现，解码时间缩放和自我修正等技术在没有外部知识的情况下显著改善了模型的表现。尽管强化学习（RL）训练的模型未能显示出自我修正的优势，但基于生成的方法在提升性能方面明显优于基于验证的方法。实验结果表明，RL训练的VLM在视觉和文本模态上仍缺乏强大的自我验证能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16500",
            "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
            "url": "https://huggingface.co/papers/2506.16500",
            "abstract": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.",
            "score": 5,
            "issue_id": 4572,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "8a6cd2aa2a56bf51",
            "authors": [
                "Samir Khaki",
                "Xiuyu Li",
                "Junxian Guo",
                "Ligeng Zhu",
                "Chenfeng Xu",
                "Konstantinos N. Plataniotis",
                "Amir Yazdanbakhsh",
                "Kurt Keutzer",
                "Song Han",
                "Zhijian Liu"
            ],
            "affiliations": [
                "Google DeepMind",
                "MIT",
                "UC Berkeley",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16500.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "SparseLoRA: Ускорение тонкой настройки LLM без потери качества",
                    "desc": "SparseLoRA - это новый метод ускорения тонкой настройки больших языковых моделей (LLM). Он использует контекстную разреженность для динамического выбора подмножества весов для вычисления потерь и градиентов. Метод основан на легковесном SVD-оценщике разреженности, не требующем обучения. Эксперименты показывают, что SparseLoRA снижает вычислительные затраты до 2,2 раз и ускоряет процесс до 1,6 раз, сохраняя точность на различных задачах."
                },
                "en": {
                    "title": "SparseLoRA: Speeding Up LLM Fine-Tuning with Smart Sparsity",
                    "desc": "SparseLoRA is a novel method designed to enhance the efficiency of fine-tuning large language models (LLMs) by utilizing a sparse subset of weights for loss and gradient calculations. Unlike previous methods that only reduce the number of trainable parameters, SparseLoRA significantly cuts down on computational costs and speeds up the fine-tuning process. It employs a lightweight, training-free singular value decomposition (SVD) estimator to dynamically select which weights to use, ensuring optimal performance across different layers and training steps. Experimental results demonstrate that SparseLoRA can reduce computational costs by up to 2.2 times and improve speed by up to 1.6 times, all while maintaining high accuracy on various tasks."
                },
                "zh": {
                    "title": "SparseLoRA：加速大规模语言模型微调的稀疏方法",
                    "desc": "SparseLoRA是一种新方法，通过动态选择稀疏权重子集来减少大规模语言模型（LLM）微调的计算成本和加速过程。与其他参数高效的微调方法相比，SparseLoRA不仅降低了内存使用，还显著提高了计算效率。该方法使用轻量级的训练无关奇异值分解（SVD）稀疏性估计器，能够在训练过程中实时选择需要计算的权重。实验结果表明，SparseLoRA在保持准确性的同时，计算成本降低了最多2.2倍，速度提升了最多1.6倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23394",
            "title": "Teaching a Language Model to Speak the Language of Tools",
            "url": "https://huggingface.co/papers/2506.23394",
            "abstract": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
            "score": 2,
            "issue_id": 4570,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "19b24559e749a260",
            "authors": [
                "Simeon Emanuilov"
            ],
            "affiliations": [
                "Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23394.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "Многоязычное расширение возможностей ИИ: точные вызовы функций на любом языке",
                    "desc": "Представлена методология адаптации языковых моделей для надежного использования инструментов на разных языках, в частности для повышения точности вызова функций на болгарском языке. Исследование включает дообучение серии моделей BgGPT на двуязычном наборе данных с примерами вызовов функций. Разработанная система TUCAN достигает значительного улучшения точности вызова функций по сравнению с базовыми моделями. Результаты демонстрируют практический подход к расширению возможностей использования инструментов за пределами англоязычных систем."
                },
                "en": {
                    "title": "Empowering Multilingual Models for Effective Tool Use",
                    "desc": "This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications."
                },
                "zh": {
                    "title": "提升多语言工具使用能力的创新方法",
                    "desc": "本文提出了一种方法，旨在提高语言模型在多语言环境中工具使用的准确性，特别是保加利亚语的功能调用准确性。大多数多语言模型在非英语语言中缺乏可靠的工具使用能力，尤其是在低资源语言中表现不佳。研究通过对BgGPT模型系列进行持续训练，使用一个包含10,035个功能调用示例的双语数据集，来增强模型的工具使用能力。最终，研究推出的TUCAN模型在功能调用准确性上比基础模型提高了28.75%，并且在响应格式上也表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23219",
            "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
            "url": "https://huggingface.co/papers/2506.23219",
            "abstract": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
            "score": 1,
            "issue_id": 4573,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "86191833a27a1741",
            "authors": [
                "Jie Feng",
                "Shengyuan Wang",
                "Tianhui Liu",
                "Yanxin Xi",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
                "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China",
                "School of Electronic and Information Engineering, Beijing Jiaotong University, China",
                "University of Helsinki, Finland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23219.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "UrbanLLaVA: Революция в анализе городских данных с помощью мультимодального ИИ",
                    "desc": "UrbanLLaVA - это мультимодальная большая языковая модель, разработанная для обработки городских данных. Модель способна эффективно работать с четырьмя типами данных одновременно, превосходя существующие модели как в одномодальных, так и в сложных кросс-модальных сценариях. Авторы предложили многоэтапную структуру обучения, которая разделяет улучшение пространственного рассуждения и изучение предметных знаний. Экспериментальные результаты показывают, что UrbanLLaVA превосходит открытые и проприетарные мультимодальные языковые модели в различных городских задачах."
                },
                "en": {
                    "title": "UrbanLLaVA: Revolutionizing Urban Data Processing with Multi-Modal AI",
                    "desc": "UrbanLLaVA is a multi-modal large language model specifically designed to handle various urban datasets, excelling in both single-modal and complex cross-modal tasks. It addresses the limitations of existing models by providing a unified framework that processes multiple types of urban data simultaneously. The model is trained using a diverse urban instruction dataset and employs a multi-stage training approach to enhance spatial reasoning and domain knowledge. Experimental results indicate that UrbanLLaVA significantly outperforms other models in urban research, demonstrating strong generalization across different cities."
                },
                "zh": {
                    "title": "UrbanLLaVA：城市数据处理的新突破",
                    "desc": "UrbanLLaVA是一种多模态大型语言模型，能够有效处理城市数据集，适用于多种任务。与现有模型相比，它在单模态和复杂跨模态场景中表现更佳。该模型通过构建多样化的城市指令数据集，并采用多阶段训练框架，提升了空间推理和领域知识的学习能力。实验结果表明，UrbanLLaVA在多个城市的任务中均优于其他开源和专有的多模态大型语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22694",
            "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
            "url": "https://huggingface.co/papers/2506.22694",
            "abstract": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.",
            "score": 1,
            "issue_id": 4573,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 июня",
                "en": "June 28",
                "zh": "6月28日"
            },
            "hash": "a9c9ccf73356a002",
            "authors": [
                "Raghavv Goel",
                "Sudhanshu Agrawal",
                "Mukul Gagrani",
                "Junyoung Park",
                "Yifan Zao",
                "He Zhang",
                "Tian Liu",
                "Yiping Yang",
                "Xin Yuan",
                "Jiuyan Lu",
                "Chris Lott",
                "Mingu Lee"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22694.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Ускорение генерации текста путем обрезки словаря",
                    "desc": "Эта статья представляет технику VocabTrim, которая улучшает спекулятивное декодирование на основе драфтера путем уменьшения словаря языковой модели-драфтера. VocabTrim реконструирует head-часть языковой модели-драфтера, оставляя только ограниченный набор наиболее часто используемых токенов из словаря целевой модели. Хотя это немного снижает коэффициент принятия, значительно уменьшается задержка при создании черновика в условиях ограниченной памяти. Метод показал увеличение скорости генерации на 16% для модели Llama-3.2-3B-Instruct в условиях ограниченной памяти."
                },
                "en": {
                    "title": "Speed Up Drafting with VocabTrim!",
                    "desc": "This paper presents VocabTrim, a technique designed to enhance drafter-based speculative decoding by optimizing the vocabulary used in the drafting process. By limiting the vocabulary to only the most frequently sampled tokens from the target model, VocabTrim reduces the inference overhead associated with drafting, particularly in memory-constrained environments. Although this approach may slightly lower the acceptance rate of generated sequences, it significantly accelerates drafting speed, especially on edge devices. The results demonstrate a notable 16% improvement in memory-bound speed-up for Llama-3 models on Spec-Bench, showcasing the effectiveness of this method."
                },
                "zh": {
                    "title": "VocabTrim：提升推测解码速度的关键技术",
                    "desc": "本文提出了一种名为VocabTrim的技术，旨在通过减少草拟语言模型的词汇量来改善基于草拟的推测解码性能。这种方法可以降低在内存受限环境中的草拟延迟，特别是对于具有大词汇量的目标语言模型。VocabTrim通过重构草拟语言模型的头部，仅保留目标模型中最常被采样的有限词汇，从而提高生成速度。尽管限制词汇会略微降低接受率，但在边缘设备上显著提高了内存受限速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21448",
            "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
            "url": "https://huggingface.co/papers/2506.21448",
            "abstract": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io.",
            "score": 0,
            "issue_id": 4573,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "fefdbdbfb0394a3c",
            "authors": [
                "Huadai Liu",
                "Jialei Wang",
                "Kaicheng Luo",
                "Wen Wang",
                "Qian Chen",
                "Zhou Zhao",
                "Wei Xue"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (HKUST)",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#audio"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Думай как звукорежиссер: ИИ создает аудио для видео с помощью рассуждений",
                    "desc": "ThinkSound - это новая система для генерации высококачественного аудио из видео с использованием рассуждений по цепочке мыслей (Chain-of-Thought) и мультимодальной большой языковой модели. Система работает в три этапа: создание базового звукового ландшафта, интерактивное уточнение звуков отдельных объектов и целенаправленное редактирование с помощью естественно-языковых инструкций. Авторы также представили датасет AudioCoT с аннотациями структурированных рассуждений, связывающих визуальный контент, текстовые описания и синтез звука. Эксперименты показали, что ThinkSound достигает наилучших результатов в генерации аудио из видео по различным метрикам."
                },
                "en": {
                    "title": "ThinkSound: Revolutionizing Video-to-Audio Generation with Chain-of-Thought Reasoning",
                    "desc": "ThinkSound is a new framework that enhances video-to-audio generation by using Chain-of-Thought reasoning with a multimodal large language model. It addresses the challenge of creating high-quality audio that accurately reflects the visual content by breaking the process into three stages: foundational foley generation, interactive object-centric refinement, and targeted editing. Each stage utilizes contextually aligned reasoning to guide the audio generation, ensuring that the soundscapes are semantically coherent and tailored to user inputs. The framework also introduces the AudioCoT dataset, which connects visual elements, text descriptions, and sound synthesis, leading to state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "ThinkSound：视频生成高保真音频的新方法",
                    "desc": "ThinkSound是一个新颖的框架，利用链式思维推理与多模态大语言模型，从视频生成高质量音频，达到了各项基准测试的最先进结果。尽管端到端的视频到音频生成技术已有显著进步，但生成真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。该框架将生成过程分为三个互补阶段：基础音效生成、交互式对象中心细化和基于自然语言指令的目标编辑。通过引入AudioCoT数据集，ThinkSound在视频到音频生成的实验中表现出色，尤其在音频指标和链式思维指标上均取得了领先成绩。"
                }
            }
        }
    ],
    "link_prev": "2025-06-30.html",
    "link_next": "2025-07-02.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.06",
        "en": "06/30",
        "zh": "6月30日"
    },
    "short_date_next": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7月2日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}