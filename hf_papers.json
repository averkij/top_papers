{
    "date": {
        "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 11",
        "zh": "11æœˆ11æ—¥"
    },
    "time_utc": "2024-11-11 04:14",
    "weekday": 0,
    "issue_id": 508,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 2,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "38beaabd86eeaa88",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ PEFT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA, (IA)^3 Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ PEFT Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° LoRA Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šæå‡å•å…ƒæµ‹è¯•ç”Ÿæˆçš„ç»æµæ€§ä¸æœ‰æ•ˆæ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒè™½ç„¶æœ‰æ•ˆï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼ŒPEFTæ–¹æ³•é€šè¿‡åªå¾®è°ƒéƒ¨åˆ†å‚æ•°æ¥é™ä½è®¡ç®—å¼€é”€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPEFTæ–¹æ³•åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æç¤ºå¾®è°ƒåœ¨æˆæœ¬å’Œèµ„æºåˆ©ç”¨ä¸Šæœ€ä¸ºæœ‰æ•ˆã€‚è®ºæ–‡è¿˜æ¯”è¾ƒäº†ä¸åŒæ¨¡å‹æ¶æ„å’Œå¤§å°ä¸‹çš„å¤šç§PEFTæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 1,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "b23d3650ace21f86",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "StdGEN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "StdGEN - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ StdGEN Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Semantic-aware Large Reconstruction Model (S-LRM), Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ†Ğ²ĞµÑ‚ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ StdGEN Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGENï¼šé«˜æ•ˆç”Ÿæˆå¯åˆ†è§£3Dè§’è‰²çš„åˆ›æ–°ç®¡é“",
                    "desc": "StdGENæ˜¯ä¸€ç§åˆ›æ–°çš„ç®¡é“ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒStdGENåœ¨å¯åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆç»†è‡´çš„3Dè§’è‰²ï¼Œä¸”å„ä¸ªè¯­ä¹‰ç»„ä»¶å¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘åˆ†ç¦»ã€‚å…¶æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚é€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼ŒStdGENå®ç°äº†é«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ç”Ÿæˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½æ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡æ¨¡å‹ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…ä»‹ç»äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMï¼Œæ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œå¹¶æä¾›è¯¦ç»†çš„è®­ç»ƒæ•°æ®å’Œåè®®ã€‚é€šè¿‡è¿™ç§å¼€æ”¾æ€§ï¼Œä½œè€…å¸Œæœ›åŠ é€Ÿä»£ç AIçš„ç ”ç©¶å’Œå¯é‡å¤çš„è¿›å±•ã€‚",
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i dÃ imÇ shÄ“ngchÃ©ng, tuÄ«lÇ rÃ¨nwÃ¹ hÃ© dÃ ilÇ xÃ¬tÇ’ng zhÅng de zhÃ²ngyÃ oxÃ¬ng. SuÄ«rÃ¡n kÄifÃ ng fÇngwÃ¨n de dÃ imÇ LLMs xÃ¬ngnÃ©ng jÃ¬nkÃ¨ zhuÄnyÇ’u mÃ³xÃ­ng, dÃ n shÃ¬hÃ© yÃ¡nge kÄ“xuÃ© yÃ¡njiÅ« de gÄo zhÃ¬liÃ ng mÃ³xÃ­ng rÃ©ngrÃ¡n yÇ’uxiÃ n. WÃ¨ile tiÃ¡nbÇ” zhÃ¨ yÄ« kÃ²ngbÃ¡i, zuÃ²zhÄ› jiÃ¨shÃ o le OpenCoder, yÄ«gÃ¨ dÇngjiÄn de dÃ imÇ LLM, xÃ¬ngnÃ©ng jÃ¬mÇ lÇngxiÄn mÃ³xÃ­ng, bÃ¬ng tÃ­gÅng xiÃ¡ngxÃ¬ de xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© xiÃ©yÃ¬. TÅngguÃ² zhÃ¨ zhÇ’ng kÄifÃ ngxÃ¬ng, zuÃ²zhÄ› xÄ«wÃ ng jiÄsÃ¹ dÃ imÇ AI de yÃ¡njiÅ« hÃ© kÄ› chÃ³ngfÃ¹ de jÃ¬nbÃ¹.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"ä»£ç ç”Ÿæˆ\", \"pinyin\": \"dÃ i mÇ shÄ“ng chÃ©ng\", \"trans\": \"code generation\"},\n    {\"word\": \"æ¨ç†ä»»åŠ¡\", \"pinyin\": \"tuÄ« lÇ rÃ¨n wÃ¹\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"ä»£ç†ç³»ç»Ÿ\", \"pinyin\": \"dÃ i lÇ xÃ¬ tÇ’ng\", \"trans\": \"proxy system\"},\n    {\"word\": \"é‡è¦æ€§\", \"pinyin\": \"zhÃ²ng yÃ o xÃ¬ng\", \"trans\": \"importance\"},\n    {\"word\": \"å¼€æ”¾è®¿é—®\", \"pinyin\": \"kÄi fÃ ng fÇng wÃ¨n\", \"trans\": \"open access\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æ¥è¿‘\", \"pinyin\": \"jiÄ“ jÃ¬n\", \"trans\": \"close to\"},\n    {\"word\": \"ä¸“æœ‰æ¨¡å‹\", \"pinyin\": \"zhuÄn yÇ’u mÃ³ xÃ­ng\", \"trans\": \"proprietary model\"},\n    {\"word\": \"é€‚åˆ\", \"pinyin\": \"shÃ¬ hÃ©\", \"trans\": \"suitable\"},\n    {\"word\": \"ä¸¥æ ¼\", \"pinyin\": \"yÃ¡n gÃ©\", \"trans\": \"strict\"},\n    {\"word\": \"ç§‘å­¦ç ”ç©¶\", \"pinyin\": \"kÄ“ xuÃ© yÃ¡n jiÅ«\", \"trans\": \"scientific research\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’u xiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"å¡«è¡¥\", \"pinyin\": \"tiÃ¡n bÇ”\", \"trans\": \"fill\"},\n    {\"word\": \"ç©ºç™½\", \"pinyin\": \"kÃ²ng bÃ¡i\", \"trans\": \"gap\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"OpenCoder\", \"pinyin\": \"OpenCoder\", \"trans\": \"OpenCoder\"},\n    {\"word\": \"é¡¶å°–\", \"pinyin\": \"dÇng jiÄn\", \"trans\": \"top-notch\"},\n    {\"word\": \"åª²ç¾\", \"pinyin\": \"pÃ¬ mÄ›i\", \"trans\": \"rival\"},\n    {\"word\": \"é¢†å…ˆæ¨¡å‹\", \"pinyin\": \"lÇng xiÄn mÃ³ xÃ­ng\", \"trans\": \"leading model\"},\n    {\"word\": \"è¯¦ç»†\", \"pinyin\": \"xiÃ¡ng xÃ¬\", \"trans\": \"detailed\"},\n    {\"word\": \"è®­ç»ƒæ•°æ®\", \"pinyin\": \"xÃ¹n liÃ n shÃ¹ jÃ¹\", \"trans\": \"training data\"},\n    {\"word\": \"åè®®\", \"pinyin\": \"xiÃ© yÃ¬\", \"trans\": \"protocol\"},\n    {\"word\": \"å¼€æ”¾æ€§\", \"pinyin\": \"kÄi fÃ ng xÃ¬ng\", \"trans\": \"openness\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"},\n    {\"word\": \"å¯é‡å¤\", \"pinyin\": \"kÄ› chÃ³ng fÃ¹\", \"trans\": \"reproducible\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"}\n]",
        "trans": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While open-access code LLMs perform nearly as well as proprietary models, high-quality models suitable for rigorous scientific research remain limited. To fill this gap, the authors introduce OpenCoder, a top-tier code LLM that matches the performance of leading models and provides detailed training data and protocols. Through this openness, the authors aim to accelerate research and reproducible progress in code AI.",
        "update_ts": "2024-11-10 10:11"
    }
}