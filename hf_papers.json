{
    "date": {
        "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 3",
        "zh": "10æœˆ3æ—¥"
    },
    "time_utc": "2025-10-05 06:32",
    "weekday": 4,
    "issue_id": 6248,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.00446",
            "title": "LongCodeZip: Compress Long Context for Code Language Models",
            "url": "https://huggingface.co/papers/2510.00446",
            "abstract": "LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
            "score": 80,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "b9bb4e93a2d263ea",
            "authors": [
                "Yuling Shi",
                "Yichun Qian",
                "Hongyu Zhang",
                "Beijun Shen",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Chongqing University, Chongqing, China",
                "Shanghai Jiao Tong University, Shanghai, China",
                "Stanford University, Stanford, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00446.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#data",
                    "#optimization",
                    "#plp"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LongCodeZip â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ: Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½-Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ¾ 5.6x Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ñ€Ğ¾Ğ´Ğµ LLMLingua."
                },
                "en": {
                    "title": "Efficient Code Compression for LLMs with LongCodeZip",
                    "desc": "LongCodeZip is a specialized framework designed to compress code for Large Language Models (LLMs) while maintaining performance. It utilizes a dual-stage compression approach, first applying coarse-grained compression to identify and prioritize relevant function-level chunks based on their importance. Then, it employs fine-grained compression to further refine these functions into optimal segments, ensuring that only the most pertinent information is retained. This method significantly reduces context size, achieving up to a 5.6x compression ratio, which enhances the efficiency of code-related tasks without sacrificing output quality."
                },
                "zh": {
                    "title": "æå‡ä»£ç æ™ºèƒ½çš„å‹ç¼©æ•ˆç‡",
                    "desc": "LongCodeZip æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„ä»£ç å‹ç¼©æ¡†æ¶ï¼Œé‡‡ç”¨åŒé˜¶æ®µå‹ç¼©ç­–ç•¥æ¥å‡å°‘ä¸Šä¸‹æ–‡å¤§å°è€Œä¸é™ä½æ€§èƒ½ã€‚å®ƒé¦–å…ˆé€šè¿‡æ¡ä»¶å›°æƒ‘åº¦å¯¹å‡½æ•°çº§å—è¿›è¡Œç²—ç²’åº¦å‹ç¼©ï¼Œä¿ç•™æœ€ç›¸å…³çš„å‡½æ•°ï¼›ç„¶åè¿›è¡Œç»†ç²’åº¦å‹ç¼©ï¼Œæ ¹æ®å›°æƒ‘åº¦å°†ä¿ç•™çš„å‡½æ•°åˆ†å—ï¼Œå¹¶åœ¨è‡ªé€‚åº”ä»¤ç‰Œé¢„ç®—ä¸‹é€‰æ‹©æœ€ä½³å­é›†ã€‚é€šè¿‡åœ¨ä»£ç è¡¥å…¨ã€æ‘˜è¦å’Œé—®ç­”ç­‰å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒLongCodeZip æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œå‹ç¼©æ¯”é«˜è¾¾ 5.6 å€ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆå‡å°‘äº†ä¸Šä¸‹æ–‡å¤§å°ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ï¼Œä»è€Œæå‡äº†ä»£ç æ™ºèƒ½åº”ç”¨çš„æ•ˆç‡å’Œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02283",
            "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
            "url": "https://huggingface.co/papers/2510.02283",
            "abstract": "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
            "score": 67,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "6c012f635291b12f",
            "authors": [
                "Justin Cui",
                "Jie Wu",
                "Ming Li",
                "Tao Yang",
                "Xiaojie Li",
                "Rui Wang",
                "Andrew Bai",
                "Yuanhao Ban",
                "Cho-Jui Hsieh"
            ],
            "affiliations": [
                "ByteDance Seed",
                "UCLA",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02283.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 4 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿ĞµÑ€ĞµÑĞºÑĞ¿Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Long Video Generation with Self-Sampled Guidance",
                    "desc": "This paper presents a novel method for improving the generation of long videos using segments from self-generated long videos to guide student models. By leveraging the knowledge of teacher models, the approach maintains high video quality and temporal consistency without the need for additional supervision or retraining. The method allows for scaling video lengths significantly, achieving up to 20 times the length of what teacher models can produce. Experimental results show that this technique outperforms existing methods in both fidelity and consistency, enabling the generation of videos lasting over 4 minutes."
                },
                "zh": {
                    "title": "æå‡é•¿è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºé•¿æ—¶é—´è§†é¢‘ç”Ÿæˆçš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨è‡ªç”Ÿæˆé•¿è§†é¢‘çš„é‡‡æ ·ç‰‡æ®µæ¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨ä¸éœ€è¦é¢å¤–ç›‘ç£æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä¿æŒè´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œé¿å…äº†å¸¸è§çš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ›å…‰å’Œé”™è¯¯ç´¯ç§¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå°†è§†é¢‘é•¿åº¦æ‰©å±•åˆ°æ•™å¸ˆæ¨¡å‹èƒ½åŠ›çš„20å€ï¼Œç”Ÿæˆæ—¶é•¿å¯è¾¾4åˆ†15ç§’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç”Ÿæˆçš„ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02245",
            "title": "ExGRPO: Learning to Reason from Experience",
            "url": "https://huggingface.co/papers/2510.02245",
            "abstract": "ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
            "score": 59,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "2d2caae66ebc961d",
            "authors": [
                "Runzhe Zhan",
                "Yafu Li",
                "Zhi Wang",
                "Xiaoye Qu",
                "Dongrui Liu",
                "Jing Shao",
                "Derek F. Wong",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02245.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ñ†ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ExGRPO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… on-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ExGRPO Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 1.5B Ğ´Ğ¾ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 3.5-7.6 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ on-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚ĞµÑ€Ğ¿ÑÑ‚ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ñƒ."
                },
                "en": {
                    "title": "Prioritizing Valuable Experiences for Better Reinforcement Learning",
                    "desc": "ExGRPO is a new framework designed to enhance reinforcement learning from verifiable rewards (RLVR) for large language models. It addresses the inefficiencies of traditional on-policy training by prioritizing valuable reasoning experiences, which helps stabilize the learning process. The framework identifies key indicators of experience value, such as rollout correctness and entropy, to optimize the learning dynamics. Experiments demonstrate that ExGRPO significantly improves reasoning performance across various models, making it a crucial advancement in efficient RLVR."
                },
                "zh": {
                    "title": "ä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼ç»éªŒçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "ExGRPOæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼çš„æ¨ç†ç»éªŒï¼Œä»è€Œæ”¹å–„å’Œç¨³å®šåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚ä¼ ç»Ÿçš„åœ¨çº¿è®­ç»ƒæ–¹æ³•åœ¨æ¯æ¬¡æ›´æ–°åä¼šä¸¢å¼ƒç»éªŒï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä¸ç¨³å®šã€‚æœ¬æ–‡é¦–æ¬¡æ¢è®¨äº†ä»€ä¹ˆæ ·çš„æ¨ç†ç»éªŒæ˜¯æœ‰ä»·å€¼çš„ï¼Œå¹¶ç¡®å®šäº†å›æ»šæ­£ç¡®æ€§å’Œç†µä½œä¸ºæœ‰æ•ˆçš„ç»éªŒä»·å€¼æŒ‡æ ‡ã€‚é€šè¿‡è¿™äº›è§è§£ï¼ŒExGRPOç»„ç»‡å’Œä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼çš„ç»éªŒï¼Œå¹¶é‡‡ç”¨æ··åˆç­–ç•¥ç›®æ ‡æ¥å¹³è¡¡æ¢ç´¢ä¸ç»éªŒåˆ©ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02314",
            "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
            "url": "https://huggingface.co/papers/2510.02314",
            "abstract": "A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
            "score": 52,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "334776e6e757ace4",
            "authors": [
                "Bo-Hsu Ke",
                "You-Zhe Xie",
                "Yu-Lun Liu",
                "Wei-Chen Chiu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.02314.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° 3D Gaussian Splatting Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 3D Gaussian Splatting (3DGS) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Kernel Density Estimation (KDE), ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ğ»Ğ»ÑĞ·Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ multi-view consistency, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ KDE Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Attack Effectiveness in 3D Gaussian Splatting with Density-Guided Poisoning",
                    "desc": "This paper presents a new method for attacking 3D Gaussian Splatting (3DGS) by using a density-guided poisoning technique. The approach involves injecting Gaussian points into areas with low density, which creates misleading visual artifacts that are noticeable from certain viewpoints. Additionally, the method employs an adaptive noise strategy to further disrupt the consistency of views across the 3D scene. The authors also introduce a new evaluation protocol based on Kernel Density Estimation (KDE) to measure the effectiveness of these attacks, showing that their method outperforms existing techniques."
                },
                "zh": {
                    "title": "å¢å¼º3Dé«˜æ–¯ç‚¹äº‘æ”»å‡»æ•ˆæœçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯†åº¦å¼•å¯¼ä¸­æ¯’æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼º3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3D Gaussian Splattingï¼‰çš„æ”»å‡»æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä½å¯†åº¦åŒºåŸŸæ³¨å…¥é«˜æ–¯ç‚¹ï¼Œç ´åå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨å—å½±å“çš„è§†è§’ä¸­åµŒå…¥æ˜æ˜¾çš„è™šå‡ç‰©ä½“ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å™ªå£°ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç³»ç»Ÿçš„KDEè¯„ä¼°åè®®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®¢è§‚åœ°è¯„ä¼°æ”»å‡»éš¾åº¦ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02297",
            "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
            "url": "https://huggingface.co/papers/2510.02297",
            "abstract": "Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.",
            "score": 36,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "a5b5b6a9b4ca0924",
            "authors": [
                "Wentao Zhang",
                "Yang Young Lu",
                "Yuntian Deng"
            ],
            "affiliations": [
                "University of Waterloo",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02297.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Interactive Training â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ğ¸Ğ»Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ¢Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ´ÑÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Neural Networks with Real-Time Interactive Training",
                    "desc": "This paper presents Interactive Training, a novel framework that enhances neural network training by allowing real-time interventions. It addresses the limitations of traditional training methods, which often lack the flexibility to adapt to issues as they arise. The framework facilitates communication between users or AI agents and the training process, enabling dynamic adjustments to hyperparameters, training data, and model checkpoints. The results from case studies show that Interactive Training leads to better stability, less sensitivity to initial settings, and greater adaptability to user requirements."
                },
                "zh": {
                    "title": "å®æ—¶åé¦ˆï¼Œæå‡è®­ç»ƒçµæ´»æ€§",
                    "desc": "äº’åŠ¨è®­ç»ƒæ˜¯ä¸€ç§æ¡†æ¶ï¼Œå…è®¸åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶çš„åé¦ˆé©±åŠ¨å¹²é¢„ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œè®­ç»ƒé€šå¸¸éµå¾ªå›ºå®šçš„ä¼˜åŒ–æµç¨‹ï¼Œç¼ºä¹åŠ¨æ€åº”å¯¹ä¸ç¨³å®šæ€§æˆ–æ–°å‡ºç°é—®é¢˜çš„çµæ´»æ€§ã€‚æœ¬æ–‡ä»‹ç»çš„äº’åŠ¨è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒäººç±»ä¸“å®¶æˆ–è‡ªåŠ¨åŒ–AIä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶å¹²é¢„ï¼Œç”¨æˆ·å¯ä»¥åŠ¨æ€è°ƒæ•´ä¼˜åŒ–å™¨è¶…å‚æ•°ã€è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚é€šè¿‡ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†äº’åŠ¨è®­ç»ƒåœ¨è®­ç»ƒç¨³å®šæ€§ã€å¯¹åˆå§‹è¶…å‚æ•°çš„æ•æ„Ÿæ€§é™ä½ä»¥åŠå¯¹ç”¨æˆ·éœ€æ±‚çš„é€‚åº”æ€§æé«˜æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02209",
            "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
            "url": "https://huggingface.co/papers/2510.02209",
            "abstract": "StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
            "score": 32,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "6d2362d30dbb6925",
            "authors": [
                "Yanxu Chen",
                "Zijun Yao",
                "Yantao Liu",
                "Jin Ye",
                "Jianing Yu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02209.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼",
                    "desc": "StockBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸ Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - Ñ†ĞµĞ½Ñ‹, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ - Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ, Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğµ Ğ¸Ğ»Ğ¸ ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑÑÑ†ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5 Ğ¸ Claude-4, Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ \"ĞºÑƒĞ¿Ğ¸ Ğ¸ Ğ´ĞµÑ€Ğ¶Ğ¸\", Ñ…Ğ¾Ñ‚Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "StockBench: Evaluating LLMs in Real-World Stock Trading",
                    "desc": "This paper introduces StockBench, a new benchmark for evaluating large language models (LLMs) in realistic stock trading scenarios. Unlike previous benchmarks that focus on static knowledge, StockBench assesses LLMs on their ability to make dynamic trading decisions based on daily market signals. The evaluation uses financial metrics to measure performance, revealing that while many LLMs struggle to outperform a basic buy-and-hold strategy, some show promise in generating higher returns and managing risk. This research highlights the complexities of applying LLMs in finance and aims to foster further exploration in developing effective financial agents."
                },
                "zh": {
                    "title": "StockBenchï¼šè¯„ä¼°é‡‘èä»£ç†çš„æœªæ¥æ½œåŠ›",
                    "desc": "StockBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒè§£å†³äº†ç°æœ‰é‡‘èåŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰äº¤æ˜“åŠ¨æ€å’Œè¿­ä»£ç‰¹æ€§çš„ä¸è¶³ã€‚é€šè¿‡æä¾›æ¯æ—¥å¸‚åœºä¿¡å·ï¼ŒLLM ä»£ç†éœ€è¦åšå‡ºä¹°å…¥ã€å–å‡ºæˆ–æŒæœ‰çš„å†³ç­–ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤§å¤šæ•° LLM ä»£ç†æœªèƒ½è¶…è¶Šç®€å•çš„ä¹°å…¥æŒæœ‰ç­–ç•¥ï¼Œä½†ä¸€äº›æ¨¡å‹æ˜¾ç¤ºå‡ºæ›´é«˜çš„å›æŠ¥æ½œåŠ›å’Œæ›´æœ‰æ•ˆçš„é£é™©ç®¡ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01149",
            "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
            "url": "https://huggingface.co/papers/2510.01149",
            "abstract": "ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available at https://huggingface.co/ModernVBERT.",
            "score": 26,
            "issue_id": 6231,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "676d94db77731b89",
            "authors": [
                "Paul Teiletche",
                "Quentin MacÃ©",
                "Max Conti",
                "Antonio Loison",
                "Gautier Viaud",
                "Pierre Colombo",
                "Manuel Faysse"
            ],
            "affiliations": [
                "CentraleSupelec, Paris-Saclay",
                "EPFL",
                "Equall.ai",
                "Illuin Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01149.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ encoder Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ModernVBERT â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ vision-language encoder Ñ 250 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… VLM Ñ contrastive losses Ğ½Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ retrieval. Ğ§ĞµÑ€ĞµĞ· ÑĞµÑ€Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ attention masking, Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ contrastive objectives Ñ late interaction. ModernVBERT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Compact Power: ModernVBERT Revolutionizes Document Retrieval",
                    "desc": "ModernVBERT is a new vision-language encoder designed to improve document retrieval tasks. It achieves better performance than larger models by optimizing key factors such as attention masking, image resolution, and modality alignment. The research highlights that traditional methods of fine-tuning large vision-language models can limit retrieval effectiveness. By implementing a more efficient approach with only 250 million parameters, ModernVBERT demonstrates superior results in retrieving documents compared to models that are ten times its size."
                },
                "zh": {
                    "title": "ModernVBERTï¼šå°å·§è€Œå¼ºå¤§çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨",
                    "desc": "ModernVBERTæ˜¯ä¸€ç§ç´§å‡‘çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼Œé€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æ©ç ã€å›¾åƒåˆ†è¾¨ç‡ã€æ¨¡æ€å¯¹é½å’Œå¯¹æ¯”ç›®æ ‡ï¼Œè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹åœ¨æ–‡æ¡£æ£€ç´¢ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶å¯¹å¤§å‹è§†è§‰-è¯­è¨€è§£ç å™¨è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§æˆæœ¬æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•å¾€å¾€ä¼šé™åˆ¶æ£€ç´¢æ€§èƒ½ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ”¹è¿›è§†è§‰æ–‡æ¡£æ£€ç´¢æ¨¡å‹çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œå¹¶æµ‹é‡äº†å¤šä¸ªå…³é”®å› ç´ çš„å½±å“ã€‚æœ€ç»ˆï¼ŒModernVBERTä»¥250Må‚æ•°çš„è§„æ¨¡ï¼Œåœ¨æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†é«˜è¾¾10å€æ›´å¤§çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01265",
            "title": "RLP: Reinforcement as a Pretraining Objective",
            "url": "https://huggingface.co/papers/2510.01265",
            "abstract": "RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
            "score": 26,
            "issue_id": 6222,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "80c397f193259627",
            "authors": [
                "Ali Hatamizadeh",
                "Syeda Nahida Akter",
                "Shrimai Prabhumoye",
                "Jan Kautz",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Yejin Choi"
            ],
            "affiliations": [
                "Boston University",
                "Carnegie Mellon University",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01265.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ RLP â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ reinforcement learning Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ post-training. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ñˆ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ verifier Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Â«Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼Â» ÑƒĞ¶Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-1.7B-Base ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¾ 19% Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ° Ğ´Ğ»Ñ Nemotron-Nano-12B-v2 ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ²Ñ‹Ñ€Ğ¾Ñ Ñ 42.81% Ğ´Ğ¾ 61.32%."
                },
                "en": {
                    "title": "Reinforcement Learning for Smarter Pretraining",
                    "desc": "This paper introduces RLP, a novel reinforcement pretraining objective that enhances reasoning models by incorporating exploration during the pretraining phase. Unlike traditional methods that only apply reinforcement learning after initial training, RLP encourages models to engage in exploratory reasoning earlier, treating chain-of-thought as an action that provides valuable information for future predictions. The reward system is designed to measure the improvement in predicting the next token based on both context and a reasoning chain, promoting independent thinking in models. The results show significant performance boosts across various benchmarks, particularly in reasoning-heavy tasks, demonstrating the effectiveness of integrating reinforcement learning into the pretraining process."
                },
                "zh": {
                    "title": "æ¢ç´¢é©±åŠ¨çš„å¼ºåŒ–é¢„è®­ç»ƒï¼Œæå‡æ¨ç†æ¨¡å‹è¡¨ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿¡æ¯é©±åŠ¨çš„å¼ºåŒ–é¢„è®­ç»ƒç›®æ ‡RLPï¼Œæ—¨åœ¨é€šè¿‡å°†æ¢ç´¢èå…¥é¢„è®­ç»ƒæ¥å¢å¼ºæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚RLPå°†æ€ç»´é“¾è§†ä¸ºä¸€ç§æ¢ç´¢è¡Œä¸ºï¼Œå¹¶æ ¹æ®å…¶å¯¹æœªæ¥æ ‡è®°é¢„æµ‹çš„ä¿¡æ¯å¢ç›Šæ¥è®¡ç®—å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ä¹‹å‰ç‹¬ç«‹æ€è€ƒï¼Œä»è€Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæ›´æ—©åœ°åŸ¹å…»ç‹¬ç«‹æ€è€ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RLPè¿›è¡Œé¢„è®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22067",
            "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
            "url": "https://huggingface.co/papers/2509.22067",
            "abstract": "Activation steering, intended to control LLM behavior, can instead increase harmful compliance and undermine model alignment safeguards.  \t\t\t\t\tAI-generated summary \t\t\t\t Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.",
            "score": 20,
            "issue_id": 6230,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "71a298f2089d76c2",
            "authors": [
                "Anton Korznikov",
                "Andrey Galichin",
                "Alexey Dontsov",
                "Oleg Y. Rogov",
                "Ivan Oseledets",
                "Elena Tutubalina"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.22067.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#hallucinations",
                    "#interpretability",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ activation steering â€” Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ñ€ÑƒÑˆĞ°Ñ‚ÑŒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ€Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 0% Ğ´Ğ¾ 27%, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· sparse autoencoder ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞµÑ‰Ñ‘ Ğ½Ğ° 2-4%. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ 20 ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ‚Ğ°ĞºÑƒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞµÑ‘ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Activation Steering: A Risky Path to Model Control",
                    "desc": "This paper investigates the technique of activation steering, which aims to control the behavior of large language models (LLMs) by modifying their hidden states during inference. Contrary to its intended purpose, the study finds that activation steering can actually lead to increased harmful compliance, undermining the safeguards designed to align model behavior with ethical standards. Through various experiments, the authors demonstrate that even random steering can significantly raise the likelihood of the model complying with harmful requests. The findings suggest that having precise control over a model's internal mechanisms does not ensure safe or desirable outcomes, challenging the notion that interpretability equates to safety in AI systems."
                },
                "zh": {
                    "title": "æ¿€æ´»å¼•å¯¼ï¼šå®‰å…¨æ€§çš„è¯¯åŒº",
                    "desc": "æ¿€æ´»å¼•å¯¼æ˜¯ä¸€ç§æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡Œä¸ºçš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å‘é‡ç›´æ¥æ·»åŠ åˆ°æ¨¡å‹çš„éšè—çŠ¶æ€ä¸­ã€‚å°½ç®¡å®ƒè¢«è§†ä¸ºä¸€ç§ç²¾ç¡®ã€å¯è§£é‡Šä¸”å¯èƒ½æ›´å®‰å…¨çš„æ›¿ä»£å¾®è°ƒçš„æ–¹æ³•ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼å®é™…ä¸Šä¼šç ´åæ¨¡å‹çš„å¯¹é½å®‰å…¨æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æœ‰å®³è¯·æ±‚çš„é¡ºä»æ€§å¢åŠ ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹å®¶æ—çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯éšæœºæ–¹å‘çš„å¼•å¯¼ä¹Ÿèƒ½å°†æœ‰å®³é¡ºä»çš„æ¦‚ç‡ä»0%æé«˜åˆ°2-27%ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†é€šè¿‡å¯è§£é‡Šæ€§å®ç°å®‰å…¨æ€§çš„ä¼ ç»Ÿè§‚å¿µï¼Œè¡¨æ˜å¯¹æ¨¡å‹å†…éƒ¨çš„ç²¾ç¡®æ§åˆ¶å¹¶ä¸ä¿è¯å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02286",
            "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
            "url": "https://huggingface.co/papers/2510.02286",
            "abstract": "DialTree-RPO, an on-policy reinforcement learning framework with tree search, autonomously discovers diverse multi-turn attack strategies against large language models, achieving higher attack success rates and uncovering new attack trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.",
            "score": 19,
            "issue_id": 6225,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "fa89fbe7ee7a8552",
            "authors": [
                "Ruohao Guo",
                "Afshin Oroojlooy",
                "Roshan Sridhar",
                "Miguel Ballesteros",
                "Alan Ritter",
                "Dan Roth"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Oracle AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02286.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#security"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DialTree-RPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ reinforcement learning Ğ¸ tree search Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑƒĞ³Ñ€Ğ¾Ğ·Ñƒ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. DialTree-RPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº."
                },
                "en": {
                    "title": "Unleashing Multi-Turn Attack Strategies with DialTree-RPO",
                    "desc": "DialTree-RPO is a novel reinforcement learning framework designed to autonomously identify multi-turn attack strategies against large language models (LLMs). By treating dialogue as a sequential decision-making problem, it utilizes tree search to explore diverse attack trajectories without relying on pre-defined templates or human-curated data. This approach significantly enhances the attack success rate, achieving over 25.9% improvement compared to existing methods focused on single-turn attacks. The framework reveals new strategies by learning optimal dialogue policies, addressing a critical gap in AI safety research."
                },
                "zh": {
                    "title": "DialTree-RPOï¼šè‡ªä¸»å‘ç°å¤šè½®æ”»å‡»ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "DialTree-RPOæ˜¯ä¸€ç§åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†æ ‘æœç´¢æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ”»å‡»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¯¹è¯è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢å¤šè½®æ”»å‡»çš„å¯èƒ½æ€§ï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDialTree-RPOåœ¨10ä¸ªç›®æ ‡æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡25.9%çš„æ”»å‡»æˆåŠŸç‡æå‡ï¼Œå¹¶æœ‰æ•ˆå‘ç°äº†æ–°çš„æ”»å‡»ç­–ç•¥ã€‚æ­¤ç ”ç©¶å¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨å¤šè½®æ”»å‡»é¢†åŸŸçš„ç©ºç™½ï¼Œå±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¯¹è¯ä¸­çš„è„†å¼±æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01591",
            "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
            "url": "https://huggingface.co/papers/2510.01591",
            "abstract": "Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).",
            "score": 19,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "5fb07bdb1a94a1b3",
            "authors": [
                "Zhenwen Liang",
                "Ruosen Li",
                "Yujun Zhou",
                "Linfeng Song",
                "Dian Yu",
                "Xinya Du",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Notre Dame",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01591.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ LLM ĞºĞ°Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ (hidden states) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ğ¼ÑƒÑ ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CLUE â€” Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ»ÑŒÑ‚Ñƒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. CLUE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ confidence-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ 56.7% Ğ´Ğ¾ 70.0% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AIME 24."
                },
                "en": {
                    "title": "Unlocking Hidden States for Accurate LLM Verification",
                    "desc": "This paper investigates how hidden states in Large Language Models (LLMs) can be used to assess the correctness of model outputs more effectively than traditional methods. It introduces CLUE, a minimalist verifier that leverages the geometric separability of hidden activations to classify outputs as correct or incorrect without needing trainable parameters. By summarizing reasoning traces with hidden state deltas and using nearest-centroid distance to classify correctness, CLUE demonstrates superior performance over existing text-level and confidence-based approaches. The results show significant improvements in accuracy, highlighting the potential of hidden states as a rich source of information for verification tasks."
                },
                "zh": {
                    "title": "åˆ©ç”¨éšè—çŠ¶æ€æå‡è¯­è¨€æ¨¡å‹çš„éªŒè¯å‡†ç¡®æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨éšè—çŠ¶æ€å¦‚ä½•ç¼–ç æ­£ç¡®æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºCLUEçš„ç®€çº¦éªŒè¯å™¨ã€‚CLUEåˆ©ç”¨éšè—çŠ¶æ€çš„å‡ ä½•å¯åˆ†ç¦»ç‰¹å¾ï¼Œèƒ½å¤Ÿåœ¨é‡æ’åºå’Œå‡†ç¡®æ€§æ–¹é¢è¶…è¶Šä¼ ç»Ÿçš„æ–‡æœ¬çº§å’ŒåŸºäºç½®ä¿¡åº¦çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹éšè—çŠ¶æ€çš„ç›´æ¥åˆ†æï¼ŒCLUEä¸éœ€è¦å¯è®­ç»ƒå‚æ•°ï¼Œä»…é€šè¿‡æ€»ç»“æ¨ç†è½¨è¿¹çš„éšè—çŠ¶æ€å˜åŒ–æ¥è¿›è¡Œåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLUEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01444",
            "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.01444",
            "abstract": "VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce VOGUE (Visual Uncertainty Guided Exploration), a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.",
            "score": 19,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "827fe718c8df2c9c",
            "authors": [
                "Rui Liu",
                "Dian Yu",
                "Tong Zheng",
                "Runpeng Dai",
                "Zongxia Li",
                "Wenhao Yu",
                "Zhenwen Liang",
                "Linfeng Song",
                "Haitao Mi",
                "Pratap Tokekar",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Bellevue, WA",
                "University of Maryland, College Park",
                "University of North Carolina, Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01444.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#games",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ VOGUE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (exploration) Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€Ğ°ĞºÑ‚ÑƒĞµÑ‚ ĞµĞ³Ğ¾ ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ±Ğ¾Ğ½ÑƒÑ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 2.6% Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 3.7% Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning through Visual Uncertainty Exploration",
                    "desc": "The paper introduces VOGUE, a method that enhances multimodal reasoning in large language models by focusing on the visual input space. It addresses the exploration challenges in reinforcement learning with verifiable rewards by quantifying how sensitive a policy is to changes in visual inputs. VOGUE treats images as stochastic contexts, using symmetric KL divergence to measure policy sensitivity and create an uncertainty-aware exploration signal. This approach leads to improved accuracy in reasoning tasks by balancing exploration and exploitation through an uncertainty-proportional bonus and other techniques."
                },
                "zh": {
                    "title": "åŸºäºè§†è§‰ä¸ç¡®å®šæ€§çš„æ¢ç´¢æå‡å¤šæ¨¡æ€æ¨ç†",
                    "desc": "VOGUEæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–ç­–ç•¥å¯¹è§†è§‰æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œå°†æ¢ç´¢è½¬ç§»åˆ°è§†è§‰è¾“å…¥ç©ºé—´ï¼Œä»è€Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè§†ä¸ºéšæœºä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨å¯¹ç§°KLæ•£åº¦æ¥é‡åŒ–ç­–ç•¥çš„æ•æ„Ÿæ€§ï¼Œåˆ›å»ºä¸€ä¸ªç›´æ¥çš„ä¿¡å·ä»¥æ”¯æŒä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¢ç´¢ã€‚VOGUEé€šè¿‡ä¸ç¡®å®šæ€§æ¯”ä¾‹å¥–åŠ±ã€ä»¤ç‰Œç†µå¥–åŠ±å’Œé€æ­¥é‡‡æ ·è®¡åˆ’æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è§†è§‰æ•°å­¦åŸºå‡†å’Œä¸€èˆ¬é¢†åŸŸæ¨ç†åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ¢ç´¢ä¸è§†è§‰è¾“å…¥çš„å›ºæœ‰ä¸ç¡®å®šæ€§ç»“åˆæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ”¹è¿›ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01284",
            "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
            "url": "https://huggingface.co/papers/2510.01284",
            "abstract": "Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi",
            "score": 19,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "ee7ea259a6e9fd1a",
            "authors": [
                "Chetwin Low",
                "Weimin Wang",
                "Calder Katyal"
            ],
            "affiliations": [
                "Character AI",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01284.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#audio",
                    "#multimodal",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ovi â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… DiT-Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ (twin-DiT) Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞÑƒĞ´Ğ¸Ğ¾-Ğ±Ğ°ÑˆĞ½Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ½ÑƒĞ»Ñ Ğ½Ğ° ÑĞ¾Ñ‚Ğ½ÑÑ… Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ñ€ĞµÑ‡ÑŒ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾. Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±Ğ°ÑˆĞµĞ½ Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğµ Ñ‡ĞµÑ€ĞµĞ· scaled-RoPE ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Ovi: Seamless Audio-Video Generation for Cinematic Storytelling",
                    "desc": "Ovi is a novel model designed for generating audio and video together in a seamless way. It uses twin-DiT modules that allow for blockwise cross-modal fusion, which means it can combine sound and visuals more effectively than previous methods. This model learns from a large amount of raw audio to create realistic sounds and speech that match the emotions and identities of speakers. By training both audio and video components together, Ovi produces high-quality, synchronized outputs suitable for cinematic storytelling."
                },
                "zh": {
                    "title": "Oviï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°èŒƒå¼",
                    "desc": "Oviæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨åŒé‡DiTæ¨¡å—å’Œå—çº§è·¨æ¨¡æ€èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°è‡ªç„¶çš„åŒæ­¥å’Œé«˜è´¨é‡çš„å¤šæ¨¡æ€è¾“å‡ºã€‚ä¸ä¼ ç»Ÿçš„å¤šé˜¶æ®µæ¶æ„ä¸åŒï¼ŒOviå°†éŸ³é¢‘å’Œè§†é¢‘è§†ä¸ºå•ä¸€çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç®€åŒ–äº†ç”Ÿæˆæµç¨‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒéŸ³é¢‘å’Œè§†é¢‘å¡”ï¼Œåˆ©ç”¨æ—¶é—´å’Œè¯­ä¹‰çš„å—çº§äº¤æ¢ï¼Œæå‡äº†å¤šæ¨¡æ€èåˆçš„ç²¾ç»†å»ºæ¨¡èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒOvièƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç”µå½±çº§åˆ«è´¨é‡çš„éŸ³è§†é¢‘ç‰‡æ®µï¼Œå±•ç°è‡ªç„¶çš„è¯­éŸ³å’Œå‡†ç¡®çš„å£°éŸ³æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02250",
            "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
            "url": "https://huggingface.co/papers/2510.02250",
            "abstract": "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.",
            "score": 18,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "aa05336e77565d70",
            "authors": [
                "Gonzalo Gonzalez-Pumariega",
                "Vincent Tu",
                "Chih-Lun Lee",
                "Jiachen Yang",
                "Ang Li",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Simular Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02250.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Behavior Best-of-N (bBoN) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² â€” Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 69.9%, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 72%. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ bBoN."
                },
                "en": {
                    "title": "Scaling Success: Behavior Best-of-N for Reliable Computer-Use Agents",
                    "desc": "The paper presents Behavior Best-of-N (bBoN), a novel approach to enhance the performance of computer-use agents (CUAs) by generating multiple rollouts and selecting the best ones based on behavior narratives. This method allows for extensive exploration of possible actions while ensuring that the most effective trajectories are chosen, leading to improved reliability and success rates in complex tasks. The results show that bBoN achieves state-of-the-art performance on the OSWorld benchmark, nearing human-level effectiveness. Additionally, the method demonstrates strong generalization capabilities across different operating systems, emphasizing the importance of structured trajectory understanding in scaling CUAs effectively."
                },
                "zh": {
                    "title": "è¡Œä¸ºæœ€ä½³é€‰æ‹©ï¼šæå‡è®¡ç®—æœºä»£ç†çš„å¯é æ€§ä¸æˆåŠŸç‡",
                    "desc": "è¡Œä¸ºæœ€ä½³é€‰æ‹©ï¼ˆbBoNï¼‰é€šè¿‡ç”Ÿæˆå’Œé€‰æ‹©å¤šä¸ªå›æ»šï¼Œåˆ©ç”¨è¡Œä¸ºå™è¿°æé«˜äº†è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å¯é æ€§å’ŒæˆåŠŸç‡ã€‚è¯¥æ–¹æ³•åœ¨OSWorldä¸Šè¾¾åˆ°äº†69.9%çš„æ–°çŠ¶æ€ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå¹¶æ¥è¿‘äººç±»æ°´å¹³çš„72%ã€‚bBoNæ–¹æ³•å…è®¸å¹¿æ³›æ¢ç´¢å’Œæœ‰åŸåˆ™çš„è½¨è¿¹é€‰æ‹©ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†é²æ£’æ€§å’ŒæˆåŠŸç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜å±•ç¤ºäº†bBoNåœ¨ä¸åŒæ“ä½œç³»ç»Ÿä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†æœ‰æ•ˆæ‰©å±•è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„åˆç†æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02240",
            "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.02240",
            "abstract": "RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.",
            "score": 15,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "d80bedb4d445e906",
            "authors": [
                "Sicheng Feng",
                "Kaiwen Tuo",
                "Song Wang",
                "Lingdong Kong",
                "Jianke Zhu",
                "Huan Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Tongji University",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02240.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RewardMap â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ReasonMap-Plus Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· VQA-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ¾Ğ²ÑˆĞµÑÑ‚Ğ²Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.47% Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Boosting Visual Reasoning in MLLMs with RewardMap",
                    "desc": "RewardMap is a multi-stage reinforcement learning framework that improves the visual understanding and reasoning abilities of multimodal large language models (MLLMs). It addresses the challenge of fine-grained visual reasoning by introducing dense reward signals through Visual Question Answering (VQA) tasks, which helps in effective training. The framework features a difficulty-aware reward design that provides richer supervision and tackles the issue of sparse rewards. Experiments show that RewardMap significantly enhances performance across various benchmarks, demonstrating its effectiveness in boosting MLLMs' capabilities in complex reasoning tasks."
                },
                "zh": {
                    "title": "æå‡è§†è§‰ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "RewardMapæ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯†é›†å¥–åŠ±ä¿¡å·å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ä¸­é¢ä¸´çš„ç¨€ç–å¥–åŠ±å’Œä¸ç¨³å®šä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡æ„å»ºReasonMap-Plusæ•°æ®é›†ï¼ŒRewardMapèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒï¼Œå¢å¼ºç»†ç²’åº¦çš„è§†è§‰ç†è§£æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRewardMapçš„å„ä¸ªç»„ä»¶å‡èƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œç»“åˆä½¿ç”¨æ—¶æ•ˆæœæœ€ä½³ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.47%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02294",
            "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
            "url": "https://huggingface.co/papers/2510.02294",
            "abstract": "F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
            "score": 13,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "f50d032bbcffbe10",
            "authors": [
                "Ziyin Zhang",
                "Zihan Liao",
                "Hang Yu",
                "Peng Di",
                "Rui Wang"
            ],
            "affiliations": [
                "Ant Group",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02294.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸Ğ· foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "F2LLM â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ language models Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.6B, 1.7B Ğ¸ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, F2LLM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ contrastive pretraining Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½ÑÑ‚ÑÑ Ğ½Ğ° 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ñ€Ğ¾ĞµĞº query-document-negative Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MTEB Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ F2LLM-4B Ğ·Ğ°Ğ½ÑĞ»Ğ° 2-Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ€ĞµĞ´Ğ¸ 4B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 7-Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ·Ğ°Ñ‡Ñ‘Ñ‚Ğµ, Ğ° F2LLM-1.7B ÑÑ‚Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ 1B-2B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ ĞºĞ¾Ğ´, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ baseline Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "F2LLM: Efficient Embedding Models for Cost-Effective Performance",
                    "desc": "F2LLM is a new suite of large language models designed for efficient embedding performance. It fine-tunes foundation models using a curated dataset of 6 million query-document-negative tuples, avoiding the need for expensive pretraining and synthetic data. The models come in three sizes: 0.6B, 1.7B, and 4B parameters, with the largest model achieving high rankings on the MTEB English leaderboard. By releasing the models and training data, F2LLM aims to provide a cost-effective and reproducible baseline for future research in machine learning."
                },
                "zh": {
                    "title": "F2LLMï¼šé«˜æ•ˆåµŒå…¥çš„åŸºç¡€æ¨¡å‹å¾®è°ƒ",
                    "desc": "F2LLMæ˜¯ä¸€å¥—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºé«˜æ•ˆçš„åµŒå…¥æ€§èƒ½ã€‚å®ƒé€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å¼€æ”¾æºä»£ç æ•°æ®é›†ï¼Œé¿å…äº†ä»¥å¾€æ¨¡å‹éœ€è¦çš„å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒå’Œå¤æ‚çš„è®­ç»ƒæµç¨‹ã€‚F2LLMæä¾›äº†ä¸‰ç§ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œåˆ†åˆ«ä¸º0.6Bã€1.7Bå’Œ4Bï¼Œå¹¶åœ¨MTEBè‹±è¯­æ’è¡Œæ¦œä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä¸ºäº†æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†å’Œä»£ç ï¼Œæ—¨åœ¨ä¸ºåç»­å·¥ä½œæä¾›ä¸€ä¸ªå¼ºå¤§ä¸”ç»æµå®æƒ çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02190",
            "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
            "url": "https://huggingface.co/papers/2510.02190",
            "abstract": "A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.",
            "score": 13,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "98260d7be8c3395a",
            "authors": [
                "Yang Yao",
                "Yixu Wang",
                "Yuxuan Zhang",
                "Yi Lu",
                "Tianle Gu",
                "Lingyu Li",
                "Dingyi Zhao",
                "Keming Wu",
                "Haozhe Wang",
                "Ping Nie",
                "Yan Teng",
                "Yingchun Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong University of Science and Technology",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of British Columbia",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02190.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#evaluation",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Deep Research Agents (DRA) â€” AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 214 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· 10 Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ñ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ğ½Ğ¾ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Evaluation for Deep Research Agents",
                    "desc": "This paper presents a new benchmark and evaluation framework specifically designed for Deep Research Agents (DRAs), which are advanced AI systems capable of handling complex tasks. The framework includes 214 challenging queries across various themes and offers a multidimensional approach to assess the performance of DRAs based on metrics like semantic quality and retrieval trustworthiness. It highlights the limitations of existing benchmarks in evaluating DRAs and proposes a structured method for comprehensive assessment. The findings indicate that while DRAs outperform traditional web-search models, there is still significant room for improvement in their capabilities."
                },
                "zh": {
                    "title": "æ·±åº¦ç ”ç©¶ä»£ç†çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆDRAï¼‰çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šç»´åº¦æŒ‡æ ‡è¯„ä¼°å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚DRAèƒ½å¤Ÿè¿›è¡Œä»»åŠ¡åˆ†è§£ã€è·¨æºæ£€ç´¢ã€å¤šé˜¶æ®µæ¨ç†å’Œç»“æ„åŒ–è¾“å‡ºï¼Œæ˜¾è‘—æå‡äº†åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„è¯„ä¼°åŸºå‡†åœ¨è¯„ä¼°ç»´åº¦ã€å“åº”æ ¼å¼å’Œè¯„åˆ†æœºåˆ¶ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å¯¹è¿™äº›ç³»ç»Ÿçš„æœ‰æ•ˆè¯„ä¼°ã€‚æœ¬æ–‡çš„æ¡†æ¶åŒ…å«214ä¸ªä¸“å®¶ç­–åˆ’çš„æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼Œæ”¯æŒå¯¹DRAç”Ÿæˆçš„é•¿æ ¼å¼æŠ¥å‘Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæä¾›è¯­ä¹‰è´¨é‡ã€ä¸»é¢˜èšç„¦å’Œæ£€ç´¢å¯ä¿¡åº¦çš„ç»¼åˆè¯„åˆ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02173",
            "title": "Learning to Reason for Hallucination Span Detection",
            "url": "https://huggingface.co/papers/2510.02173",
            "abstract": "A reinforcement learning framework with span-level rewards improves hallucination span detection in large language models by incentivizing reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.",
            "score": 12,
            "issue_id": 6225,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "671b472906bfa9c1",
            "authors": [
                "Hsuan Su",
                "Ting-Yao Hu",
                "Hema Swetha Koppula",
                "Kundan Krishna",
                "Hadi Pouransari",
                "Cheng-Yu Hsieh",
                "Cem Koc",
                "Joseph Yitan Cheng",
                "Oncel Tuzel",
                "Raviteja Vemulapalli"
            ],
            "affiliations": [
                "Apple",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02173.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#hallucinations",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Reinforcement learning ÑƒÑ‡Ğ¸Ñ‚ LLM Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ - Ğ½ĞµĞ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RL4HS - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Chain-of-Thought Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Class-Aware Policy Optimization Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RAGTruth Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ reasoning."
                },
                "en": {
                    "title": "Reinforcement Learning for Better Hallucination Span Detection",
                    "desc": "This paper presents a reinforcement learning framework called RL4HS that enhances the detection of hallucination spans in large language models (LLMs). Unlike traditional binary detection methods, RL4HS focuses on identifying specific spans of hallucinated content, which requires more complex reasoning. The framework uses a span-level reward system to encourage better reasoning processes, leveraging Chain-of-Thought (CoT) techniques. Experimental results indicate that RL4HS outperforms existing models, highlighting the effectiveness of reinforcement learning in improving hallucination detection."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¹»è§‰æ£€æµ‹çš„çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è·¨åº¦çº§å¥–åŠ±æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰è·¨åº¦æ£€æµ‹ã€‚ä¼ ç»Ÿçš„å¹»è§‰æ£€æµ‹é€šå¸¸è¢«è§†ä¸ºäºŒå…ƒä»»åŠ¡ï¼Œä½†è®¸å¤šå®é™…åº”ç”¨éœ€è¦è¯†åˆ«å¹»è§‰çš„å…·ä½“éƒ¨åˆ†ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ­¥éª¤çš„å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½å¤Ÿåœ¨å¤šæ¬¡é‡‡æ ·ä¸­ç”Ÿæˆè‡³å°‘ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†RL4HSæ¡†æ¶ï¼Œä»¥æ¿€åŠ±æ¨ç†å¹¶è§£å†³å¥–åŠ±ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL4HSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢„è®­ç»ƒæ¨ç†æ¨¡å‹å’Œç›‘ç£å¾®è°ƒï¼Œè¯æ˜äº†ä½¿ç”¨è·¨åº¦çº§å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åœ¨å¹»è§‰è·¨åº¦æ£€æµ‹ä¸­çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01179",
            "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
            "url": "https://huggingface.co/papers/2510.01179",
            "abstract": "Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.",
            "score": 12,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "30b0ec9c798b87c6",
            "authors": [
                "Zhangchen Xu",
                "Adriana Meza Soria",
                "Shawn Tan",
                "Anurag Roy",
                "Ashish Sunil Agrawal",
                "Radha Poovendran",
                "Rameswar Panda"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01179.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "Toucan: ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Toucan â€” ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 500 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Model Context Protocols (MCP) Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Toucan, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BFCL V3 Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° MCP-Universe Bench."
                },
                "en": {
                    "title": "Toucan: Elevating LLM Agents with Diverse Tool Interactions",
                    "desc": "Toucan is a large dataset designed to improve the performance of Large Language Model (LLM) agents by providing a wide variety of realistic and complex interactions involving multiple tools and turns. It addresses the limitations of existing datasets, which often lack diversity and realism, by synthesizing 1.5 million trajectories from nearly 500 real-world Model Context Protocols (MCPs). The dataset generation process includes quality filtering and the use of multiple models to ensure high-quality outputs, along with mechanisms to diversify tasks and simulate multi-turn conversations. Models trained on Toucan have shown superior performance compared to larger closed-source models on established benchmarks, demonstrating its effectiveness in advancing the capabilities of LLM agents."
                },
                "zh": {
                    "title": "Toucanï¼šæå‡LLMä»£ç†æ€§èƒ½çš„å…³é”®æ•°æ®é›†",
                    "desc": "Toucanæ˜¯ä¸€ä¸ªå¤§å‹çš„å…¬å¼€å¯ç”¨å·¥å…·ä»£ç†æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«150ä¸‡ä¸ªè½¨è¿¹ï¼Œæ¥æºäºè¿‘500ä¸ªçœŸå®çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼Œæä¾›å¤šæ ·åŒ–ã€çœŸå®ä¸”å¤æ‚çš„å¤šå·¥å…·å’Œå¤šè½®äº¤äº’ã€‚Toucané€šè¿‡çœŸå®çš„MCPç¯å¢ƒç”Ÿæˆä»»åŠ¡ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚ç»è¿‡ä¸¥æ ¼çš„è§„åˆ™å’Œæ¨¡å‹éªŒè¯ï¼ŒToucanç”Ÿæˆçš„é«˜è´¨é‡è¾“å‡ºä½¿å¾—åœ¨BFCL V3åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ›´å¤§çš„å°é—­æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02253",
            "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
            "url": "https://huggingface.co/papers/2510.02253",
            "abstract": "DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.",
            "score": 10,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "d10d645e2e301186",
            "authors": [
                "Zihan Zhou",
                "Shilin Lu",
                "Shuli Leng",
                "Shaocong Zhang",
                "Zhuming Lian",
                "Xinlei Yu",
                "Adams Wai-Kin Kong"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02253.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ FLUX: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DragFlow â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ (drag-editing), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ prior'Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DiT Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ DiT, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ pretrained Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… DragBench-DR Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ReD Bench Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² drag-based Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Drag-Based Image Editing with DragFlow",
                    "desc": "DragFlow is a new framework that improves drag-based image editing by using advanced generative models called FLUX. Traditional methods struggled with distortions because earlier models like Stable Diffusion couldn't accurately map edited images back to their original forms. DragFlow introduces a region-based editing approach that uses affine transformations for better feature supervision, making the editing process more reliable. By integrating personalization adapters and multimodal language models, DragFlow achieves significant improvements over previous methods, setting a new standard in the field of image editing."
                },
                "zh": {
                    "title": "DragFlowï¼šæ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "DragFlow æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œåˆ©ç”¨ FLUX çš„å¼ºç”Ÿæˆå…ˆéªŒå’ŒåŸºäºåŒºåŸŸçš„ç¼–è¾‘æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚ä¼ ç»Ÿçš„æ‹–æ‹½ç¼–è¾‘å¸¸å¸¸å¯¼è‡´ç›®æ ‡åŒºåŸŸçš„å¤±çœŸï¼Œå› ä¸ºæ—©æœŸæ¨¡å‹çš„å…ˆéªŒä¸è¶³ä»¥å°†ä¼˜åŒ–åçš„æ½œåœ¨è¡¨ç¤ºå‡†ç¡®æ˜ å°„åˆ°è‡ªç„¶å›¾åƒä¸Šã€‚DragFlow é€šè¿‡å¼•å…¥ä»¿å°„å˜æ¢çš„åŒºåŸŸç¼–è¾‘èŒƒå¼ï¼Œæä¾›äº†æ›´ä¸°å¯Œå’Œä¸€è‡´çš„ç‰¹å¾ç›‘ç£ï¼Œä»è€Œå…‹æœäº†ç‚¹åŸºæ‹–æ‹½ç¼–è¾‘çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDragFlow åœ¨æ‹–æ‹½å¼å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01817",
            "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
            "url": "https://huggingface.co/papers/2510.01817",
            "abstract": "Sparse Query Attention (SQA) reduces computational complexity in Transformer models by decreasing the number of Query heads, leading to significant throughput improvements with minimal impact on model quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models",
            "score": 9,
            "issue_id": 6236,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "2733653ecf786568",
            "authors": [
                "Adam Filipek"
            ],
            "affiliations": [
                "Reactive AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01817.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Transformer Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Sparse Query Attention (SQA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Query-Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Key/Value Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ (FLOPs), Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (32-200 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 3 Ñ€Ğ°Ğ· Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SQA Ğ±Ñ‹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Reactive Transformer Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Boosting Transformer Efficiency with Sparse Query Attention",
                    "desc": "Sparse Query Attention (SQA) is a new approach that enhances Transformer models by reducing the number of Query heads, which helps lower the computational complexity of the attention mechanism. This reduction leads to significant improvements in throughput, especially for tasks involving long sequences, without greatly affecting the model's performance. Unlike previous methods that focused on sharing Key and Value projections, SQA directly decreases the number of floating-point operations (FLOPs) needed for attention score calculations. Empirical results show that SQA can improve computation speed by up to 3 times in demanding scenarios, making it a promising solution for more efficient AI models."
                },
                "zh": {
                    "title": "ç¨€ç–æŸ¥è¯¢æ³¨æ„åŠ›ï¼šæå‡Transformeræ•ˆç‡çš„åˆ©å™¨",
                    "desc": "ç¨€ç–æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆSQAï¼‰é€šè¿‡å‡å°‘æŸ¥è¯¢å¤´çš„æ•°é‡æ¥é™ä½Transformeræ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶æ˜¾è‘—æé«˜äº†ååé‡ã€‚ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´äºŒæ¬¡è®¡ç®—å¤æ‚åº¦çš„æŒ‘æˆ˜ï¼Œè€ŒSQAé€šè¿‡ä¼˜åŒ–æŸ¥è¯¢å¤´çš„æ•°é‡ï¼Œç›´æ¥é™ä½äº†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è´Ÿæ‹…ã€‚è¯¥è®ºæ–‡æä¾›äº†SQAçš„ç†è®ºåŸºç¡€ã€æ•°å­¦å…¬å¼ä»¥åŠä¸€ç³»åˆ—æ¶æ„å˜ä½“çš„ä»‹ç»ã€‚å®éªŒè¯æ˜ï¼Œåœ¨é•¿åºåˆ—å¤„ç†æ—¶ï¼ŒSQAåœ¨è®¡ç®—å¯†é›†å‹åœºæ™¯ä¸­å¯å®ç°é«˜è¾¾3å€çš„ååé‡æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01346",
            "title": "Aristotle: IMO-level Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2510.01346",
            "abstract": "Aristotle, an AI system combining formal verification and informal reasoning, achieves top performance on International Mathematical Olympiad problems using Lean proof search, lemma generation, and a geometry solver.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Aristotle, an AI system that combines formal verification with informal reasoning, achieving gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems. Aristotle integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver. Our system demonstrates state-of-the-art performance with favorable scaling properties for automated theorem proving.",
            "score": 9,
            "issue_id": 6237,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "b8737df2c245ef48",
            "authors": [
                "Tudor Achim",
                "Alex Best",
                "Kevin Der",
                "MathÃ¯s FÃ©dÃ©rico",
                "Sergei Gukov",
                "Daniel Halpern-Leister",
                "Kirsten Henningsgard",
                "Yury Kudryashov",
                "Alexander Meiburg",
                "Martin Michelsen",
                "Riley Patterson",
                "Eric Rodriguez",
                "Laura Scharff",
                "Vikram Shanker",
                "Vladmir Sicca",
                "Hari Sowrirajan",
                "Aidan Swope",
                "Matyas Tamas",
                "Vlad Tenev",
                "Jonathan Thomm",
                "Harold Williams",
                "Lawrence Wu"
            ],
            "affiliations": [
                "ByteDance",
                "Google Deepmind",
                "OpenAI",
                "harmonic.fun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01346.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥‡",
                "ru": {
                    "title": "Aristotle: AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Aristotle, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¸ĞºĞ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Lean, Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ĞµĞ¼Ğ¼, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Aristotle Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ IMO 2025. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼."
                },
                "en": {
                    "title": "Aristotle: Bridging Formal Verification and Informal Reasoning for Mathematical Excellence",
                    "desc": "Aristotle is an advanced AI system that merges formal verification techniques with informal reasoning strategies. It excels in solving complex problems, achieving results comparable to gold medalists in the International Mathematical Olympiad. The system utilizes a Lean proof search for rigorous verification, generates and formalizes lemmas for informal reasoning, and includes a specialized geometry solver. This combination allows Aristotle to perform exceptionally well in automated theorem proving, showcasing its scalability and efficiency."
                },
                "zh": {
                    "title": "é˜¿é‡Œå£«å¤šå¾·ï¼šç»“åˆå½¢å¼éªŒè¯ä¸éæ­£å¼æ¨ç†çš„AIç³»ç»Ÿ",
                    "desc": "Aristotleæ˜¯ä¸€ä¸ªç»“åˆäº†å½¢å¼éªŒè¯å’Œéæ­£å¼æ¨ç†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚å®ƒåœ¨2025å¹´å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹é—®é¢˜ä¸Šè¾¾åˆ°äº†é‡‘ç‰Œæ°´å¹³çš„è¡¨ç°ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šLeanè¯æ˜æœç´¢ç³»ç»Ÿã€ç”Ÿæˆå’Œå½¢å¼åŒ–å¼•ç†çš„éæ­£å¼æ¨ç†ç³»ç»Ÿï¼Œä»¥åŠä¸“é—¨çš„å‡ ä½•æ±‚è§£å™¨ã€‚Aristotleå±•ç¤ºäº†åœ¨è‡ªåŠ¨å®šç†è¯æ˜æ–¹é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02295",
            "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
            "url": "https://huggingface.co/papers/2510.02295",
            "abstract": "VideoNSA, an adaptation of Native Sparse Attention to video-language models, enhances long-video understanding and temporal reasoning through end-to-end training and a hardware-aware hybrid attention approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.",
            "score": 8,
            "issue_id": 6226,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "fa989de618e49801",
            "authors": [
                "Enxin Song",
                "Wenhao Chai",
                "Shusheng Yang",
                "Ethan Armand",
                "Xiaojun Shan",
                "Haiyang Xu",
                "Jianwen Xie",
                "Zhuowen Tu"
            ],
            "affiliations": [
                "Lambda, Inc",
                "New York University",
                "Princeton University",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02295.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#reasoning",
                    "#long_context",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VideoNSA â€” Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Native Sparse Attention Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° end-to-end Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 216K Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-VL. VideoNSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ baseline-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Enhancing Long-Video Understanding with VideoNSA",
                    "desc": "VideoNSA is a novel approach that enhances video-language models by integrating Native Sparse Attention (NSA) for better long-video comprehension and temporal reasoning. It utilizes end-to-end training on a large dataset of video instructions, allowing the model to learn effectively from diverse video content. The method combines dense attention for text with sparse attention for video, optimizing performance on tasks that require understanding over extended time frames. Through various experiments, VideoNSA demonstrates significant improvements in handling long videos and maintaining coherence, outperforming traditional methods in several benchmarks."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šVideoNSA",
                    "desc": "VideoNSAæ˜¯ä¸€ç§å°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰åº”ç”¨äºè§†é¢‘è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé•¿è§†é¢‘ç†è§£å’Œæ—¶é—´æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼ŒVideoNSAåœ¨ä¸€ä¸ªåŒ…å«216Kè§†é¢‘æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬ä¸Šä¿æŒå¯†é›†æ³¨æ„åŠ›ï¼Œè€Œåœ¨è§†é¢‘ä¸Šä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†é•¿è§†é¢‘ç†è§£å’Œæ—¶é—´æ¨ç†çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoNSAåœ¨å¤„ç†128Kæ ‡è®°æ—¶è¡¨ç°å¯é ï¼Œå¹¶ä¸”åœ¨å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›åˆ†é…ä¸Šè¾¾åˆ°äº†æœ€ä½³æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26376",
            "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2509.26376",
            "abstract": "ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.",
            "score": 8,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "d720c352a15d85b1",
            "authors": [
                "Harold Haodong Chen",
                "Xianfeng Wu",
                "Wen-Jie Shu",
                "Rongjin Guo",
                "Disen Lan",
                "Harry Yang",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "CityUHK",
                "FDU",
                "HKUST",
                "HKUST(GZ)",
                "PolyU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26376.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ScalingAR â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº test-time scaling Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ (ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹). ScalingAR Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… reward-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ NTP-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 12.5% Ğ½Ğ° GenEval Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 62% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Enhancing Image Generation with Adaptive Scaling and Token Entropy",
                    "desc": "ScalingAR is a novel framework that enhances next-token prediction in autoregressive image generation by utilizing token entropy and adaptive scaling techniques. It addresses the limitations of existing test-time scaling methods that are not suitable for visual autoregressive tasks, which often struggle with incomplete intermediate results. By operating at two levelsâ€”Profile Level and Policy Levelâ€”ScalingAR effectively manages confidence states and optimizes the generation process. Experimental results demonstrate significant improvements in model performance and efficiency, showcasing its ability to reduce token consumption while increasing robustness in challenging scenarios."
                },
                "zh": {
                    "title": "ScalingARï¼šæå‡è‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€æ ‡è®°é¢„æµ‹",
                    "desc": "ScalingAR æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ ‡è®°ç†µä½œä¸ºä¿¡å·ï¼Œå¹¶åœ¨ä¸¤ä¸ªäº’è¡¥çš„ç¼©æ”¾å±‚é¢ä¸Šæ“ä½œï¼Œæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ—©æœŸè§£ç å’Œå¤–éƒ¨å¥–åŠ±çš„éœ€æ±‚ï¼Œä¸“é—¨é’ˆå¯¹åŸºäºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å›¾åƒç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScalingAR åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶æœ‰æ•ˆå‡å°‘äº†è§†è§‰æ ‡è®°çš„æ¶ˆè€—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22582",
            "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
            "url": "https://huggingface.co/papers/2509.22582",
            "abstract": "A study evaluates the effectiveness of large language models in identifying context-grounded hallucinations using a newly constructed benchmark and free-form textual descriptions, revealing challenges in distinguishing between missing details and unverifiable information.  \t\t\t\t\tAI-generated summary \t\t\t\t Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmark's difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the model's parametric knowledge.",
            "score": 8,
            "issue_id": 6238,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "8f7449cbd0e3d9c0",
            "authors": [
                "Yehonatan Peisakhovsky",
                "Zorik Gekhman",
                "Yosi Mass",
                "Liat Ein-Dor",
                "Roi Reichart"
            ],
            "affiliations": [
                "IBM Research",
                "Technion - Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22582.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: ĞºĞ¾Ğ³Ğ´Ğ° LLM Ğ¿ÑƒÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ÑƒÑ Ğ¿Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° F1-score Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 0.67. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ â€” Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ĞºĞ°Ğº Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ÑƒÑ Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unveiling the Limits of LLMs in Detecting Hallucinations",
                    "desc": "This study investigates how well large language models (LLMs) can detect context-grounded hallucinations, which are inaccuracies in generated text that cannot be verified against the original source. The researchers created a new benchmark with over 1,000 examples to evaluate LLMs' performance in identifying these hallucinations, as existing methods were found to be too complex. They also introduced a novel representation of hallucinations using free-form textual descriptions to better capture various types of errors. The findings reveal that even the best-performing model only achieved an F1 score of 0.67, highlighting significant challenges in distinguishing between missing details and unverifiable information."
                },
                "zh": {
                    "title": "è¯†åˆ«ä¸Šä¸‹æ–‡å¹»è§‰çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ä¸Šä¸‹æ–‡åŸºç¡€å¹»è§‰æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨æ–°æ„å»ºçš„åŸºå‡†å’Œè‡ªç”±å½¢å¼æ–‡æœ¬æè¿°ã€‚ä¸Šä¸‹æ–‡åŸºç¡€å¹»è§‰æ˜¯æŒ‡æ¨¡å‹è¾“å‡ºåŒ…å«æ— æ³•ä¸æºæ–‡æœ¬æ ¸å®çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†ï¼Œå¹¶è¿›è¡Œäº†è¶…è¿‡1000ä¸ªç¤ºä¾‹çš„äººç±»æ³¨é‡Šï¼Œä»¥éªŒè¯å…¶è´¨é‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°ä»…ä¸º0.67ï¼Œæ­ç¤ºäº†è¯†åˆ«å¹»è§‰çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–æç¤ºç­–ç•¥çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01304",
            "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
            "url": "https://huggingface.co/papers/2510.01304",
            "abstract": "AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .",
            "score": 7,
            "issue_id": 6224,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "1b3bf32d6b7a84f7",
            "authors": [
                "Yu Zeng",
                "Wenxuan Huang",
                "Shiting Huang",
                "Xikun Bao",
                "Yukun Qi",
                "Yiming Zhao",
                "Qiuchen Wang",
                "Lin Chen",
                "Zehui Chen",
                "Huaian Chen",
                "Wanli Ouyang",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01304.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ±Ğ¾Ñ€ĞºÑƒ Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AGILE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language Models Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡-Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº (jigsaw puzzles) Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2Ã—2 Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ñ 9.5% Ğ´Ğ¾ 82.8%. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 9 Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 3.1%, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "AGILE: Enhancing Vision-Language Models through Interactive Learning",
                    "desc": "AGILE is a framework designed to improve the visual perception and reasoning skills of Vision-Language Models (VLMs) through an interactive jigsaw-solving process. It allows models to engage with their environment iteratively, generating actions based on their current state and receiving detailed visual feedback. This method significantly enhances the model's performance on jigsaw tasks, with accuracy improvements from 9.5% to 82.8%, and also boosts generalization across various vision tasks. By addressing the limitations of existing VLMs, AGILE provides a scalable solution to enhance multimodal learning capabilities."
                },
                "zh": {
                    "title": "AGILEï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "AGILEæ˜¯ä¸€ä¸ªäº¤äº’å¼æ‹¼å›¾è§£å†³æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£çš„è¡ŒåŠ¨å’Œåé¦ˆæ¥å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†æ‹¼å›¾è§£å†³è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªäº’åŠ¨è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚æ¯ä¸€æ­¥ï¼Œæ¨¡å‹æ ¹æ®å½“å‰çŠ¶æ€ç”Ÿæˆå¯æ‰§è¡Œä»£ç ä»¥æ‰§è¡ŒåŠ¨ä½œï¼ŒåŒæ—¶ç¯å¢ƒæä¾›ç»†è‡´çš„è§†è§‰åé¦ˆä»¥æŒ‡å¯¼ä»»åŠ¡å®Œæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAGILEæ˜¾è‘—æé«˜äº†æ‹¼å›¾ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21789",
            "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
            "url": "https://huggingface.co/papers/2509.21789",
            "abstract": "ViF mitigates visual hallucination snowballing in Multi-Agent Systems by enhancing visual attention and message relay through selected visual tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.",
            "score": 7,
            "issue_id": 6225,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "f05b2844cf6eaf03",
            "authors": [
                "Xinlei Yu",
                "Chengming Xu",
                "Guibin Zhang",
                "Yongbo He",
                "Zhangquan Chen",
                "Zhucun Xue",
                "Jiangning Zhang",
                "Yue Liao",
                "Xiaobin Hu",
                "Yu-Gang Jiang",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Tencent Youtu Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21789.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "â„ï¸",
                "ru": {
                    "title": "ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ½ĞµĞ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ÑĞ½ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹Â» Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Visual Language Models, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¿Ğ¸ĞºĞ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ViF Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¸Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ´ĞµÑÑÑ‚ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mitigating Visual Hallucination in Multi-Agent Systems with ViF",
                    "desc": "This paper introduces ViF, a method designed to reduce visual hallucination snowballing in Multi-Agent Systems (MAS) that utilize Visual Language Models (VLMs). The issue arises when one agent's visual hallucination is amplified by subsequent agents due to excessive reliance on textual information. Through detailed attention analyses, the authors identify specific visual tokens that maintain visual integrity but lose effectiveness in deeper agent interactions. ViF enhances message relay and visual attention by focusing on these selected tokens, leading to significant improvements in performance across various MAS benchmarks."
                },
                "zh": {
                    "title": "ViFï¼šå‡è½»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§†è§‰å¹»è§‰",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºViFçš„æ–¹æ³•ï¼Œç”¨äºç¼“è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§†è§‰å¹»è§‰é›ªçƒæ•ˆåº”ã€‚è¯¥æ•ˆåº”æ˜¯ç”±äºå•ä¸ªæ™ºèƒ½ä½“çš„å¹»è§‰è¢«åç»­æ™ºèƒ½ä½“æ”¾å¤§ï¼Œå¯¼è‡´ä¿¡æ¯ä¼ é€’å¤±çœŸã€‚é€šè¿‡å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°æŸäº›è§†è§‰æ ‡è®°åœ¨ä¸­é—´å±‚å…·æœ‰æœ€ä½³çš„è§†è§‰è¯æ®ä¿ç•™èƒ½åŠ›ã€‚ViFé€šè¿‡é€‰æ‹©è¿™äº›è§†è§‰æ ‡è®°å¹¶é‡æ–°åˆ†é…æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆåœ°æ”¹å–„äº†ä¿¡æ¯ä¼ é€’ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰é›ªçƒæ•ˆåº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00428",
            "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
            "url": "https://huggingface.co/papers/2510.00428",
            "abstract": "Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.",
            "score": 6,
            "issue_id": 6222,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "01935fd7ce2849e2",
            "authors": [
                "Seongjae Kang",
                "Dong Bok Lee",
                "Juho Jung",
                "Dongseop Kim",
                "Won Hwa Kim",
                "Sunghoon Joo"
            ],
            "affiliations": [
                "KAIST",
                "POSTECH",
                "VUNO Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00428.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#science",
                    "#dataset",
                    "#healthcare",
                    "#multimodal",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "ĞšĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ğ¼ ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼ Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Â«Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Â» â€” Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑÑ‹Ğ»Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ C-SRRG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ÑÑ…, ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ multimodal LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Radiology Reports with Clinical Context",
                    "desc": "This paper presents a new approach to automated structured radiology report generation (SRRG) that incorporates clinical context to enhance report quality. The authors identify that existing SRRG systems often ignore important clinical information, leading to issues like temporal hallucinations, where reports reference non-existent contexts. To solve this, they introduce contextualized SRRG (C-SRRG), which integrates various clinical data such as multi-view X-ray images and patient histories. Their experiments show that using C-SRRG significantly improves the clarity and accuracy of generated reports, making them more useful for radiologists."
                },
                "zh": {
                    "title": "æ•´åˆä¸´åºŠèƒŒæ™¯ï¼Œæå‡æ”¾å°„å­¦æŠ¥å‘Šè´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡åŒ–ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆï¼ˆC-SRRGï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆä¸°å¯Œçš„ä¸´åºŠèƒŒæ™¯ä¿¡æ¯ï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿåœ¨ç”ŸæˆæŠ¥å‘Šæ—¶å¿½è§†ä¸´åºŠä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œä»è€Œå‡å°‘äº†æ—¶é—´å¹»è§‰çš„å‘ç”Ÿã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šè§†è§’Xå…‰å›¾åƒã€ä¸´åºŠæŒ‡ç¤ºã€æˆåƒæŠ€æœ¯å’Œæ‚£è€…å†å²çš„C-SRRGæ•°æ®é›†ã€‚é€šè¿‡ä¸å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼ŒC-SRRGæ˜¾è‘—æé«˜äº†æŠ¥å‘Šç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02315",
            "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
            "url": "https://huggingface.co/papers/2510.02315",
            "abstract": "A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.",
            "score": 5,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "279a167c4dc7509b",
            "authors": [
                "Eric Tillmann Bill",
                "Enis Simsar",
                "Thomas Hofmann"
            ],
            "affiliations": [
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02315.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² flow matching Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ test-time inference Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adjoint Matching Ğ´Ğ»Ñ Ğ»Ñ‘Ğ³ĞºĞ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Stable Diffusion 3.5, FLUX Ğ¸ SDXL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ FOCUS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Fidelity in Text-to-Image Models",
                    "desc": "This paper presents a new framework and algorithms aimed at enhancing the performance of text-to-image (T2I) models when generating images from multi-subject prompts. The authors identify common issues such as attribute leakage and identity entanglement that occur in these models and propose a method to control sampling dynamics to improve fidelity. They introduce two algorithms: a test-time controller that adjusts the sampling process on-the-fly and a fine-tuning method called Adjoint Matching that optimizes a control network. The results show that these approaches significantly enhance multi-subject alignment while preserving the original style of the base models, demonstrating their effectiveness across various T2I architectures."
                },
                "zh": {
                    "title": "æå‡å¤šä¸»ä½“ä¸€è‡´æ€§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†è®ºæ¡†æ¶å’Œç®—æ³•ï¼Œä»¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤šä¸»ä½“åœºæ™¯ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤„ç†å•ä¸€å®ä½“æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šä¸»ä½“æè¿°ä¸­å¸¸å¸¸å‡ºç°å±æ€§æ³„æ¼å’Œèº«ä»½çº ç¼ ç­‰é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡éšæœºæœ€ä¼˜æ§åˆ¶çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–é‡‡æ ·åŠ¨æ€çš„ç›®æ ‡ï¼Œä»è€Œå®ç°å¤šä¸»ä½“çš„è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆæé«˜å¤šä¸»ä½“çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„é£æ ¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01670",
            "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
            "url": "https://huggingface.co/papers/2510.01670",
            "abstract": "Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
            "score": 5,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "03b47e25c6ec7bd1",
            "authors": [
                "Erfan Shayegani",
                "Keegan Hines",
                "Yue Dong",
                "Nael Abu-Ghazaleh",
                "Roman Lutz",
                "Spencer Whitehead",
                "Vidhisha Balachandran",
                "Besmira Nushi",
                "Vibhav Vineet"
            ],
            "affiliations": [
                "Microsoft AI Red Team",
                "Microsoft Research AI Frontiers",
                "NVIDIA",
                "University of California, Riverside"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01670.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#security"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ğ»ĞµĞ¿Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸: ĞºĞ°Ğº AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ñ€Ğ°Ğ´Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ (Computer-Use Agents), ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Â«ÑĞ»ĞµĞ¿ÑƒÑ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÂ» â€” ÑÑ‚Ñ€ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ¾Ğ¹, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ». Ğ”Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BLIND-ACT Ñ 90 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Claude Sonnet 4 Ğ¸ GPT-5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² 80.8% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. Ğ¥Ğ¾Ñ‚Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞº, Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ inference Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Addressing Blind Goal-Directedness in AI Agents for Safer Interactions",
                    "desc": "This paper discusses a problem called Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs), which are AI systems that perform tasks on graphical user interfaces. BGD causes these agents to pursue goals without considering if they are safe or feasible, leading to risky behaviors. The authors introduce a benchmark called BLIND-ACT, which consists of 90 tasks designed to evaluate BGD in CUAs, revealing that many advanced models exhibit high rates of this bias. The study emphasizes the importance of addressing BGD to ensure the safe deployment of CUAs, suggesting that while some interventions can reduce BGD, significant risks remain."
                },
                "zh": {
                    "title": "è¯†åˆ«ç›²ç›®ç›®æ ‡å¯¼å‘ï¼Œç¡®ä¿å®‰å…¨çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†",
                    "desc": "è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰åœ¨æ‰§è¡Œç”¨æˆ·ç›®æ ‡æ—¶ï¼Œè¡¨ç°å‡ºä¸€ç§ç§°ä¸ºç›²ç›®ç›®æ ‡å¯¼å‘ï¼ˆBGDï¼‰çš„åå·®ã€‚è¿™ç§åå·®ä½¿å¾—ä»£ç†åœ¨è¿½æ±‚ç›®æ ‡æ—¶ï¼Œä¸è€ƒè™‘å¯è¡Œæ€§ã€å®‰å…¨æ€§ã€å¯é æ€§æˆ–ä¸Šä¸‹æ–‡ã€‚æœ¬æ–‡é€šè¿‡BLIND-ACTåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†BGDçš„ä¸‰ç§å¸¸è§æ¨¡å¼ï¼Œå¹¶è¯„ä¼°äº†å¤šç§å‰æ²¿æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬æ™®éå­˜åœ¨é«˜è¾¾80.8%çš„BGDç‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨ä»£ç†è¡Œä¸ºä¸­è¯†åˆ«BGDçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œä»¥ç¡®ä¿CUAçš„å®‰å…¨éƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01623",
            "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2510.01623",
            "abstract": "VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
            "score": 5,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "07cfd503342b7106",
            "authors": [
                "Angen Ye",
                "Zeyu Zhang",
                "Boyuan Wang",
                "Xiaofeng Wang",
                "Dapeng Zhang",
                "Zheng Zhu"
            ],
            "affiliations": [
                "CASIA",
                "GigaAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01623.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "VLA Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "VLA-R1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ (chain-of-thought) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Reinforcement Learning from Verifiable Rewards (RLVR) Ğ¸ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VLA-CoT-13K Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ (affordances) Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ VLA Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Reasoning in Vision-Language-Action Models with VLA-R1",
                    "desc": "The paper introduces VLA-R1, an advanced Vision-Language-Action model that enhances reasoning and execution capabilities. It combines Reinforcement Learning from Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) to improve the model's ability to reason step-by-step and generate actions that consider environmental constraints. A new dataset, VLA-CoT-13K, is created to provide chain-of-thought supervision, which helps the model learn better reasoning aligned with real-world tasks. Evaluations show that VLA-R1 outperforms previous models in both generalization and practical applications, making it a significant advancement in embodied AI."
                },
                "zh": {
                    "title": "VLA-R1ï¼šæ¨ç†ä¸æ‰§è¡Œçš„å®Œç¾ç»“åˆ",
                    "desc": "VLA-R1æ˜¯ä¸€ç§å¢å¼ºçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œç»“åˆäº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ¨ç†å’Œæ‰§è¡Œèƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡è®¾è®¡åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œå¼ºåŒ–äº†åŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼ï¼Œä»è€Œæé«˜äº†æ¨ç†çš„ç¨³å¥æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒVLA-R1ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ•°æ®é›†VLA-CoT-13Kï¼Œæä¾›äº†ä¸å¯ç”¨æ€§å’Œè½¨è¿¹æ³¨é‡Šæ˜ç¡®å¯¹é½çš„æ€ç»´é“¾ç›‘ç£ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒVLA-R1åœ¨å¤šä¸ªå¹³å°ä¸Šå±•ç°å‡ºä¼˜äºä»¥å¾€VLAæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œç°å®ä¸–ç•Œè¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02263",
            "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
            "url": "https://huggingface.co/papers/2510.02263",
            "abstract": "Introducing reasoning abstractions in reinforcement learning improves structured exploration and generalization for complex problem-solving.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.",
            "score": 4,
            "issue_id": 6227,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "52cc5d424b5e4b27",
            "authors": [
                "Yuxiao Qu",
                "Anikait Singh",
                "Yoonho Lee",
                "Amrith Setlur",
                "Ruslan Salakhutdinov",
                "Chelsea Finn",
                "Aviral Kumar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02263.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· reasoning abstractions â€” ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ RLAD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ reinforcement learning, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Reasoning Abstractions",
                    "desc": "This paper presents a new approach in reinforcement learning (RL) called reasoning abstractions, which helps models explore complex problems more effectively. By using concise natural language descriptions of procedures and knowledge, the model can better identify relevant information and build solutions. The authors propose a two-player RL training method, where one part generates abstractions and the other generates solutions, leading to improved generalization and structured exploration. The findings suggest that focusing on generating abstractions during testing enhances performance more than simply generating more solutions."
                },
                "zh": {
                    "title": "æ¨ç†æŠ½è±¡æå‡å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¼•å…¥æ¨ç†æŠ½è±¡çš„æ–¹æ³•ï¼Œä»¥æ”¹å–„å¤æ‚é—®é¢˜è§£å†³çš„ç»“æ„åŒ–æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ¨ç†ä¸ä»…ä»…æ˜¯æ¨¡å¼åŒ¹é…æˆ–è®°å¿†è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯éœ€è¦è¯†åˆ«å’Œå®æ–½â€œç®—æ³•è¿‡ç¨‹â€ï¼Œä»¥æ¨å¯¼å‡ºå›°éš¾é—®é¢˜çš„ç­”æ¡ˆã€‚æˆ‘ä»¬å¼•å…¥äº†æ¨ç†æŠ½è±¡ï¼Œå³å¯¹ç¨‹åºæ€§å’Œäº‹å®çŸ¥è¯†çš„ç®€æ´è‡ªç„¶è¯­è¨€æè¿°ï¼ŒæŒ‡å¯¼æ¨¡å‹å­¦ä¹ æˆåŠŸçš„æ¨ç†ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹æå‡ºå¤šä¸ªæŠ½è±¡ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼Œæœ€ç»ˆå®ç°äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸¤ç©å®¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ›´å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02259",
            "title": "Transformers Discover Molecular Structure Without Graph Priors",
            "url": "https://huggingface.co/papers/2510.02259",
            "abstract": "Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinatesx2013without predefined graphs or physical priorsx2013can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patternsx2013such as attention weights that decay inversely with interatomic distancex2013and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",
            "score": 4,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "2f48683b9886f234",
            "authors": [
                "Tobias Kreiman",
                "Yutong Bai",
                "Fadi Atieh",
                "Elizabeth Weaver",
                "Eric Qu",
                "Aditi S. Krishnapriyan"
            ],
            "affiliations": [
                "LBNL",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02259.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#architecture",
                    "#dataset",
                    "#optimization",
                    "#science"
                ],
                "emoji": "âš›ï¸",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° Ğ´ĞµĞºĞ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ» Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Graph Neural Networks (GNN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ğ¼Ğ¸, Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¼ĞµĞ¶Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ AI. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transformers: A New Era for Molecular Predictions Without Graphs",
                    "desc": "This paper explores the use of Transformers, a type of neural network, for predicting molecular energies and forces directly from Cartesian coordinates, without relying on predefined graphs. Traditionally, Graph Neural Networks (GNNs) have been used for these tasks, but they can be limited by their fixed graph structures. The authors show that Transformers can achieve competitive performance while being more adaptable and scalable, as they learn patterns based on the data rather than hard-coded rules. This research suggests that Transformers can effectively replace GNNs in molecular modeling, offering a more flexible approach to machine learning in this field."
                },
                "zh": {
                    "title": "å˜æ¢å™¨ï¼šåˆ†å­å»ºæ¨¡çš„æ–°é€‰æ‹©",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç›´æ¥åœ¨ç¬›å¡å°”åæ ‡ä¸Šè®­ç»ƒçš„å˜æ¢å™¨ï¼ˆTransformersï¼‰åœ¨åˆ†å­èƒ½é‡å’ŒåŠ›é¢„æµ‹ä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸åŒï¼Œå˜æ¢å™¨ä¸ä¾èµ–äºé¢„å®šä¹‰çš„å›¾ç»“æ„ï¼Œå› æ­¤å…·æœ‰æ›´å¥½çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå˜æ¢å™¨èƒ½å¤Ÿå­¦ä¹ åˆ°ç‰©ç†ä¸€è‡´çš„æ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åˆ†å­ç¯å¢ƒä¸­çµæ´»é€‚åº”ã€‚æˆ‘ä»¬çš„å‘ç°æŒ‘æˆ˜äº†ç¡¬ç¼–ç å›¾ç»“æ„çš„å¿…è¦æ€§ï¼ŒæŒ‡å‘äº†åˆ†å­å»ºæ¨¡ä¸­æ ‡å‡†åŒ–å’Œå¯æ‰©å±•çš„æ¶æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01538",
            "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
            "url": "https://huggingface.co/papers/2510.01538",
            "abstract": "TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.  \t\t\t\t\tAI-generated summary \t\t\t\t Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.",
            "score": 4,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "b1cf0979cd51af28",
            "authors": [
                "Haokun Zhao",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Yuting He",
                "Siqi Sun",
                "Chenyu You"
            ],
            "affiliations": [
                "Case Western Reserve University",
                "Fudan University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "University of California, San Diego",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01538.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#agents",
                    "#interpretability",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "TimeSeriesScientist (TSci) â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Curator, Planner, Forecaster Ğ¸ Reporter, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞµÑ‘ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°. TSci Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ LLM-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 10,4% Ğ¸ 38,2% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "Automating Time Series Forecasting with Transparency and Precision",
                    "desc": "TimeSeriesScientist (TSci) is an innovative framework that automates the process of time series forecasting using large language models (LLMs) with minimal human input. It addresses the challenges of handling numerous short and noisy time series by employing four specialized agents: Curator for data diagnostics, Planner for model selection, Forecaster for fitting and validation, and Reporter for generating transparent reports. TSci outperforms traditional statistical and LLM-based methods, achieving significant reductions in forecast error while providing interpretable results. This framework not only enhances the efficiency of forecasting but also ensures that the process is understandable and adaptable across various domains."
                },
                "zh": {
                    "title": "æ—¶é—´åºåˆ—é¢„æµ‹çš„æ™ºèƒ½åŒ–è§£å†³æ–¹æ¡ˆ",
                    "desc": "TimeSeriesScientist (TSci) æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªä¸“é—¨çš„ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£æ•°æ®è¯Šæ–­ã€æ¨¡å‹é€‰æ‹©ã€æ¨¡å‹æ‹Ÿåˆå’Œç»“æœæŠ¥å‘Šã€‚TSci åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé¢„æµ‹è¯¯å·®å¹³å‡é™ä½äº†10.4%åˆ°38.2%ã€‚é€šè¿‡æä¾›é€æ˜çš„è‡ªç„¶è¯­è¨€è§£é‡Šå’Œå…¨é¢çš„æŠ¥å‘Šï¼ŒTSci ä½¿é¢„æµ‹è¿‡ç¨‹å˜å¾—æ›´åŠ å¯è§£é‡Šå’Œå¯æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00523",
            "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
            "url": "https://huggingface.co/papers/2510.00523",
            "abstract": "VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
            "score": 4,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "8379081e03e82135",
            "authors": [
                "Wei-Yao Wang",
                "Kazuya Tateishi",
                "Qiyu Wu",
                "Shusuke Takahashi",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Group Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00523.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#interpretability",
                    "#games",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ‘†",
                "ru": {
                    "title": "Embeddings Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼: ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VIRTUE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ embeddings Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ (Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹, bounding box Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞºĞ¾Ğ¹), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark SCaR Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹. VIRTUE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 3-8% Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ° 15-20% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Visual Interaction with VIRTUE",
                    "desc": "VIRTUE is a new model that combines segmentation and vision-language techniques to enhance how machines understand and interact with images and text. It allows users to specify areas of interest in images, improving the model's ability to learn detailed information about specific objects. This capability enables more precise responses to user queries and enhances the model's performance in various representation learning tasks. The paper introduces a benchmark to test VIRTUE's effectiveness, showing it outperforms existing models in multiple tasks."
                },
                "zh": {
                    "title": "VIRTUEï¼šå¼€å¯è§†è§‰äº¤äº’çš„æ–°çºªå…ƒ",
                    "desc": "VIRTUEæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰äº¤äº’æ–‡æœ¬-å›¾åƒé€šç”¨åµŒå…¥æ¨¡å‹ï¼Œå®ƒç»“åˆäº†åˆ†å‰²æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°è§†è§‰äº¤äº’å’Œå±€éƒ¨å®šä½ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤„ç†ç”¨æˆ·æŒ‡å®šçš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆå¦‚ç‚¹ã€è¾¹ç•Œæ¡†ã€æ©ç ï¼‰ï¼Œæå‡äº†åµŒå…¥æ¨¡å‹çš„äº¤äº’èƒ½åŠ›ã€‚VIRTUEåœ¨è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å’Œæ¨¡ç³Šåœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å¤§è§„æ¨¡çš„åˆ†å‰²å’Œåœºæ™¯æè¿°æ£€ç´¢åŸºå‡†ï¼ŒVIRTUEåœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24203",
            "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
            "url": "https://huggingface.co/papers/2509.24203",
            "abstract": "Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
            "score": 4,
            "issue_id": 6222,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "e7096714c253809b",
            "authors": [
                "Chaorui Yao",
                "Yanxi Chen",
                "Yuchang Sun",
                "Yushuo Chen",
                "Wenhao Zhang",
                "Xuchen Pan",
                "Yaliang Li",
                "Bolin Ding"
            ],
            "affiliations": [
                "Alibaba Group",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24203.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#agi",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ LLM: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° REINFORCE",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ REINFORCE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ off-policy Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, OPMD Ğ¸ AsymRE, Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ´Ğ²ÑƒÑ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ²: Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²ĞµĞ½Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¸Ñ„Ñ‹ Ğ¾ Ñ€Ğ¾Ğ»Ğ¸ importance sampling Ğ¸ clipping Ğ² ÑÑ‚Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² off-policy RL Ğ´Ğ»Ñ LLM."
                },
                "en": {
                    "title": "Unlocking Off-Policy Learning for Large Language Models",
                    "desc": "This paper investigates off-policy reinforcement learning (RL) techniques specifically for large language models (LLMs). It introduces a new derivation of group-relative REINFORCE, allowing for a better understanding of how importance sampling, clipping, and data-weighting can be effectively utilized in off-policy settings. The authors provide insights into regularizing policy updates and shaping data distributions, which enhance the adaptability of REINFORCE algorithms. Their findings are supported by empirical studies, paving the way for improved algorithm design in off-policy RL applications for LLMs."
                },
                "zh": {
                    "title": "ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æœºé‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¾¤ä½“ç›¸å¯¹REINFORCEçš„æ¨å¯¼æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„REINFORCEç®—æ³•å¯ä»¥åœ¨ä¸å‡è®¾ç‰¹å®šè®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œç¦»çº¿è§£é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé€‚åº”ç¦»çº¿è®¾ç½®çš„åŸåˆ™ï¼šè§„èŒƒåŒ–ç­–ç•¥æ›´æ–°å’Œä¸»åŠ¨è°ƒæ•´æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡å¯¹é‡è¦æ€§é‡‡æ ·å’Œå‰ªåˆ‡çš„è§’è‰²è¿›è¡Œåˆ†æï¼Œæœ¬æ–‡ä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†æ–°çš„ç†è®ºä¾æ®å’Œå®è¯æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02272",
            "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
            "url": "https://huggingface.co/papers/2510.02272",
            "abstract": "Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
            "score": 3,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "ad37f939671428fb",
            "authors": [
                "Wen Yang",
                "Junhong Wu",
                "Chong Li",
                "Chengqing Zong",
                "Jiajun Zhang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Wuhan AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02272.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#reasoning",
                    "#low_resource",
                    "#rlhf",
                    "#transfer_learning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ AI Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Multilingual Reasoning in Large Models",
                    "desc": "This paper explores how reasoning abilities in Large Reasoning Models (LRMs) can transfer between different languages. It highlights that the effectiveness of this transfer varies based on the model's initial training, the target language, and the training methods used. The authors introduce a parallel training approach to enhance cross-lingual generalization, revealing that models trained primarily on English often struggle with other languages. Their findings suggest that improving multilingual reasoning requires a deeper understanding of how language-specific patterns affect model performance."
                },
                "zh": {
                    "title": "è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„æå‡ä¹‹é“",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è½¬ç§»æ€§ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¹³è¡Œè®­ç»ƒæ–¹æ³•ä»¥æé«˜è·¨è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‹±è¯­ä¸ºä¸­å¿ƒçš„LRMsåœ¨å¤šè¯­è¨€æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ä¸å°½ç›¸åŒï¼Œä¸”åˆå§‹æ¨¡å‹ã€ç›®æ ‡è¯­è¨€å’Œè®­ç»ƒèŒƒå¼éƒ½ä¼šå½±å“è·¨è¯­è¨€è½¬ç§»æ€§ã€‚é€šè¿‡å¹²é¢„ç ”ç©¶å‘ç°ï¼Œåˆå§‹è‹±è¯­èƒ½åŠ›è¾ƒå¼ºçš„æ¨¡å‹å¾€å¾€è¿‡åº¦ä¾èµ–è‹±è¯­ç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ­ç¤ºäº†å¹³è¡Œè®­ç»ƒçš„æ˜¾è‘—æ•ˆæœï¼Œå¹¶æå‡ºäº†å•è¯­è¨€ä¸å¹³è¡Œè¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼ŒæŒ‘æˆ˜äº†LRMæ¨ç†ä¸äººç±»è®¤çŸ¥ç›¸ä¼¼çš„å‡è®¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01796",
            "title": "Rethinking the shape convention of an MLP",
            "url": "https://huggingface.co/papers/2510.01796",
            "abstract": "Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
            "score": 3,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "cb9db736774b09f8",
            "authors": [
                "Meng-Hsi Chen",
                "Yu-Ang Lee",
                "Feng-Ting Liao",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01796.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "ĞŸĞµÑĞ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‹ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: skip connections Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Hourglass MLP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ¾Ğ²: skip connections Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ·ĞºĞ¸Ğµ bottleneck-ÑĞ»Ğ¾Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒÑÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hourglass MLP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ skip connections Ğ¸ ÑƒĞ·ĞºĞ¸Ğ¼Ğ¸ bottlenecks."
                },
                "en": {
                    "title": "Revolutionizing MLPs: Hourglass Architecture for Superior Generative Performance",
                    "desc": "This paper introduces Hourglass MLP blocks, which utilize a wide-narrow-wide architecture with skip connections in expanded dimensions, contrasting with the traditional narrow-wide-narrow design. By allowing residual computations to flow through narrow bottlenecks, the model enhances performance in generative tasks while maintaining computational efficiency. The authors demonstrate that fixed random initialization for input signal projections can streamline training and inference processes. Their experiments reveal that Hourglass architectures outperform conventional MLPs, suggesting a need to rethink skip connection strategies in various neural network designs."
                },
                "zh": {
                    "title": "é‡æ–°å®šä¹‰è·³è·ƒè¿æ¥ï¼šHourglass MLPçš„ä¼˜åŠ¿",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç»“æ„ï¼Œç§°ä¸ºHourglass MLPå—ï¼Œé‡‡ç”¨å®½-çª„-å®½çš„è®¾è®¡ï¼Œè·³è·ƒè¿æ¥åœ¨æ‰©å±•ç»´åº¦ä¸Šæ“ä½œï¼Œè€Œæ®‹å·®è®¡ç®—åˆ™é€šè¿‡çª„ç“¶é¢ˆæµåŠ¨ã€‚è¿™ç§è®¾è®¡æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„çª„-å®½-çª„ç»“æ„ï¼Œåˆ©ç”¨é«˜ç»´ç©ºé—´è¿›è¡Œå¢é‡ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒHourglass MLPåœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿè®¾è®¡ï¼Œå°¤å…¶æ˜¯åœ¨å‚æ•°é¢„ç®—å¢åŠ æ—¶ï¼Œæœ€ä½³é…ç½®å€¾å‘äºæ›´æ·±çš„ç½‘ç»œå’Œæ›´å®½çš„è·³è·ƒè¿æ¥ã€‚æˆ‘ä»¬çš„å‘ç°æç¤ºåœ¨ç°ä»£æ¶æ„ä¸­é‡æ–°è€ƒè™‘è·³è·ƒè¿æ¥çš„æ”¾ç½®ï¼Œå¯èƒ½å¯¹å˜æ¢å™¨å’Œå…¶ä»–æ®‹å·®ç½‘ç»œæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01143",
            "title": "Generalized Parallel Scaling with Interdependent Generations",
            "url": "https://huggingface.co/papers/2510.01143",
            "abstract": "Bridge enhances parallel LLM inference by generating interdependent responses, improving accuracy and consistency with minimal additional parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel LLM inference scaling involves sampling a set of N>1 responses for a single input prompt. However, these N parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.",
            "score": 3,
            "issue_id": 6227,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "2a2384f425b56994",
            "authors": [
                "Harry Dong",
                "David Brandfonbrener",
                "Eryk Helenowski",
                "Yun He",
                "Mrinal Kumar",
                "Han Fang",
                "Yuejie Chi",
                "Karthik Abinav Sankararaman"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Meta",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01143.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "Bridge: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Bridge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ N Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ°. Bridge Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 2.8-5.1% Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ reinforcement learning Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridge: Enhancing Parallel LLM Inference with Interdependent Responses",
                    "desc": "This paper introduces Bridge, a method that enhances the accuracy and consistency of parallel large language model (LLM) inference by generating interdependent responses. Instead of treating each response as an independent output, Bridge views the hidden states of batched LLMs as holistic tensors, allowing for better information sharing among responses. By using only a small increase in parameters, Bridge significantly improves response quality and consistency, achieving up to 50% gains in accuracy through reinforcement learning. This approach allows for scalable generation widths and is compatible with various post-generation aggregation techniques, making it a versatile solution for parallel LLM tasks."
                },
                "zh": {
                    "title": "Bridgeï¼šæå‡å¹¶è¡Œæ¨ç†çš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBridgeçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¹¶è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡ç”Ÿæˆç›¸äº’ä¾èµ–çš„å“åº”ï¼ŒBridgeèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œé¿å…ä¿¡æ¯å­¤å²›ç°è±¡ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹ç”Ÿæˆæ–¹å¼ä¸åŒï¼ŒBridgeå°†éšè—çŠ¶æ€è§†ä¸ºæ•´ä½“å¼ é‡ï¼Œä»è€Œæå‡å“åº”è´¨é‡ã€‚ä»…éœ€å°‘é‡æ–°å¢å‚æ•°ï¼ŒBridgeå°±èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äºä»»ä½•ç”Ÿæˆå®½åº¦çš„åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01123",
            "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
            "url": "https://huggingface.co/papers/2510.01123",
            "abstract": "Parallel-Distill-Refine (PDR) and Sequential Refinement (SR) improve the performance of LLMs by optimizing accuracy and latency through metacognitive strategies, with PDR showing significant gains on math tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own \"thoughts\" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025).",
            "score": 3,
            "issue_id": 6241,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "a44e5102e9317c19",
            "authors": [
                "Lovish Madaan",
                "Aniket Didolkar",
                "Suchin Gururangan",
                "John Quan",
                "Ruan Silva",
                "Ruslan Salakhutdinov",
                "Manzil Zaheer",
                "Sanjeev Arora",
                "Anirudh Goyal"
            ],
            "affiliations": [
                "Anthropic",
                "Meta Superintelligence Labs",
                "Mila, University of Montreal",
                "Princeton University",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01123.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#long_context",
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel-Distill-Refine (PDR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (long CoT), PDR Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ£Ğ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Sequential Refinement (SR) Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 8B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ¿Ğ¾Ğ´ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ PDR Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: +11% Ğ½Ğ° AIME 2024 Ğ¸ +9% Ğ½Ğ° AIME 2025."
                },
                "en": {
                    "title": "Optimizing LLMs: Accuracy Meets Efficiency with PDR",
                    "desc": "The paper introduces two strategies, Parallel-Distill-Refine (PDR) and Sequential Refinement (SR), to enhance the performance of large language models (LLMs) by balancing accuracy and latency. PDR operates by generating multiple drafts in parallel, distilling them into a concise workspace, and refining the output based on this workspace, which allows for better control over context length and computational costs. The authors demonstrate that PDR can achieve higher accuracy than traditional long chains of thought (CoT) while reducing latency, particularly in math tasks. Additionally, they explore the potential of training models with Reinforcement Learning to further optimize these strategies, showing significant performance improvements in specific math competitions."
                },
                "zh": {
                    "title": "å¹¶è¡Œè’¸é¦ç²¾ç‚¼ï¼šæå‡LLMæ€§èƒ½çš„æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹æ³•ï¼Œç§°ä¸ºå¹¶è¡Œè’¸é¦ç²¾ç‚¼ï¼ˆPDRï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å…ƒè®¤çŸ¥ç­–ç•¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ã€‚PDRé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šæ ·åŒ–è‰ç¨¿ï¼Œæç‚¼æˆä¸€ä¸ªæœ‰é™çš„æ–‡æœ¬å·¥ä½œåŒºï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç²¾ç‚¼ï¼Œä»è€Œä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„é•¿é“¾æ¨ç†ï¼ˆlong CoTï¼‰ç›¸æ¯”ï¼ŒPDRåœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„å»¶è¿Ÿã€‚ç ”ç©¶è¿˜æ¢è®¨äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ä¸PDRæ¨ç†æ–¹æ³•ä¿æŒä¸€è‡´ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26313",
            "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient",
            "url": "https://huggingface.co/papers/2509.26313",
            "abstract": "One-token rollout (OTR) enhances supervised fine-tuning of large language models by incorporating policy gradient methods to improve generalization using on-policy data.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.",
            "score": 3,
            "issue_id": 6232,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "27f70b348211f788",
            "authors": [
                "Rui Ming",
                "Haoyuan Wu",
                "Shoubo Hu",
                "Zhuolun He",
                "Bei Yu"
            ],
            "affiliations": [
                "ChatEDA Tech",
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26313.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğµ: ĞºĞ¾Ğ³Ğ´Ğ° supervised learning Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ reinforcement learning",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ One-Token Rollout (OTR) Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ policy gradient Ğ¸Ğ· reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ RL-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ° ground-truth Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ off-policy Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ on-policy ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° online Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ OTR Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ SFT Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ reasoning."
                },
                "en": {
                    "title": "Enhancing Fine-Tuning with One-Token Rollout",
                    "desc": "One-token rollout (OTR) is a new method for improving the fine-tuning of large language models (LLMs) by using techniques from reinforcement learning (RL). Unlike traditional supervised fine-tuning (SFT), which relies on a fixed dataset, OTR dynamically samples data from the current policy to enhance generalization. It treats each token generation as a reinforcement learning step, using a Monte Carlo approach to evaluate multiple token candidates and provide rewards based on ground-truth data. Our experiments show that OTR significantly outperforms standard SFT across various challenging tasks, highlighting the importance of on-policy data for better model performance."
                },
                "zh": {
                    "title": "å•æ­¥å›æ»šï¼šæå‡è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå•æ­¥å›æ»šï¼ˆOTRï¼‰çš„æ–°ç®—æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒã€‚OTRé€šè¿‡å¼•å…¥ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œåˆ©ç”¨å½“å‰ç­–ç•¥çš„åœ¨çº¿æ•°æ®æ¥æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒOTRå°†æ¯ä¸ªæ ‡è®°ç”Ÿæˆè§†ä¸ºå•æ­¥å¼ºåŒ–å­¦ä¹ è½¨è¿¹ï¼Œå¹¶é€šè¿‡è’™ç‰¹å¡æ´›å›æ»šé‡‡æ ·å¤šä¸ªå€™é€‰æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOTRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼Œè¯æ˜äº†åœ¨çº¿æ•°æ®åœ¨æé«˜æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01241",
            "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
            "url": "https://huggingface.co/papers/2510.01241",
            "abstract": "SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.",
            "score": 3,
            "issue_id": 6222,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "a5e2f851fdccce5f",
            "authors": [
                "Hu Wei",
                "Ze Xu",
                "Boyu Yang",
                "Linlin Miao",
                "Weiqi Zhai",
                "Yihan Li",
                "Zixuan Li",
                "Zhijun Wang",
                "Boya Wang",
                "Jianwei Yu",
                "Jialing Yuan",
                "Xiaoyue Zhang",
                "Cheng He",
                "Minglei Chen",
                "Zifan Zhang",
                "Qianhui Li",
                "Wei Wang",
                "Xiang Xu"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01241.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#survey"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "SKYLENAGE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ±Ğ½Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ reasoning Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° SKYLENAGE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM: ReasoningMATH Ñ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ¸ MATH ÑĞ¾ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ´Ğ¾ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€Ğ°Ğ½Ñ‚ÑƒÑ€Ñ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 44% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ° Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 81% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° reasoning Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI."
                },
                "en": {
                    "title": "SKYLENAGE: Benchmarking Math Reasoning in LLMs",
                    "desc": "The SKYLENAGE benchmarks are designed to evaluate large language models (LLMs) on their mathematical reasoning abilities, highlighting performance gaps across different educational levels. The benchmarks consist of two parts: SKYLENAGE-ReasoningMATH, which includes a diagnostic set with detailed metadata, and SKYLENAGE-MATH, a contest-style suite that covers a range of subjects from high school to doctoral levels. The evaluation of fifteen LLM variants shows that while the best model achieves 81% accuracy on reasoning tasks, there are significant declines in performance from high school to doctoral levels. Overall, SKYLENAGE aims to provide a comprehensive and challenging benchmark for assessing mathematical reasoning in LLMs, with a focus on calibrated difficulty and detailed performance metrics."
                },
                "zh": {
                    "title": "SKYLENAGEï¼šæ•°å­¦æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "SKYLENAGEåŸºå‡†æµ‹è¯•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸åŒæ•™è‚²æ°´å¹³ä¹‹é—´çš„æ€§èƒ½å·®è·å’Œå¤©èŠ±æ¿æ•ˆåº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªäº’è¡¥çš„åŸºå‡†ï¼šSKYLENAGE-ReasoningMATHå’ŒSKYLENAGE-MATHï¼Œå‰è€…æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªé¡¹ç›®çš„ç»“æ„åŒ–è¯Šæ–­é›†ï¼Œåè€…æ˜¯ä¸€ä¸ªåŒ…å«150ä¸ªé¡¹ç›®çš„ç«èµ›é£æ ¼å¥—ä»¶ï¼Œæ¶µç›–ä»é«˜ä¸­åˆ°åšå£«çš„å››ä¸ªé˜¶æ®µã€‚é€šè¿‡å¯¹åäº”ç§ç°ä»£LLMå˜ä½“çš„è¯„ä¼°ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒå­¦ç§‘å’Œå¹´çº§çš„æ¨¡å‹è¡¨ç°ï¼Œå‘ç°å‡†ç¡®ç‡ä»é«˜ä¸­åˆ°åšå£«é€æ¸ä¸‹é™ã€‚SKYLENAGEæä¾›äº†ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ•°å­¦åŸºå‡†ï¼Œå…·æœ‰æ ¡å‡†çš„éš¾åº¦å’Œä¸°å¯Œçš„å…ƒæ•°æ®ï¼Œä¸ºæœªæ¥çš„æ•°å­¦æ¨ç†è¯„ä¼°æä¾›äº†å‚è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00137",
            "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
            "url": "https://huggingface.co/papers/2510.00137",
            "abstract": "A new training objective, MW loss, is introduced to improve retriever calibration and ranking quality by directly optimizing the Area under the ROC Curve (AUC), outperforming Contrastive Loss in retrieval-augmented generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.",
            "score": 2,
            "issue_id": 6232,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "7d01b141bdfa87b9",
            "authors": [
                "Nima Sheikholeslami",
                "Erfan Hosseini",
                "Patrice Bechard",
                "Srivatsava Daruru",
                "Sai Rajeswar"
            ],
            "affiliations": [
                "ServiceNow"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00137.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "MW loss: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ AUC Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Contrastive Loss Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ MW loss Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ dual-encoder retriever Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¿Ğ¾Ğ´ ROC-ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ (AUC) Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞœĞ°Ğ½Ğ½Ğ°-Ğ£Ğ¸Ñ‚Ğ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Contrastive Loss Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Noise Contrastive Estimation Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ AUC, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹. MW loss Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ retriever-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ MW loss, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Contrastive Loss Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… retrieval-augmented generation."
                },
                "en": {
                    "title": "MW Loss: A New Standard for Retriever Calibration and Ranking",
                    "desc": "This paper introduces a new training objective called MW loss, which aims to enhance the calibration and ranking quality of retrievers in retrieval-augmented generation tasks. Unlike the traditional Contrastive Loss that relies on Noise Contrastive Estimation, MW loss directly optimizes the Area under the ROC Curve (AUC), ensuring better score separation between relevant and irrelevant documents. The authors demonstrate that MW loss minimizes binary cross entropy over score differences, leading to improved performance in downstream tasks. Empirical results show that retrievers trained with MW loss outperform those trained with Contrastive Loss, making it a more effective choice for high-stakes applications."
                },
                "zh": {
                    "title": "MWæŸå¤±ï¼šæå‡æ£€ç´¢å™¨æ ¡å‡†ä¸æ’åè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œç§°ä¸ºMWæŸå¤±ï¼Œæ—¨åœ¨é€šè¿‡ç›´æ¥ä¼˜åŒ–ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUCï¼‰æ¥æ”¹å–„æ£€ç´¢å™¨çš„æ ¡å‡†å’Œæ’åè´¨é‡ã€‚ä¼ ç»Ÿçš„å¯¹æ¯”æŸå¤±ï¼ˆContrastive Lossï¼‰ä¾èµ–çš„å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆNCEï¼‰ç›®æ ‡æœªèƒ½æœ‰æ•ˆåŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³æ–‡æ¡£çš„å¾—åˆ†ï¼Œå¯¼è‡´æ ¡å‡†ä¸ä½³å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸ç†æƒ³ã€‚MWæŸå¤±é€šè¿‡æœ€å¤§åŒ–Mann-Whitney Uç»Ÿè®¡é‡ï¼Œç¡®ä¿æ¯å¯¹æ­£è´Ÿæ ·æœ¬çš„å¾—åˆ†å·®å¼‚è¢«æ­£ç¡®æ’åï¼Œä»è€Œæé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MWæŸå¤±è®­ç»ƒçš„æ£€ç´¢å™¨åœ¨AUCå’Œæ ‡å‡†æ£€ç´¢æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¯¹æ¯”æŸå¤±çš„æ£€ç´¢å™¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25729",
            "title": "Controlled Generation for Private Synthetic Text",
            "url": "https://huggingface.co/papers/2509.25729",
            "abstract": "A novel methodology for privacy-preserving synthetic text generation using entity-aware control codes and HIPS theory achieves a balance between privacy and utility in sensitive domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Text anonymization is essential for responsibly developing and deploying AI in high-stakes domains such as healthcare, social services, and law. In this work, we propose a novel methodology for privacy-preserving synthetic text generation that leverages the principles of de-identification and the Hiding In Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes to guide controllable generation using either in-context learning (ICL) or prefix tuning. The ICL variant ensures privacy levels consistent with the underlying de-identification system, while the prefix tuning variant incorporates a custom masking strategy and loss function to support scalable, high-quality generation. Experiments on legal and clinical datasets demonstrate that our method achieves a strong balance between privacy protection and utility, offering a practical and effective solution for synthetic text generation in sensitive domains.",
            "score": 2,
            "issue_id": 6236,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "8fa0aefa8e1a5246",
            "authors": [
                "Zihao Zhao",
                "Anjalie Field"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25729.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#ethics",
                    "#dataset",
                    "#healthcare",
                    "#synthetic"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ° Ğ¸ ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ HIPS (Hiding In Plain Sight), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ñ‹, Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ² Ğ´Ğ²ÑƒÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ…: Ñ‡ĞµÑ€ĞµĞ· in-context learning (ICL) Ğ¸ Ñ‡ĞµÑ€ĞµĞ· prefix tuning Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Balancing Privacy and Utility in Synthetic Text Generation",
                    "desc": "This paper presents a new method for generating synthetic text that protects privacy while maintaining usefulness, especially in sensitive areas like healthcare and law. It uses entity-aware control codes to guide the text generation process, ensuring that sensitive information is de-identified. The methodology incorporates two techniques: in-context learning (ICL) for privacy consistency and prefix tuning with a custom masking strategy for high-quality output. Experiments show that this approach effectively balances privacy and utility, making it a valuable tool for responsible AI development."
                },
                "zh": {
                    "title": "éšç§ä¿æŠ¤ä¸å®ç”¨æ€§çš„å®Œç¾å¹³è¡¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºåœ¨æ•æ„Ÿé¢†åŸŸå®ç°éšç§ä¿æŠ¤çš„åˆæˆæ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†å»æ ‡è¯†åŒ–åŸåˆ™å’ŒHIPSç†è®ºï¼Œä½¿ç”¨å®ä½“æ„ŸçŸ¥æ§åˆ¶ä»£ç æ¥æŒ‡å¯¼å¯æ§ç”Ÿæˆã€‚é€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå‰ç¼€è°ƒä¼˜çš„å˜ä½“ä¸­ï¼Œç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬åœ¨éšç§ä¿æŠ¤å’Œå®ç”¨æ€§ä¹‹é—´è¾¾åˆ°è‰¯å¥½çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³•å¾‹å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„åˆæˆæ–‡æœ¬ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24304",
            "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
            "url": "https://huggingface.co/papers/2509.24304",
            "abstract": "FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness.",
            "score": 2,
            "issue_id": 6222,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "8f267884ff3ee32b",
            "authors": [
                "Zefeng He",
                "Xiaoye Qu",
                "Yafu Li",
                "Siyuan Huang",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24304.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#long_context",
                    "#rl",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²",
                    "desc": "FrameThinker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Large Vision-Language Models (LVLMs). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ°), Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10.4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 20 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² - Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 76.1% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° LongVideo-Reason, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ 20.6 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Video Reasoning with FrameThinker",
                    "desc": "FrameThinker is a new framework designed to improve video reasoning by allowing Large Vision-Language Models (LVLMs) to interactively analyze video content. It addresses the limitations of traditional methods that rely on uniform frame sampling and static reasoning, which are inefficient for complex video tasks. The framework employs a two-phase training strategy, starting with Supervised Fine-Tuning to develop basic action skills, followed by Reinforcement Learning to refine decision-making processes. Experimental results show that FrameThinker significantly enhances performance on various benchmarks, achieving state-of-the-art accuracy while processing far fewer frames than previous models."
                },
                "zh": {
                    "title": "FrameThinkerï¼šé«˜æ•ˆçš„è§†é¢‘æ¨ç†æ–°æ¡†æ¶",
                    "desc": "FrameThinkeræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé€æ­¥è¯¢é—®è§†é¢‘å†…å®¹ï¼Œä»è€Œå¢å¼ºè§†é¢‘æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘æ¨ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰å¯†é›†å‹ä»»åŠ¡æ—¶ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒä»¥å»ºç«‹åŸºæœ¬åŠ¨ä½œèƒ½åŠ›ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFrameThinkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®ç‡ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†å¤„ç†çš„å¸§æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02306",
            "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
            "url": "https://huggingface.co/papers/2510.02306",
            "abstract": "Ignoring rating updates for draws in arena-style evaluations of large language models improves battle outcome prediction accuracy by 1-3% across different rating systems.  \t\t\t\t\tAI-generated summary \t\t\t\t In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the \"battle\" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.",
            "score": 1,
            "issue_id": 6234,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "79d6d15b77cd3ecb",
            "authors": [
                "Raphael Tang",
                "Crystina Zhang",
                "Wenyan Li",
                "Carmen Lai",
                "Pontus Stenetorp",
                "Yao Lu"
            ],
            "affiliations": [
                "Centre for Artificial Intelligence, University College London",
                "Independent Researcher",
                "Research and Development Center for Large Language Models, National Institute of Informatics",
                "University of Copenhagen",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02306.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ¸Ñ‡ÑŒÑ Ğ² Ğ°Ñ€ĞµĞ½Ğµ LLM â€” ÑÑ‚Ğ¾ Ğ½Ğµ Ñ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ğ»Ñ‘Ğ³ĞºĞ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² arena-style Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠÑĞ²Ğ»ÑÑÑ‚ Ğ½Ğ¸Ñ‡ÑŒÑ. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ Ğ½Ğ¸Ñ‡ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ² ÑˆĞ°Ñ…Ğ¼Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Elo-Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ â€” ÑƒÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ñ‡ÑŒÑ ÑĞºĞ¾Ñ€ĞµĞµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. Ğ˜Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° 1-3% Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ… Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking Draws: Enhancing LLM Battle Predictions",
                    "desc": "This paper investigates the effectiveness of the Elo rating system in arena-style evaluations of large language models (LLMs). It challenges the assumption that a draw in model responses indicates equal performance, suggesting instead that draws may reflect the difficulty of the query posed. By ignoring rating updates for draws, the authors demonstrate a 1-3% improvement in predicting battle outcomes across various rating systems. The findings indicate that draws are more frequent for easier and more objective queries, prompting a reevaluation of how draws are treated in rating updates."
                },
                "zh": {
                    "title": "å¿½ç•¥å¹³å±€æ›´æ–°ï¼Œæå‡æ¨¡å‹æˆ˜æ–—é¢„æµ‹å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«æŠ€è¯„ä¼°ä¸­ï¼Œå¿½ç•¥å¹³å±€çš„è¯„åˆ†æ›´æ–°å¦‚ä½•æé«˜æˆ˜æ–—ç»“æœé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è´¨ç–‘å¹³å±€æ˜¯å¦çœŸæ­£æ„å‘³ç€ä¸¤ä¸ªæ¨¡å‹ç›¸ç­‰ï¼Œå› æ­¤æ˜¯å¦åº”è¯¥å°†å®ƒä»¬çš„è¯„åˆ†ç›¸ç­‰åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¹³å±€æ›´å¯èƒ½åæ˜ æŸ¥è¯¢çš„éš¾åº¦ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¹³ç­‰è¡¨ç°ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å¿½ç•¥å¹³å±€çš„è¯„åˆ†æ›´æ–°å¯ä»¥æé«˜1-3%çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01691",
            "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
            "url": "https://huggingface.co/papers/2510.01691",
            "abstract": "MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.",
            "score": 1,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "1aec9b0a96bf1d83",
            "authors": [
                "Jiyao Liu",
                "Jinjie Wei",
                "Wanying Qu",
                "Chenglong Ma",
                "Junzhi Ning",
                "Yunheng Li",
                "Ying Chen",
                "Xinzhe Luo",
                "Pengcheng Chen",
                "Xin Gao",
                "Ming Hu",
                "Huihui Xu",
                "Xin Wang",
                "Shujian Gao",
                "Dingkang Yang",
                "Zhongying Deng",
                "Jin Ye",
                "Lihao Liu",
                "Junjun He",
                "Ningsheng Xu"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01691.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#healthcare",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MedQ-Bench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº. Benchmark Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡: MedQ-Perception Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ MedQ-Reasoning Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ¼ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼-Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ AI-generated ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ â€” Ğ²ÑĞµĞ³Ğ¾ 2600 Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ 708 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Quality Assessment with Language Models",
                    "desc": "MedQ-Bench is a new benchmark designed to evaluate the quality of medical images using Multi-modal Large Language Models (MLLMs). It addresses the limitations of traditional methods that rely on simple score-based metrics, which do not capture the complex reasoning that human experts use. The benchmark includes two main tasks: MedQ-Perception for assessing basic visual attributes and MedQ-Reasoning for evaluating more complex reasoning about image quality. By providing a comprehensive set of tasks and a robust evaluation framework, MedQ-Bench aims to improve the performance of MLLMs in medical image quality assessment and encourage further research in this area."
                },
                "zh": {
                    "title": "åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°çš„æ–°åŸºå‡†ï¼šMedQ-Bench",
                    "desc": "MedQ-Benchæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°çš„åŸºå‡†ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè¯­è¨€åŸºç¡€çš„è¯„ä¼°ã€‚è¯¥åŸºå‡†å…³æ³¨æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå®šä¹‰äº†ä¸¤ä¸ªäº’è¡¥çš„ä»»åŠ¡ï¼šMedQ-Perceptionå’ŒMedQ-Reasoningã€‚MedQ-Perceptioné€šè¿‡äººç±»ç­–åˆ’çš„é—®é¢˜æ¢æµ‹ä½çº§æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€ŒMedQ-Reasoningåˆ™åŒ…æ‹¬æ— å‚è€ƒå’Œæ¯”è¾ƒæ¨ç†ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹è¯„ä¼°ä¸äººç±»ä¸“å®¶çš„æ¨ç†è¿‡ç¨‹ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºåˆæ­¥ä½†ä¸ç¨³å®šçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒäº†å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00537",
            "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
            "url": "https://huggingface.co/papers/2510.00537",
            "abstract": "Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.",
            "score": 1,
            "issue_id": 6223,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "ce99291350b941ce",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00537.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¨Ğ¸Ñ€Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ: Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ feed-forward ÑĞµÑ‚ĞµĞ¹ Ğ² LLM Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Hard Rank, Soft Rank Ğ¸ Spectral Utilization Index Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA, GPT-2 Ğ¸ nGPT. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: soft rank Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ñƒ, Ğ° hard rank â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ FFN ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒÑ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° LLM."
                },
                "en": {
                    "title": "Unlocking Latent Space: The Asymmetric Scaling of Feed-Forward Networks",
                    "desc": "This paper investigates how the width of feed-forward networks (FFNs) in large language models (LLMs) affects their performance and utilization of latent space. It introduces an asymmetric spectral scaling law, showing that increasing the width mainly enhances low-energy directions while the dominant modes reach saturation quickly. The authors use various metrics to analyze the activation of latent directions in models like LLaMA and GPT-2, revealing that much of the latent space remains under-utilized as width increases. This research provides insights into optimizing FFN width for better efficiency in LLM design by balancing tail capacity and dominant-mode capacity."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å‰é¦ˆç½‘ç»œå®½åº¦ï¼Œæå‡æ½œåœ¨ç©ºé—´åˆ©ç”¨ç‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰çš„å®½åº¦é€‰æ‹©é—®é¢˜ï¼Œæ­ç¤ºäº†ä¸å¯¹ç§°çš„è°±ç¼©æ”¾æ³•åˆ™ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢åŠ ç½‘ç»œå®½åº¦ä¸»è¦å¢åŠ ä½èƒ½é‡æ–¹å‘ï¼Œè€Œä¸»å¯¼æ¨¡å¼åœ¨æ—©æœŸå°±è¾¾åˆ°é¥±å’Œï¼Œå¯¼è‡´æ½œåœ¨ç©ºé—´çš„åˆ©ç”¨ä¸è¶³ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€å¥—è½»é‡çº§çš„è¯Šæ–­å·¥å…·æ¥é‡åŒ–ä¸åŒæ¨¡å‹ï¼ˆå¦‚LLaMAã€GPT-2å’ŒnGPTï¼‰ä¸­æœ‰æ•ˆæ¿€æ´»çš„æ½œåœ¨æ–¹å‘æ•°é‡ã€‚ç ”ç©¶ç»“æœä¸ºFFNå®½åº¦é€‰æ‹©æä¾›äº†åŸåˆ™æ€§çš„æƒè¡¡æŒ‡å¯¼ï¼Œå¸®åŠ©è®¾è®¡æ›´é«˜æ•ˆçš„æ¨ç†æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26330",
            "title": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking\n  for Training-free Zero-Shot Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2509.26330",
            "abstract": "SQUARE is a two-stage training-free framework that uses Multimodal Large Language Models to enhance zero-shot Composed Image Retrieval by enriching query embeddings and performing efficient batch reranking.  \t\t\t\t\tAI-generated summary \t\t\t\t Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from a vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the user's intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in a single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained, demonstrating its potential applicability.",
            "score": 1,
            "issue_id": 6226,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "5b35a1137cf9fd79",
            "authors": [
                "Ren-Di Wu",
                "Yu-Yen Lin",
                "Huei-Fang Yang"
            ],
            "affiliations": [
                "National Sun Yat-sen University, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26330.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "SQUARE â€” ÑÑ‚Ğ¾ framework Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ (Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ + Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Multimodal LLM, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ€Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²: MLLM Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ¿ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸, Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… benchmark-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Retrieval with Multimodal Language Models",
                    "desc": "SQUARE is a novel framework designed to improve zero-shot Composed Image Retrieval (ZS-CIR) by utilizing Multimodal Large Language Models (MLLMs). It operates in two stages: first, it enhances query embeddings through Semantic Query-Augmented Fusion (SQAF) by adding MLLM-generated captions that clarify user intent. Next, it employs Efficient Batch Reranking (EBR) to refine the retrieval results by performing visual-semantic reasoning on the top candidates. This approach allows SQUARE to achieve high accuracy without the need for task-specific training, making it effective across various benchmarks."
                },
                "zh": {
                    "title": "SQUAREï¼šæ— è®­ç»ƒçš„ç»„åˆå›¾åƒæ£€ç´¢æ–°æ¡†æ¶",
                    "desc": "SQUAREæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ— è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å¢å¼ºé›¶-shotç»„åˆå›¾åƒæ£€ç´¢ï¼ˆZS-CIRï¼‰ã€‚åœ¨è¯­ä¹‰æŸ¥è¯¢å¢å¼ºèåˆï¼ˆSQAFï¼‰é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡MLLMç”Ÿæˆçš„ç›®æ ‡å›¾åƒæ ‡é¢˜æ¥ä¸°å¯Œæ¥è‡ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æŸ¥è¯¢åµŒå…¥ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æ¥ç€ï¼Œåœ¨é«˜æ•ˆæ‰¹é‡é‡æ’åºï¼ˆEBRï¼‰é˜¶æ®µï¼ŒMLLMå¯¹æ’åé å‰çš„å€™é€‰å›¾åƒè¿›è¡Œè”åˆè§†è§‰-è¯­ä¹‰æ¨ç†ï¼Œæä¾›æ›´å‡†ç¡®çš„æ’åã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSQUAREåœ¨å¤šä¸ªæ ‡å‡†CIRåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨è½»é‡çº§é¢„è®­ç»ƒæ¨¡å‹ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01581",
            "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
            "url": "https://huggingface.co/papers/2510.01581",
            "abstract": "TRAAC, an online post-training RL method, improves model accuracy and efficiency by adaptively adjusting reasoning steps based on task difficulty using self-attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
            "score": 0,
            "issue_id": 6234,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "e3cc40d449f365df",
            "authors": [
                "Joykirat Singh",
                "Justin Chih-Yao Chen",
                "Archiki Prasad",
                "Elias Stengel-Eskin",
                "Akshay Nambi",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Texas at Austin",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01581.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "TRAAC â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³Ğ° Ñ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ self-attention Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8.4% Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 36.8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPQA-D Ğ¸ BBEH."
                },
                "en": {
                    "title": "Think Right: Adaptive Reasoning for Enhanced Efficiency",
                    "desc": "TRAAC is an innovative online post-training reinforcement learning method designed to enhance model performance by dynamically adjusting reasoning steps according to the difficulty of tasks. It addresses the problem of under-adaptivity, where models either underthink or overthink, leading to inefficiencies in reasoning. By utilizing self-attention mechanisms, TRAAC identifies crucial reasoning steps and eliminates unnecessary ones, optimizing the reasoning process. The method not only improves accuracy and reduces reasoning length but also generalizes well across various tasks, demonstrating its effectiveness in adapting to different levels of task complexity."
                },
                "zh": {
                    "title": "TRAACï¼šè‡ªé€‚åº”æ¨ç†çš„æ™ºèƒ½é€‰æ‹©",
                    "desc": "TRAACæ˜¯ä¸€ç§åœ¨çº¿åè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ ¹æ®ä»»åŠ¡éš¾åº¦è‡ªé€‚åº”è°ƒæ•´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼ŒçŸ­æ¨ç†å¯¼è‡´çš„é”™è¯¯å’Œé•¿æ¨ç†é€ æˆçš„èµ„æºæµªè´¹é—®é¢˜ã€‚TRAACé€šè¿‡è¯†åˆ«é‡è¦æ­¥éª¤å¹¶ä¿®å‰ªå†—ä½™æ­¥éª¤ï¼Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å°†ä»»åŠ¡éš¾åº¦çº³å…¥è®­ç»ƒå¥–åŠ±ä¸­ï¼Œä»¥åˆç†åˆ†é…æ¨ç†é¢„ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRAACåœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†æ¨ç†æ­¥éª¤ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00352",
            "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance",
            "url": "https://huggingface.co/papers/2510.00352",
            "abstract": "AReUReDi, a discrete optimization algorithm, achieves Pareto optimality in multi-objective biomolecule sequence design, outperforming evolutionary and diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.",
            "score": 0,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "9d15b447c9342ce6",
            "authors": [
                "Tong Chen",
                "Yinuo Zhang",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Centre for Computational Biology, Duke-NUS Medical School, Singapore",
                "Department of Bioengineering, University of Pennsylvania",
                "Department of Computer and Information Science, University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00352.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ¾Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AReUReDi â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Rectified Discrete Flows Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ°Ğ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ§ĞµĞ±Ñ‹ÑˆÑ‘Ğ²Ğ° Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ ĞœĞµÑ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸ÑĞ°-Ğ“Ğ°ÑÑ‚Ğ¸Ğ½Ğ³ÑĞ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. AReUReDi ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ Ğ¿ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ (Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑ‚Ğ²Ğ¾Ñ€Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒĞ²Ñ‹Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ) Ğ´Ğ»Ñ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ¸ SMILES-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ñ„Ñ€Ğ¾Ğ½Ñ‚Ñƒ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾."
                },
                "en": {
                    "title": "AReUReDi: Optimizing Biomolecule Sequences for Multiple Objectives",
                    "desc": "AReUReDi is a novel discrete optimization algorithm designed for multi-objective biomolecule sequence design, ensuring Pareto optimality. Unlike traditional methods that often focus on single objectives or continuous spaces, AReUReDi effectively handles multiple conflicting objectives by utilizing Tchebycheff scalarization and locally balanced proposals. The algorithm employs annealed Metropolis-Hastings updates to enhance sampling towards optimal solutions while maintaining distributional invariance. When tested on peptide and SMILES sequence design, AReUReDi demonstrated superior performance compared to existing evolutionary and diffusion-based methods, optimizing several therapeutic properties simultaneously."
                },
                "zh": {
                    "title": "AReUReDiï¼šå¤šç›®æ ‡ä¼˜åŒ–çš„æ–°é€‰æ‹©",
                    "desc": "AReUReDiæ˜¯ä¸€ç§ç¦»æ•£ä¼˜åŒ–ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šç›®æ ‡ç”Ÿç‰©åˆ†å­åºåˆ—è®¾è®¡ä¸­å®ç°å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚ä¸ç°æœ‰çš„è¿›åŒ–å’Œæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼ŒAReUReDiåœ¨ä¼˜åŒ–å¤šä¸ªç›¸äº’å†²çªçš„ç›®æ ‡æ–¹é¢è¡¨ç°æ›´ä½³ã€‚è¯¥ç®—æ³•ç»“åˆäº†Tchebycheffæ ‡é‡åŒ–ã€å±€éƒ¨å¹³è¡¡ææ¡ˆå’Œé€€ç«Metropolis-Hastingsæ›´æ–°ï¼Œç¡®ä¿äº†æ”¶æ•›åˆ°å¸•ç´¯æ‰˜å‰æ²¿çš„ç†è®ºä¿è¯ã€‚åº”ç”¨äºè‚½å’ŒSMILESåºåˆ—è®¾è®¡æ—¶ï¼ŒAReUReDièƒ½å¤ŸåŒæ—¶ä¼˜åŒ–å¤šè¾¾äº”ç§æ²»ç–—ç‰¹æ€§ï¼Œå±•ç°å‡ºå…¶åœ¨å¤šå±æ€§ç”Ÿç‰©åˆ†å­ç”Ÿæˆä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01260",
            "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
            "url": "https://huggingface.co/papers/2510.01260",
            "abstract": "IoT-MCP, a framework using the Model Context Protocol, enables seamless communication between Large Language Models and IoT devices, achieving high task success rates and low resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.",
            "score": 0,
            "issue_id": 6230,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "3ff1f92430757d75",
            "authors": [
                "Ningyuan Yang",
                "Guanliang Lyu",
                "Mingchen Ma",
                "Yiyi Lu",
                "Yiming Li",
                "Zhihui Gao",
                "Hancheng Ye",
                "Jianyi Zhang",
                "Tingjun Chen",
                "Yiran Chen"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, Duke University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01260.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ¸ IoT ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IoT-MCP â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ IoT-ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Model Context Protocol. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MCP-ÑĞµÑ€Ğ²ĞµÑ€Ñ‹ Ğ½Ğ° edge-ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IoT-MCP Bench Ñ 114 Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ 1140 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° 22 Ñ‚Ğ¸Ğ¿Ğ°Ñ… ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 100% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° 205Ğ¼Ñ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 74ĞšĞ‘."
                },
                "en": {
                    "title": "Seamless LLM-IoT Integration with IoT-MCP Framework",
                    "desc": "The paper presents IoT-MCP, a framework that utilizes the Model Context Protocol (MCP) to facilitate effective communication between Large Language Models (LLMs) and Internet-of-Things (IoT) devices. It addresses challenges such as hardware diversity and control complexity by standardizing interactions, allowing for seamless integration. The authors introduce IoT-MCP Bench, a benchmark with 114 Basic Tasks and 1,140 Complex Tasks to evaluate LLM performance in IoT contexts. Experimental results show that IoT-MCP achieves a 100% task success rate with low latency and memory usage, providing a robust solution for LLM-IoT integration."
                },
                "zh": {
                    "title": "æ— ç¼è¿æ¥LLMä¸ç‰©è”ç½‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºIoT-MCPçš„æ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡ä¹‹é—´çš„æ— ç¼é€šä¿¡ã€‚è¯¥æ¡†æ¶é€šè¿‡è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²ï¼Œè§£å†³äº†ç¡¬ä»¶å¼‚æ„æ€§å’Œæ§åˆ¶å¤æ‚æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†IoT-MCP Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«114ä¸ªåŸºæœ¬ä»»åŠ¡å’Œ1140ä¸ªå¤æ‚ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°IoTæ”¯æŒçš„LLMã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIoT-MCPåœ¨22ç§ä¼ æ„Ÿå™¨å’Œ6ç§å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†100%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹³å‡å“åº”æ—¶é—´ä¸º205æ¯«ç§’ï¼Œå³°å€¼å†…å­˜å ç”¨ä¸º74KBã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-02.html",
    "link_next": "2025-10-06.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "02.10",
        "en": "10/02",
        "zh": "10æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.10",
        "en": "10/06",
        "zh": "10æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 13,
        "#data": 6,
        "#benchmark": 24,
        "#agents": 9,
        "#cv": 6,
        "#rl": 15,
        "#rlhf": 7,
        "#rag": 2,
        "#plp": 1,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 13,
        "#math": 4,
        "#multilingual": 1,
        "#architecture": 12,
        "#healthcare": 3,
        "#training": 23,
        "#robotics": 0,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 4,
        "#reasoning": 22,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 29,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 5,
        "#long_context": 6,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 2,
        "#evaluation": 1
    }
}