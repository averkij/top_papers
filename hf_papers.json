{
    "date": "15 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-15 02:47",
    "issue_id": 107,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.10139",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.10139",
            "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.",
            "score": 20,
            "issue_id": 107,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "MMIE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —á–µ—Ä–µ–¥—É—é—â–∏–º—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (LVLMs). –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 20 000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ 12 –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π. MMIE –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —á–µ—Ä–µ–¥—É—é—â–∏–µ—Å—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤–æ–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ü–µ–Ω–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö.",
                "tags": [
                    "#LVLMs",
                    "#–∏–Ω—Ç–µ—Ä–ª–∏–≤–∏–Ω–≥–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å",
                    "#–æ—Ü–µ–Ω–∫–∞–ú–æ–¥–µ–ª–µ–π"
                ],
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üîÑ",
                "title": "MMIE: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–¥—É—é—â–∏—Ö—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09584",
            "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2410.09584",
            "abstract": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.",
            "score": 15,
            "issue_id": 107,
            "pub_date": "2024-10-12",
            "pub_date_ru": "12 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "VIF-RAG - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Å–∏—Å—Ç–µ–º–∞—Ö RAG. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø—Ä–∞–≤–∏–ª–∞ –∏—Ö –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç VIF-RAG-QA –∏ —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ FollowRAG –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG-—Å–∏—Å—Ç–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ VIF-RAG –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.",
                "tags": [
                    "#instruction-following",
                    "#synthetic-data-generation",
                    "#rag-evaluation"
                ],
                "categories": [
                    "#nlp",
                    "#rag",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üß†",
                "title": "VIF-RAG: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é RAG-—Å–∏—Å—Ç–µ–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07985",
            "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
            "url": "https://huggingface.co/papers/2410.07985",
            "abstract": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.",
            "score": 9,
            "issue_id": 107,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ–ª–∏–º–ø–∏–∞–¥. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç 4428 –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 33 –ø–æ–¥–¥–æ–º–µ–Ω–∞ –∏ 10 —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ —Å–∞–º—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ OpenAI o1-mini –∏ OpenAI o1-preview, –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å —Ä–µ—à–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 60.54% –∏ 52.55% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#olympiad-math",
                    "#benchmark-dataset",
                    "#llm-evaluation"
                ],
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#nlp"
                ],
                "emoji": "üßÆ",
                "title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: –æ–ª–∏–º–ø–∏–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∫–∞–∫ —Ç–µ—Å—Ç –Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09732",
            "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
            "url": "https://huggingface.co/papers/2410.09732",
            "abstract": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/",
            "score": 8,
            "issue_id": 107,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "LOKI - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 18 —Ç—ã—Å—è—á –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –≤–∏–¥–µ–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, 3D, —Ç–µ–∫—Å—Ç—É –∏ –∞—É–¥–∏–æ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 26 –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —á–µ—Ç–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. LOKI –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LMM —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á–∏ –≥—Ä—É–±–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤—ã–±–æ—Ä–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ 28 –º–æ–¥–µ–ª–µ–π LMM –Ω–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ, –≤—ã—è–≤–∏–≤ –∏—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "tags": [
                    "#syntheticDataDetection",
                    "#multimodalBenchmark",
                    "#LMMevaluation"
                ],
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üïµÔ∏è",
                "title": "LOKI: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤ –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07752",
            "title": "TVBench: Redesigning Video-Language Evaluation",
            "url": "https://huggingface.co/papers/2410.07752",
            "abstract": "Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.",
            "score": 1,
            "issue_id": 107,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –º–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TVBench, —Ç—Ä–µ–±—É—é—â–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å TVBench, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è.",
                "tags": [
                    "#video-language-models",
                    "#temporal-reasoning",
                    "#benchmark-evaluation"
                ],
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "üé•",
                "title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09335",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
            "url": "https://huggingface.co/papers/2410.09335",
            "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.",
            "score": 0,
            "issue_id": 107,
            "pub_date": "2024-10-12",
            "pub_date_ru": "12 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–≤–æ–¥–∫–∏ (SFT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∏ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–≤—É—Ö –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ SFT. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–ª–∏–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –±–æ–ª–µ–µ —Å–ª–∞–±—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#dataSelection",
                    "#SFT",
                    "#largeScaleDatasets"
                ],
                "categories": [
                    "#nlp",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "üîç",
                "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        }
    ],
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫ÜBaichuan-OmniÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ7BÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂíåÂàÜÊûêÂõæÂÉè„ÄÅËßÜÈ¢ë„ÄÅÈü≥È¢ëÂíåÊñáÊú¨ÔºåÊèê‰æõÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫í‰ΩìÈ™åÂíåÂº∫Â§ßÊÄßËÉΩ„ÄÇÊñáÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÂ§öÊ®°ÊÄÅËÆ≠ÁªÉÊñπÊ°àÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂØπÈΩêÂíåÂ§ö‰ªªÂä°ÂæÆË∞É‰∏§‰∏™Èò∂ÊÆµÔºå‰ΩøËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜËßÜËßâÂíåÈü≥È¢ëÊï∞ÊçÆÔºåÂπ∂Âú®ÂêÑÁßçÂÖ®Ê®°ÊÄÅÂíåÂ§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ",
        "pinyin": "zh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le Baichuan-Omni, yƒ´ g√® kƒÅi yu√°n de 7B du≈ç m√≥ t√†i d√† y«î y√°n m√≥ x√≠ng, n√©ng g√≤u t√≥ng sh√≠ ch«î l«ê h√© fƒìn xƒ´ t√∫ xi√†ng, sh√¨ p«ên, yƒ´n p«ên h√© w√©n bƒõn, t√≠ g≈çng xiƒÅn j√¨n de du≈ç m√≥ t√†i jiƒÅo h√π t«ê y√†n h√© qi√°ng d√† x√≠ng n√©ng. w√©n zhƒÅng t√≠ ch≈´ le yƒ´ zh«íng y«íu xi√†o de du≈ç m√≥ t√†i x√πn li√†n fƒÅng √†n, t≈çng gu√≤ du≈ç m√≥ t√†i du√¨ q√≠ h√© du≈ç r√®n w«î ti√°o li«éng √®r g√® jiƒì du√†n, sh«ê y«î y√°n m√≥ x√≠ng n√©ng g√≤u y«íu xi√†o ch«î l«ê sh√¨ ju√© h√© yƒ´n p«ên sh√π j√π, b√¨ng z√†i g√® zh«íng qu√°n m√≥ t√†i h√© du≈ç m√≥ t√†i bƒõn zh«în c√® sh√¨ zh≈çng bi«éo xi√†n ch≈´ s√®.",
        "vocab": "[\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ shu√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"Â§ßËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"d√† y«îy√°n m√≥x√≠ng\", \"trans\": \"large language model\"},\n    {\"word\": \"Â§ÑÁêÜ\", \"pinyin\": \"ch«îl«ê\", \"trans\": \"process\"},\n    {\"word\": \"ÂàÜÊûê\", \"pinyin\": \"fƒìnxƒ´\", \"trans\": \"analyze\"},\n    {\"word\": \"ÂÖàËøõ\", \"pinyin\": \"xiƒÅnj√¨n\", \"trans\": \"advanced\"},\n    {\"word\": \"‰∫§‰∫í\", \"pinyin\": \"jiƒÅoh√π\", \"trans\": \"interaction\"},\n    {\"word\": \"‰ΩìÈ™å\", \"pinyin\": \"t«êy√†n\", \"trans\": \"experience\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ngn√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"ÊúâÊïà\", \"pinyin\": \"y«íuxi√†o\", \"trans\": \"effective\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πnli√†n\", \"trans\": \"training\"},\n    {\"word\": \"ÊñπÊ°à\", \"pinyin\": \"fƒÅng'√†n\", \"trans\": \"scheme\"},\n    {\"word\": \"ÂØπÈΩê\", \"pinyin\": \"du√¨q√≠\", \"trans\": \"alignment\"},\n    {\"word\": \"ÂæÆË∞É\", \"pinyin\": \"wƒìiti√°o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"Èò∂ÊÆµ\", \"pinyin\": \"jiƒìdu√†n\", \"trans\": \"stage\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"Èü≥È¢ë\", \"pinyin\": \"yƒ´np√≠n\", \"trans\": \"audio\"},\n    {\"word\": \"Êï∞ÊçÆ\", \"pinyin\": \"sh√πj√π\", \"trans\": \"data\"},"
    }
}