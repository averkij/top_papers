{
    "date": {
        "ru": "6 декабря",
        "en": "December 6",
        "zh": "12月6日"
    },
    "time_utc": "2024-12-06 04:13",
    "weekday": 4,
    "issue_id": 981,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.04454",
            "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
            "url": "https://huggingface.co/papers/2412.04454",
            "abstract": "Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.",
            "score": 13,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a088657cce2c618c",
            "authors": [
                "Yiheng Xu",
                "Zekun Wang",
                "Junli Wang",
                "Dunjie Lu",
                "Tianbao Xie",
                "Amrita Saha",
                "Doyen Sahoo",
                "Tao Yu",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce Research",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04454.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#games",
                    "#multimodal",
                    "#agents",
                    "#training",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Aguvis: Автономный ГИП-агент на чистом компьютерном зрении",
                    "desc": "Авторы представляют Aguvis - унифицированную систему для автономных агентов графического интерфейса пользователя (ГИП), работающую на основе компьютерного зрения. Система использует наблюдения на основе изображений и привязку инструкций на естественном языке к визуальным элементам, что обеспечивает кросс-платформенную генерализацию. Aguvis включает явное планирование и рассуждение для улучшения навигации в сложных цифровых средах. Эксперименты показывают, что Aguvis превосходит существующие методы в офлайн и онлайн сценариях, достигая полной автономности без использования внешних моделей."
                },
                "en": {
                    "title": "Aguvis: Revolutionizing Autonomous GUI Interaction with Pure Vision",
                    "desc": "This paper presents Aguvis, a novel framework designed for autonomous GUI agents that operates purely on visual inputs. Unlike previous methods that depend on textual representations, Aguvis utilizes image-based observations and natural language instructions to interact with GUI elements. The framework incorporates explicit planning and reasoning, allowing it to effectively navigate and perform tasks in complex digital environments. Through extensive testing, Aguvis demonstrates superior performance compared to existing methods, marking a significant advancement in the development of fully autonomous GUI agents."
                },
                "zh": {
                    "title": "Aguvis：完全自主的视觉GUI代理",
                    "desc": "本论文介绍了一种名为Aguvis的框架，旨在自动化图形用户界面（GUI）任务。该框架基于纯视觉的方法，能够在不同平台上操作，克服了传统方法的局限性。Aguvis通过图像观察和自然语言指令的结合，增强了模型的规划和推理能力，使其能够独立导航和与复杂数字环境互动。实验结果表明，Aguvis在离线和在线场景中均超越了现有的最先进方法，成为首个完全自主的纯视觉GUI代理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04455",
            "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
            "url": "https://huggingface.co/papers/2412.04455",
            "abstract": "Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments.",
            "score": 8,
            "issue_id": 981,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a00b427eb116e4bf",
            "authors": [
                "Enshen Zhou",
                "Qi Su",
                "Cheng Chi",
                "Zhizheng Zhang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Lu Sheng",
                "He Wang"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "School of Computer Science, Peking University",
                "School of Software, Beihang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04455.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#optimization",
                    "#cv",
                    "#security"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Код как монитор: умное обнаружение ошибок для роботов",
                    "desc": "В статье представлен метод Code-as-Monitor (CaM), использующий визуально-языковую модель для обнаружения и предотвращения ошибок в робототехнических системах. CaM формулирует задачи как набор пространственно-временных ограничений и использует сгенерированный код для их оценки в режиме реального времени. Метод вводит элементы ограничений для абстрагирования связанных сущностей, что упрощает отслеживание и облегчает визуальное программирование. Эксперименты показывают, что CaM превосходит базовые методы по успешности и времени выполнения задач в различных условиях."
                },
                "en": {
                    "title": "Revolutionizing Robotic Failure Detection with Code-as-Monitor",
                    "desc": "This paper presents Code-as-Monitor (CaM), a new method for detecting and preventing failures in robotic systems that operate in unpredictable environments. CaM uses a vision-language model (VLM) to handle both reactive and proactive failure detection by treating these tasks as spatio-temporal constraint satisfaction problems. The method improves monitoring accuracy and efficiency by introducing compact geometric elements that represent constraints, making it easier to track and manage them visually. Experimental results demonstrate that CaM significantly outperforms existing methods, achieving higher success rates and faster execution times in both simulated and real-world scenarios."
                },
                "zh": {
                    "title": "智能监控：提升机器人系统的故障检测与预防",
                    "desc": "本文提出了一种名为Code-as-Monitor（CaM）的新方法，用于自动检测和预防闭环机器人系统中的开放集故障。该方法利用视觉-语言模型（VLM）将反应性和主动性故障检测任务统一为时空约束满足问题。通过生成的代码进行实时监控，CaM在准确性和效率上都有显著提升。实验结果表明，CaM在严重干扰下的成功率提高了28.7%，执行时间减少了31.8%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04467",
            "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
            "url": "https://huggingface.co/papers/2412.04467",
            "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .",
            "score": 8,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "5539efbb0d3e8e80",
            "authors": [
                "Senqiao Yang",
                "Yukang Chen",
                "Zhuotao Tian",
                "Chengyao Wang",
                "Jingyao Li",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HITSZ",
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04467.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#interpretability",
                    "#multimodal",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "VisionZip: Сжимаем визуальные токены, ускоряем ИИ",
                    "desc": "VisionZip - это новый метод, который улучшает эффективность визуально-языковых моделей путем выбора наиболее информативных визуальных токенов. Он решает проблему избыточности токенов, генерируемых популярными кодировщиками изображений, такими как CLIP и SigLIP. VisionZip значительно повышает производительность и скорость вывода моделей, особенно в задачах понимания изображений и видео, а также в многоходовых диалогах. Авторы призывают сообщество сосредоточиться на извлечении лучших визуальных признаков, а не просто на увеличении длины токенов."
                },
                "en": {
                    "title": "Streamlining Visual Tokens for Enhanced Efficiency in Vision-Language Models",
                    "desc": "This paper presents VisionZip, a method designed to reduce redundancy in visual tokens used in vision-language models. By selecting only the most informative tokens, VisionZip improves computational efficiency without sacrificing performance. The method shows significant improvements in both model inference speed and overall accuracy, outperforming previous state-of-the-art techniques by at least 5%. The authors emphasize the importance of optimizing visual feature extraction rather than simply increasing token length."
                },
                "zh": {
                    "title": "VisionZip：高效减少视觉标记冗余的创新方法",
                    "desc": "最近，视觉语言模型的进展通过增加视觉标记的长度来提高性能，但这也显著增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在显著的冗余。为了解决这个问题，我们提出了VisionZip，这是一种简单而有效的方法，可以选择一组信息丰富的标记输入到语言模型中，从而减少视觉标记的冗余，提高效率，同时保持模型性能。实验结果表明，VisionZip在几乎所有设置中比之前的最先进方法提高了至少5%的性能，并显著提升了模型推理速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01506",
            "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
            "url": "https://huggingface.co/papers/2412.01506",
            "abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.",
            "score": 7,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "c35e5586d464b27c",
            "authors": [
                "Jianfeng Xiang",
                "Zelong Lv",
                "Sicheng Xu",
                "Yu Deng",
                "Ruicheng Wang",
                "Bowen Zhang",
                "Dong Chen",
                "Xin Tong",
                "Jiaolong Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01506.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная генерация 3D-объектов с помощью структурированного латентного представления",
                    "desc": "Статья представляет новый метод генерации 3D-объектов высокого качества. Основой метода является унифицированное структурированное латентное представление (SLAT), позволяющее декодировать различные выходные форматы. SLAT интегрирует разреженную 3D-сетку с плотными мультиракурсными визуальными признаками, извлеченными из мощной фундаментальной модели компьютерного зрения. Для генерации 3D-объектов используются трансформеры с выпрямленным потоком, адаптированные для SLAT, обученные на большом наборе данных из 500 тысяч разнообразных объектов."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation with SLAT!",
                    "desc": "This paper presents a new method for creating high-quality 3D assets using a unified Structured LATent (SLAT) representation. The SLAT allows for flexible decoding into various formats like Radiance Fields, 3D Gaussians, and meshes by combining a sparse 3D grid with detailed visual features from a vision model. The authors utilize rectified flow transformers designed for SLAT, training models with up to 2 billion parameters on a large dataset of 500,000 diverse 3D objects. The results show significant improvements in quality and versatility, enabling local 3D editing and output format selection that previous models could not achieve."
                },
                "zh": {
                    "title": "灵活高效的3D资产生成新方法",
                    "desc": "我们提出了一种新颖的3D生成方法，用于多功能和高质量的3D资产创建。该方法的核心是统一的结构化潜在(SLAT)表示，能够解码为不同的输出格式，如辐射场、3D高斯和网格。通过将稀疏的3D网格与从强大的视觉基础模型中提取的密集多视角视觉特征相结合，我们全面捕捉了结构（几何）和纹理（外观）信息，同时在解码过程中保持灵活性。我们的模型使用针对SLAT的整流流变换器进行3D生成，训练了高达20亿参数的模型，生成的结果在文本或图像条件下的质量显著超过现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03632",
            "title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy",
            "url": "https://huggingface.co/papers/2412.03632",
            "abstract": "Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to optimization difficulties and scarce high-quality 3D data. In this paper, we propose the first adapter-based solution for multi-view image generation, and introduce MV-Adapter, a versatile plug-and-play adapter that enhances T2I models and their derivatives without altering the original network structure or feature space. By updating fewer parameters, MV-Adapter enables efficient training and preserves the prior knowledge embedded in pre-trained models, mitigating overfitting risks. To efficiently model the 3D geometric knowledge within the adapter, we introduce innovative designs that include duplicated self-attention layers and parallel attention architecture, enabling the adapter to inherit the powerful priors of the pre-trained models to model the novel 3D knowledge. Moreover, we present a unified condition encoder that seamlessly integrates camera parameters and geometric information, facilitating applications such as text- and image-based 3D generation and texturing. MV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion XL (SDXL), and demonstrates adaptability and versatility. It can also be extended to arbitrary view generation, enabling broader applications. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its efficiency, adaptability and versatility.",
            "score": 6,
            "issue_id": 981,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "fe0891c026484abc",
            "authors": [
                "Zehuan Huang",
                "Yuan-Chen Guo",
                "Haoran Wang",
                "Ran Yi",
                "Lizhuang Ma",
                "Yan-Pei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "School of Software, Beihang University",
                "Shanghai Jiao Tong University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03632.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#3d"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "MV-Adapter: Эффективная генерация многоракурсных изображений без переобучения",
                    "desc": "Статья представляет MV-Adapter - первое адаптерное решение для генерации многоракурсных изображений. Этот плагин улучшает модели текст-в-изображение без изменения их структуры, сохраняя предобученные знания и снижая риск переобучения. MV-Adapter использует инновационные подходы, включая дублированные слои самовнимания и параллельную архитектуру внимания, для эффективного моделирования 3D-геометрии. Решение достигает высокого качества генерации многоракурсных изображений с разрешением 768 пикселей на основе Stable Diffusion XL."
                },
                "en": {
                    "title": "Efficient Multi-View Image Generation with MV-Adapter",
                    "desc": "This paper introduces MV-Adapter, a novel adapter-based approach for multi-view image generation that avoids invasive changes to pre-trained text-to-image (T2I) models. By updating fewer parameters, MV-Adapter enhances the efficiency of training while preserving the knowledge from pre-trained models, thus reducing the risk of overfitting. The design incorporates advanced features like duplicated self-attention layers and a unified condition encoder to effectively integrate 3D geometric information. MV-Adapter not only achieves high-quality multi-view generation but also allows for broader applications in arbitrary view generation."
                },
                "zh": {
                    "title": "高效多视角图像生成的新标准",
                    "desc": "现有的多视角图像生成方法通常需要对预训练的文本到图像模型进行大幅修改，并且需要完全微调，这导致了高计算成本和图像质量下降。本文提出了一种基于适配器的多视角图像生成解决方案，MV-Adapter是一种可插拔的适配器，可以增强文本到图像模型，而无需改变原始网络结构。通过更新更少的参数，MV-Adapter实现了高效训练，并保留了预训练模型中的先验知识，降低了过拟合风险。我们还引入了统一的条件编码器，能够无缝整合相机参数和几何信息，促进文本和图像基础的3D生成和纹理处理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01820",
            "title": "Towards Universal Soccer Video Understanding",
            "url": "https://huggingface.co/papers/2412.01820",
            "abstract": "As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "fa9f7e4132ee5026",
            "authors": [
                "Jiayuan Rao",
                "Haoning Wu",
                "Hao Jiang",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Alibaba Group, China",
                "CMIC, Shanghai Jiao Tong University, China",
                "School of Artificial Intelligence, Shanghai Jiao Tong University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01820.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "⚽",
                "ru": {
                    "title": "MatchVision: революция в компьютерном зрении для футбола",
                    "desc": "В этой статье представлена многомодальная модель MatchVision для анализа футбольных видео. Авторы создали крупнейший датасет SoccerReplay-1988, содержащий видео и аннотации 1988 полных матчей. MatchVision использует пространственно-временную информацию из видео и превосходит существующие модели в задачах классификации событий, генерации комментариев и распознавания нарушений. Эксперименты показали значительное улучшение производительности по сравнению с предыдущими подходами."
                },
                "en": {
                    "title": "Revolutionizing Soccer Video Analysis with MatchVision",
                    "desc": "This paper presents a new framework for understanding soccer videos using machine learning. It introduces SoccerReplay-1988, a large dataset containing videos and annotations from nearly 2,000 soccer matches, which helps in training models. The authors also develop MatchVision, a visual-language model that processes spatiotemporal data from soccer videos and performs well in tasks like event classification and commentary generation. The results show that MatchVision outperforms existing models, setting a new standard for research in sports video analysis."
                },
                "zh": {
                    "title": "足球视频理解的新标准",
                    "desc": "这篇论文旨在开发一个全面的多模态框架，用于理解足球视频。我们引入了SoccerReplay-1988，这是迄今为止最大的多模态足球数据集，包含1988场完整比赛的视频和详细注释。我们还提出了足球领域的首个视觉-语言基础模型MatchVision，能够利用足球视频中的时空信息，并在多个下游任务中表现出色。通过广泛的实验和消融研究，MatchVision在事件分类、评论生成和多视角犯规识别等任务上均表现出最先进的性能，显示了我们提出的数据和模型的优越性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03679",
            "title": "Evaluating Language Models as Synthetic Data Generators",
            "url": "https://huggingface.co/papers/2412.03679",
            "abstract": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "2b80634d3f712590",
            "authors": [
                "Seungone Kim",
                "Juyoung Suk",
                "Xiang Yue",
                "Vijay Viswanathan",
                "Seongyun Lee",
                "Yizhong Wang",
                "Kiril Gashteovski",
                "Carolin Lawrence",
                "Sean Welleck",
                "Graham Neubig"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "KAIST AI",
                "NEC Laboratories Europe",
                "Ss. Cyril and Methodius University of Skopje",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03679.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AgoraBench: Новый взгляд на генерацию данных языковыми моделями",
                    "desc": "В статье представлен AgoraBench - новый бенчмарк для оценки способности языковых моделей генерировать синтетические данные. Исследователи провели масштабный эксперимент, сгенерировав 1,26 миллиона обучающих примеров с помощью 6 различных ЯМ и обучив 99 студенческих моделей. Результаты показали, что разные ЯМ имеют свои сильные стороны в генерации данных, и эта способность не всегда коррелирует с навыками решения задач. Авторы также выявили ключевые факторы, влияющие на качество генерируемых данных, включая формат вывода и выбор модели."
                },
                "en": {
                    "title": "Unlocking the Power of Language Models in Data Generation",
                    "desc": "This paper introduces AgoraBench, a benchmark designed to evaluate the data generation capabilities of various language models (LMs). It highlights the importance of synthetic data in enhancing the performance of LMs, especially in post-training scenarios. The study synthesizes a large dataset using six different LMs and trains 99 student models to analyze their data generation strengths and weaknesses. Key findings indicate that while some LMs excel in creating new problems, others are better at refining existing ones, and that data generation quality is influenced by several intrinsic features rather than just the LM's problem-solving skills."
                },
                "zh": {
                    "title": "评估语言模型数据生成能力的新基准",
                    "desc": "随着合成数据在语言模型后期训练中的使用增加，语言模型生成高质量数据的能力变得与直接解决问题的能力同样重要。以往的研究主要集中在开发有效的数据生成方法，但缺乏对不同语言模型作为数据生成器的系统比较。为了解决这个问题，我们提出了AgoraBench，一个提供标准化设置和指标的基准，用于评估语言模型的数据生成能力。我们的研究发现，不同语言模型在数据生成方面具有独特的优势，并且数据生成能力与解决问题的能力并不总是相关，而是与数据质量的多个内在特征有关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04106",
            "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
            "url": "https://huggingface.co/papers/2412.04106",
            "abstract": "Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "7b2720f9a6c27027",
            "authors": [
                "Haoning Wu",
                "Ziheng Zhao",
                "Ya Zhang",
                "Weidi Xie",
                "Yanfeng Wang"
            ],
            "affiliations": [
                "CMIC, Shanghai Jiao Tong University, China",
                "School of Artificial Intelligence, Shanghai Jiao Tong University, China",
                "Shanghai AI Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04106.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#healthcare",
                    "#synthetic",
                    "#cv",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Генерация синтетических МРТ для обучения сегментации без разметки",
                    "desc": "В этой статье представлен новый подход к синтезу медицинских изображений для неаннотированных модальностей с использованием генеративных моделей. Авторы создали большой датасет MedGen-1M, содержащий радиологические изображения с текстовыми описаниями и частичной разметкой органов. Они разработали диффузионную модель MRGen для генерации МРТ-изображений на основе текстовых подсказок и масок. Эксперименты показали эффективность этого подхода для обучения моделей сегментации на неаннотированных модальностях МРТ."
                },
                "en": {
                    "title": "Generating Synthetic Data for Medical Image Segmentation",
                    "desc": "This paper presents a novel approach to medical image segmentation by using generative models to create synthetic data for modalities that lack mask annotations. The authors introduce a large dataset called MedGen-1M, which includes radiology images with modality labels and some organ mask annotations, facilitating research in medical image generation. They propose a diffusion-based data engine named MRGen that generates MR images based on text prompts and available masks, allowing for the training of segmentation models on unannotated data. Extensive experiments demonstrate the effectiveness of MRGen in synthesizing training samples and improving segmentation performance across various MRI modalities."
                },
                "zh": {
                    "title": "利用生成模型推动医学图像分割的创新",
                    "desc": "这篇论文探讨了在医学图像分割中使用生成模型的新方法，特别是针对未标注模态的数据合成。研究者们收集并整理了一个大型的放射学图像-文本数据集MedGen-1M，包含模态标签、属性、区域和器官信息，以及部分器官的掩膜注释。论文中提出了一种基于扩散的生成数据引擎MRGen，可以根据文本提示和掩膜生成缺乏掩膜注释的MR图像，从而训练未标注模态的分割模型。通过广泛的实验，结果表明该数据引擎能够有效合成训练样本，推动MRI分割向未标注模态的扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04280",
            "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing",
            "url": "https://huggingface.co/papers/2412.04280",
            "abstract": "We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution 1024 times 1024 content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "4467ff5ceea9cb1d",
            "authors": [
                "Jinbin Bai",
                "Wei Chow",
                "Ling Yang",
                "Xiangtai Li",
                "Juncheng Li",
                "Hanwang Zhang",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "Peking University",
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04280.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "HumanEdit: Редактирование изображений с человеческим подходом",
                    "desc": "HumanEdit - это набор данных для редактирования изображений на основе текстовых инструкций, созданный с участием людей. Он включает 5751 изображение с масками и инструкциями шести типов: действие, добавление, подсчет, отношение, удаление и замена. Датасет отличается высоким качеством и разнообразием, так как создавался с привлечением аннотаторов и администраторов. HumanEdit предназначен для обучения и оценки моделей машинного обучения в задачах редактирования изображений."
                },
                "en": {
                    "title": "HumanEdit: Elevating Image Editing with Human-Centric Instructions",
                    "desc": "HumanEdit is a newly created dataset aimed at improving instruction-guided image editing by incorporating significant human feedback. Unlike previous datasets that lacked sufficient human input, HumanEdit utilizes human annotators to create data pairs and provide valuable feedback, ensuring alignment with human preferences. The dataset consists of 5,751 high-resolution images and includes six types of editing instructions, allowing for a wide range of image manipulation tasks. By offering detailed instructions and masks, HumanEdit sets a new standard for versatility and accuracy in image editing research."
                },
                "zh": {
                    "title": "HumanEdit：精准多样的图像编辑数据集",
                    "desc": "HumanEdit是一个高质量的人类奖励数据集，专门用于指导图像编辑。与以往的大规模编辑数据集不同，HumanEdit通过人类注释者构建数据对，并由管理员提供反馈，确保数据与人类偏好的对齐。该数据集包含5751张图像，经过2500多个小时的人工努力，涵盖六种不同类型的编辑指令，支持多样化的图像编辑任务。HumanEdit为未来的研究提供了一个新的基准，推动图像编辑领域的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04062",
            "title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality",
            "url": "https://huggingface.co/papers/2412.04062",
            "abstract": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "44d216e3fa2fb345",
            "authors": [
                "Yefei He",
                "Feng Chen",
                "Yuanyu He",
                "Shaoxuan He",
                "Hong Zhou",
                "Kaipeng Zhang",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory, China",
                "The University of Adelaide, Australia",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04062.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "ZipAR: Ускоряем генерацию изображений параллельным декодированием",
                    "desc": "ZipAR - это новый подход к ускорению авторегрессионной генерации изображений без дополнительного обучения модели. Метод основан на наблюдении, что пространственно удаленные области изображения имеют минимальную взаимозависимость. ZipAR позволяет параллельно декодировать токены, соответствующие смежным областям, что значительно сокращает количество прямых проходов модели. Эксперименты показали, что ZipAR может уменьшить число прямых проходов на 91% для модели Emu3-Gen без переобучения."
                },
                "en": {
                    "title": "Accelerate Image Generation with Parallel Decoding!",
                    "desc": "This paper introduces ZipAR, a new framework designed to speed up the process of generating images using auto-regressive models without the need for retraining. It leverages the observation that images have local structures, allowing for the parallel decoding of visual tokens in adjacent regions. By implementing a 'next-set prediction' approach, ZipAR can decode multiple tokens at once, significantly cutting down the number of forward passes needed to create an image. Experiments show that this method can reduce forward passes by up to 91%, greatly enhancing generation efficiency."
                },
                "zh": {
                    "title": "ZipAR：加速自回归视觉生成的高效解码框架",
                    "desc": "本文提出了一种名为ZipAR的框架，旨在加速自回归视觉生成。该框架不需要重新训练，能够实现并行解码，利用图像的局部结构特性。通过在列维度上并行解码空间相邻区域的视觉标记，ZipAR显著减少了生成图像所需的前向传播次数。实验结果表明，ZipAR在Emu3-Gen模型上可以将前向传播次数减少多达91%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04431",
            "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
            "url": "https://huggingface.co/papers/2412.04431",
            "abstract": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.",
            "score": 1,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "f297f288187d4cc4",
            "authors": [
                "Jian Han",
                "Jinlai Liu",
                "Yi Jiang",
                "Bin Yan",
                "Yuqi Zhang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04431.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Infinity: новый уровень генерации изображений с бесконечным словарем",
                    "desc": "Представлена модель Infinity - битовая визуальная авторегрессионная модель для генерации фотореалистичных изображений по текстовому описанию. Infinity использует бесконечный словарь токенов и механизм битовой самокоррекции, что значительно улучшает качество генерации. Модель превосходит ведущие диффузионные модели по ряду метрик, включая GenEval и ImageReward. Infinity генерирует качественное изображение 1024x1024 за 0.8 секунды, что делает ее самой быстрой моделью text-to-image на данный момент."
                },
                "en": {
                    "title": "Infinity: Redefining Text-to-Image Generation with Infinite Vocabulary",
                    "desc": "Infinity is a novel Bitwise Visual AutoRegressive Model designed to create high-resolution, photorealistic images based on language instructions. It introduces an infinite-vocabulary tokenizer and classifier, along with a bitwise self-correction mechanism, which enhances the detail and quality of generated images. By scaling both the tokenizer and transformer sizes, Infinity significantly improves upon traditional visual autoregressive models. It sets new performance records in text-to-image generation, outperforming leading diffusion models and achieving faster generation times without additional optimization."
                },
                "zh": {
                    "title": "Infinity：无限可能的视觉生成模型",
                    "desc": "本文介绍了Infinity，一种基于位的视觉自回归模型，能够根据语言指令生成高分辨率、逼真的图像。Infinity在位元令牌预测框架下重新定义了视觉自回归模型，采用无限词汇量的令牌器和分类器，以及位元自我校正机制，显著提升了生成能力和细节表现。通过理论上将令牌器的词汇量扩展到无限，并同时扩展变换器的规模，我们的方法相比传统的VAR模型释放了强大的扩展能力。Infinity在自回归文本到图像模型中创下新纪录，超越了顶级扩散模型，如SD3-Medium和SDXL。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01169",
            "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
            "url": "https://huggingface.co/papers/2412.01169",
            "abstract": "We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.",
            "score": 1,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "7f3df6f7d4733664",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Zichun Liao",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "Salesforce AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01169.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "OmniFlow: универсальная модель для мультимодальной генерации",
                    "desc": "OmniFlow - это новая генеративная модель, разработанная для задач генерации любого типа данных в любой другой тип, например текст-в-изображение или аудио-в-изображение. Модель развивает фреймворк выпрямленного потока (rectified flow) для работы с совместным распределением нескольких модальностей. OmniFlow превосходит предыдущие модели в широком спектре задач генерации. Авторы предлагают новую архитектуру на основе MMDiT из Stable Diffusion 3, а также изучают оптимальные параметры трансформеров с выпрямленным потоком для генерации аудио и текста."
                },
                "en": {
                    "title": "OmniFlow: Bridging Modalities for Any-to-Any Generation",
                    "desc": "OmniFlow is a new generative model that can create outputs across different types of data, like turning text into images or audio. It builds on the rectified flow (RF) framework, allowing it to understand and generate multiple types of data together. This model not only improves performance on tasks like text-to-image and text-to-audio synthesis but also introduces a way for users to control how different data types relate to each other in the generated results. Additionally, it features a unique architecture that enhances existing models and provides insights into optimizing generative tasks across various modalities."
                },
                "zh": {
                    "title": "OmniFlow：多模态生成的灵活解决方案",
                    "desc": "OmniFlow是一种新型生成模型，旨在处理任意到任意的生成任务，如文本到图像、文本到音频和音频到图像的合成。它在文本到图像模型中改进了修正流（RF）框架，以处理多种模态的联合分布。OmniFlow在多种任务上超越了之前的任意到任意模型，提供了灵活的模态对齐控制机制。我们的研究还探讨了修正流变换器在大规模音频和文本生成中的设计选择，为优化不同模态的性能提供了宝贵的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04315",
            "title": "Densing Law of LLMs",
            "url": "https://huggingface.co/papers/2412.04315",
            "abstract": "Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments, and the scaling trend is becoming increasingly unsustainable. This paper introduces the concept of ``capacity density'' as a new metric to evaluate the quality of the LLMs across different scales and describes the trend of LLMs in terms of both effectiveness and efficiency. To calculate the capacity density of a given target LLM, we first introduce a set of reference models and develop a scaling law to predict the downstream performance of these reference models based on their parameter sizes. We then define the effective parameter size of the target LLM as the parameter size required by a reference model to achieve equivalent performance, and formalize the capacity density as the ratio of the effective parameter size to the actual parameter size of the target LLM. Capacity density provides a unified framework for assessing both model effectiveness and efficiency. Our further analysis of recent open-source base LLMs reveals an empirical law (the densing law)that the capacity density of LLMs grows exponentially over time. More specifically, using some widely used benchmarks for evaluation, the capacity density of LLMs doubles approximately every three months. The law provides new perspectives to guide future LLM development, emphasizing the importance of improving capacity density to achieve optimal results with minimal computational overhead.",
            "score": 0,
            "issue_id": 981,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "4a5189762d711a19",
            "authors": [
                "Chaojun Xiao",
                "Jie Cai",
                "Weilin Zhao",
                "Guoyang Zeng",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "ModelBest Inc.",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04315.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Плотность мощности: новый путь к эффективным языковым моделям",
                    "desc": "Статья представляет новую метрику 'плотность мощности' для оценки качества больших языковых моделей (LLM) разного масштаба. Авторы вводят понятие эффективного размера параметров модели и формулируют плотность мощности как отношение эффективного размера к фактическому. Анализ современных LLM выявил эмпирический закон экспоненциального роста плотности мощности со временем. Предложенный подход предоставляет новый взгляд на развитие LLM, подчеркивая важность повышения плотности мощности для достижения оптимальных результатов при минимальных вычислительных затратах."
                },
                "en": {
                    "title": "Maximizing Performance with Minimal Resources: The Capacity Density Revolution",
                    "desc": "This paper discusses the challenges of training and using large language models (LLMs) as they grow in size. It introduces a new metric called 'capacity density' to evaluate LLMs based on their effectiveness and efficiency. The authors develop a scaling law to predict how well different models perform relative to their size and define capacity density as the ratio of effective parameter size to actual parameter size. Their findings suggest that the capacity density of LLMs is increasing rapidly, which could guide future improvements in model design to optimize performance while reducing resource use."
                },
                "zh": {
                    "title": "提升容量密度，优化大型语言模型的效率与效果",
                    "desc": "大型语言模型（LLMs）在人工智能领域取得了重要进展，但随着模型规模的增加，训练和推理的效率面临巨大挑战。本文提出了“容量密度”这一新指标，用于评估不同规模LLMs的质量，并描述了LLMs在有效性和效率方面的趋势。通过引入一组参考模型并开发缩放法则，本文计算了目标LLM的有效参数大小，并将容量密度定义为有效参数大小与实际参数大小的比率。我们的分析表明，LLMs的容量密度每三个月大约翻倍，为未来的LLM开发提供了新的视角，强调了提高容量密度的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04424",
            "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
            "url": "https://huggingface.co/papers/2412.04424",
            "abstract": "We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2's visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL's visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL",
            "score": 0,
            "issue_id": 981,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "18a7fac6be50215d",
            "authors": [
                "Jiuhai Chen",
                "Jianwei Yang",
                "Haiping Wu",
                "Dianqi Li",
                "Jianfeng Gao",
                "Tianyi Zhou",
                "Bin Xiao"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04424.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#hallucinations",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Florence-VL: Новый уровень понимания изображений для языковых моделей",
                    "desc": "Florence-VL представляет собой новое семейство мультимодальных больших языковых моделей (MLLM) с улучшенными визуальными представлениями, созданными с помощью Florence-2 - генеративной базовой модели компьютерного зрения. В отличие от широко используемых трансформеров CLIP, Florence-2 способна захватывать различные уровни и аспекты визуальных характеристик. Авторы предлагают новую архитектуру слияния признаков и инновационный рецепт обучения, эффективно интегрирующий визуальные характеристики Florence-2 в предобученные языковые модели. Florence-VL достигает значительных улучшений по сравнению с существующими передовыми MLLM в различных мультимодальных и ориентированных на зрение тестах."
                },
                "en": {
                    "title": "Florence-VL: Bridging Vision and Language with Depth-Breath Fusion",
                    "desc": "Florence-VL is a new type of multimodal large language model that enhances visual understanding using advanced features from the Florence-2 vision model. Unlike traditional models that rely on contrastive learning, Florence-2 captures a wider range of visual details, making it adaptable for various tasks. The model employs a unique feature-fusion method called depth-breath fusion (DBFusion) to combine visual information from different levels and prompts effectively. As a result, Florence-VL outperforms existing models in multiple benchmarks related to vision and language tasks, and its training methods are available for further research."
                },
                "zh": {
                    "title": "Florence-VL：多模态语言模型的新突破",
                    "desc": "Florence-VL是一种新型的多模态大型语言模型，结合了Florence-2生成的丰富视觉表示。与传统的对比学习训练的CLIP风格视觉变换器不同，Florence-2能够捕捉不同层次和方面的视觉特征，更加灵活地适应各种下游任务。我们提出了一种新颖的特征融合架构和创新的训练方案，有效地将Florence-2的视觉特征整合到预训练的语言模型中。Florence-VL在多种多模态和视觉中心基准测试中显著提升了性能，展示了其在视觉-语言对齐方面的优势。"
                }
            }
        }
    ],
    "link_prev": "2024-12-05.html",
    "link_next": "2024-12-09.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12月5日"
    },
    "short_date_next": {
        "ru": "09.12",
        "en": "12/09",
        "zh": "12月9日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。",
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "pinyin": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de kuàngjià SNOOPI, zhǐ zài gǎijìn dān bù kuòsàn móxíng de zhǐdǎo jīzhì. Xiànyǒu fāngfǎ zài chǔlǐ bùtóng kuòsàn móxíng gǔjià shí biǎoxiàn bùículai, qiě bù zhīchí fùmiàn tíshì zhǐdǎo. SNOOPI tōngguò PG-SB hé NASA liǎng zhǒng fāngfǎ jiějué le zhèxiē wèntí. Shíyàn jiéguǒ xiǎnshì, SNOOPI xiǎnzhù tíshēng le jīzhǔn móxíng de xíngnéng, dá dào le xīn de zuìjiā shuǐpíng.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '改进', 'pinyin': 'gǎijìn', 'trans': 'improve'},\n{'word': '单步', 'pinyin': 'dānbù', 'trans': 'single-step'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '指导', 'pinyin': 'zhǐdǎo', 'trans': 'guidance'},\n{'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': '处理', 'pinyin': 'chǔlǐ', 'trans': 'handle'},\n{'word': '不同', 'pinyin': 'bùtóng', 'trans': 'different'},\n{'word': '骨架', 'pinyin': 'gǔjià', 'trans': 'skeleton'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '不稳定', 'pinyin': 'bùwěndìng', 'trans': 'unstable'},\n{'word': '且', 'pinyin': 'qiě', 'trans': 'and'},\n{'word': '不支持', 'pinyin': 'bù zhīchí', 'trans': 'not support'},\n{'word': '负面', 'pinyin': 'fùmiàn', 'trans': 'negative'},\n{'word': '提示', 'pinyin': 'tíshì', 'trans': 'prompt'},\n{'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through'},\n{'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'},\n{'word': 'NASA', 'pinyin': '', 'trans': 'NASA'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'},\n{'word': '这些', 'pinyin': 'zhèxiē', 'trans': 'these'},\n{'word': '问题', 'pinyin': 'wèntí', 'trans': 'problems'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'},\n{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'},\n{'word': '达到', 'pinyin': 'dádào', 'trans': 'reach'},\n{'word': '新的', 'pinyin': 'xīn de', 'trans': 'new'},\n{'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'},\n{'word': '水平', 'pinyin': 'shuǐpíng', 'trans': 'level'}]",
        "trans": "This article introduces a new framework called SNOOPI, aimed at improving the guidance mechanism of single-step diffusion models. Existing methods perform unstably when handling different diffusion model backbones and do not support negative prompt guidance. SNOOPI addresses these issues through the PG-SB and NASA methods. Experimental results demonstrate that SNOOPI significantly enhances the performance of benchmark models, achieving new best-in-class levels.",
        "update_ts": "2024-12-05 09:11"
    }
}