{
    "date": {
        "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 23",
        "zh": "2æœˆ23æ—¥"
    },
    "time_utc": "2026-02-23 11:35",
    "weekday": 0,
    "issue_id": 1179,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.10693",
            "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
            "url": "https://huggingface.co/papers/2602.10693",
            "abstract": "VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO",
            "score": 152,
            "issue_id": 1174,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "62b483e658d37cc6",
            "authors": [
                "Guobin Shen",
                "Chenxiao Zhao",
                "Xiang Cheng",
                "Lei Huang",
                "Xing Yu"
            ],
            "affiliations": [
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10693.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#reasoning",
                    "#math",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸",
                    "desc": "VESPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ±ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ importance sampling Ñ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ¾Ğ¹ ÑĞ´Ñ€Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Stabilizing LLM Training with VESPO",
                    "desc": "VESPO is a method designed to improve the stability of training large language models (LLMs) in reinforcement learning (RL) settings. It addresses issues like policy divergence caused by asynchronous training and mismatches between training and inference. By using a variational approach with variance reduction, VESPO corrects distribution shifts without relying on length normalization. Experiments demonstrate that VESPO can maintain stable training even under extreme conditions and improves performance across various model types."
                },
                "zh": {
                    "title": "VESPOï¼šç¨³å®šå¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
                    "desc": "VESPOæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚å®ƒé€šè¿‡å˜åˆ†å½¢å¼ç»“åˆæ–¹å·®å‡å°‘æŠ€æœ¯ï¼Œæ¥ä¿®æ­£ç­–ç•¥çš„åå·®ï¼Œè€Œä¸éœ€è¦é•¿åº¦å½’ä¸€åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç›´æ¥åœ¨åºåˆ—çº§åˆ«çš„é‡è¦æ€§æƒé‡ä¸Šæ“ä½œï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVESPOåœ¨é«˜è¾¾64å€çš„è¿‡æ—¶æ¯”ç‡å’Œå®Œå…¨å¼‚æ­¥æ‰§è¡Œä¸‹ï¼Œä»èƒ½ä¿æŒç¨³å®šçš„è®­ç»ƒæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08354",
            "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
            "url": "https://huggingface.co/papers/2602.08354",
            "abstract": "Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
            "score": 68,
            "issue_id": 1173,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "3e2572a9c799d2c1",
            "authors": [
                "Zixuan Huang",
                "Xin Xia",
                "Yuxi Ren",
                "Jianbin Zheng",
                "Xuanda Wang",
                "Zhixia Zhang",
                "Hongyan Xie",
                "Songshi Liang",
                "Zehao Chen",
                "Xuefeng Xiao",
                "Fuzhen Zhuang",
                "Jianxin Li",
                "Yikun Ban",
                "Deqing Wang"
            ],
            "affiliations": [
                "Beihang University",
                "Bytedance China",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08354.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°ÑÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ÑÑÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾ Ğ·Ğ½Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ ÑÑ‚Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ SAGE â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ SAGE Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Efficient Reasoning with SAGE-RL",
                    "desc": "This paper discusses the limitations of large reasoning models (LRMs) when using Long Chains of Thought (CoTs), which can lead to inefficiencies and inaccuracies in reasoning tasks. It reveals that LRMs have an inherent ability to determine optimal stopping points during reasoning, a capability that is often masked by current sampling methods. To address this, the authors propose SAGE (Self-Aware Guided Efficient Reasoning), a new sampling paradigm that leverages this stopping ability to improve reasoning efficiency. By integrating SAGE into group-based reinforcement learning as SAGE-RL, the authors demonstrate significant enhancements in both accuracy and efficiency for LRMs on complex mathematical problems."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œæå‡å‡†ç¡®æ€§ä¸æ•ˆç‡",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å¯¼è‡´å†—ä½™ï¼Œå½±å“è®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒé•¿çš„æ¨ç†é“¾ä¸æ­£ç¡®æ€§å¹¶æ— æ˜æ˜¾å…³è”ï¼Œåè€Œå¯èƒ½é™ä½å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†SAGEï¼ˆè‡ªæˆ‘æ„è¯†å¼•å¯¼é«˜æ•ˆæ¨ç†ï¼‰ï¼Œä¸€ç§æ–°é¢–çš„é‡‡æ ·èŒƒå¼ï¼Œèƒ½å¤Ÿé‡Šæ”¾æ¨ç†æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ½œåŠ›ã€‚é€šè¿‡å°†SAGEä¸åŸºäºç»„çš„å¼ºåŒ–å­¦ä¹ ç»“åˆï¼ŒSAGE-RLæ˜¾è‘—æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.18422",
            "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control",
            "url": "https://huggingface.co/papers/2602.18422",
            "abstract": "A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.",
            "score": 6,
            "issue_id": 1173,
            "pub_date": "2026-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "99c0d74c2622ff49",
            "authors": [
                "Linxi Xie",
                "Lisong C. Sun",
                "Ashley Neall",
                "Tong Wu",
                "Shengqu Cai",
                "Gordon Wetzstein"
            ],
            "affiliations": [
                "NYU Shanghai",
                "Stanford University",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.18422.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¥½",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ÑƒĞº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ÑƒĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ñ€ÑƒĞº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ²ĞºĞ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Interaction with Human-Centric Video Models",
                    "desc": "This paper presents a novel human-centric video world model that enhances user interaction in virtual environments by utilizing tracked head and hand poses. Unlike traditional models that rely on basic input methods, this approach allows for more precise control, enabling users to perform complex actions with their hands. The authors develop a bidirectional video diffusion model that generates realistic egocentric environments, which are then distilled into an interactive system. Evaluation results show that users experience better task performance and feel more in control compared to existing models."
                },
                "zh": {
                    "title": "ä»¥äººä¸ºä¸­å¿ƒçš„çµæ´»è§†é¢‘äº¤äº’æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè¿½è¸ªçš„å¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»è€Œå®ç°çµæ´»çš„äº¤äº’ã€‚ç°æœ‰çš„è§†é¢‘ä¸–ç•Œæ¨¡å‹åªèƒ½æ¥å—ç²—ç•¥çš„æ§åˆ¶ä¿¡å·ï¼Œå¦‚æ–‡æœ¬æˆ–é”®ç›˜è¾“å…¥ï¼Œé™åˆ¶äº†å…¶åœ¨å…·èº«äº¤äº’ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„3Då¤´éƒ¨å’Œæ‰‹éƒ¨æ§åˆ¶æœºåˆ¶ï¼Œèƒ½å¤Ÿæ”¯æŒçµæ´»çš„æ‰‹-ç‰©ä½“äº¤äº’ã€‚é€šè¿‡è®­ç»ƒåŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å°†å…¶æç‚¼ä¸ºå› æœäº¤äº’ç³»ç»Ÿï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä»»åŠ¡è¡¨ç°æå‡å’Œæ›´é«˜çš„æ§åˆ¶æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.18071",
            "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots",
            "url": "https://huggingface.co/papers/2602.18071",
            "abstract": "EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.",
            "score": 2,
            "issue_id": 1173,
            "pub_date": "2026-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "04911bee98ce08d2",
            "authors": [
                "Boyuan An",
                "Zhexiong Wang",
                "Yipeng Wang",
                "Jiaqi Li",
                "Sihang Li",
                "Jing Zhang",
                "Chen Feng"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.18071.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°",
                    "desc": "EgoPush â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ, ÑÑƒĞ¶Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "EgoPush: Smart Robot Manipulation in Cluttered Spaces",
                    "desc": "EgoPush is a framework designed for robots to manipulate objects in cluttered spaces using only an egocentric camera. It focuses on learning policies that understand the relationships between objects rather than their exact positions, which helps in navigating complex environments. The framework employs a reinforcement learning approach where a knowledgeable teacher guides a simpler student model, allowing the robot to learn effective actions based on visual cues. By breaking down tasks into smaller stages and using specific rewards, EgoPush improves the robot's ability to complete long-term rearrangement tasks successfully."
                },
                "zh": {
                    "title": "EgoPushï¼šåœ¨æ‚ä¹±ç¯å¢ƒä¸­å®ç°æ™ºèƒ½é‡æ’çš„æœºå™¨äººæ¡†æ¶",
                    "desc": "EgoPush æ˜¯ä¸€ä¸ªæœºå™¨äººæ“ä½œæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ„ŸçŸ¥é©±åŠ¨çš„ç­–ç•¥å­¦ä¹ ï¼Œåœ¨æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç‰©ä½“çš„éæŠ“å–é‡æ’ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å•ä¸ªè‡ªæˆ‘ä¸­å¿ƒæ‘„åƒå¤´ï¼Œè®¾è®¡äº†ä¸€ä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ½œåœ¨ç©ºé—´ï¼Œç¼–ç ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ç©ºé—´å…³ç³»ï¼Œè€Œä¸æ˜¯ç»å¯¹ä½ç½®ã€‚EgoPush é€šè¿‡å°†é‡æ’ä»»åŠ¡åˆ†è§£ä¸ºé˜¶æ®µæ€§å­é—®é¢˜ï¼Œå¹¶ä½¿ç”¨é˜¶æ®µå±€éƒ¨çš„å¥–åŠ±æ¥è§£å†³é•¿æœŸä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ“ä½œçš„æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgoPush åœ¨æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­å®ç°äº†é›¶æ ·æœ¬çš„ä»¿çœŸåˆ°ç°å®è½¬ç§»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.18432",
            "title": "SARAH: Spatially Aware Real-time Agentic Humans",
            "url": "https://huggingface.co/papers/2602.18432",
            "abstract": "A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  \t\t\t\t\tAI-generated summary \t\t\t\t As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.",
            "score": 1,
            "issue_id": 1173,
            "pub_date": "2026-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "fea213b1786adad9",
            "authors": [
                "Evonne Ng",
                "Siwei Zhang",
                "Zhang Chen",
                "Michael Zollhoefer",
                "Alexander Richard"
            ],
            "affiliations": [
                "Meta Reality Labs Redmond, WA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.18432.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚ĞµĞ»Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¶ĞµÑÑ‚Ñ‹ Ñ Ñ€ĞµÑ‡ÑŒÑ, Ğ½Ğ¾ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚ Ğ¸ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ñ Ğ½Ğ° ĞµĞ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 300 FPS Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Embody 3D Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚ Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ VR-ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Real-Time Spatial Awareness for Conversational Agents in VR",
                    "desc": "This paper presents a novel approach for creating real-time, spatially-aware motion in embodied agents for virtual reality applications. By combining a causal transformer-based variational autoencoder with flow matching, the method allows agents to respond dynamically to user movements and maintain natural gaze. The architecture enables full-body motion generation that aligns gestures with speech while orienting the agent based on the user's position. The proposed system achieves high performance, processing over 300 frames per second, and demonstrates state-of-the-art motion quality in live VR environments."
                },
                "zh": {
                    "title": "å®æ—¶ç©ºé—´æ„ŸçŸ¥å¯¹è¯åŠ¨ä½œçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå› æœå˜æ¢å™¨çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç»“åˆæµåŒ¹é…æŠ€æœ¯ï¼Œå®ç°äº†å®æ—¶çš„ç©ºé—´æ„ŸçŸ¥å¯¹è¯åŠ¨ä½œï¼Œé€‚ç”¨äºè™šæ‹Ÿç°å®åº”ç”¨ä¸­çš„å…·èº«ä»£ç†ã€‚å½“å‰çš„æ–¹æ³•ç¼ºä¹ç©ºé—´æ„è¯†ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„ä½ç½®å’ŒéŸ³é¢‘ç”Ÿæˆå…¨èº«åŠ¨ä½œï¼Œä½¿ä»£ç†ä¸ä»…èƒ½ä¸ç”¨æˆ·å¯¹è¯ï¼Œè¿˜èƒ½è‡ªç„¶åœ°è½¬å‘ç”¨æˆ·å¹¶ä¿æŒç›®å…‰æ¥è§¦ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ³¨è§†è¯„åˆ†æœºåˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è°ƒæ•´çœ¼ç¥æ¥è§¦çš„å¼ºåº¦ï¼ŒåŒæ—¶æ¨¡å‹ä»æ•°æ®ä¸­æ•æ‰è‡ªç„¶çš„ç©ºé—´å¯¹é½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨Embody 3Dæ•°æ®é›†ä¸Šä»¥è¶…è¿‡300 FPSçš„é€Ÿåº¦å®ç°äº†æœ€å…ˆè¿›çš„åŠ¨ä½œè´¨é‡ï¼ŒéªŒè¯äº†åœ¨å®æ—¶è™šæ‹Ÿç°å®ç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.17186",
            "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
            "url": "https://huggingface.co/papers/2602.17186",
            "abstract": "Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.",
            "score": 1,
            "issue_id": 1176,
            "pub_date": "2026-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "baddeec8722ce1cf",
            "authors": [
                "Seulbi Lee",
                "Sangheum Hwang"
            ],
            "affiliations": [
                "Seoul National University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.17186.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ²ĞºĞ»Ğ°Ğ´ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Visual Information Gain (VIG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VIG Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Visual Information Gain",
                    "desc": "This paper introduces the Visual Information Gain (VIG) metric, which quantifies how much visual input helps reduce uncertainty in predictions made by vision-language models. It addresses the common issue of language bias in large vision-language models (LVLMs) that often rely on textual information rather than visual evidence. By using VIG, the authors can analyze the contribution of individual training samples and tokens, identifying which visual elements enhance model performance. The proposed VIG-guided selective training method focuses on high-VIG samples, leading to improved visual grounding and reduced reliance on language, ultimately enhancing model accuracy with less training data."
                },
                "zh": {
                    "title": "æå‡è§†è§‰åŸºç¡€ï¼Œå‡å°‘è¯­è¨€åè§çš„é€‰æ‹©æ€§è®­ç»ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºè§†è§‰ä¿¡æ¯å¢ç›Šï¼ˆVIGï¼‰ï¼Œç”¨äºé‡åŒ–è§†è§‰è¾“å…¥å¯¹é¢„æµ‹ä¸ç¡®å®šæ€§çš„è´¡çŒ®ã€‚é€šè¿‡ä½¿ç”¨VIGï¼Œç ”ç©¶è€…èƒ½å¤Ÿåœ¨æ ·æœ¬å’Œæ ‡è®°å±‚é¢è¿›è¡Œç»†è‡´åˆ†æï¼Œè¯†åˆ«å‡ºä¸è§†è§‰ç›¸å…³çš„å…ƒç´ ï¼Œå¦‚é¢œè‰²ã€ç©ºé—´å…³ç³»å’Œå±æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜å…ˆé€‰æ‹©é«˜VIGæ ·æœ¬å’Œæ ‡è®°ï¼Œå®æ–½é€‰æ‹©æ€§è®­ç»ƒï¼Œä»è€Œæ”¹å–„è§†è§‰åŸºç¡€å’Œå‡å°‘è¯­è¨€åè§ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å‡å°‘ç›‘ç£çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16742",
            "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2602.16742",
            "abstract": "DeepVision-103K dataset enhances multimodal reasoning capabilities of large models through diverse mathematical content and visual elements.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce DeepVision-103K, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: https://huggingface.co/datasets/skylenage/DeepVision-103K{this url}.",
            "score": 1,
            "issue_id": 1177,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "23877a45dc3a3e08",
            "authors": [
                "Haoxiang Sun",
                "Lizhen Xu",
                "Bing Zhao",
                "Wotao Yin",
                "Wei Wang",
                "Boyu Yang",
                "Rui Wang",
                "Hu Wei"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16742.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DeepVision-103K, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 103 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° DeepVision, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ."
                },
                "en": {
                    "title": "Empowering Multimodal Reasoning with DeepVision-103K",
                    "desc": "The DeepVision-103K dataset is designed to improve the reasoning abilities of large multimodal models by providing a wide range of mathematical content and visual elements. It addresses the limitations of existing datasets, which often lack diversity and comprehensive coverage, by offering a rich collection of K12 mathematical topics and knowledge points. Models trained with this dataset demonstrate significant improvements in performance on multimodal mathematical tasks and show strong generalization to broader reasoning challenges. The results indicate that DeepVision-103K effectively enhances visual perception, reflection, and reasoning skills in these models, making it a valuable resource for advancing multimodal reasoning capabilities."
                },
                "zh": {
                    "title": "DeepVision-103Kï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ•°æ®é›†",
                    "desc": "DeepVision-103K æ•°æ®é›†é€šè¿‡å¤šæ ·çš„æ•°å­¦å†…å®¹å’Œè§†è§‰å…ƒç´ ï¼Œå¢å¼ºäº†å¤§å‹æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†ä¸“ä¸ºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒè€Œè®¾è®¡ï¼Œæ¶µç›–äº†ä¸°å¯Œçš„ K12 æ•°å­¦ä¸»é¢˜å’ŒçŸ¥è¯†ç‚¹ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒDeepVision-103K æä¾›äº†æ›´é«˜çš„æ•°æ®å¤šæ ·æ€§å’Œè¦†ç›–é¢ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä¸€èˆ¬å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­æœ‰æ•ˆæ³›åŒ–ï¼ŒéªŒè¯äº† DeepVision çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.18312",
            "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty",
            "url": "https://huggingface.co/papers/2602.18312",
            "abstract": "Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.",
            "score": 0,
            "issue_id": 1173,
            "pub_date": "2026-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "73b0690b05d7ed43",
            "authors": [
                "Zhaoming Xie",
                "Kevin Karol",
                "Jessica Hodgins"
            ],
            "affiliations": [
                "Robotics AI Institute, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.18312.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#robotics",
                    "#optimization",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑˆÑ‚Ñ€Ğ°Ñ„ ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ½Ğ° ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Linear Policy Net, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Smooth Motion Imitation with Efficient Learning",
                    "desc": "This paper presents a method to improve reinforcement learning policies by using an action Jacobian penalty, which helps to reduce unrealistic high-frequency control signals in motion imitation tasks. The authors introduce a new architecture called Linear Policy Net (LPN) that minimizes computational overhead while allowing for faster convergence and efficient inference. By directly penalizing changes in actions relative to changes in the simulated state, the method eliminates the need for extensive tuning typically required in existing approaches. The results show that LPN can effectively learn smooth motion policies for complex tasks, including dynamic movements on a physical robot."
                },
                "zh": {
                    "title": "é€šè¿‡çº¿æ€§ç­–ç•¥ç½‘ç»œä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨åŠ¨ä½œé›…å¯æ¯”æƒ©ç½šæ¥æ¶ˆé™¤ä¸ç°å®çš„é«˜é¢‘ä¿¡å·ï¼Œä»è€Œæ”¹å–„ç­–ç•¥çš„å­¦ä¹ æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„çº¿æ€§ç­–ç•¥ç½‘ç»œæ¶æ„ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦å’Œæ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•æ— éœ€ç‰¹å®šä»»åŠ¡çš„è°ƒä¼˜ï¼Œèƒ½å¤Ÿç›´æ¥é€šè¿‡è‡ªåŠ¨å¾®åˆ†æ¥æƒ©ç½šåŠ¨ä½œå˜åŒ–ï¼Œç”Ÿæˆå¹³æ»‘çš„æ§åˆ¶ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŠ¨æ€è¿åŠ¨æ¨¡ä»¿ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬åç©ºç¿»å’Œå„ç§æŒ‘æˆ˜æ€§çš„è·‘é…·æŠ€èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-20.html",
    "link_next": "2026-02-24.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "20.02",
        "en": "02/20",
        "zh": "2æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 5,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}