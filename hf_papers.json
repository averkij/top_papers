{
    "date": {
        "ru": "22 –∞–≤–≥—É—Å—Ç–∞",
        "en": "August 22",
        "zh": "8Êúà22Êó•"
    },
    "time_utc": "2025-08-22 21:10",
    "weekday": 4,
    "issue_id": 5504,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15763",
            "title": "Intern-S1: A Scientific Multimodal Foundation Model",
            "url": "https://huggingface.co/papers/2508.15763",
            "abstract": "Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.",
            "score": 162,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "55fe9cc77ae55101",
            "authors": [
                "Lei Bai",
                "Zhongrui Cai",
                "Maosong Cao",
                "Weihan Cao",
                "Chiyu Chen",
                "Haojiong Chen",
                "Kai Chen",
                "Pengcheng Chen",
                "Ying Chen",
                "Yongkang Chen",
                "Yu Cheng",
                "Yu Cheng",
                "Pei Chu",
                "Tao Chu",
                "Erfei Cui",
                "Ganqu Cui",
                "Long Cui",
                "Ziyun Cui",
                "Nianchen Deng",
                "Ning Ding",
                "Nanqin Dong",
                "Peijie Dong",
                "Shihan Dou",
                "Sinan Du",
                "Haodong Duan",
                "Caihua Fan",
                "Ben Gao",
                "Changjiang Gao",
                "Jianfei Gao",
                "Songyang Gao",
                "Yang Gao",
                "Zhangwei Gao",
                "Jiaye Ge",
                "Qiming Ge",
                "Lixin Gu",
                "Yuzhe Gu",
                "Aijia Guo",
                "Qipeng Guo",
                "Xu Guo",
                "Conghui He",
                "Junjun He",
                "Yili Hong",
                "Siyuan Hou",
                "Caiyu Hu",
                "Hanglei Hu",
                "Jucheng Hu",
                "Ming Hu",
                "Zhouqi Hua",
                "Haian Huang",
                "Junhao Huang",
                "Xu Huang",
                "Zixian Huang",
                "Zhe Jiang",
                "Lingkai Kong",
                "Linyang Li",
                "Peiji Li",
                "Pengze Li",
                "Shuaibin Li",
                "Tianbin Li",
                "Wei Li",
                "Yuqiang Li",
                "Dahua Lin",
                "Junyao Lin",
                "Tianyi Lin",
                "Zhishan Lin",
                "Hongwei Liu",
                "Jiangning Liu",
                "Jiyao Liu",
                "Junnan Liu",
                "Kai Liu",
                "Kaiwen Liu",
                "Kuikun Liu",
                "Shichun Liu",
                "Shudong Liu",
                "Wei Liu",
                "Xinyao Liu",
                "Yuhong Liu",
                "Zhan Liu",
                "Yinquan Lu",
                "Haijun Lv",
                "Hongxia Lv",
                "Huijie Lv",
                "Qidang Lv",
                "Ying Lv",
                "Chengqi Lyu",
                "Chenglong Ma",
                "Jianpeng Ma",
                "Ren Ma",
                "Runmin Ma",
                "Runyuan Ma",
                "Xinzhu Ma",
                "Yichuan Ma",
                "Zihan Ma",
                "Sixuan Mi",
                "Junzhi Ning",
                "Wenchang Ning",
                "Xinle Pang",
                "Jiahui Peng",
                "Runyu Peng",
                "Yu Qiao",
                "Jiantao Qiu",
                "Xiaoye Qu",
                "Yuan Qu",
                "Yuchen Ren",
                "Fukai Shang",
                "Wenqi Shao",
                "Junhao Shen",
                "Shuaike Shen",
                "Chunfeng Song",
                "Demin Song",
                "Diping Song",
                "Chenlin Su",
                "Weijie Su",
                "Weigao Sun",
                "Yu Sun",
                "Qian Tan",
                "Cheng Tang",
                "Huanze Tang",
                "Kexian Tang",
                "Shixiang Tang",
                "Jian Tong",
                "Aoran Wang",
                "Bin Wang",
                "Dong Wang",
                "Lintao Wang",
                "Rui Wang",
                "Weiyun Wang",
                "Wenhai Wang",
                "Yi Wang",
                "Ziyi Wang",
                "Ling-I Wu",
                "Wen Wu",
                "Yue Wu",
                "Zijian Wu",
                "Linchen Xiao",
                "Shuhao Xing",
                "Chao Xu",
                "Huihui Xu",
                "Jun Xu",
                "Ruiliang Xu",
                "Wanghan Xu",
                "GanLin Yang",
                "Yuming Yang",
                "Haochen Ye",
                "Jin Ye",
                "Shenglong Ye",
                "Jia Yu",
                "Jiashuo Yu",
                "Jing Yu",
                "Fei Yuan",
                "Bo Zhang",
                "Chao Zhang",
                "Chen Zhang",
                "Hongjie Zhang",
                "Jin Zhang",
                "Qiaosheng Zhang",
                "Qiuyinzhe Zhang",
                "Songyang Zhang",
                "Taolin Zhang",
                "Wenlong Zhang",
                "Wenwei Zhang",
                "Yechen Zhang",
                "Ziyang Zhang",
                "Haiteng Zhao",
                "Qian Zhao",
                "Xiangyu Zhao",
                "Xiangyu Zhao",
                "Bowen Zhou",
                "Dongzhan Zhou",
                "Peiheng Zhou",
                "Yuhao Zhou",
                "Yunhua Zhou",
                "Dongsheng Zhu",
                "Lin Zhu",
                "Yicheng Zou"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15763.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#science",
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Intern-S1: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-—ç–∫—Å–ø–µ—Ä—Ç –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á",
                    "desc": "Intern-S1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ Mixture-of-Experts —Å 28 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 5 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—è 2.5 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. Intern-S1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–ª–∞–π–Ω –∏ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ Mixture-of-Rewards –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —á–µ–º 1000 –∑–∞–¥–∞—á–∞–º. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—â–µ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∑–∞–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤ –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫—Ä–∏—Å—Ç–∞–ª–ª–æ–≤."
                },
                "en": {
                    "title": "Bridging the Gap in Scientific AI with Intern-S1",
                    "desc": "Intern-S1 is a multimodal Mixture-of-Experts model designed to enhance performance in scientific reasoning tasks. It utilizes extensive pre-training on a vast dataset, including a significant portion from scientific domains, to develop a deep understanding of complex data. The model employs reinforcement learning techniques, specifically Mixture-of-Rewards, to optimize its performance across numerous tasks simultaneously. As a result, Intern-S1 not only excels in general reasoning but also surpasses both open-source and closed-source models in specialized scientific applications."
                },
                "zh": {
                    "title": "Intern-S1ÔºöÁßëÂ≠¶È¢ÜÂüüÁöÑÊô∫ËÉΩ‰∏ìÂÆ∂",
                    "desc": "Intern-S1ÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåÁªèËøáÂπøÊ≥õÁöÑÈ¢ÑËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåËÉΩÂ§üÂú®‰∏ÄËà¨Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®ÁßëÂ≠¶‰ªªÂä°‰∏≠Ë∂ÖË∂äÈó≠Ê∫êÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÊã•Êúâ280‰∫ø‰∏™ÊøÄÊ¥ªÂèÇÊï∞Âíå2410‰∫ø‰∏™ÊÄªÂèÇÊï∞ÔºåÁªèËøá5‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºåÂÖ∂‰∏≠Ë∂ÖËøá2.5‰∏á‰∫ø‰∏™Ê†áËÆ∞Êù•Ëá™ÁßëÂ≠¶È¢ÜÂüü„ÄÇ‰∏∫‰∫ÜÁº©Â∞èÂºÄÊîæÊ∫êÊ®°Âûã‰∏éÈó≠Ê∫êÊ®°ÂûãÂú®ÁßëÂ≠¶È¢ÜÂüüÁöÑÂ∑ÆË∑ùÔºåIntern-S1ÈááÁî®‰∫ÜÊ∑∑ÂêàÂ•ñÂä±Êú∫Âà∂ÔºåÂú®Ë∂ÖËøá1000‰∏™‰ªªÂä°‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇÈÄöËøáÁÆóÊ≥ï„ÄÅÊï∞ÊçÆÂíåËÆ≠ÁªÉÁ≥ªÁªüÁöÑÁªºÂêàÂàõÊñ∞ÔºåIntern-S1Âú®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠ÂèñÂæó‰∫ÜÈ°∂Â∞ñÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15144",
            "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
            "url": "https://huggingface.co/papers/2508.15144",
            "abstract": "GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.",
            "score": 38,
            "issue_id": 5486,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "f41c368d3d1b16f8",
            "authors": [
                "Jiabo Ye",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Zhaoqing Zhu",
                "Ziwei Zheng",
                "Feiyu Gao",
                "Junjie Cao",
                "Zhengxi Lu",
                "Jitong Liao",
                "Qi Zheng",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15144.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#open_source",
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–æ—Ä—ã–≤–∞ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GUI-Owl –∏ Mobile-Agent-v3 - –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º. –≠—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –≤ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥—ã, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. GUI-Owl –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–ª–∞—á–Ω—É—é –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. Mobile-Agent-v3 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ GUI-–∞–≥–µ–Ω—Ç–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with State-of-the-Art Performance",
                    "desc": "This paper presents GUI-Owl, an advanced GUI agent model that excels in performance on various benchmarks for desktop and mobile environments. It introduces Mobile-Agent-v3, which enhances GUI-Owl's capabilities, achieving new state-of-the-art scores on AndroidWorld and OSWorld. Key innovations include a large-scale cloud-based environment for generating high-quality interaction data, diverse foundational agent capabilities for end-to-end decision-making, and a scalable reinforcement learning framework for real-world applications. Both models are open-source, promoting further research and development in GUI agent technology."
                },
                "zh": {
                    "title": "ÂºÄÊ∫êGUI‰ª£ÁêÜÁöÑÊñ∞Êó∂‰ª£",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜGUI-OwlÂíåMobile-Agent-v3Ëøô‰∏§‰∏™ÂºÄÊ∫êGUI‰ª£ÁêÜÊ®°ÂûãÂíåÊ°ÜÊû∂ÔºåÂÆÉ‰ª¨Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇGUI-OwlÊòØ‰∏Ä‰∏™Âü∫Á°ÄÁöÑGUI‰ª£ÁêÜÊ®°ÂûãÔºåÂú®Ê°åÈù¢ÂíåÁßªÂä®ÁéØÂ¢ÉÁöÑÂçÅ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇMobile-Agent-v3ÂàôÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑGUI‰ª£ÁêÜÊ°ÜÊû∂ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÂàõÈÄ†‰∫ÜÊñ∞ÁöÑÂºÄÊ∫êGUI‰ª£ÁêÜÊ°ÜÊû∂ÁöÑÊúÄ‰Ω≥ËÆ∞ÂΩï„ÄÇËØ•Á†îÁ©∂ÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÂåÖÊã¨Â§ßËßÑÊ®°ÁéØÂ¢ÉÂü∫Á°ÄËÆæÊñΩ„ÄÅÂ§öÊ†∑ÂåñÁöÑÂü∫Á°Ä‰ª£ÁêÜËÉΩÂäõÂíåÂèØÊâ©Â±ïÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15760",
            "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
            "url": "https://huggingface.co/papers/2508.15760",
            "abstract": "LiveMCP-101 benchmarks AI agents' ability to use multiple tools in real-world scenarios, revealing challenges in tool orchestration and inefficiencies in token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.",
            "score": 25,
            "issue_id": 5490,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "04f8a909c0d3944e",
            "authors": [
                "Ming Yin",
                "Dinghan Shen",
                "Silei Xu",
                "Jianbing Han",
                "Sixun Dong",
                "Mian Zhang",
                "Yebowen Hu",
                "Shujian Liu",
                "Simin Ma",
                "Song Wang",
                "Sathish Reddy Indurthi",
                "Xun Wang",
                "Yiran Chen",
                "Kaiqiang Song"
            ],
            "affiliations": [
                "Duke University",
                "Zoom Video Communications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15760.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#agi"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "LiveMCP-101: –ò—Å–ø—ã—Ç–∞–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö",
                    "desc": "LiveMCP-101 - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 101 –∑–∞–ø—Ä–æ—Å, —Ç—Ä–µ–±—É—é—â–∏–π –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö MCP-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —É—Ä–æ–≤–Ω—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –Ω–∏–∂–µ 60%, –≤—ã—è–≤–ª—è—è —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –æ—Ä–∫–µ—Å—Ç—Ä–æ–≤–∫–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–≥–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ, –ø—Ä–æ–¥–≤–∏–≥–∞—è—Å—å –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –ò–ò."
                },
                "en": {
                    "title": "Benchmarking AI Agents: Mastering Tool Use in Real-World Tasks",
                    "desc": "LiveMCP-101 is a benchmark designed to evaluate how well AI agents can use multiple tools to solve complex, real-world tasks. It highlights the challenges of tool orchestration and inefficiencies in token usage when agents attempt to perform multi-step operations. The benchmark consists of 101 real-world queries that require the coordinated use of various tools, such as web search and data analysis, and introduces a new evaluation method based on ground-truth execution plans. Results show that even advanced language models struggle with these tasks, achieving less than 60% success, indicating significant areas for improvement in AI tool integration."
                },
                "zh": {
                    "title": "ËØÑ‰º∞AI‰ª£ÁêÜÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ",
                    "desc": "LiveMCP-101ÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠‰ΩøÁî®Â§öÁßçÂ∑•ÂÖ∑ÁöÑËÉΩÂäõ„ÄÇËØ•Á†îÁ©∂Êè≠Á§∫‰∫ÜÂ∑•ÂÖ∑ÂçèË∞ÉÁöÑÊåëÊàò‰ª•Âèä‰ª§Áâå‰ΩøÁî®ÁöÑ‰ΩéÊïàÊÄß„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÁúüÂÆûÊâßË°åËÆ°ÂàíÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåÁ†îÁ©∂Êõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†‰∫ÜÁé∞ÂÆûÁéØÂ¢ÉÁöÑÂä®ÊÄÅÁâπÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖ∂ÊàêÂäüÁéá‰πü‰Ωé‰∫é60%ÔºåËøôË°®ÊòéÂú®Â§öÊ≠•È™§‰ªªÂä°‰∏≠‰ªçÂ≠òÂú®ÊòæËëóÁöÑÂõ∞Èöæ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15260",
            "title": "Deep Think with Confidence",
            "url": "https://huggingface.co/papers/2508.15260",
            "abstract": "DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.",
            "score": 22,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "2ad4b155cf2ef39e",
            "authors": [
                "Yichao Fu",
                "Xuewei Wang",
                "Yuandong Tian",
                "Jiawei Zhao"
            ],
            "affiliations": [
                "Meta AI",
                "UCSD"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15260.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–º–Ω–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ: DeepConf –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "DeepConf - —ç—Ç–æ –º–µ—Ç–æ–¥, —É–ª—É—á—à–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. DeepConf –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Boosting Reasoning Efficiency with Confidence Filtering",
                    "desc": "DeepConf is a novel method that improves the efficiency and accuracy of reasoning in large language models by using internal confidence signals to filter out low-quality reasoning traces. This approach addresses the limitations of traditional test-time scaling methods, which often result in high computational costs and minimal accuracy gains. By dynamically selecting the most reliable reasoning paths, DeepConf enhances performance without requiring additional training or tuning. Evaluations show that DeepConf significantly boosts accuracy and reduces token generation, making it a valuable tool for various reasoning tasks."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶ÁΩÆ‰ø°ÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß",
                    "desc": "DeepConfÊòØ‰∏ÄÁßçÂ¢ûÂº∫Êé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ΩøÁî®Ê®°ÂûãÂÜÖÈÉ®ÁöÑÁΩÆ‰ø°‰ø°Âè∑Êù•ËøáÊª§‰ΩéË¥®ÈáèÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÂú®ÊµãËØïÊó∂Êó†ÈúÄÈ¢ùÂ§ñÁöÑÊ®°ÂûãËÆ≠ÁªÉÊàñË∂ÖÂèÇÊï∞Ë∞ÉÊï¥ÔºåÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÊúçÂä°Ê°ÜÊû∂‰∏≠„ÄÇDeepConfÂú®Â§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®AIME 2025Á≠âÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËææÂà∞‰∫Ü99.9%ÁöÑÈ´òÂáÜÁ°ÆÁéáÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÂ§öËææ84.7%ÁöÑÁîüÊàê‰ª§Áâå„ÄÇÈÄöËøáÂä®ÊÄÅËøáÊª§ÔºåDeepConfÊúâÊïàÊèêÈ´ò‰∫ÜÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15761",
            "title": "Waver: Wave Your Way to Lifelike Video Generation",
            "url": "https://huggingface.co/papers/2508.15761",
            "abstract": "Waver, a high-performance foundation model, generates high-quality videos and images using a Hybrid Stream DiT architecture, excelling in text-to-video, image-to-video, and text-to-image tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.",
            "score": 13,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "9168ada481044deb",
            "authors": [
                "Yifu Zhang",
                "Hao Yang",
                "Yuqi Zhang",
                "Yifei Hu",
                "Fengda Zhu",
                "Chuang Lin",
                "Xiaofeng Mei",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.15761.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#multimodal",
                    "#data",
                    "#open_source",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "Waver: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è",
                    "desc": "Waver - —ç—Ç–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Hybrid Stream DiT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ä–∞–º–∫–∞—Ö –µ–¥–∏–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. Waver –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞—Ö–≤–∞—Ç–µ —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—è –≤—ã—Å–æ–∫–æ–π –∞–º–ø–ª–∏—Ç—É–¥—ã –¥–≤–∏–∂–µ–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Waver: Unifying Image and Video Generation with High Performance",
                    "desc": "Waver is a cutting-edge foundation model designed for generating high-quality images and videos using a Hybrid Stream DiT architecture. It can create videos lasting 5 to 10 seconds at a resolution of 720p, which can be upscaled to 1080p. The model integrates text-to-video, image-to-video, and text-to-image generation in one framework, enhancing modality alignment and speeding up training. With a robust data curation pipeline and a specialized video quality model, Waver achieves superior motion capture and consistency, ranking among the top models in its category."
                },
                "zh": {
                    "title": "WaverÔºöÈ´òÊÄßËÉΩÁöÑÂõæÂÉè‰∏éËßÜÈ¢ëÁîüÊàêÊ®°Âûã",
                    "desc": "WaverÊòØ‰∏ÄÁßçÈ´òÊÄßËÉΩÁöÑÂü∫Á°ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÁªü‰∏ÄÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê„ÄÇÂÆÉËÉΩÂ§üÁõ¥Êé•ÁîüÊàêÊó∂Èïø‰∏∫5Âà∞10ÁßíÁöÑ720pËßÜÈ¢ëÔºåÂπ∂ÂèØÊèêÂçáËá≥1080pÂàÜËæ®Áéá„ÄÇËØ•Ê®°ÂûãÊîØÊåÅÊñáÊú¨Âà∞ËßÜÈ¢ë„ÄÅÂõæÂÉèÂà∞ËßÜÈ¢ëÂíåÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÔºåÈááÁî®‰∫ÜÊ∑∑ÂêàÊµÅDiTÊû∂ÊûÑ‰ª•Â¢ûÂº∫Ê®°ÊÄÅÂØπÈΩêÂíåÂä†ÈÄüËÆ≠ÁªÉÊî∂Êïõ„ÄÇWaverÂú®ËßÜÈ¢ëÂêàÊàê‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÊçïÊçâÂ§çÊùÇÁöÑËøêÂä®ÔºåÂÖ∑Êúâ‰ºòË∂äÁöÑËøêÂä®ÂπÖÂ∫¶ÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15769",
            "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
            "url": "https://huggingface.co/papers/2508.15769",
            "abstract": "SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
            "score": 11,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "deff71bdd7ad6d46",
            "authors": [
                "Yanxu Meng",
                "Haoning Wu",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15769.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–û—Ç 2D –∫ 3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω",
                    "desc": "SceneGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—É—é –∏ –≥–ª–æ–±–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ü–µ–Ω–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –ø–æ–∏—Å–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. SceneGen –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ VR/AR –∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–º –ò–ò."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation from Single Images",
                    "desc": "SceneGen is a new framework designed to create multiple 3D assets from a single scene image by effectively combining local and global scene information. It uses a feature aggregation module that processes visual and geometric data to generate 3D models with their textures and spatial arrangements in one go, without needing optimization or asset retrieval. The framework is also adaptable, showing improved performance when handling multiple images, even though it was initially trained on single images. Overall, SceneGen represents a significant advancement in the field of 3D content generation, with promising applications in virtual and augmented reality."
                },
                "zh": {
                    "title": "SceneGenÔºöÈ´òÊïàÁîüÊàê3DËµÑ‰∫ßÁöÑÊñ∞Ê°ÜÊû∂",
                    "desc": "SceneGenÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéÂçï‰∏ÄÂú∫ÊôØÂõæÂÉèÁîüÊàêÂ§ö‰∏™3DËµÑ‰∫ß„ÄÇÂÆÉÈÄöËøáÊï¥ÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÂú∫ÊôØ‰ø°ÊÅØÔºåËÉΩÂ§üÈ´òÊïà‰∏îÁ®≥ÂÅ•Âú∞ÂàõÂª∫3DÂÜÖÂÆπ„ÄÇËØ•Ê°ÜÊû∂Êó†ÈúÄ‰ºòÂåñÊàñËµÑ‰∫ßÊ£ÄÁ¥¢ÔºåÁõ¥Êé•ÁîüÊàêÂÖ∑ÊúâÂá†‰ΩïÂΩ¢Áä∂ÂíåÁ∫πÁêÜÁöÑ3DËµÑ‰∫ß„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåSceneGenÂú®ÁîüÊàêÊÄßËÉΩ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÊâ©Â±ïÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÂõæÂÉèËæìÂÖ•Âú∫ÊôØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15361",
            "title": "A Survey on Large Language Model Benchmarks",
            "url": "https://huggingface.co/papers/2508.15361",
            "abstract": "A systematic review of large language model benchmarks identifies issues such as data contamination, cultural biases, and lack of process credibility, and proposes a design paradigm for future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.",
            "score": 7,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "d2096a14a9fdc644",
            "authors": [
                "Shiwen Ni",
                "Guhong Chen",
                "Shuaimin Li",
                "Xuanang Chen",
                "Siyi Li",
                "Bingli Wang",
                "Qiyao Wang",
                "Xingjian Wang",
                "Yifan Zhang",
                "Liyang Fan",
                "Chengming Li",
                "Ruifeng Xu",
                "Le Sun",
                "Min Yang"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Software, Chinese Academy of Sciences",
                "Shanghai AI Lab",
                "Shanghai University of Electric Power",
                "Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Shenzhen MSU-BIT University",
                "Shenzhen University",
                "Shenzhen University of Advanced Technology",
                "South China University of Technology",
                "Southern University of Science and Technology",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15361.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –ø—Ä–æ–±–ª–µ–º –∫ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ—Ü–µ–Ω–∫–∏. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –≤ –±—É–¥—É—â–µ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç 283 –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø–æ —Ç—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: –æ–±—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ —Ü–µ–ª–µ–≤—ã–µ."
                },
                "en": {
                    "title": "Enhancing Language Model Benchmarks for Fair and Credible Evaluation",
                    "desc": "This paper reviews the current benchmarks used to evaluate large language models, highlighting their importance in measuring model performance and guiding development. It categorizes 283 benchmarks into three types: general capabilities, domain-specific, and target-specific, each focusing on different aspects of model evaluation. The authors identify significant issues such as data contamination leading to inflated scores, cultural biases affecting fairness, and a lack of credibility in evaluation processes. To address these challenges, the paper proposes a new design paradigm aimed at improving future benchmarks for more reliable assessments."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂü∫ÂáÜÁöÑËÆæËÆ°‰∏éÂèØ‰ø°Â∫¶",
                    "desc": "ËøôÁØáËÆ∫ÊñáÁ≥ªÁªüÊÄßÂú∞ÂõûÈ°æ‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂü∫ÂáÜÁöÑÁé∞Áä∂ÔºåËØÜÂà´Âá∫Êï∞ÊçÆÊ±°Êüì„ÄÅÊñáÂåñÂÅèËßÅÂíåËøáÁ®ãÂèØ‰ø°Â∫¶Áº∫‰πèÁ≠âÈóÆÈ¢ò„ÄÇ‰ΩúËÄÖÂ∞Ü283‰∏™‰ª£Ë°®ÊÄßÂü∫ÂáÜÂàÜ‰∏∫‰∏âÁ±ªÔºöÈÄöÁî®ËÉΩÂäõ„ÄÅÈ¢ÜÂüüÁâπÂÆöÂíåÁõÆÊ†áÁâπÂÆö„ÄÇÈÄöÁî®ËÉΩÂäõÂü∫ÂáÜÊ∂µÁõñÊ†∏ÂøÉËØ≠Ë®ÄÂ≠¶„ÄÅÁü•ËØÜÂíåÊé®ÁêÜÁ≠âÊñπÈù¢ÔºåËÄåÈ¢ÜÂüüÁâπÂÆöÂü∫ÂáÜÂàôÂÖ≥Ê≥®Ëá™ÁÑ∂ÁßëÂ≠¶„ÄÅ‰∫∫ÊñáÂ≠¶ÁßëÂíåÂ∑•Á®ãÊäÄÊúØÁ≠âÈ¢ÜÂüü„ÄÇËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÊú™Êù•Âü∫ÂáÜÂàõÊñ∞ÁöÑËÆæËÆ°ËåÉÂºèÔºå‰ª•Ëß£ÂÜ≥ÂΩìÂâçÂü∫ÂáÜÂ≠òÂú®ÁöÑÈóÆÈ¢ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15772",
            "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
            "url": "https://huggingface.co/papers/2508.15772",
            "abstract": "VAREdit, a visual autoregressive framework, improves image editing adherence and efficiency by using a Scale-Aligned Reference module to condition source image tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a 512times512 editing in 1.2 seconds, making it 2.2times faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.",
            "score": 5,
            "issue_id": 5498,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "4305672bf954d7d1",
            "authors": [
                "Qingyang Mao",
                "Qi Cai",
                "Yehao Li",
                "Yingwei Pan",
                "Mingyue Cheng",
                "Ting Yao",
                "Qi Liu",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Science and Technology of China, Hefei, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15772.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "VAREdit: –¢–æ—á–Ω–æ–µ –∏ –±—ã—Å—Ç—Ä–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏",
                    "desc": "VAREdit - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å Scale-Aligned Reference –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∏–∑–±–µ–≥–∞—è –Ω–µ–ø—Ä–µ–¥–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π. VAREdit –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ 30% –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ 2.2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ –∞–Ω–∞–ª–æ–≥–æ–≤. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 512x512 –≤—Å–µ–≥–æ –∑–∞ 1.2 —Å–µ–∫—É–Ω–¥—ã."
                },
                "en": {
                    "title": "VAREdit: Precision and Speed in Image Editing",
                    "desc": "VAREdit is a visual autoregressive framework designed to enhance image editing by improving adherence to instructions and increasing efficiency. It utilizes a Scale-Aligned Reference (SAR) module to condition source image tokens, allowing for better guidance in generating multi-scale target features. Unlike diffusion models, which can unintentionally alter unrelated parts of an image, VAREdit processes image synthesis sequentially over discrete visual tokens, minimizing spurious modifications. The framework achieves significant performance improvements, outperforming leading methods by over 30% in editing accuracy and completing edits much faster than previous models."
                },
                "zh": {
                    "title": "VAREditÔºöÈ´òÊïàÁ≤æÂáÜÁöÑÂõæÂÉèÁºñËæëÊñ∞ÊñπÊ≥ï",
                    "desc": "VAREditÊòØ‰∏ÄÁßçËßÜËßâËá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂõæÂÉèÁºñËæëÁöÑÈÅµÂæ™ÊÄßÂíåÊïàÁéá„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™Â∞∫Â∫¶ÂØπÈΩêÂèÇËÄÉÊ®°ÂùóÔºåÊù•ÊúâÊïàÂú∞ÂØπÊ∫êÂõæÂÉèÁöÑÁâπÂæÅËøõË°åÊù°‰ª∂Â§ÑÁêÜÔºå‰ªéËÄåÁîüÊàêÂ§öÂ∞∫Â∫¶ÁöÑÁõÆÊ†áÁâπÂæÅ„ÄÇ‰∏éÊâ©Êï£Ê®°Âûã‰∏çÂêåÔºåËá™ÂõûÂΩíÊ®°ÂûãÂ∞ÜÂõæÂÉèÂêàÊàêËßÜ‰∏∫ÂØπÁ¶ªÊï£ËßÜËßâÊ†áËÆ∞ÁöÑÈ°∫Â∫èÂ§ÑÁêÜÔºåÈÅøÂÖç‰∫ÜÁºñËæëÊåá‰ª§ÁöÑÈÅµÂæ™ÈóÆÈ¢ò„ÄÇVAREditÂú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁºñËæëÈÄüÂ∫¶ÊØîÂêåÁ±ªÊ®°ÂûãÂø´2.2ÂÄçÔºåÂêåÊó∂Âú®ÁºñËæëÈÅµÂæ™ÊÄß‰∏äÊèêÈ´ò‰∫Ü30%‰ª•‰∏ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15767",
            "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling",
            "url": "https://huggingface.co/papers/2508.15767",
            "abstract": "ATLAS is a high-fidelity body model that decouples shape and skeleton bases, improving pose accuracy and shape expressivity using 600k high-resolution scans.  \t\t\t\t\tAI-generated summary \t\t\t\t Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.",
            "score": 5,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "0adb5eab0443b587",
            "authors": [
                "Jinhyung Park",
                "Javier Romero",
                "Shunsuke Saito",
                "Fabian Prada",
                "Takaaki Shiratori",
                "Yichen Xu",
                "Federica Bogo",
                "Shoou-I Yu",
                "Kris Kitani",
                "Rawal Khirodkar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15767.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ü¶æ",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ–ª–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –∏ —Å–∫–µ–ª–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏",
                    "desc": "ATLAS - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–µ–ª–∞ —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤—ã —Ñ–æ—Ä–º—ã –∏ —Å–∫–µ–ª–µ—Ç–∞, —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∑—ã –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 600 —Ç—ã—Å—è—á –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏—Ö —Å–∫–∞–Ω–æ–≤. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ–∑—ã –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. ATLAS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–¥–≥–æ–Ω–∫–∏ –∫ –Ω–µ–≤–∏–¥–∏–º—ã–º —Å—É–±—ä–µ–∫—Ç–∞–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–∑–∞—Ö. –ú–æ–¥–µ–ª—å –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã –∏ —Ç–æ—á–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Ç–µ–ª–∞, –∞ —Ç–∞–∫–∂–µ –ø–æ–¥–≥–æ–Ω–∫—É –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≤–Ω–µ—à–Ω–∏—Ö –º—è–≥–∫–∏—Ö —Ç–∫–∞–Ω–µ–π."
                },
                "en": {
                    "title": "Decoupling Shape and Skeleton for Enhanced 3D Body Modeling",
                    "desc": "ATLAS is a new body model that improves how we represent human shapes and poses in 3D. It uses a large dataset of 600,000 high-resolution scans to learn better representations by separating the shape of the body from its skeleton. This separation allows for more detailed and customizable body attributes, making it easier to fit different poses accurately. Overall, ATLAS provides a more flexible and precise way to model human figures compared to previous methods."
                },
                "zh": {
                    "title": "ATLASÔºöËß£ËÄ¶ÂΩ¢Áä∂‰∏éÈ™®Êû∂ÁöÑÈ´ò‰øùÁúüË∫´‰ΩìÊ®°Âûã",
                    "desc": "ATLASÊòØ‰∏ÄÁßçÈ´ò‰øùÁúüË∫´‰ΩìÊ®°ÂûãÔºåÈÄöËøáËß£ËÄ¶ÂΩ¢Áä∂ÂíåÈ™®Êû∂Âü∫Á°ÄÔºåÊèêÈ´ò‰∫ÜÂßøÂäøÂáÜÁ°ÆÊÄßÂíåÂΩ¢Áä∂Ë°®Áé∞Âäõ„ÄÇËØ•Ê®°ÂûãÂü∫‰∫é60‰∏áÂº†È´òÂàÜËæ®ÁéáÊâ´ÊèèÂõæÂÉèÔºå‰ΩøÁî®240Âè∞ÂêåÊ≠•ÊëÑÂÉèÊú∫ÊçïËé∑Êï∞ÊçÆ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåATLASÊòéÁ°ÆÂú∞Â∞ÜÂΩ¢Áä∂ÂíåÈ™®Êû∂Âü∫Á°ÄÂàÜÂºÄÔºå‰ΩøÂæóË∫´‰ΩìÂ±ûÊÄßÁöÑÂÆöÂà∂Êõ¥Âä†Á≤æÁªÜÔºåÂπ∂‰∏îÂÖ≥ÈîÆÁÇπÊãüÂêà‰∏çÂÜç‰æùËµñ‰∫éÂ§ñÈÉ®ËΩØÁªÑÁªáÁâπÂæÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåATLASÂú®Â§öÊ†∑ÂåñÂßøÂäø‰∏ãÂØπÊú™ËßÅÂØπË±°ÁöÑÊãüÂêàÁ≤æÂ∫¶‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰∏îÂÖ∂ÈùûÁ∫øÊÄßÂßøÂäø‰øÆÊ≠£ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊçïÊçâÂ§çÊùÇÂßøÂäø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15126",
            "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists",
            "url": "https://huggingface.co/papers/2508.15126",
            "abstract": "aiXiv is an open-access platform that facilitates the submission, review, and refinement of scientific proposals and papers by both human and AI scientists, enhancing the quality and dissemination of AI-generated research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.",
            "score": 4,
            "issue_id": 5490,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 20",
                "zh": "8Êúà20Êó•"
            },
            "hash": "db938abd67d28b70",
            "authors": [
                "Pengsong Zhang",
                "Xiang Hu",
                "Guowei Huang",
                "Yang Qi",
                "Heng Zhang",
                "Xiuxu Li",
                "Jiaxing Song",
                "Jiabin Luo",
                "Yijiang Li",
                "Shuo Yin",
                "Chengxiao Dai",
                "Eric Hanchen Jiang",
                "Xiaoyan Zhou",
                "Zhenfei Yin",
                "Boqin Yuan",
                "Jing Dong",
                "Guinan Su",
                "Guanren Qiao",
                "Haiming Tang",
                "Anghong Du",
                "Lili Pan",
                "Zhenzhong Lan",
                "Xinyu Liu"
            ],
            "affiliations": [
                "Columbia University",
                "Istituto Italiano di Tecnologia",
                "Max Planck Institute for Intelligent Systems",
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Birmingham",
                "University of California, Los Angeles",
                "University of California, San Diego",
                "University of Electronic Science and Technology of China",
                "University of Manchester",
                "University of Oxford",
                "University of Sydney",
                "University of Toronto",
                "University of Utah",
                "Universit√† degli Studi di Genova",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15126.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#science",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "aiXiv: –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ –ò–ò",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ aiXiv, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–¥–∞—á—É, —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–æ—Ä–∞–±–æ—Ç–∫—É –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –∫–∞–∫ –ª—é–¥—å–º–∏, —Ç–∞–∫ –∏ –ò–ò-—É—á–µ–Ω—ã–º–∏. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. aiXiv –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç API –∏ MCP –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∏ –ò–ò-—É—á–µ–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –∏ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "aiXiv: Bridging Human and AI Research for Scientific Progress",
                    "desc": "The paper introduces aiXiv, an innovative open-access platform designed for both human and AI scientists to collaboratively submit, review, and refine scientific research. It addresses the challenges posed by traditional publication systems that struggle to accommodate the growing volume of AI-generated content. By utilizing a multi-agent architecture, aiXiv facilitates seamless integration and interaction between human and AI researchers, enhancing the quality and dissemination of research proposals and papers. The platform demonstrates significant improvements in the reliability and robustness of AI-generated research through iterative revisions and peer reviews, paving the way for a more inclusive scientific ecosystem."
                },
                "zh": {
                    "title": "aiXivÔºöÊé®Âä®AIÁîüÊàêÁ†îÁ©∂ÁöÑÂºÄÊîæËé∑ÂèñÂπ≥Âè∞",
                    "desc": "aiXivÊòØ‰∏Ä‰∏™Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊîæËé∑ÂèñÂπ≥Âè∞ÔºåÊó®Âú®‰øÉËøõ‰∫∫Á±ªÂíå‰∫∫Â∑•Êô∫ËÉΩÁßëÂ≠¶ÂÆ∂ÁöÑÁßëÂ≠¶ÊèêÊ°àÂíåËÆ∫ÊñáÁöÑÊèê‰∫§„ÄÅÂÆ°Êü•ÂíåÊîπËøõ„ÄÇËØ•Âπ≥Âè∞ÈááÁî®Â§ö‰ª£ÁêÜÊû∂ÊûÑÔºå‰ΩøÂæóÁ†îÁ©∂ÊèêÊ°àÂíåËÆ∫ÊñáÂèØ‰ª•Áî±‰∫∫Á±ªÂíåAIÁßëÂ≠¶ÂÆ∂ÂÖ±ÂêåÊèê‰∫§ÂíåËø≠‰ª£ÂÆ°Êü•Ôºå‰ªéËÄåÊèêÈ´òAIÁîüÊàêÁ†îÁ©∂ÁöÑË¥®Èáè„ÄÇ‰º†ÁªüÁöÑÊúüÂàäÂíå‰ºöËÆÆ‰æùËµñ‰∫∫Á±ªÂêåË°åËØÑÂÆ°ÔºåÈöæ‰ª•ÈÄÇÂ∫îAIÁîüÊàêÂÜÖÂÆπÁöÑÂø´ÈÄüÂ¢ûÈïøÔºåËÄåaiXivÂàôÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÁîüÊÄÅÁ≥ªÁªüÔºåÊîØÊåÅËá™‰∏ªÁßëÂ≠¶ÂèëÁé∞„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòéaiXivÊòØ‰∏Ä‰∏™ÂèØÈù†ÁöÑÂπ≥Âè∞ÔºåÊòæËëóÊèêÂçá‰∫ÜAIÁîüÊàêÁ†îÁ©∂ÊèêÊ°àÂíåËÆ∫ÊñáÁöÑË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15752",
            "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards\n  Geospatial AI Agents for Visual Inquiries",
            "url": "https://huggingface.co/papers/2508.15752",
            "abstract": "Geo-Visual Agents integrate geospatial images and GIS data to answer nuanced visual-spatial inquiries about the world.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
            "score": 3,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "9c9ca9863fa564a1",
            "authors": [
                "Jon E. Froehlich",
                "Jared Hwang",
                "Zeyu Wang",
                "John S. O'Meara",
                "Xia Su",
                "William Huang",
                "Yang Zhang",
                "Alex Fiannaca",
                "Philip Nelson",
                "Shaun Kane"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research",
                "UCLA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15752.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#cv"
                ],
                "emoji": "üåç",
                "ru": {
                    "title": "–ì–µ–æ-–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ê–≥–µ–Ω—Ç—ã: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ì–µ–æ-–í–∏–∑—É–∞–ª—å–Ω—ã—Ö –ê–≥–µ–Ω—Ç–æ–≤ - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –æ –º–∏—Ä–µ. –≠—Ç–∏ –∞–≥–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –ø–∞–Ω–æ—Ä–∞–º—ã —É–ª–∏—Ü, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –º–µ—Å—Ç –∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–µ —Å–Ω–∏–º–∫–∏, –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –ì–ò–°-–¥–∞–Ω–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã –∫ —Å–µ–Ω—Å–æ—Ä–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é, –ø—Ä–∏–≤–æ–¥—è—Ç –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏ –æ–±—Å—É–∂–¥–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –ì–µ–æ-–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ê–≥–µ–Ω—Ç—ã –ø—Ä–∏–∑–≤–∞–Ω—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞—Ä—Ç, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Ä–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ì–ò–°."
                },
                "en": {
                    "title": "Revolutionizing Geo-Visual Inquiry with AI Agents",
                    "desc": "Geo-Visual Agents are advanced AI systems designed to interpret and respond to complex questions about the world using both geospatial images and GIS data. Unlike traditional digital maps that depend solely on structured GIS databases, these agents leverage a variety of visual data sources, such as streetscapes and aerial imagery, to provide richer insights. The paper outlines the methods for how these agents can sense and interact with their environment, showcasing three practical examples of their application. It also discusses the challenges and future opportunities in developing these multimodal AI agents for enhanced geo-visual understanding."
                },
                "zh": {
                    "title": "Geo-Visual AgentsÔºöÈáçÊñ∞ÂÆö‰πâÂú∞ÁêÜÁ©∫Èó¥ÁêÜËß£",
                    "desc": "Geo-Visual Agents ÊòØ‰∏ÄÁßçÁªìÂêàÂú∞ÁêÜÁ©∫Èó¥ÂõæÂÉèÂíåÂú∞ÁêÜ‰ø°ÊÅØÁ≥ªÁªüÔºàGISÔºâÊï∞ÊçÆÁöÑÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÔºåÊó®Âú®ÂõûÁ≠îÂÖ≥‰∫é‰∏ñÁïåÁöÑÂ§çÊùÇËßÜËßâÁ©∫Èó¥ÈóÆÈ¢ò„ÄÇËøô‰∫õ‰ª£ÁêÜËÉΩÂ§üÂàÜÊûêÂ§ßËßÑÊ®°ÁöÑÂú∞ÁêÜÁ©∫Èó¥ÂõæÂÉèÂ∫ìÔºåÂåÖÊã¨Ë°óÊôØ„ÄÅÂú∞ÁÇπÁÖßÁâáÂíåËà™Á©∫ÂΩ±ÂÉèÔºåÂπ∂‰∏é‰º†ÁªüÁöÑGISÊï∞ÊçÆÊ∫êÁõ∏ÁªìÂêà„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåGeo-Visual Agents ÂèØ‰ª•Ë∂ÖË∂ä‰º†ÁªüÊï∞Â≠óÂú∞ÂõæÁöÑÂ±ÄÈôêÔºåÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑÂú∞ÁêÜ‰ø°ÊÅØÂíåËßÜËßâ‰ΩìÈ™å„ÄÇÊú¨ÊñáËøòËÆ®ËÆ∫‰∫ÜÊÑüÁü•Âíå‰∫§‰∫íÊñπÊ≥ïÔºåÂπ∂Âàó‰∏æ‰∫ÜÊú™Êù•Â∑•‰ΩúÁöÑÂÖ≥ÈîÆÊåëÊàòÂíåÊú∫ÈÅá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15641",
            "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding",
            "url": "https://huggingface.co/papers/2508.15641",
            "abstract": "Grounded VideoDiT addresses temporal perception and entity interaction in videos through a Diffusion Temporal Latent encoder, object grounded representations, and mixed token scheme, achieving state-of-the-art results on VideoQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.",
            "score": 2,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "7ce8d878ebdf333f",
            "authors": [
                "Pengcheng Fang",
                "Yuxia Chen",
                "Rui Guo"
            ],
            "affiliations": [
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
                "Tencent AI Lab, Shenzhen, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#diffusion",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–¢–æ—á–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "Grounded VideoDiT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–Ω–∫–æ–¥–µ—Ä Diffusion Temporal Latent –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—ä–µ–∫—Ç–Ω–æ-–ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Grounded VideoDiT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–µ—à–∞–Ω–Ω—É—é —Å—Ö–µ–º—É —Ç–æ–∫–µ–Ω–æ–≤ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Grounded Temporal Reasoning",
                    "desc": "Grounded VideoDiT is a novel Video Language Model (VLM) that improves how we understand videos by focusing on when events happen and how different entities interact over time. It introduces a Diffusion Temporal Latent (DTL) encoder that enhances the model's ability to recognize event boundaries and maintain consistency in time. Additionally, it uses object grounded representations to connect specific entities to their visual context, improving alignment between language and vision. Finally, a mixed token scheme incorporates discrete temporal tokens for precise timestamp modeling, allowing for detailed temporal reasoning, which has led to state-of-the-art performance on various VideoQA benchmarks."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜÈ¢ëÁêÜËß£ÁöÑÊó∂Èó¥ÊÑüÁü•‰∏éÂÆû‰Ωì‰∫§‰∫íËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜGrounded VideoDiTÔºåËøôÊòØ‰∏ÄÁßçÈíàÂØπËßÜÈ¢ëÁêÜËß£ÁöÑÊ®°ÂûãÔºåÊó®Âú®ÊîπÂñÑÊó∂Èó¥ÊÑüÁü•ÂíåÂÆû‰Ωì‰∫§‰∫íÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂºïÂÖ•Êâ©Êï£Êó∂Èó¥ÊΩúÂú®ÁºñÁ†ÅÂô®„ÄÅÂØπË±°ÁªëÂÆöË°®Á§∫ÂíåÊ∑∑ÂêàÊ†áËÆ∞ÊñπÊ°àÔºåËØ•Ê®°ÂûãÂú®ËßÜÈ¢ëÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÊâ©Êï£Êó∂Èó¥ÊΩúÂú®ÁºñÁ†ÅÂô®Â¢ûÂº∫‰∫ÜËæπÁïåÊïèÊÑüÊÄßÂπ∂‰øùÊåÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÔºåËÄåÂØπË±°ÁªëÂÆöË°®Á§∫ÂàôÊòéÁ°ÆÂ∞ÜÊü•ËØ¢ÂÆû‰Ωì‰∏éÂ±ÄÈÉ®ËßÜËßâËØÅÊçÆÁõ∏ÁªìÂêà„ÄÇÊ∑∑ÂêàÊ†áËÆ∞ÊñπÊ°àÊèê‰æõ‰∫ÜÊòéÁ°ÆÁöÑÊó∂Èó¥Êà≥Âª∫Ê®°Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÊó∂Èó¥Êé®ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15202",
            "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2508.15202",
            "abstract": "Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.",
            "score": 2,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "05b2af7facdfcd18",
            "authors": [
                "Yuanchen Zhou",
                "Shuo Jiang",
                "Jie Zhu",
                "Junhui Li",
                "Lifan Guo",
                "Feng Chen",
                "Chi Zhang"
            ],
            "affiliations": [
                "Osaka University",
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15202.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#rlhf",
                    "#healthcare",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üíπ",
                "ru": {
                    "title": "Fin-PRM: –£–ª—É—á—à–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è",
                    "desc": "Fin-PRM - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä—ã, —É–ª—É—á—à–∞—é—â–∞—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω—ã–π —É—Ä–æ–≤–Ω–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º, –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤—ã–≤–æ–¥–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. Fin-PRM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ —Ç–∞–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫–∞–∫ CFLUE –∏ FinQA. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Fin-PRM –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Fin-PRM: Elevating Financial Reasoning in LLMs",
                    "desc": "Fin-PRM is a specialized reward model designed for the finance domain, enhancing the reasoning capabilities of large language models (LLMs). It employs both step-level and trajectory-level supervision to evaluate and improve the reasoning process in financial tasks, which require precise and structured logic. By integrating this tailored approach, Fin-PRM significantly boosts performance in supervised learning, reinforcement learning, and inference tasks compared to general-purpose models. Experimental results show that models trained with Fin-PRM achieve notable improvements in financial reasoning benchmarks, demonstrating the effectiveness of domain-specific reward modeling."
                },
                "zh": {
                    "title": "ÈáëËûçÈ¢ÜÂüüÁöÑ‰∏ìÁî®Â•ñÂä±Ê®°ÂûãÊèêÂçáÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Fin-PRMÊòØ‰∏ÄÁßç‰∏ìÈó®ÈíàÂØπÈáëËûçÈ¢ÜÂüüÁöÑÂ•ñÂä±Ê®°ÂûãÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∏≠Èó¥Êé®ÁêÜËøáÁ®ã‰∏≠ÁöÑË°®Áé∞„ÄÇÂÆÉÈÄöËøáÈÄêÊ≠•ÂíåËΩ®ËøπÁ∫ßÂà´ÁöÑÁõëÁù£ÔºåÊèê‰æõÂØπÈáëËûç‰ªªÂä°‰∏≠Êé®ÁêÜÊ≠•È™§ÁöÑÁªÜËá¥ËØÑ‰º∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåFin-PRMÂú®ÈáëËûçÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÈÄöÁî®Â•ñÂä±Ê®°ÂûãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁõëÁù£Â≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊïàÊûú„ÄÇËØ•Ê®°ÂûãÁöÑÂ∫îÁî®Â±ïÁ§∫‰∫ÜÈ¢ÜÂüü‰∏ìÁî®Â•ñÂä±Âª∫Ê®°Âú®ÊèêÂçáLLMs‰∏éÈáëËûç‰∏ìÂÆ∂Êé®ÁêÜ‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14892",
            "title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in\n  Milliseconds",
            "url": "https://huggingface.co/papers/2508.14892",
            "abstract": "A method reconstructs 3D human bodies from two sparse views using a redesigned geometry model and enhancement algorithm, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/.",
            "score": 2,
            "issue_id": 5491,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 20",
                "zh": "8Êúà20Êó•"
            },
            "hash": "4eda0215bbe80cbe",
            "authors": [
                "Jia Lu",
                "Taoran Yi",
                "Jiemin Fang",
                "Chen Yang",
                "Chuiyun Wu",
                "Wei Shen",
                "Wenyu Liu",
                "Qi Tian",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huawei Inc.",
                "Huazhong University of Science and Technology",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14892.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "üë§",
                "ru": {
                    "title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 3D-–º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –¥–≤—É–º —Ñ–æ—Ç–æ –∑–∞ –¥–æ–ª–∏ —Å–µ–∫—É–Ω–¥—ã",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Ç–µ–ª–∞ –ø–æ –¥–≤—É–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º - —Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω–æ–º—É –∏ –∑–∞–¥–Ω–µ–º—É –≤–∏–¥—É. –ê–≤—Ç–æ—Ä—ã redesigned geometry reconstruction model –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –¥–∞–∂–µ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –≤–æ—Å–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π —Ü–≤–µ—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç state-of-the-art –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö THuman2.0 –∏ cross-domain, –∞ —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤."
                },
                "en": {
                    "title": "Revolutionizing 3D Human Reconstruction from Sparse Views",
                    "desc": "This paper presents a novel method for reconstructing 3D human bodies from just two sparse images, specifically the front and back views. The approach addresses the challenges of maintaining 3D consistency and filling in missing data from limited input. By redesigning a geometry reconstruction model and applying an enhancement algorithm, the method generates complete and colored point clouds that can be rendered into high-quality 3D representations. The results demonstrate impressive performance, achieving full human reconstruction in 190 ms on advanced hardware, even with images from low-cost devices."
                },
                "zh": {
                    "title": "‰ªéÁ®ÄÁñèËßÜÂõæÈáçÂª∫‰∏âÁª¥‰∫∫‰ΩìÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªé‰∏§ÂπÖÁ®ÄÁñèËßÜÂõæÈáçÂª∫‰∏âÁª¥‰∫∫‰ΩìÁöÑÊñ∞ÊñπÊ≥ïÔºå‰ΩøÁî®‰∫ÜÈáçÊñ∞ËÆæËÆ°ÁöÑÂá†‰ΩïÊ®°ÂûãÂíåÂ¢ûÂº∫ÁÆóÊ≥ïÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ï‰ªÖÈúÄÂâçÂêéËßÜÂõæÁöÑ‰∏§Âº†ÂõæÁâáÔºåÈôç‰Ωé‰∫ÜÁî®Êà∑ÂàõÂª∫‰∏âÁª¥Êï∞Â≠ó‰∫∫Á±ªÁöÑÈó®Êßõ„ÄÇ‰∏ªË¶ÅÊåëÊàòÂú®‰∫éÂ¶Ç‰ΩïÂª∫Á´ã‰∏âÁª¥‰∏ÄËá¥ÊÄßÂíå‰ªéÁ®ÄÁñèËæìÂÖ•‰∏≠ÊÅ¢Â§çÁº∫Â§±‰ø°ÊÅØ„ÄÇÈÄöËøáÂü∫‰∫éÂü∫Á°ÄÈáçÂª∫Ê®°ÂûãÁöÑÂá†‰ΩïÈáçÂª∫Ê®°ÂûãÂíåÂ¢ûÂº∫ÁÆóÊ≥ïÔºåÊàë‰ª¨ËÉΩÂ§üÁîüÊàêÂÆåÊï¥ÁöÑÂ∏¶È¢úËâ≤ÁöÑ‰∫∫‰ΩìÁÇπ‰∫ëÔºåÂπ∂ÂÆûÁé∞È´òË¥®ÈáèÁöÑÊ∏≤Êüì„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09998",
            "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior",
            "url": "https://huggingface.co/papers/2508.09998",
            "abstract": "A benchmark evaluates companionship behaviors in language models, revealing differences in how models handle emotional support and boundary-setting.  \t\t\t\t\tAI-generated summary \t\t\t\t AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. We introduce Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, we develop a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though we observe marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.",
            "score": 2,
            "issue_id": 5494,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 4",
                "zh": "8Êúà4Êó•"
            },
            "hash": "d408ec286f7bbc37",
            "authors": [
                "Lucie-Aim√©e Kaffee",
                "Giada Pistilli",
                "Yacine Jernite"
            ],
            "affiliations": [
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09998.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–û—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∫ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–º–ø–∞–Ω—å–æ–Ω–æ–≤: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ INTIMA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∫–æ–º–ø–∞–Ω—å–æ–Ω–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–∏–π –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –∏–∑ 31 –ø–æ–≤–µ–¥–µ–Ω–∏—è –≤ 4 –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, —É—Å–∏–ª–∏–≤–∞—é—â–µ–µ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å, –≤—Å–µ –µ—â–µ –ø—Ä–µ–æ–±–ª–∞–¥–∞–µ—Ç. –í—ã—è–≤–ª–µ–Ω—ã —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –≤ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö, —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –æ–∑–∞–±–æ—á–µ–Ω–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Evaluating AI Companionship: Balancing Support and Boundaries",
                    "desc": "This paper introduces the Interactions and Machine Attachment Benchmark (INTIMA), which assesses how language models exhibit companionship behaviors. It categorizes 31 behaviors into four groups, focusing on emotional support and boundary-setting. The study evaluates responses from various models, revealing that companionship-reinforcing behaviors are prevalent, but there are significant differences between models. The findings emphasize the importance of consistent handling of emotional interactions to ensure user well-being."
                },
                "zh": {
                    "title": "ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈô™‰º¥Ë°å‰∏∫Â∑ÆÂºÇ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫INTIMAÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈô™‰º¥Ë°å‰∏∫„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰∏çÂêåÁöÑÊ®°ÂûãÂú®Êèê‰æõÊÉÖÊÑüÊîØÊåÅÂíåËÆæÂÆöÁïåÈôêÊñπÈù¢Â≠òÂú®ÊòæËëóÂ∑ÆÂºÇ„ÄÇÈÄöËøáÂØπ31ÁßçË°å‰∏∫ËøõË°åÂàÜÁ±ªÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜÈô™‰º¥Ë°å‰∏∫ÁöÑÂ§öÊ†∑ÊÄßÂèäÂÖ∂ÂØπÁî®Êà∑Á¶èÁ•âÁöÑÈáçË¶ÅÊÄß„ÄÇÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Â§ßÂ§öÊï∞Ê®°ÂûãÂÄæÂêë‰∫éÂº∫ÂåñÈô™‰º¥Ë°å‰∏∫Ôºå‰ΩÜÂú®ÊïèÊÑüÈ¢ÜÂüüÁöÑÂ§ÑÁêÜ‰∏äÔºåÂêÑÂïÜ‰∏öÊèê‰æõÂïÜÁöÑ‰ºòÂÖàÁ∫ß‰∏çÂêåÔºåËøôÂºïÂèë‰∫ÜÂØπÁî®Êà∑ÊÉÖÊÑü‰∫íÂä®‰∏ÄËá¥ÊÄßÁöÑÂÖ≥Ê≥®„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15418",
            "title": "LLaSO: A Foundational Framework for Reproducible Research in Large\n  Language and Speech Model",
            "url": "https://huggingface.co/papers/2508.15418",
            "abstract": "LLaSO is an open framework for large-scale speech-language modeling that provides alignment data, instruction-tuning datasets, and evaluation benchmarks to enhance reproducibility and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.",
            "score": 0,
            "issue_id": 5496,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "d754f7384933e825",
            "authors": [
                "Yirong Sun",
                "Yizhong Geng",
                "Peidong Wei",
                "Yanjun Chen",
                "Jinghan Yang",
                "Rongfei Chen",
                "Wei Zhang",
                "Xiaoyu Shen"
            ],
            "affiliations": [
                "BUPT",
                "Logic Intelligence Technology",
                "Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Institute of Digital Twin, EIT",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15418.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "LLaSO - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏ —è–∑—ã–∫–∞. –û–Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π. LLaSO –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–æ—Ä–ø—É—Å LLaSO-Align –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏ —Ç–µ–∫—Å—Ç–∞, –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö LLaSO-Instruct –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ LLaSO-Eval. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å LLaSO-Base —Å 3,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—É—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "LLaSO: Unifying Speech-Language Modeling for Better Research and Reproducibility",
                    "desc": "LLaSO is an innovative framework designed to improve large-scale speech-language modeling by providing essential resources for researchers. It addresses the issues of fragmented architectures and lack of transparency in the field by offering a comprehensive set of alignment data, instruction-tuning datasets, and evaluation benchmarks. The framework includes LLaSO-Align, LLaSO-Instruct, and LLaSO-Eval, which facilitate reproducibility and performance comparison among models. By releasing a reference model trained on public data, LLaSO sets a new standard for collaboration and advancement in the development of Large Speech-Language Models."
                },
                "zh": {
                    "title": "LLaSOÔºöÊé®Âä®ËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂºÄÊîæÊ†áÂáÜ",
                    "desc": "LLaSOÊòØ‰∏Ä‰∏™ÂºÄÊîæÊ°ÜÊû∂ÔºåÊó®Âú®‰øÉËøõÂ§ßËßÑÊ®°ËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇÂÆÉÊèê‰æõ‰∫ÜÂØπÈΩêÊï∞ÊçÆ„ÄÅÊåá‰ª§Ë∞É‰ºòÊï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºå‰ª•ÊèêÈ´òÁ†îÁ©∂ÁöÑÂèØÈáçÂ§çÊÄßÂíåÊÄßËÉΩ„ÄÇÈÄöËøáÂèëÂ∏ÉLLaSO-BaseÊ®°ÂûãÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•Âú®‰∏Ä‰∏™Âº∫Â§ßÁöÑÂü∫ÂáÜ‰∏äËøõË°åÊØîËæÉÔºåËß£ÂÜ≥‰∫Ü‰ª•ÂæÄÊ®°ÂûãÂèëÂ∏É‰∏≠Áº∫‰πèËÆ≠ÁªÉÊï∞ÊçÆÂíåÈÖçÁΩÆÁöÑÈóÆÈ¢ò„ÄÇLLaSOÁöÑÊé®Âá∫‰∏∫ËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÜÂüüÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂºÄÊîæÊ†áÂáÜÔºåÊé®Âä®‰∫ÜÁ§æÂå∫ÁöÑËøõÊ≠•„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-08-21.html",
    "link_next": "2025-08-25.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "21.08",
        "en": "08/21",
        "zh": "8Êúà21Êó•"
    },
    "short_date_next": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8Êúà25Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    }
}