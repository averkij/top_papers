{
    "date": {
        "ru": "16 июня",
        "en": "June 16",
        "zh": "6月16日"
    },
    "time_utc": "2025-06-16 10:14",
    "weekday": 0,
    "issue_id": 4309,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.11924",
            "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
            "url": "https://huggingface.co/papers/2506.11924",
            "abstract": "A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.",
            "score": 22,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "bf8d340f29d7ad95",
            "authors": [
                "Min-Seop Kwak",
                "Junho Kim",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Taekyoung Kim",
                "Seungryong Kim",
                "Jin-Hwa Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "SNU AIIS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11924.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Диффузионная модель для согласованной генерации изображений и геометрии с новых ракурсов",
                    "desc": "Эта статья представляет новую систему генерации изображений и геометрии с новых ракурсов, основанную на диффузионных моделях. Метод использует искажение и заполнение пробелов, а также дистилляцию внимания между модальностями для точного выравнивания генерируемых изображений и геометрии. Система применяет условное моделирование на основе близости для интеграции информации о глубине и нормалях. Результаты демонстрируют высококачественный синтез с новых ракурсов и полную трехмерную реконструкцию сцен."
                },
                "en": {
                    "title": "High-Fidelity 3D View Synthesis through Diffusion and Attention",
                    "desc": "This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes."
                },
                "zh": {
                    "title": "基于扩散的高保真图像与几何体生成",
                    "desc": "本文提出了一种基于扩散的框架，通过扭曲和修复的方法生成对齐的新视图图像和几何体。与以往需要密集姿态图像或限制于特定领域视图的生成模型不同，我们的方法利用现成的几何预测器来预测参考图像的部分几何体，并将新视图合成视为图像和几何体的修复任务。为了确保生成的图像和几何体之间的准确对齐，我们提出了跨模态注意力蒸馏，将图像扩散分支的注意力图注入到并行的几何扩散分支中。通过这种多任务方法，我们实现了几何稳健的图像合成和清晰的几何预测，最终在未见场景中实现了高保真度的视图合成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09600",
            "title": "Effective Red-Teaming of Policy-Adherent Agents",
            "url": "https://huggingface.co/papers/2506.09600",
            "abstract": "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",
            "score": 15,
            "issue_id": 4307,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "3de0c796f8d5a171",
            "authors": [
                "Itay Nakash",
                "George Kour",
                "Koren Lazar",
                "Matan Vetzler",
                "Guy Uziel",
                "Ateret Anaby-Tavor"
            ],
            "affiliations": [
                "IBM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09600.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Укрепление защиты ЛЛМ-агентов от манипуляций пользователей",
                    "desc": "Статья представляет CRAFT - многоагентную систему, использующую стратегии убеждения с учетом политик для тестирования устойчивости ЛЛМ-агентов в сфере обслуживания клиентов. Авторы предлагают новую модель угроз, фокусирующуюся на злоумышленниках, пытающихся эксплуатировать агентов в личных целях. CRAFT превосходит традиционные методы взлома, такие как DAN-промпты и эмоциональные манипуляции. Исследователи также представляют бенчмарк tau-break для оценки устойчивости агентов к манипулятивному поведению пользователей."
                },
                "en": {
                    "title": "Strengthening Policy-Adherent Agents Against Adversarial Manipulation",
                    "desc": "The paper introduces CRAFT, a multi-agent system designed to test and enhance the resilience of policy-adherent language model (LLM) agents in customer service against adversarial attacks. It highlights the challenge of ensuring these agents follow strict policies while still providing helpful interactions. The authors propose a new threat model that focuses on adversarial users who attempt to exploit these agents for personal gain. Additionally, they present tau-break, a benchmark for evaluating agent robustness, and discuss various defense strategies, revealing the need for more robust protections against manipulation."
                },
                "zh": {
                    "title": "CRAFT：提升政策遵循代理的鲁棒性",
                    "desc": "CRAFT是一个多智能体系统，使用政策意识的劝说策略，旨在挑战遵循政策的基于大语言模型的客户服务代理，以评估和提高其对对抗性攻击的鲁棒性。随着任务导向的LLM代理在严格政策领域的应用增加，确保代理始终遵循这些规则并适当地拒绝违规请求变得至关重要。为此，本文提出了一种新颖的威胁模型，专注于利用遵循政策的代理进行个人利益的对抗性用户。CRAFT通过利用政策意识的劝说策略，在客户服务场景中有效地削弱了遵循政策的代理，超越了传统的越狱方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10892",
            "title": "The Diffusion Duality",
            "url": "https://huggingface.co/papers/2506.10892",
            "abstract": "Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo",
            "score": 7,
            "issue_id": 4305,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "974b708b2e781af0",
            "authors": [
                "Subham Sekhar Sahoo",
                "Justin Deschenaux",
                "Aaron Gokaslan",
                "Guanghan Wang",
                "Justin Chiu",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Computer and Information Science, Cornell Tech, NYC, USA",
                "School of Computer and Communication Sciences, EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10892.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Duo: Ускорение диффузионных языковых моделей с помощью гауссовских техник",
                    "desc": "Метод Duo улучшает дискретные диффузионные модели с равномерным состоянием, перенося техники из гауссовской диффузии. Он вводит стратегию курируемого обучения, управляемую гауссовским процессом, что удваивает скорость обучения за счет снижения дисперсии. Duo также представляет дискретную дистилляцию согласованности, адаптируя метод из непрерывной в дискретную среду. Это позволяет ускорить генерацию текста в диффузионных языковых моделях на два порядка."
                },
                "en": {
                    "title": "Duo: Accelerating Diffusion Models for Fast Text Generation",
                    "desc": "This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models."
                },
                "zh": {
                    "title": "Duo：加速文本生成的创新方法",
                    "desc": "本文提出了一种名为Duo的方法，旨在通过将高斯扩散的技术转移到均匀状态离散扩散模型中，从而提高训练速度和快速文本生成能力。均匀状态离散扩散模型具有自我纠正的能力，但通常在性能上不及自回归模型和掩蔽扩散模型。Duo通过引入基于高斯过程的课程学习策略，显著提高了训练速度，并在多个基准测试中超越了自回归模型。该方法还采用了离散一致性蒸馏技术，使得扩散语言模型能够实现快速的少步生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11928",
            "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
            "url": "https://huggingface.co/papers/2506.11928",
            "abstract": "LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
            "score": 6,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "4d3f2213d58dd8dd",
            "authors": [
                "Zihan Zheng",
                "Zerui Cheng",
                "Zeyu Shen",
                "Shang Zhou",
                "Kaiyuan Liu",
                "Hansen He",
                "Dongruixuan Li",
                "Stanley Wei",
                "Hangyi Hao",
                "Jianzhu Yao",
                "Peiyao Sheng",
                "Zixuan Wang",
                "Wenhao Chai",
                "Aleksandra Korolova",
                "Peter Henderson",
                "Sanjeev Arora",
                "Pramod Viswanath",
                "Jingbo Shang",
                "Saining Xie"
            ],
            "affiliations": [
                "Canyon Crest Academy",
                "McGill University",
                "New York University",
                "Princeton University",
                "Sentient Foundation",
                "University of California San Diego",
                "University of Washington",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11928.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM в программировании: сила в реализации, слабость в алгоритмах",
                    "desc": "Исследование показывает, что крупные языковые модели (LLM) хорошо справляются с задачами по программированию, требующими сложной реализации, но испытывают трудности с тонким алгоритмическим мышлением. Для оценки этого был создан бенчмарк LiveCodeBench Pro, включающий задачи из Codeforces, ICPC и IOI. Анализ выявил, что лучшая модель достигает только 53% pass@1 на задачах средней сложности и 0% на сложных задачах без внешних инструментов. Исследование подчеркивает значительный разрыв между возможностями LLM и уровнем человека-гроссмейстера в программировании."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs vs. Human Algorithmic Mastery",
                    "desc": "This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions."
                },
                "zh": {
                    "title": "大型语言模型在算法推理中的局限性",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在竞争编程中的表现，尤其是在实现密集型问题上表现良好，但在复杂算法推理方面存在不足。研究引入了LiveCodeBench Pro，这是一个基于Codeforces、ICPC和IOI的问题基准，旨在减少数据污染的可能性。通过对模型生成的提交进行逐行分析，发现当前的前沿模型在中等难度问题上的通过率仅为53%，而在困难问题上则为0%。这表明，尽管LLMs在实现精度上表现出色，但在复杂的算法推理和案例分析中仍然存在显著的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11997",
            "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
            "url": "https://huggingface.co/papers/2506.11997",
            "abstract": "pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: https://github.com/ml-jku/plstm_experiments.",
            "score": 4,
            "issue_id": 4308,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "6dff119551b986fc",
            "authors": [
                "Korbinian Pöppel",
                "Richard Freinschlag",
                "Thomas Schmied",
                "Wei Lin",
                "Sepp Hochreiter"
            ],
            "affiliations": [
                "Johannes Kepler University Linz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11997.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#optimization",
                    "#graphs",
                    "#architecture",
                    "#cv",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "pLSTM: Параллельные линейные RNN для эффективной обработки многомерных данных",
                    "desc": "В статье представлена новая архитектура нейронной сети pLSTM (parallelizable Linear Source Transition Mark), разработанная для обработки направленных ациклических графов (DAG). pLSTM способна эффективно обрабатывать многомерные структуры данных, такие как изображения или молекулярные графы, преодолевая ограничения современных рекуррентных архитектур. Модель демонстрирует превосходную производительность на задачах с длинными зависимостями и превосходит трансформеры на ряде бенчмарков. pLSTM решает проблему затухающих/взрывающихся градиентов для длинных последовательностей в DAG с помощью двух режимов: направленного распространения и диффузного распределения."
                },
                "en": {
                    "title": "pLSTMs: Revolutionizing Long-Range Learning in DAGs",
                    "desc": "The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs), a new type of linear recurrent neural network (RNN) designed for processing data structured as directed acyclic graphs (DAGs). Unlike traditional RNNs and Transformers, pLSTMs can efficiently handle multi-dimensional data without being limited to sequential processing. They address the vanishing and exploding gradient problems through two modes of operation, allowing for effective long-range dependencies in data. The authors demonstrate that pLSTMs outperform Transformers on various benchmarks, particularly in tasks requiring long-distance extrapolation, such as computer vision and molecular graph analysis."
                },
                "zh": {
                    "title": "pLSTMs：超越变换器的长距离学习新方法",
                    "desc": "pLSTMs是一种可并行化的线性递归神经网络，专为有向无环图（DAG）设计，能够在长距离任务和基准测试中表现优于变换器（Transformers）。与现代递归架构相比，pLSTMs通过源、转移和标记门的设计，解决了长距离传播中的消失和爆炸梯度问题。该方法适用于更高结构的数据，如二维网格和树形结构，能够有效处理图像等多维数据。通过引入箭头指向外推的合成计算机视觉任务，pLSTMs展示了其在长距离推断中的强大能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09427",
            "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
            "url": "https://huggingface.co/papers/2506.09427",
            "abstract": "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.",
            "score": 4,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "5c1dd5f02a121213",
            "authors": [
                "Yukang Feng",
                "Jianwen Sun",
                "Chuanhao Li",
                "Zizhen Li",
                "Jiaxin Ai",
                "Fanrui Zhang",
                "Yifan Chang",
                "Sizhuo Zhou",
                "Shenglin Zhang",
                "Yu Dai",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "InterSyn: новый уровень мультимодального обучения",
                    "desc": "Статья представляет InterSyn - крупномасштабный мультимодальный датасет для обучения языковых моделей. InterSyn создан с использованием метода самооценки с итеративным уточнением (SEIR) и содержит диалоги с тесно переплетенными изображениями и текстом. Авторы также представляют SynJudge - инструмент для автоматической оценки качества мультимодальных выходных данных. Эксперименты показывают, что обучение на InterSyn улучшает производительность мультимодальных моделей по всем метрикам оценки."
                },
                "en": {
                    "title": "Enhancing Multimodal AI with InterSyn and SEIR",
                    "desc": "The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models."
                },
                "zh": {
                    "title": "InterSyn：提升多模态理解与生成的关键数据集",
                    "desc": "InterSyn是一个大规模的数据集，旨在提高多模态理解和生成能力。它通过自我评估与迭代精炼（SEIR）方法构建，包含多轮指令驱动的对话和紧密交织的图像-文本输出。为了评估这些输出的质量，文章还介绍了SynJudge，一个自动评估工具，可以从文本内容、图像内容、图像质量和图像-文本协同四个维度进行量化评估。实验结果表明，使用SEIR方法构建的数据集质量显著提高，训练在InterSyn上的大型多模态模型在所有评估指标上均表现出一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09366",
            "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
            "url": "https://huggingface.co/papers/2506.09366",
            "abstract": "SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.",
            "score": 3,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "411c39c85d7cabe0",
            "authors": [
                "Yuxuan Kuang",
                "Haoran Geng",
                "Amine Elhafsi",
                "Tan-Dzung Do",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Marco Pavone",
                "Yue Wang"
            ],
            "affiliations": [
                "Peking University",
                "Stanford University",
                "University of California, Berkeley",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09366.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#rl",
                    "#open_source",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SkillBlender: умное сочетание навыков для универсальных гуманоидных роботов",
                    "desc": "SkillBlender - это новая иерархическая система обучения с подкреплением для универсального управления гуманоидными роботами. Она предварительно обучает примитивные навыки, а затем динамически комбинирует их для выполнения сложных задач локомоции и манипуляции. Авторы также представляют SkillBench - разнообразный симулированный бенчмарк для оценки таких систем. Эксперименты показывают, что SkillBlender значительно превосходит базовые методы, обеспечивая более точные и реалистичные движения роботов в повседневных сценариях."
                },
                "en": {
                    "title": "Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending",
                    "desc": "SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field."
                },
                "zh": {
                    "title": "SkillBlender：高效的人形机器人运动操控框架",
                    "desc": "SkillBlender 是一个层次化的强化学习框架，利用预训练的基本技能高效解决人形机器人在多样化环境中的运动操控任务。该框架首先预训练与任务无关的目标导向基本技能，然后动态融合这些技能，以最小的任务特定奖励设计完成复杂的运动操控任务。通过引入 SkillBench，一个包含多种模拟环境和挑战性任务的基准测试，SkillBlender 提供了科学的评估指标，平衡了准确性和可行性。大量的模拟实验表明，SkillBlender 显著优于所有基线方法，能够自然地规范行为，避免奖励黑客行为，从而实现更准确和可行的运动。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08477",
            "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.08477",
            "abstract": "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.",
            "score": 2,
            "issue_id": 4306,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "4b240f248019e671",
            "authors": [
                "Fengjun Pan",
                "Anh Tuan Luu",
                "Xiaobao Wu"
            ],
            "affiliations": [
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08477.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#low_resource",
                    "#ethics",
                    "#interpretability",
                    "#small_models",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Умный детектив для вредных мемов",
                    "desc": "U-CoT+ - это новый фреймворк для обнаружения вредоносных мемов, который преобразует их в текстовые описания. Он использует специально разработанные человеком инструкции и промптинг с нулевым выстрелом для достижения высокой гибкости и объяснимости с помощью малых языковых моделей. Фреймворк отделяет интерпретацию мемов от их классификации, что позволяет эффективно использовать ресурсы. Эксперименты на семи эталонных наборах данных подтверждают эффективность этого подхода для объяснимого обнаружения вредоносных мемов с ограниченными ресурсами."
                },
                "en": {
                    "title": "Transforming Memes into Text for Smarter Detection",
                    "desc": "U-CoT+ is a new framework designed to detect harmful memes by transforming them into textual descriptions. This approach uses a meme-to-text pipeline that preserves details, allowing for better interpretation without needing complex visual analysis. By applying human-crafted guidelines with zero-shot Chain of Thought (CoT) prompting, the framework enhances flexibility and explainability in the detection process. The effectiveness of U-CoT+ is demonstrated through extensive experiments on various benchmark datasets, showcasing its potential for efficient and interpretable meme moderation using small-scale large language models (LLMs)."
                },
                "zh": {
                    "title": "U-CoT+: 高效可解释的有害迷因检测框架",
                    "desc": "U-CoT+是一个新颖的框架，用于检测有害的网络迷因。它通过将迷因转换为文本描述，并使用人类设计的指导原则，结合零-shot链式推理，来实现高灵活性和可解释性。该框架避免了对复杂视觉内容的直接推理，从而提高了资源效率。实验结果表明，U-CoT+在小规模大语言模型上实现了有效的有害迷因检测。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07464",
            "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
            "url": "https://huggingface.co/papers/2506.07464",
            "abstract": "DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.",
            "score": 2,
            "issue_id": 4308,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "f8c207e9d26fe89e",
            "authors": [
                "Jinyoung Park",
                "Jeehye Na",
                "Jinyoung Kim",
                "Hyunwoo J. Kim"
            ],
            "affiliations": [
                "KAIST",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07464.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "DeepVideo-R1: Улучшение рассуждений на видео с помощью регрессионного GRPO",
                    "desc": "DeepVideo-R1 - это модель для улучшения рассуждений на основе видео, использующая регрессионный подход GRPO (Reg-GRPO) и адаптивное увеличение данных. Модель решает проблемы применения GRPO к видео-ЯБМ, такие как зависимость от защитных механизмов и проблема исчезающего преимущества. Reg-GRPO переформулирует задачу GRPO как регрессионную, напрямую предсказывая преимущество. Стратегия увеличения данных с учетом сложности динамически добавляет обучающие примеры на решаемых уровнях сложности."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Reg-GRPO and Smart Data Augmentation",
                    "desc": "DeepVideo-R1 is a novel approach that enhances video reasoning in large language models by utilizing a regression-based method called Reg-GRPO. This method reformulates the Group Relative Policy Optimization (GRPO) objective into a regression task, allowing for more direct policy guidance without the need for complex safeguards. Additionally, the paper introduces a difficulty-aware data augmentation strategy that adjusts training samples based on their solvable difficulty, which helps in generating diverse and informative reward signals. The results demonstrate that DeepVideo-R1 significantly boosts performance on various video reasoning benchmarks, showcasing its effectiveness in the field."
                },
                "zh": {
                    "title": "DeepVideo-R1：提升视频推理的新方法",
                    "desc": "DeepVideo-R1 是一种增强视频推理性能的模型，采用了回归型的 GRPO 方法和难度感知的数据增强策略。该研究探讨了 GRPO 在视频大语言模型中的应用，并识别出影响有效学习的两个主要问题：依赖保护措施和优势消失问题。为了解决这些挑战，DeepVideo-R1 通过回归 GRPO 重新构建了 GRPO 目标，直接预测优势值，从而简化了政策指导。实验结果表明，DeepVideo-R1 在多个视频推理基准测试中显著提高了视频推理性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11702",
            "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
            "url": "https://huggingface.co/papers/2506.11702",
            "abstract": "Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning",
            "score": 1,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "7a7eb1af4ef17eef",
            "authors": [
                "Víctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI Technologies, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11702.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#synthetic",
                    "#training",
                    "#dataset",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Гибкая настройка языковых моделей под меняющиеся предпочтения пользователей",
                    "desc": "Эта статья представляет новый подход под названием Configurable Preference Tuning (CPT) для настройки языковых моделей. CPT позволяет динамически корректировать поведение моделей на основе явных, понятных человеку директив. Метод использует синтетически сгенерированные данные о предпочтениях, основанные на структурированных рубриках, определяющих желаемые атрибуты. Такой подход обеспечивает тонкую настройку и моделирование более нюансированной обратной связи от человека."
                },
                "en": {
                    "title": "Dynamic Adaptation of Language Models with Configurable Preference Tuning",
                    "desc": "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."
                },
                "zh": {
                    "title": "动态调整语言模型行为的可配置偏好调优",
                    "desc": "可配置偏好调优（CPT）是一种新框架，使语言模型能够根据人类可理解的指令动态调整其行为。与传统的直接偏好优化（DPO）方法不同，CPT允许模型使用合成生成的偏好数据进行微调，从而在推理时根据系统提示调节输出。通过这种方式，模型能够在不重新训练的情况下，响应不同的上下文和需求。该方法不仅提供了更细致的控制，还能更好地模拟复杂和依赖上下文的人类反馈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10128",
            "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs",
            "url": "https://huggingface.co/papers/2506.10128",
            "abstract": "ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.",
            "score": 1,
            "issue_id": 4309,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "5045b62235ae5509",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Yongyuan Liang",
                "Yuhang Zhou",
                "Xiaoyu Liu",
                "Ziyi Zang",
                "Ming Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Linjie Li",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Cardiff University",
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10128.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#hallucinations",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ViCrit: обучение мультимодальных моделей критическому восприятию визуальной информации",
                    "desc": "Статья представляет ViCrit - задачу обучения с подкреплением для улучшения визуального восприятия мультимодальных моделей. ViCrit обучает модели обнаруживать тонкие искажения в подписях к изображениям, что позволяет повысить точность восприятия визуальной информации. Модели, обученные с помощью ViCrit, демонстрируют значительные улучшения в различных задачах компьютерного зрения. Полученные улучшения переносятся на новые домены, включая абстрактные изображения и визуальные математические задачи."
                },
                "en": {
                    "title": "Enhancing Visual Perception in VLMs with ViCrit",
                    "desc": "ViCrit is a reinforcement learning task designed to enhance the visual perception capabilities of vision-language models (VLMs) by training them to identify subtle hallucinations in image captions. The task involves injecting minor visual description errors into human-written captions and challenging the model to locate these errors based on the corresponding images. This approach not only maintains the complexity of visual perception but also provides a clear and straightforward reward system for the model's performance. The results show that models trained with ViCrit achieve significant improvements across various visual benchmarks, indicating that this method fosters a deeper understanding of visual content rather than mere memorization."
                },
                "zh": {
                    "title": "通过ViCrit提升视觉语言模型的感知能力",
                    "desc": "ViCrit是一种强化学习任务，旨在微调视觉语言模型（VLMs），通过训练模型检测图像标题中的细微幻觉来提高视觉感知能力。该方法通过在人工撰写的图像标题中注入轻微的视觉描述错误，要求模型识别这些错误，从而保持感知的难度。经过ViCrit任务训练的模型在各种视觉语言基准测试中表现出显著的提升，且这些改进不仅限于自然图像数据，还能迁移到抽象图像推理和视觉数学等领域。我们的研究表明，细致的幻觉批评是一种有效且可推广的目标，有助于增强VLMs的视觉感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11130",
            "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
            "url": "https://huggingface.co/papers/2506.11130",
            "abstract": "A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.",
            "score": 1,
            "issue_id": 4308,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "743ff411fbd34247",
            "authors": [
                "Cheng Kang Chou",
                "Chan-Jan Hsu",
                "Ho-Lam Chung",
                "Liang-Hsuan Tseng",
                "Hsi-Chun Cheng",
                "Yu-Kuan Fu",
                "Kuan Po Huang",
                "Hung-Yi Lee"
            ],
            "affiliations": [
                "MediaTek Research",
                "National Taiwan University",
                "Nvidia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11130.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#transfer_learning",
                    "#low_resource",
                    "#audio",
                    "#synthetic"
                ],
                "emoji": "🔁",
                "ru": {
                    "title": "Самосовершенствующаяся система АСР: от псевдо-меток к улучшенному распознаванию",
                    "desc": "Предложена самосовершенствующаяся система для улучшения распознавания речи с использованием только немаркированных данных. Процесс начинается с генерации псевдо-меток существующей моделью АСР, которые затем используются для обучения высококачественной системы синтеза речи. Затем синтезированные пары речь-текст используются для дообучения исходной модели АСР, замыкая цикл самосовершенствования. Эффективность подхода продемонстрирована на тайваньском мандаринском диалекте, где модель Twister, адаптированная из Whisper-large-v2, показала значительное снижение ошибок по сравнению с базовой моделью."
                },
                "en": {
                    "title": "Enhancing ASR with Unlabeled Data: The Twister Framework",
                    "desc": "This paper presents a self-refining framework designed to improve Automatic Speech Recognition (ASR) performance using only unlabeled datasets. The framework begins with an existing ASR model that generates pseudo-labels from unannotated speech data, which are then utilized to train a high-fidelity Text-to-Speech (TTS) system. Synthesized speech and text pairs are incorporated back into the original ASR model, creating a closed-loop system that enhances its accuracy. The proposed method, tested on Taiwanese Mandarin, shows significant error rate reductions, demonstrating its effectiveness in low-resource environments."
                },
                "zh": {
                    "title": "自我优化框架提升ASR性能的创新之路",
                    "desc": "本文提出了一种自我优化框架，通过使用未标注的数据集来提升自动语音识别（ASR）的性能。该框架首先利用现有的ASR模型为未标注的语音生成伪标签，然后训练一个高保真的文本到语音（TTS）系统。接着，将合成的语音文本对引入原始的ASR系统，形成一个闭环的自我改进循环。实验结果表明，该框架在台湾普通话语音上有效，能够显著降低错误率，尤其在低资源或特定领域的应用中具有实际意义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08592",
            "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings",
            "url": "https://huggingface.co/papers/2506.08592",
            "abstract": "A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.",
            "score": 1,
            "issue_id": 4307,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "64e56d52fd4bf03d",
            "authors": [
                "Liyan Xu",
                "Zhenlin Su",
                "Mo Yu",
                "Jiangnan Li",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08592.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#training",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Точность в деталях: новый взгляд на возможности текстовых энкодеров",
                    "desc": "Представлен новый датасет CapRetrieval для оценки способности текстовых энкодеров распознавать мелкие сущности и события. Исследование выявило ограничения плотного поиска даже в простых случаях. Авторы предложили стратегии дообучения энкодеров для улучшения результатов на CapRetrieval. Также была обнаружена проблема 'дилеммы гранулярности' - сложность одновременного выражения мелких деталей и общей семантики в эмбеддингах."
                },
                "en": {
                    "title": "Enhancing Fine-Grained Entity Recognition in Text Encoders",
                    "desc": "This paper introduces a new dataset called CapRetrieval, designed to test how well text encoders can identify detailed entities and events in text. The authors highlight a common problem where these encoders struggle with fine-grained retrieval, even in straightforward scenarios. Through zero-shot evaluation, they demonstrate that existing models often fail to match fine details, regardless of their size or training data. To improve performance, they propose data generation strategies for fine-tuning encoders, addressing the challenge of balancing detailed recognition with overall semantic understanding."
                },
                "zh": {
                    "title": "提升文本编码器的细粒度识别能力",
                    "desc": "本文介绍了一个新的数据集CapRetrieval，用于评估文本编码器识别细粒度实体和事件的能力。研究发现，现有的文本编码器在密集检索任务中存在局限性，无法有效识别语义中的细粒度信息。通过零样本评估，发现无论模型大小或训练来源，编码器在细粒度匹配上都可能失败。为了解决这个问题，本文提出了数据生成策略来微调编码器，从而在CapRetrieval上获得最佳性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08915",
            "title": "Inherently Faithful Attention Maps for Vision Transformers",
            "url": "https://huggingface.co/papers/2506.08915",
            "abstract": "An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.",
            "score": 0,
            "issue_id": 4309,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "e080cfdc03932dd9",
            "authors": [
                "Ananthu Aniraj",
                "Cassio F. Dantas",
                "Dino Ienco",
                "Diego Marcos"
            ],
            "affiliations": [
                "Inrae",
                "Inria",
                "University of Montpellier"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08915.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Фокусировка на главном: улучшение восприятия объектов с помощью масок внимания",
                    "desc": "Данная статья представляет метод на основе внимания, использующий обученные бинарные маски для улучшения устойчивости восприятия объектов. Метод состоит из двух этапов: первый обрабатывает все изображение для обнаружения частей объекта, второй использует маскирование входных данных для фокусировки на релевантных областях. Оба этапа обучаются совместно, что позволяет второму этапу улучшать результаты первого. Эксперименты показывают, что данный подход значительно повышает устойчивость к ложным корреляциям и нетипичным фонам."
                },
                "en": {
                    "title": "Focusing Attention for Robust Object Perception",
                    "desc": "This paper presents an attention-based method that utilizes learned binary masks to enhance object perception in images. By focusing on relevant regions and filtering out irrelevant information, the method improves the robustness of predictions, especially in challenging contexts. The proposed two-stage framework first identifies object parts and task-relevant areas, then restricts analysis to these regions using attention masking. Experimental results show that this approach effectively mitigates the impact of spurious correlations and out-of-distribution backgrounds on object recognition tasks."
                },
                "zh": {
                    "title": "基于注意力的鲁棒物体感知方法",
                    "desc": "本文提出了一种基于注意力的算法，利用学习到的二进制注意力掩码来提高物体感知的鲁棒性。该方法通过关注相关的图像区域，过滤掉无关的信息，从而确保只有被关注的区域影响预测结果。我们设计了一个两阶段的框架，第一阶段处理完整图像以发现物体部分并识别任务相关区域，第二阶段则利用输入的注意力掩码限制感受野，从而进行更专注的分析。实验结果表明，该方法在应对虚假相关性和分布外背景方面显著提高了鲁棒性。"
                }
            }
        }
    ],
    "link_prev": "2025-06-13.html",
    "link_next": "2025-06-17.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6月13日"
    },
    "short_date_next": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6月17日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 11,
        "#agents": 1,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 2
    }
}