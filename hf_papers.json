{
    "date": {
        "ru": "24 января",
        "en": "January 24",
        "zh": "1月24日"
    },
    "time_utc": "2025-01-24 03:12",
    "weekday": 4,
    "issue_id": 1841,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.13926",
            "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
            "url": "https://huggingface.co/papers/2501.13926",
            "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
            "score": 1,
            "issue_id": 1841,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "61611cbe661736ff",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Chengzhuo Tong",
                "Zhizheng Zhao",
                "Peng Gao",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "MMLab",
                "MiuLar Lab",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13926.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Рассуждения по цепочке мыслей открывают новые горизонты в генерации изображений",
                    "desc": "Статья исследует применение рассуждений по цепочке мыслей (Chain-of-Thought) для улучшения автореграссивной генерации изображений. Авторы предлагают три метода: масштабирование вычислений во время тестирования, оптимизацию предпочтений модели и интеграцию этих техник. Они также представляют новые модели вознаграждения PARM и PARM++, специально разработанные для генерации изображений. Результаты показывают значительное улучшение производительности базовой модели Show-o на 24% по сравнению с эталоном GenEval."
                },
                "en": {
                    "title": "Enhancing Image Generation with Chain-of-Thought Reasoning",
                    "desc": "This paper explores the use of Chain-of-Thought (CoT) reasoning to improve autoregressive image generation models. It investigates three main techniques: enhancing verification through increased computation, aligning model preferences using Direct Preference Optimization (DPO), and combining these methods for better outcomes. The authors introduce the Potential Assessment Reward Model (PARM) and its enhanced version PARM++, which help assess and correct image generation steps. The results show a significant performance boost, achieving a 24% improvement on the GenEval benchmark compared to previous models."
                },
                "zh": {
                    "title": "链式思维提升图像生成性能",
                    "desc": "本文探讨了链式思维（CoT）推理在自回归图像生成中的应用潜力。我们提出了三种技术：测试时计算的扩展、与直接偏好优化（DPO）对齐模型偏好，以及这些技术的整合。研究结果表明，这些方法可以有效结合，显著提升图像生成性能。此外，我们提出了潜力评估奖励模型（PARM）和PARM++，专门用于自回归图像生成，进一步提高了生成质量。"
                }
            }
        }
    ],
    "link_prev": "2025-01-23.html",
    "link_next": "2025-01-27.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1月23日"
    },
    "short_date_next": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了两种推理模型：DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 通过大规模强化学习训练，展示了出色的推理能力，但存在可读性差和语言混合的问题。为了解决这些问题，我们开发了 DeepSeek-R1，它在强化学习之前进行多阶段训练和冷启动数据处理。DeepSeek-R1 在推理任务上的表现与 OpenAI-o1-1217 相当。我们开源了这些模型和六个基于 Qwen 和 Llama 的压缩模型。",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "pinyin": "我们介绍了两种推理模型：DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 通过大规模强化学习训练，展示了出色的推理能力，但存在可读性差和语言混合的问题。为了解决这些问题，我们开发了 DeepSeek-R1，它在强化学习之前进行多阶段训练和冷启动数据处理。DeepSeek-R1 在推理任务上的表现与 OpenAI-o1-1217 相当。我们开源了这些模型和六个基于 Qwen 和 Llama 的压缩模型。\n\nWǒmen jièshào le liǎng zhǒng tuīlǐ móxíng: DeepSeek-R1-Zero hé DeepSeek-R1. DeepSeek-R1-Zero tōngguò dà guīmó qiángzhù xuéxí xùnliàn, zhǎnshì le chūsè de tuīlǐ nénglì, dàn cúnzài kědúxìng chà hé yǔyán hùnhé de wèntí. Wèile jiějué zhèxiē wèntí, wǒmen kāifā le DeepSeek-R1, tā zài qiángzhù xuéxí zhīqián jìnxíng duō jiēduàn xùnliàn hé lěng qǐdòng shùjù chǔlǐ. DeepSeek-R1 zài tuīlǐ rènwù shàng de biǎoxiàn yǔ OpenAI-o1-1217 xiāngdāng. Wǒmen kāiyuán le zhèxiē móxíng hé liù gè jīyú Qwen hé Llama de yāsuō móxíng.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '强化学习', 'pinyin': 'qiáng​huà​xué​xí', 'trans': 'reinforcement learning'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'}, {'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'}, {'word': '可读性', 'pinyin': 'kě​dú​xìng', 'trans': 'readability'}, {'word': '语言混合', 'pinyin': 'yǔ​yán​hùn​hé', 'trans': 'language mixing'}, {'word': '开发', 'pinyin': 'kāifā', 'trans': 'develop'}, {'word': '多阶段', 'pinyin': 'duō​jiē​duàn', 'trans': 'multi-stage'}, {'word': '冷启动', 'pinyin': 'lěng​qǐ​dòng', 'trans': 'cold start'}, {'word': '数据处理', 'pinyin': 'shù​jù​chǔ​lǐ', 'trans': 'data processing'}, {'word': '表现', 'pinyin': 'biǎo​xiàn', 'trans': 'performance'}, {'word': '相当', 'pinyin': 'xiāng​dāng', 'trans': 'equivalent'}, {'word': '开源', 'pinyin': 'kāi​yuán', 'trans': 'open source'}, {'word': '基于', 'pinyin': 'jī​yú', 'trans': 'based on'}, {'word': '压缩', 'pinyin': 'yā​suō', 'trans': 'compression'}]",
        "trans": "We introduced two reasoning models: DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, trained through large-scale reinforcement learning, demonstrated excellent reasoning capabilities but suffered from poor readability and language mixing issues. To address these problems, we developed DeepSeek-R1, which undergoes multi-stage training and cold start data processing before reinforcement learning. DeepSeek-R1 performs comparably to OpenAI-o1-1217 in reasoning tasks. We have open-sourced these models along with six compressed models based on Qwen and Llama.",
        "update_ts": "2025-01-23 09:10"
    }
}