{
    "date": {
        "ru": "8 июля",
        "en": "July 8",
        "zh": "7月8日"
    },
    "time_utc": "2025-07-08 13:29",
    "weekday": 1,
    "issue_id": 4703,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.03724",
            "title": "MemOS: A Memory OS for AI System",
            "url": "https://huggingface.co/papers/2507.03724",
            "abstract": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.",
            "score": 70,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "5a64c779be945671",
            "authors": [
                "Zhiyu Li",
                "Shichao Song",
                "Chenyang Xi",
                "Hanyu Wang",
                "Chen Tang",
                "Simin Niu",
                "Ding Chen",
                "Jiawei Yang",
                "Chunyu Li",
                "Qingchen Yu",
                "Jihao Zhao",
                "Yezhaohui Wang",
                "Peng Liu",
                "Zehao Lin",
                "Pengyuan Wang",
                "Jiahao Huo",
                "Tianyi Chen",
                "Kai Chen",
                "Kehang Li",
                "Zhen Tao",
                "Junpeng Ren",
                "Huayi Lai",
                "Hao Wu",
                "Bo Tang",
                "Zhenren Wang",
                "Zhaoxin Fan",
                "Ningyu Zhang",
                "Linfeng Zhang",
                "Junchi Yan",
                "Mingchuan Yang",
                "Tong Xu",
                "Wei Xu",
                "Huajun Chen",
                "Haofeng Wang",
                "Hongkang Yang",
                "Wentao Zhang",
                "Zhi-Qin John Xu",
                "Siheng Chen",
                "Feiyu Xiong"
            ],
            "affiliations": [
                "Beihang University",
                "Institute for Advanced Algorithms Research, Shanghai",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Peking University",
                "Renmin University of China",
                "Research Institute of China Telecom",
                "Shanghai Jiao Tong University",
                "Tongji University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03724.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#rag",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MemOS: операционная система памяти для более умных и адаптивных языковых моделей",
                    "desc": "MemOS - это операционная система памяти для больших языковых моделей (LLM), предложенная для улучшения управления памятью. Она позволяет эффективно хранить и извлекать информацию, а также способствует непрерывному обучению и персонализированному моделированию. MemOS вводит концепцию MemCube как базовой единицы памяти, содержащей как контент, так и метаданные. Система объединяет представление, планирование и эволюцию различных типов памяти, обеспечивая гибкость и эффективность работы LLM."
                },
                "en": {
                    "title": "MemOS: Revolutionizing Memory Management for LLMs",
                    "desc": "MemOS is a proposed memory operating system designed to improve memory management in Large Language Models (LLMs). It addresses the limitations of existing models that rely on static parameters and short-term context by introducing a structured memory layer that enhances storage and retrieval capabilities. The system utilizes MemCubes, which encapsulate memory content along with metadata, allowing for flexible transitions between different memory types. This approach not only increases computational efficiency but also supports continual learning and personalized modeling by managing knowledge across various temporal scales."
                },
                "zh": {
                    "title": "MemOS：为大型语言模型提供智能内存管理",
                    "desc": "MemOS是一种为大型语言模型（LLMs）设计的内存操作系统，旨在改善内存管理。它通过统一表示、调度和演变不同类型的内存，支持高效的存储和检索。MemOS引入了MemCube作为基本单元，封装了内存内容和元数据，允许灵活的内存类型转换。该系统为LLMs提供了可控性、可塑性和可演化性，促进了持续学习和个性化建模。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00994",
            "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
            "url": "https://huggingface.co/papers/2507.00994",
            "abstract": "Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.",
            "score": 43,
            "issue_id": 4700,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "3445ad02ac25de31",
            "authors": [
                "Hippolyte Gisserot-Boukhlef",
                "Nicolas Boizard",
                "Manuel Faysse",
                "Duarte M. Alves",
                "Emmanuel Malherbe",
                "André F. T. Martins",
                "Céline Hudelot",
                "Pierre Colombo"
            ],
            "affiliations": [
                "Artefact Research Center",
                "Diabolocom",
                "Equall",
                "Illuin Technology",
                "Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
                "Instituto de Telecomunicações",
                "MICS, CentraleSupélec, Université Paris-Saclay",
                "Unbabel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00994.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#optimization",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация предобучения энкодеров: синергия CLM и MLM",
                    "desc": "Исследование сравнивает эффективность методов предобучения энкодеров на основе маскированного языкового моделирования (MLM) и каузального языкового моделирования (CLM). Авторы проводят масштабные эксперименты с моделями разного размера, оценивая их производительность на задачах представления текста. Результаты показывают, что MLM в целом дает лучшие результаты, но CLM более эффективен с точки зрения данных и стабильности дообучения. Предлагается двухфазная стратегия обучения, сочетающая CLM и MLM, которая достигает оптимальной производительности при фиксированном вычислительном бюджете."
                },
                "en": {
                    "title": "Unlocking Text Representation: CLM Meets MLM for Optimal Performance",
                    "desc": "This paper investigates the effectiveness of Causal Language Modeling (CLM) compared to traditional Masked Language Modeling (MLM) for training text representations in natural language processing (NLP). The authors conduct extensive experiments with various model sizes and training strategies, revealing that while MLM generally performs better, CLM models are more efficient with data and offer greater stability during fine-tuning. They propose a biphasic training approach that first uses CLM and then MLM, which optimizes performance within a limited computational budget. Additionally, leveraging existing pretrained CLM models can significantly reduce the resources needed to achieve high-quality encoder models."
                },
                "zh": {
                    "title": "双相训练策略：CLM与MLM的最佳结合",
                    "desc": "本文探讨了文本表示学习在自然语言处理中的重要性。研究表明，使用因果语言模型（CLM）预训练的解码器模型可以有效地作为编码器，且在文本表示基准测试中常常超越传统的编码器。尽管使用掩码语言模型（MLM）训练通常在文本表示任务中表现更好，但CLM训练的模型在数据效率和微调稳定性方面表现更佳。我们提出了一种双相训练策略，先应用CLM再应用MLM，在固定的计算预算下实现最佳性能，并且在使用现有的预训练CLM模型初始化时，进一步降低了训练顶级编码器模型的计算负担。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05163",
            "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
            "url": "https://huggingface.co/papers/2507.05163",
            "abstract": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.",
            "score": 31,
            "issue_id": 4693,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "e1f4c8e83495db53",
            "authors": [
                "Yutian Chen",
                "Shi Guo",
                "Tianshuo Yang",
                "Lihe Ding",
                "Xiuyuan Yu",
                "Jinwei Gu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "NVIDIA",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05163.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Высокоскоростная 4D-съемка обычными камерами",
                    "desc": "Предлагается система высокоскоростной 4D-съемки с использованием камер с низкой частотой кадров и асинхронным захватом. Система повышает эффективную частоту кадров до 100-200 FPS путем смещения времени начала съемки для разных камер. Для устранения артефактов, вызванных реконструкцией по малому числу ракурсов, применяется генеративная модель на основе видео-диффузии. Экспериментальные результаты показывают значительное улучшение качества высокоскоростной 4D-реконструкции по сравнению с синхронной съемкой."
                },
                "en": {
                    "title": "Revolutionizing 4D Capture: High-Speed Reconstruction with Low FPS Cameras",
                    "desc": "This paper presents a novel system for capturing high-speed 4D scenes using low frame rate cameras. It introduces an asynchronous capture technique that effectively increases the frame rate by staggering the start times of multiple cameras, achieving rates of 100-200 FPS from a base of 25 FPS. Additionally, the authors propose a video-diffusion-based generative model to correct artifacts in the sparse 4D reconstruction, ensuring better detail and temporal consistency. Experimental results show that this approach significantly improves the quality of high-speed 4D reconstructions compared to traditional synchronous methods."
                },
                "zh": {
                    "title": "低帧率相机实现高速度4D重建的创新方案",
                    "desc": "本研究提出了一种高速度的4D捕捉系统，利用低帧率相机进行异步捕捉和视频扩散基础的伪影修正，从而提高重建质量。传统的4D捕捉系统通常帧率低于30 FPS，直接从低帧率输入进行高速度运动的4D重建会导致不理想的结果。我们的方法通过异步捕捉方案，将相机的启动时间错开，提升了有效帧率，达到100-200 FPS的效果。处理方面，我们提出了一种基于视频扩散的生成模型，修复4D稀疏视图重建中产生的伪影，显著改善了重建的细节和时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04447",
            "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge",
            "url": "https://huggingface.co/papers/2507.04447",
            "abstract": "DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.",
            "score": 28,
            "issue_id": 4696,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "8fbe1a8248768baa",
            "authors": [
                "Wenyao Zhang",
                "Hongsi Liu",
                "Zekun Qi",
                "Yunnan Wang",
                "XinQiang Yu",
                "Jiazhao Zhang",
                "Runpei Dong",
                "Jiawei He",
                "He Wang",
                "Zhizheng Zhang",
                "Li Yi",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "affiliations": [
                "EIT",
                "Galbot",
                "PKU",
                "SJTU",
                "THU",
                "UIUC",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04447.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#agents",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "DreamVLA: Интеллектуальное планирование действий робота на основе комплексного анализа окружающего мира",
                    "desc": "DreamVLA - это новая система управления роботами, использующая комплексный подход к прогнозированию и планированию действий. Она объединяет зрение, язык и действие (VLA) с прогнозированием знаний о мире для улучшения манипуляций робота. Система использует механизм внимания для разделения динамической, пространственной и семантической информации. DreamVLA применяет диффузионный трансформер для моделирования распределения будущих действий робота."
                },
                "en": {
                    "title": "Enhancing Robot Manipulation with DreamVLA: Clear Action Planning through World Knowledge",
                    "desc": "DreamVLA is a new framework designed to enhance robot manipulation by integrating world knowledge with a vision-language-action (VLA) model. It addresses the limitations of existing methods by providing a clear and organized representation of dynamic, spatial, and semantic information, which is crucial for effective action planning. The framework uses a dynamic-region-guided approach to predict world knowledge, allowing robots to form abstract reasoning chains before executing actions. Additionally, a diffusion-based transformer is employed to ensure that action representations remain distinct and free from interference during training, leading to improved performance in both real-world and simulated tasks."
                },
                "zh": {
                    "title": "DreamVLA：提升机器人操作的智能框架",
                    "desc": "DreamVLA 是一种新颖的视觉-语言-动作（VLA）框架，旨在通过整合全面的世界知识来改善机器人操作。它引入了动态区域引导的世界知识预测，结合空间和语义线索，为动作规划提供了紧凑而全面的表示。该框架通过块状结构注意机制，减少动态、空间和语义信息之间的干扰，确保每个表示的清晰和分离。此外，DreamVLA 采用扩散式变换器来建模未来动作的条件分布，从而提高了机器人任务的成功率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05197",
            "title": "Pre-Trained Policy Discriminators are General Reward Models",
            "url": "https://huggingface.co/papers/2507.05197",
            "abstract": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  \t\t\t\t\tAI-generated summary \t\t\t\t We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.",
            "score": 25,
            "issue_id": 4694,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "88d62db0ed894120",
            "authors": [
                "Shihan Dou",
                "Shichun Liu",
                "Yuming Yang",
                "Yicheng Zou",
                "Yunhua Zhou",
                "Shuhao Xing",
                "Chenhao Huang",
                "Qiming Ge",
                "Demin Song",
                "Haijun Lv",
                "Songyang Gao",
                "Chengqi Lv",
                "Enyu Zhou",
                "Honglin Guo",
                "Zhiheng Xi",
                "Wenwei Zhang",
                "Qipeng Guo",
                "Qi Zhang",
                "Xipeng Qiu",
                "Xuanjing Huang",
                "Tao Gui",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05197.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "POLAR: Революция в моделировании вознаграждений для обучения с подкреплением",
                    "desc": "В статье представлен новый метод моделирования вознаграждений в обучении с подкреплением, названный Policy Discriminative Learning (POLAR). POLAR обучает модель вознаграждения различать идентичные политики и дискриминировать различные, что позволяет захватывать относительную разницу между политиками. Эмпирические результаты показывают, что POLAR значительно превосходит традиционные методы, улучшая точность предпочтений и производительность политик в различных задачах. Метод демонстрирует надежные возможности обобщения и масштабирования, что делает его перспективным направлением для разработки сильных и общих моделей вознаграждения."
                },
                "en": {
                    "title": "POLAR: Revolutionizing Reward Modeling through Policy Comparison",
                    "desc": "The paper introduces Policy Discriminative Learning (POLAR), a new method for reward modeling in reinforcement learning that focuses on comparing policies rather than relying on absolute preferences. By treating reward modeling as a policy discriminator, POLAR effectively generates reward signals that guide the training policy towards a target policy with desired behaviors. This approach allows for scalable pre-training of reward models, which can discern between similar and different policies, enhancing their performance significantly. Empirical results demonstrate that POLAR outperforms traditional methods, showing improved accuracy and robust generalization in various tasks."
                },
                "zh": {
                    "title": "POLAR：提升奖励模型性能的新方法",
                    "desc": "POLAR是一种可扩展的奖励建模方法，通过策略比较来增强奖励模型的性能并在强化学习中实现稳健的泛化。它将奖励建模视为策略鉴别器，量化两个策略之间的差异，以生成奖励信号，指导训练策略朝向具有期望行为的目标策略。与传统的绝对偏好方法不同，POLAR捕捉一个策略与任意目标策略之间的相对差异，适合于建模通用的排名关系。实验结果表明，POLAR显著优于传统的非预训练方法，提升了奖励模型的表现，展示了其在强化学习中的强大泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03483",
            "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
            "url": "https://huggingface.co/papers/2507.03483",
            "abstract": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.",
            "score": 19,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "a916ca78a2bd6196",
            "authors": [
                "Zhiheng Xi",
                "Guanyu Li",
                "Yutao Fan",
                "Honglin Guo",
                "Yufang Liu",
                "Xiaoran Fan",
                "Jiaqi Liu",
                "Jingchao Ding",
                "Wangmeng Zuo",
                "Zhenfei Yin",
                "Lei Bai",
                "Tao Ji",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "Harbin Institute of Technology",
                "Oxford",
                "Shanghai AI Laboratory",
                "University of Sydney",
                "Yimudata"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03483.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый инструмент для оценки мультидисциплинарных рассуждений ИИ",
                    "desc": "Статья представляет BMMR - масштабный двуязычный мультимодальный датасет для оценки рассуждений в различных дисциплинах. Он содержит 110 тысяч вопросов университетского уровня по 300 предметам, включая тесты, вопросы с открытым ответом и заполнение пропусков. Авторы также предлагают BMMR-Verifier для точной оценки цепочек рассуждений моделей. Эксперименты показывают, что даже современные модели оставляют значительный простор для улучшений в мультидисциплинарных рассуждениях."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with BMMR Dataset",
                    "desc": "This paper presents BMMR, a comprehensive dataset designed to enhance the reasoning abilities of large multimodal models (LMMs) across various disciplines. It includes 110,000 college-level questions from 300 subjects, formatted in multiple-choice, fill-in-the-blank, and open-ended styles, sourced from both print and digital media. The dataset is divided into BMMR-Eval for evaluation and BMMR-Train for training, with a focus on improving reasoning in diverse domains beyond just mathematics. Additionally, the authors introduce BMMR-Verifier, a tool for detailed assessment of reasoning paths, revealing significant gaps in current models' performance and highlighting the need for further research in multidisciplinary reasoning."
                },
                "zh": {
                    "title": "推动多模态模型的跨学科推理能力",
                    "desc": "本文介绍了BMMR，一个大规模的双语、多模态、多学科推理数据集，旨在帮助开发和评估大型多模态模型（LMMs）。该数据集包含110,000个大学水平的问题，涵盖300个联合国教科文组织定义的学科，问题形式多样，包括选择题、填空题和开放式问答。数据经过人工筛选和过滤，并为每个实例配备高质量的推理路径，分为BMMR-Eval和BMMR-Train两部分，以支持多学科知识和推理的评估与研究。我们还提出了基于过程的多学科验证器（BMMR-Verifier），用于对推理路径进行准确和细致的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02029",
            "title": "RoboBrain 2.0 Technical Report",
            "url": "https://huggingface.co/papers/2507.02029",
            "abstract": "RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.",
            "score": 17,
            "issue_id": 4698,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "5d72b9df64404714",
            "pdf_title_img": "assets/pdf/title_img/2507.02029.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RoboBrain 2.0: Универсальный ИИ для воплощенных задач",
                    "desc": "RoboBrain 2.0 - это новая модель искусственного интеллекта для воплощенных задач, объединяющая восприятие, рассуждение и планирование в физических средах. Модель имеет гетерогенную архитектуру с визуальным энкодером и языковой моделью, доступна в вариантах 7B и 32B параметров. RoboBrain 2.0 демонстрирует ведущие результаты на широком спектре пространственных и временных тестов, превосходя предыдущие открытые и проприетарные модели. Модель поддерживает ключевые возможности воплощенного ИИ, включая пространственное понимание и временное принятие решений."
                },
                "en": {
                    "title": "RoboBrain 2.0: Unifying Vision and Language for Advanced Embodied AI",
                    "desc": "RoboBrain 2.0 is a cutting-edge vision-language foundation model that integrates perception, reasoning, and planning for complex tasks in physical environments. It features a heterogeneous architecture with both a vision encoder and a language model, available in two sizes: a lightweight 7B model and a powerful 32B model. The 32B variant excels in various embodied reasoning tasks, outperforming previous models in spatial and temporal benchmarks. This model aims to enhance embodied AI research and facilitate the development of generalist embodied agents, with resources available for further exploration."
                },
                "zh": {
                    "title": "RoboBrain 2.0：推动实体AI的未来",
                    "desc": "RoboBrain 2.0 是一种视觉-语言基础模型，旨在统一感知、推理和规划，以应对复杂的实体任务。它有两个版本：轻量级的7B模型和全规模的32B模型，采用异构架构，结合了视觉编码器和语言模型。尽管体积小，RoboBrain 2.0 在多种实体推理任务中表现出色，特别是在空间和时间基准测试中，32B版本的性能领先于之前的开源和专有模型。该模型支持关键的现实世界实体AI能力，如空间理解和时间决策，推动了实体AI研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04009",
            "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
            "url": "https://huggingface.co/papers/2507.04009",
            "abstract": "A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.",
            "score": 12,
            "issue_id": 4698,
            "pub_date": "2025-07-05",
            "pub_date_card": {
                "ru": "5 июля",
                "en": "July 5",
                "zh": "7月5日"
            },
            "hash": "fd1930ff40937fac",
            "authors": [
                "Ziyang Miao",
                "Qiyu Sun",
                "Jingyuan Wang",
                "Yuchen Gong",
                "Yaowei Zheng",
                "Shiqi Li",
                "Richong Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Computer Science and Engineering, Beihang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04009.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтез данных для дообучения LLM с помощью Easy Dataset",
                    "desc": "Easy Dataset - это унифицированный фреймворк для синтеза данных для дообучения больших языковых моделей (LLM) из неструктурированных документов с помощью графического интерфейса. Он позволяет пользователям настраивать модели извлечения текста и стратегии разбиения для преобразования необработанных документов в связные текстовые фрагменты. Затем Easy Dataset использует подход к промптингу на основе персон для генерации разнообразных пар вопрос-ответ с помощью общедоступных LLM. Эксперименты показывают, что дообучение LLM на синтезированном наборе данных значительно улучшает производительность в конкретной предметной области, сохраняя при этом общие знания."
                },
                "en": {
                    "title": "Transforming Unstructured Data into Domain-Specific Knowledge",
                    "desc": "The paper introduces Easy Dataset, a framework designed to create fine-tuning data from unstructured documents using a user-friendly graphical interface and large language models (LLMs). It addresses the challenge of obtaining high-quality domain-specific data by allowing users to configure text extraction and chunking methods easily. The framework employs a persona-driven prompting technique to generate varied question-answer pairs, enhancing the dataset's richness. Experiments demonstrate that fine-tuning LLMs with this synthesized data significantly boosts their performance in specific domains while retaining their general knowledge capabilities."
                },
                "zh": {
                    "title": "轻松合成数据，提升模型表现",
                    "desc": "Easy Dataset是一个统一框架，旨在从非结构化文档中合成微调数据，以提高大语言模型（LLMs）在特定领域的表现，同时保持其通用知识。该框架通过直观的图形用户界面（GUI）使用户能够轻松配置文本提取模型和分块策略，将原始文档转化为连贯的文本块。接着，Easy Dataset利用基于角色的提示方法，生成多样化的问题-答案对，使用公开可用的LLMs。实验结果表明，在合成数据集上微调LLMs显著提高了其在特定领域的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03253",
            "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
            "url": "https://huggingface.co/papers/2507.03253",
            "abstract": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose RefineX, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines.",
            "score": 12,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "6f3d1aa17a4188e7",
            "authors": [
                "Baolong Bi",
                "Shenghua Liu",
                "Xingzhang Ren",
                "Dayiheng Liu",
                "Junyang Lin",
                "Yiwei Wang",
                "Lingrui Mei",
                "Junfeng Fang",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "National University of Singapore",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03253.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "RefineX: хирургическая точность в улучшении данных для ИИ",
                    "desc": "RefineX - это масштабируемый фреймворк для улучшения качества данных предобучения больших языковых моделей путем программного редактирования. Он позволяет эффективно и точечно улучшать качество данных, сохраняя при этом разнообразие и естественность исходного текста. RefineX обучает модель уточнения, которая может систематически улучшать каждый экземпляр в корпусе в масштабе. Эксперименты показывают, что модели, обученные на данных, улучшенных с помощью RefineX, превосходят модели, обученные на необработанных, отфильтрованных или альтернативно улучшенных данных по различным задачам."
                },
                "en": {
                    "title": "RefineX: Precision Editing for Superior Language Model Training",
                    "desc": "RefineX is a new framework designed to enhance the quality of pre-training data for large language models (LLMs) through targeted programmatic editing. It addresses the challenge of improving data quality at scale by allowing for precise modifications rather than broad document-level changes. This method preserves the diversity and naturalness of the text while ensuring efficient processing. Evaluations show that models trained with RefineX consistently outperform those trained on raw or traditionally refined data across various tasks, demonstrating its effectiveness in optimizing pre-training data."
                },
                "zh": {
                    "title": "RefineX：提升预训练数据质量的可扩展框架",
                    "desc": "RefineX是一个可扩展的框架，旨在通过程序化编辑提高大型语言模型预训练数据的质量。该框架解决了数据质量提升与处理效率之间的权衡问题，能够进行高效且细致的数据精炼。RefineX通过最小化编辑的删除程序，提炼出高质量的专家指导的端到端精炼结果，从而系统性地改善语料库中的每个实例。实验表明，RefineX在多个下游任务中表现优于使用原始、过滤或其他精炼数据训练的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05108",
            "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration",
            "url": "https://huggingface.co/papers/2507.05108",
            "abstract": "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement to 94.25\\% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
            "score": 8,
            "issue_id": 4698,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "1fe680b6a13d9932",
            "authors": [
                "Yuyi Zhang",
                "Peirong Zhang",
                "Zhenhua Yang",
                "Pengyu Yan",
                "Yongxin Shi",
                "Pengwei Liu",
                "Fengjun Guo",
                "Lianwen Jin"
            ],
            "affiliations": [
                "INTSIG-SCUT Joint Lab on Document Analysis and Recognition",
                "Intsig Information Co., Ltd.",
                "SCUT-Zhuhai Institute of Modern Industrial Innovation",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05108.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#data"
                ],
                "emoji": "📜",
                "ru": {
                    "title": "AutoHDR: Возрождение истории через искусственный интеллект",
                    "desc": "Эта статья представляет новый подход к автоматизированной реставрации исторических документов под названием AutoHDR. Авторы создали полностраничный набор данных FPHDR, содержащий реальные и синтетические изображения с различными уровнями повреждений. AutoHDR использует трехэтапный процесс, включающий локализацию повреждений с помощью OCR, предсказание контекста текста с использованием vision-language моделей и авторегрессивное восстановление внешнего вида. Метод значительно улучшает точность OCR для сильно поврежденных документов, достигая 84.05% без вмешательства человека и 94.25% при человеко-машинном взаимодействии."
                },
                "en": {
                    "title": "Revolutionizing Historical Document Restoration with AutoHDR",
                    "desc": "This paper introduces a new approach to restoring historical documents that have been damaged over time. It presents a dataset called FPHDR, which includes thousands of images with detailed annotations for various damage levels. The proposed method, AutoHDR, uses a three-stage process that combines damage detection, text prediction, and appearance restoration, mimicking the work of historians. The results show a significant improvement in OCR accuracy, demonstrating the effectiveness of this automated solution in preserving cultural heritage."
                },
                "zh": {
                    "title": "自动化历史文献修复的创新之路",
                    "desc": "历史文献是宝贵的文化遗产，但由于撕裂、水侵蚀和氧化等原因，经历了严重的退化。现有的历史文献修复方法主要集中在单一模态或有限规模的修复，无法满足实际需求。为了解决这个问题，我们提出了一个全页历史文献修复数据集（FPHDR）和一种新颖的自动化修复解决方案（AutoHDR）。AutoHDR通过三个阶段模拟历史学家的修复工作流程，显著提高了严重损坏文档的OCR准确率，推动了自动化历史文献修复的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04952",
            "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
            "url": "https://huggingface.co/papers/2507.04952",
            "abstract": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  \t\t\t\t\tAI-generated summary \t\t\t\t The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.",
            "score": 7,
            "issue_id": 4695,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "8eed78adf2fbda3a",
            "authors": [
                "Chenchen Zhang",
                "Yuhang Li",
                "Can Xu",
                "Jiaheng Liu",
                "Ao Liu",
                "Shihui Hu",
                "Dengpeng Wu",
                "Guanhua Huang",
                "Kejiao Li",
                "Qi Yi",
                "Ruibin Xiong",
                "Haotian Zhu",
                "Yuanxing Zhang",
                "Yuhao Jiang",
                "Yue Zhang",
                "Zenan Xu",
                "Bohui Zhai",
                "Guoxiang He",
                "Hebin Li",
                "Jie Zhao",
                "Le Zhang",
                "Lingyun Tan",
                "Pengyu Guo",
                "Xianshu Pang",
                "Yang Ruan",
                "Zhifeng Zhang",
                "Zhonghu Wang",
                "Ziyan Xu",
                "Zuopu Yin",
                "Wiggin Zhou",
                "Chayse Zhou",
                "Fengzong Lian"
            ],
            "affiliations": [
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04952.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Автоматизированная оценка визуального кода с помощью мультимодальных языковых моделей",
                    "desc": "ArtifactsBench - это новый фреймворк для автоматизированной оценки качества генерации визуального кода. Он использует временные скриншоты и мультимодальную языковую модель в качестве судьи для оценки. Фреймворк включает в себя бенчмарк из 1825 разнообразных задач и оценивает более 30 ведущих языковых моделей. ArtifactsBench достигает 94.4% согласованности ранжирования с золотым стандартом WebDev Arena и более 90% попарного согласия с экспертами-людьми."
                },
                "en": {
                    "title": "Automating Quality Assessment in Visual Code Generation",
                    "desc": "ArtifactsBench is a new framework designed to evaluate the quality of visual code generation by using temporal screenshots and a multimodal language model as a judge. It addresses the limitations of existing benchmarks that only focus on algorithmic correctness, ignoring the visual and interactive aspects crucial for user experiences. By programmatically rendering artifacts and capturing their dynamic behavior, ArtifactsBench provides a comprehensive assessment through a detailed checklist. The framework has been tested on 1,825 tasks and shows high consistency with human evaluations, making it a valuable tool for improving generative models in web development."
                },
                "zh": {
                    "title": "ArtifactsBench：自动化视觉代码生成评估的新标准",
                    "desc": "ArtifactsBench是一个新颖的基准和评估框架，旨在自动评估视觉代码生成的质量。它通过时间截图和多模态语言模型评判，捕捉生成的视觉工件的动态行为。该框架使用细致的任务清单来确保评估的全面性和可重复性，并构建了一个包含1825个多样化任务的新基准。我们的自动评估与人类专家的评分高度一致，标志着ArtifactsBench成为可靠的自动化评估人类感知质量的首个框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03745",
            "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2507.03745",
            "abstract": "A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>",
            "score": 7,
            "issue_id": 4696,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "8ca0a425a1ddc352",
            "authors": [
                "Akio Kodaira",
                "Tingbo Hou",
                "Ji Hou",
                "Masayoshi Tomizuka",
                "Yue Zhao"
            ],
            "affiliations": [
                "Meta",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03745.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "StreamDiT: Реальновременная генерация видео с помощью ИИ",
                    "desc": "StreamDiT - это модель генерации потокового видео, основанная на трансформерных диффузионных моделях. Она позволяет генерировать видео в реальном времени с высокой согласованностью контента и визуальным качеством. Обучение StreamDiT основано на методе flow matching с использованием движущегося буфера и смешанного обучения с различными схемами разбиения буферизованных кадров. После дистилляции модель достигает производительности реального времени 16 кадров в секунду на одном GPU при разрешении 512p."
                },
                "en": {
                    "title": "Real-Time Video Generation with StreamDiT",
                    "desc": "StreamDiT is a novel streaming video generation model that leverages transformer-based diffusion techniques to create high-quality videos in real-time. Unlike traditional models that generate short clips offline, StreamDiT can produce continuous video streams, making it suitable for interactive applications. The model employs flow matching with a moving buffer and mixed training strategies to enhance both content consistency and visual fidelity. With 4 billion parameters, StreamDiT achieves impressive performance, generating videos at 16 frames per second while maintaining a resolution of 512p."
                },
                "zh": {
                    "title": "实时视频生成的新突破：StreamDiT",
                    "desc": "本文提出了一种名为StreamDiT的流媒体视频生成模型，基于变换器的扩散模型，能够实现实时视频生成，同时保持高内容一致性和视觉质量。通过引入移动缓冲区，StreamDiT的训练采用流匹配的方法，设计了不同分区方案的混合训练，以提升生成视频的质量。该模型使用了具有不同时间嵌入和窗口注意力的adaLN DiT架构，并通过多步蒸馏方法优化性能。最终，经过蒸馏的模型在单个GPU上以16帧每秒的速度实现了实时性能，能够生成512p分辨率的视频流。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04590",
            "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
            "url": "https://huggingface.co/papers/2507.04590",
            "abstract": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.",
            "score": 4,
            "issue_id": 4694,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "a417297c3b4c5459",
            "authors": [
                "Rui Meng",
                "Ziyan Jiang",
                "Ye Liu",
                "Mingyi Su",
                "Xinyi Yang",
                "Yuepeng Fu",
                "Can Qin",
                "Zeyuan Chen",
                "Ran Xu",
                "Caiming Xiong",
                "Yingbo Zhou",
                "Wenhu Chen",
                "Semih Yavuz"
            ],
            "affiliations": [
                "Salesforce Research",
                "Tsinghua University",
                "UC Santa Barbara",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04590.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rag",
                    "#survey",
                    "#benchmark",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Единая модель эмбеддингов для всех визуальных форматов",
                    "desc": "VLM2Vec-V2 - это унифицированная система для создания эмбеддингов различных визуальных форматов, включая видео и документы. Модель демонстрирует высокую эффективность на новых задачах и превосходит существующие бенчмарки для изображений. VLM2Vec-V2 обучена на расширенном наборе данных MMEB-V2, который включает задачи поиска визуальных документов, поиска видео, временной привязки, классификации видео и ответов на вопросы по видео. Эксперименты показывают, что модель обобщается на различные мультимодальные задачи и закладывает основу для более масштабируемого и адаптивного обучения представлений."
                },
                "en": {
                    "title": "Unified Embeddings for All Visual Forms!",
                    "desc": "The paper introduces VLM2Vec-V2, a new framework designed to learn embeddings for various visual forms, including videos and documents. This model enhances the capabilities of existing multimodal embedding models, which have primarily focused on natural images. By establishing a comprehensive benchmark called MMEB-V2, the authors evaluate the model on new tasks such as video retrieval and visual document retrieval. The results show that VLM2Vec-V2 not only excels in these new tasks but also outperforms previous models on traditional image benchmarks, demonstrating its versatility and effectiveness in real-world applications."
                },
                "zh": {
                    "title": "统一多模态嵌入学习的新框架",
                    "desc": "本文提出了一种统一框架VLM2Vec-V2，用于学习多种视觉形式（如视频和文档）的嵌入。该模型在新任务上表现出色，并在图像的现有基准上有所提升。我们引入了MMEB-V2基准，扩展了五种新任务类型，包括视觉文档检索和视频分类等。通过广泛的实验，VLM2Vec-V2展示了其在多模态嵌入学习中的强大能力，为未来的研究和实际应用奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03607",
            "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification",
            "url": "https://huggingface.co/papers/2507.03607",
            "abstract": "A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descriptions. Built on RoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and achieves over 82% accuracy in predicting severity categories, enabling faster and more consistent triage ahead of manual CVSS scoring. The model and dataset are open-source and integrated into the Vulnerability-Lookup service.",
            "score": 4,
            "issue_id": 4698,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "7fd058ff4a7cb43a",
            "authors": [
                "Cédric Bonhomme",
                "Alexandre Dulaunoy"
            ],
            "affiliations": [
                "Computer Incident Response Center Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03607.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#architecture",
                    "#dataset",
                    "#data",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Искусственный интеллект на страже кибербезопасности: автоматическая оценка уязвимостей ПО",
                    "desc": "Представлена модель VLAI на основе трансформера, которая предсказывает уровни серьезности уязвимостей программного обеспечения непосредственно из текстовых описаний. VLAI построена на архитектуре RoBERTa и обучена на более чем 600 000 реальных уязвимостей. Модель достигает точности более 82% в предсказании категорий серьезности, что позволяет проводить более быструю и последовательную сортировку перед ручной оценкой CVSS. VLAI и набор данных являются открытыми и интегрированы в сервис Vulnerability-Lookup."
                },
                "en": {
                    "title": "Transforming Vulnerability Assessment with VLAI",
                    "desc": "This paper introduces VLAI, a transformer-based model designed to predict the severity levels of software vulnerabilities from textual descriptions. Utilizing the RoBERTa architecture, VLAI has been fine-tuned on a large dataset of over 600,000 real-world vulnerabilities, achieving an impressive accuracy of over 82% in classifying severity categories. This model significantly improves the efficiency and consistency of vulnerability triage processes, allowing for quicker assessments before manual Common Vulnerability Scoring System (CVSS) evaluations. Additionally, both the model and the dataset are made open-source and are integrated into the Vulnerability-Lookup service for broader accessibility."
                },
                "zh": {
                    "title": "智能预测软件漏洞严重性",
                    "desc": "本文介绍了一种基于变换器的模型VLAI，该模型能够直接从文本描述中预测软件漏洞的严重性等级。VLAI基于RoBERTa模型，经过对超过60万个真实漏洞的微调，达到了超过82%的严重性分类准确率。这一模型的应用可以提高漏洞分类的效率和一致性，帮助在手动CVSS评分之前进行更快速的评估。该模型和数据集都是开源的，并已集成到漏洞查询服务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04036",
            "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
            "url": "https://huggingface.co/papers/2507.04036",
            "abstract": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.",
            "score": 3,
            "issue_id": 4693,
            "pub_date": "2025-07-05",
            "pub_date_card": {
                "ru": "5 июля",
                "en": "July 5",
                "zh": "7月5日"
            },
            "hash": "79b10f5eed3bd7e4",
            "authors": [
                "Jingwei Shi",
                "Zeyu Zhang",
                "Biao Wu",
                "Yanjie Liang",
                "Meng Fang",
                "Ling Chen",
                "Yang Zhao"
            ],
            "affiliations": [
                "AI Geeks, Australia",
                "Australian Artificial Intelligence Institute, Australia",
                "La Trobe University, Australia",
                "University of Liverpool, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04036.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Искусственный интеллект создает презентации на уровне человека",
                    "desc": "Статья представляет PresentAgent - мультимодального агента, преобразующего длинные документы в видеопрезентации с озвучкой. Система использует модульный конвейер для сегментации документа, создания слайдов, генерации речи и компоновки видео. Для оценки качества выходных данных авторы разработали фреймворк PresentEval на основе визуально-языковых моделей. Эксперименты показали, что PresentAgent приближается к уровню человека по всем метрикам оценки."
                },
                "en": {
                    "title": "Transforming Text into Engaging Videos with PresentAgent",
                    "desc": "PresentAgent is a multimodal agent designed to convert long documents into engaging presentation videos with synchronized audio. Unlike traditional methods that only create static slides or text summaries, this approach generates dynamic visual and spoken content that resembles human presentations. It utilizes a modular pipeline for document segmentation, slide rendering, and narration generation, ensuring high-quality audio-visual alignment. The effectiveness of PresentAgent is evaluated using PresentEval, a framework that assesses video quality based on content fidelity, visual clarity, and audience comprehension, demonstrating its potential to enhance the accessibility of information."
                },
                "zh": {
                    "title": "将文档转化为生动演示的智能体",
                    "desc": "本文介绍了一种名为PresentAgent的多模态智能体，它能够将长篇文档转化为带有旁白的演示视频。与现有方法仅能生成静态幻灯片或文本摘要不同，我们的方法能够生成与人类演示风格相似的同步视觉和语音内容。PresentAgent采用模块化流程，系统地对输入文档进行分段，规划和渲染幻灯片风格的视觉框架，并利用大型语言模型和文本转语音模型生成上下文相关的旁白。我们还提出了PresentEval评估框架，通过视觉-语言模型对视频进行全面评分，验证了PresentAgent在内容真实性、视觉清晰度和观众理解力等方面接近人类水平的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05259",
            "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing",
            "url": "https://huggingface.co/papers/2507.05259",
            "abstract": "X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.",
            "score": 2,
            "issue_id": 4696,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "2db1a2de292203d6",
            "authors": [
                "Chun-Hsiao Yeh",
                "Yilin Wang",
                "Nanxuan Zhao",
                "Richard Zhang",
                "Yuheng Li",
                "Yi Ma",
                "Krishna Kumar Singh"
            ],
            "affiliations": [
                "Adobe",
                "HKU",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05259.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "X-Planner: умное планирование для точного редактирования изображений",
                    "desc": "X-Planner - это система планирования на основе мультимодальной большой языковой модели для редактирования изображений по текстовым инструкциям. Она разбивает сложные указания на точные подинструкции, обеспечивая локализованные правки с сохранением идентичности объектов. X-Planner автоматически генерирует типы правок и маски сегментации для каждой подинструкции. Система достигает наилучших результатов на существующих бенчмарках и новом тесте сложного редактирования."
                },
                "en": {
                    "title": "X-Planner: Simplifying Complex Image Edits with Precision",
                    "desc": "X-Planner is a planning system that uses a multimodal large language model to improve text-guided image editing. It breaks down complex editing instructions into simpler sub-instructions, which helps in making precise edits while preserving the identity of the images. This system reduces the need for manual masks and minimizes unintended changes by generating accurate edit types and segmentation masks automatically. X-Planner has shown to achieve top performance on established benchmarks, demonstrating its effectiveness in handling intricate editing tasks."
                },
                "zh": {
                    "title": "X-Planner：精准分解复杂指令的图像编辑系统",
                    "desc": "X-Planner 是一个利用多模态大语言模型的规划系统，能够将复杂的文本引导图像编辑指令分解为精确的子指令。这种方法确保了编辑的局部性和身份保留，避免了不必要的编辑错误。X-Planner 通过链式思维推理，系统地将复杂指令简化为更清晰的子指令，并自动生成编辑类型和分割掩码，减少了人工干预。该系统在现有基准测试和新引入的复杂编辑基准上都取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04562",
            "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
            "url": "https://huggingface.co/papers/2507.04562",
            "abstract": "State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against human superforecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of superforecasters.",
            "score": 1,
            "issue_id": 4696,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "27146571110c725d",
            "authors": [
                "Janna Lu"
            ],
            "affiliations": [
                "Department of Economics, George Mason University, Fairfax, VA 22030"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04562.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Искусственный интеллект vs суперпрогнозисты: битва предсказателей будущего",
                    "desc": "Современные большие языковые модели (LLM) были протестированы на задачах прогнозирования будущих событий. Результаты показали, что LLM достигают более высоких оценок Брайера, чем обычные люди, но все еще значительно уступают группе суперпрогнозистов. Исследование проводилось на 464 вопросах прогнозирования с платформы Metaculus. Это подчеркивает, что способности LLM к прогнозированию все еще требуют улучшения."
                },
                "en": {
                    "title": "LLMs Lag Behind Humans in Forecasting Accuracy",
                    "desc": "This paper evaluates the forecasting abilities of state-of-the-art large language models (LLMs) against human superforecasters. Despite LLMs showing impressive performance in various tasks, their accuracy in predicting future events is still lacking. The study uses Brier scores to compare the predictions of LLMs on 464 questions from Metaculus with those made by human experts. The results indicate that while LLMs may achieve scores that seem better than average human predictions, they still fall short when compared to the best human forecasters."
                },
                "zh": {
                    "title": "大型语言模型在预测中的局限性",
                    "desc": "本研究评估了最新的大型语言模型（LLMs）在预测问题上的表现，发现其准确性低于人类超级预测者。尽管LLMs在多种任务中展现出卓越的能力，但它们在预测未来事件方面的能力仍然缺乏深入研究。通过对Metaculus的464个预测问题进行评估，研究发现前沿模型的Brier分数表面上超过了人类群体，但仍显著低于超级预测者的表现。该研究揭示了当前LLMs在预测任务中的局限性，强调了人类预测者的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04376",
            "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents",
            "url": "https://huggingface.co/papers/2507.04376",
            "abstract": "As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.",
            "score": 1,
            "issue_id": 4696,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "a7ed25fb7089c612",
            "authors": [
                "Georgios Ioannides",
                "Christos Constantinou",
                "Vinija Jain",
                "Aman Chadha",
                "Aaron Elkins"
            ],
            "affiliations": [
                "Amazon GenAI, USA",
                "James Silberrad Brown Center for Artificial Intelligence, Carnegie Mellon University",
                "James Silberrad Brown Center for Artificial Intelligence, USA",
                "University of Bristol"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04376.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#agi",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "MOD-X: Новый стандарт для взаимодействия ИИ-агентов",
                    "desc": "Статья представляет MOD-X - новую архитектурную концепцию для обеспечения взаимодействия между ИИ-агентами. MOD-X предлагает многоуровневую архитектуру с универсальной шиной сообщений, управлением состоянием и механизмами безопасности на основе блокчейна. Ключевые инновации включают модель публикации-подписки, семантическое обнаружение возможностей и динамическую оркестровку рабочих процессов. Эта архитектура отвечает растущей потребности в действительно децентрализованных, совместимых экосистемах агентов, способных эффективно масштабироваться без необходимости централизованной координации."
                },
                "en": {
                    "title": "MOD-X: Bridging AI Agents for Seamless Collaboration",
                    "desc": "This paper presents MOD-X, a new framework designed to improve communication between different AI agents. It introduces a layered architecture that includes a Universal Message Bus and blockchain security, allowing diverse agents to work together seamlessly. MOD-X enhances interoperability by using a publish-subscribe model and dynamic workflow orchestration, making it easier for agents with different capabilities to collaborate. The framework aims to create decentralized ecosystems of specialized agents that can scale without central control."
                },
                "zh": {
                    "title": "MOD-X：构建智能代理的互操作性新框架",
                    "desc": "随着人工智能系统从单一模型演变为专门化代理的生态系统，标准化通信协议的需求变得越来越重要。本文提出了MOD-X（模块化开放去中心化交换），这是一个新的架构框架，旨在解决现有协议的关键限制。MOD-X采用分层架构，配备通用消息总线、全面的状态管理、翻译能力和基于区块链的安全机制。其创新之处在于发布-订阅通信模型、语义能力发现和动态工作流编排，提供了一个连接理论形式与实际实施的框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03336",
            "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
            "url": "https://huggingface.co/papers/2507.03336",
            "abstract": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.",
            "score": 1,
            "issue_id": 4694,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "a22f17539601dde1",
            "authors": [
                "Ashutosh Hathidara",
                "Julien Yu",
                "Sebastian Schreiber"
            ],
            "affiliations": [
                "SAP Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03336.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#data",
                    "#alignment",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "DiaFORGE: точные вызовы API через синтез диалогов и дообучение LLM",
                    "desc": "DiaFORGE - это фреймворк для улучшения способности больших языковых моделей (LLM) точно вызывать корпоративные API через синтез диалогов и дообучение с учителем. Он включает трехэтапный процесс: синтез диалогов, дообучение моделей и оценку готовности к реальному использованию. На тестовом наборе DiaBENCH модели, обученные с помощью DiaFORGE, повышают успешность вызова инструментов на 27 процентных пунктов по сравнению с GPT-4 и на 49 пунктов по сравнению с Claude-3.5-Sonnet. Авторы также выпустили открытый корпус из 5000 корпоративных API-спецификаций с проверенными диалогами для дальнейших исследований."
                },
                "en": {
                    "title": "Empowering LLMs to Accurately Invoke APIs with DiaFORGE",
                    "desc": "DiaFORGE is a framework designed to improve how large language models (LLMs) interact with enterprise APIs by resolving ambiguities in user requests. It consists of a three-stage process that includes generating multi-turn dialogues to help the model differentiate between similar tools, fine-tuning the model with supervised learning using reasoning traces, and evaluating the model's performance in real-world scenarios. The results show that models trained with DiaFORGE significantly outperform existing models like GPT-4o and Claude-3.5-Sonnet in successfully invoking tools. Additionally, DiaFORGE provides a valuable resource by releasing a corpus of enterprise API specifications and validated dialogues to aid future research."
                },
                "zh": {
                    "title": "提升API调用准确性的对话框架",
                    "desc": "DiaFORGE是一个消歧义框架，旨在提高大型语言模型在对话中准确调用企业API的能力。该框架包括三个阶段：首先合成以角色为驱动的多轮对话，帮助助手区分相似工具；其次对开源模型进行监督微调，利用3B到70B参数的推理轨迹；最后通过动态评估套件测试模型在真实环境中的表现。通过DiaFORGE训练的模型在工具调用成功率上比GPT-4o提高了27个百分点，比Claude-3.5-Sonnet提高了49个百分点。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04285",
            "title": "SeqTex: Generate Mesh Textures in Video Sequence",
            "url": "https://huggingface.co/papers/2507.04285",
            "abstract": "SeqTex leverages pretrained video foundation models to directly generate high-fidelity UV texture maps through a sequence generation approach, enhancing 3D texture generation with superior consistency and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.",
            "score": 0,
            "issue_id": 4701,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "be8c2e50f0eeca2f",
            "authors": [
                "Ze Yuan",
                "Xin Yu",
                "Yangtian Sun",
                "Yuan-Chen Guo",
                "Yan-Pei Cao",
                "Ding Liang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "HKU",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04285.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "SeqTex: прямая генерация UV-текстур с помощью видео-моделей",
                    "desc": "SeqTex - это новый метод генерации текстур для 3D-моделей, использующий предобученные видео-модели. Он генерирует UV-развертки текстур напрямую, без промежуточных этапов, что улучшает согласованность и выравнивание текстур. SeqTex переформулирует задачу как проблему генерации последовательностей, что позволяет модели изучать совместное распределение многоракурсных рендеров и UV-текстур. Метод достигает наилучших результатов в генерации 3D-текстур по изображениям и текстовым описаниям."
                },
                "en": {
                    "title": "SeqTex: Direct UV Texture Generation with Video Model Power",
                    "desc": "SeqTex is a new framework that uses pretrained video models to create high-quality UV texture maps directly, improving the process of 3D texture generation. Traditional methods often struggle with limited datasets and generate textures in two stages, which can lead to errors and inconsistencies. By treating the generation of UV textures as a sequence problem, SeqTex learns to connect multi-view images and UV textures more effectively. This approach, along with innovative design features, allows SeqTex to produce detailed and consistent textures without needing additional processing steps."
                },
                "zh": {
                    "title": "SeqTex：直接生成高保真 UV 纹理图的创新框架",
                    "desc": "SeqTex 是一个新颖的端到端框架，利用预训练的视频基础模型直接生成高保真 UV 纹理图。与以往方法不同，SeqTex 将任务重新定义为序列生成问题，从而学习多视图渲染和 UV 纹理的联合分布。这种设计有效地将视频基础模型中的一致性图像空间先验转移到 UV 领域。通过多种架构创新，SeqTex 在生成高质量 3D 纹理时实现了更好的一致性和对齐，且无需后处理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02659",
            "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
            "url": "https://huggingface.co/papers/2507.02659",
            "abstract": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the ``one drafter for all'' paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.",
            "score": 0,
            "issue_id": 4694,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "356734d41c5a5e65",
            "authors": [
                "Ramchalam Kinattinkara Ramakrishnan",
                "Zhaocong Yuan",
                "Shaojie Zhuo",
                "Chen Feng",
                "Yicheng Lin",
                "Chenzheng Su",
                "Xiaopeng Zhang"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02659.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Один черновик для всех: ускорение и адаптация языковых моделей",
                    "desc": "OmniDraft - это унифицированная система для ускорения работы языковых моделей. Она решает проблему несоответствия словарей между черновой и целевой моделями, используя онлайн-кэш n-грамм и гибридную дистилляцию. OmniDraft позволяет одной черновой модели работать с различными целевыми моделями и адаптироваться к пользовательским данным. Система особенно подходит для LLM-приложений на устройствах, где важны эффективность и кастомизация."
                },
                "en": {
                    "title": "One Draft Model for All Target Models!",
                    "desc": "OmniDraft is a new framework designed to solve the problem of cross-vocabulary mismatch between draft models and target models in machine learning applications. It allows a single draft model to work with different target models dynamically, improving decoding speed and efficiency. The framework uses an online n-gram cache and hybrid distillation fine-tuning to adapt to user data and enhance performance. OmniDraft is particularly beneficial for on-device large language model (LLM) applications, where it can significantly reduce latency and improve user customization."
                },
                "zh": {
                    "title": "一个草稿模型，适配所有目标模型",
                    "desc": "OmniDraft是一个统一框架，旨在解决跨词汇不匹配问题，并通过允许单一草稿模型与多种目标模型动态交互来提高解码速度。该框架特别适用于在线部署环境，能够使单一草稿模型与任何目标模型兼容，并根据用户数据动态调整。通过引入在线n-gram缓存和混合蒸馏微调，OmniDraft有效解决了草稿模型与目标模型之间的词汇不匹配问题。该方法在数学推理、编码和文本生成任务中表现出色，显著提高了解码效率。"
                }
            }
        }
    ],
    "link_prev": "2025-07-07.html",
    "link_next": "2025-07-09.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "07.07",
        "en": "07/07",
        "zh": "7月7日"
    },
    "short_date_next": {
        "ru": "09.07",
        "en": "07/09",
        "zh": "7月9日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 7,
        "#benchmark": 8,
        "#agents": 5,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 1,
        "#agi": 3,
        "#games": 6,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 11,
        "#survey": 2,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}