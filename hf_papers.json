{
    "date": {
        "ru": "24 сентября",
        "en": "September 24",
        "zh": "9月24日"
    },
    "time_utc": "2025-09-24 03:26",
    "weekday": 2,
    "issue_id": 6053,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.19249",
            "title": "Reinforcement Learning on Pre-Training Data",
            "url": "https://huggingface.co/papers/2509.19249",
            "abstract": "Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
            "score": 6,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "7ee5ca9be200b064",
            "authors": [
                "Siheng Li",
                "Kejiao Li",
                "Zenan Xu",
                "Guanhua Huang",
                "Evander Yang",
                "Kun Li",
                "Haoyuan Wu",
                "Jiajia Wu",
                "Zihao Zheng",
                "Chenchen Zhang",
                "Kun Shi",
                "Kyrierl Deng",
                "Qi Yi",
                "Ruibin Xiong",
                "Tingqiang Xu",
                "Yuhao Jiang",
                "Jianfeng Yan",
                "Yuyuan Zeng",
                "Guanghui Xu",
                "Jinbao Xue",
                "Zhijiang Xu",
                "Zheng Fang",
                "Shuai Li",
                "Qibin Liu",
                "Xiaoxue Li",
                "Zhuoyu Li",
                "Yangyu Tao",
                "Fei Gao",
                "Cheng Jiang",
                "Bo Chao Wang",
                "Kai Liu",
                "Jianchen Zhu",
                "Wai Lam",
                "Wayyt Wang",
                "Bo Zhou",
                "Di Wang"
            ],
            "affiliations": [
                "HunYuan Infra Team",
                "LLM Department, Tencent",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19249.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RLPT: Усиление языковых моделей через самообучение на предобученных данных",
                    "desc": "Метод обучения с подкреплением на предварительно обученных данных (RLPT) оптимизирует большие языковые модели, позволяя им автономно исследовать значимые траектории в предобученных данных. RLPT использует цель рассуждения о следующем сегменте, вознаграждая модель за точное предсказание последующих текстовых сегментов на основе предыдущего контекста. Этот подход позволяет масштабировать обучение с подкреплением на предобученных данных, способствуя развитию более обобщаемых навыков рассуждения. Эксперименты показывают значительное улучшение производительности моделей на различных тестах, включая математические рассуждения."
                },
                "en": {
                    "title": "Autonomous Learning for Enhanced Reasoning in Language Models",
                    "desc": "Reinforcement Learning on Pre-Training data (RLPT) is a novel approach that enhances large language models (LLMs) by allowing them to learn from pre-training data without needing human annotations. It uses reinforcement learning to autonomously explore meaningful data trajectories, which helps improve the model's reasoning abilities. Unlike traditional methods that rely on supervised learning, RLPT derives reward signals directly from the data, focusing on predicting subsequent text segments based on prior context. This method not only boosts performance on various reasoning tasks but also shows promise for scaling with increased computational resources."
                },
                "zh": {
                    "title": "自主探索，提升推理能力的强化学习新方法",
                    "desc": "强化学习在预训练数据上的应用（RLPT）通过自主探索预训练数据中的有意义轨迹，优化大型语言模型，提升其通用推理能力，而无需人工标注。与传统的监督学习方法不同，RLPT允许策略从预训练数据中学习，并通过强化学习提高能力。该方法通过预测后续文本段落来构建奖励信号，鼓励在更广泛的上下文中探索更丰富的轨迹。实验结果表明，RLPT在多个模型上显著提升了推理性能，展示了其在大型语言模型优化中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18154",
            "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
            "url": "https://huggingface.co/papers/2509.18154",
            "abstract": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
            "score": 6,
            "issue_id": 6052,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "e46263baf17f8869",
            "authors": [
                "Tianyu Yu",
                "Zefan Wang",
                "Chongyi Wang",
                "Fuwei Huang",
                "Wenshuo Ma",
                "Zhihui He",
                "Tianchi Cai",
                "Weize Chen",
                "Yuxiang Huang",
                "Yuanqian Zhao",
                "Bokai Xu",
                "Junbo Cui",
                "Yingjing Xu",
                "Liqing Ruan",
                "Luoyuan Zhang",
                "Hanyu Liu",
                "Jingkun Tang",
                "Hongyuan Liu",
                "Qining Guo",
                "Wenhao Hu",
                "Bingxiang He",
                "Jie Zhou",
                "Jie Cai",
                "Ji Qi",
                "Zonghao Guo",
                "Chi Chen",
                "Guoyang Zeng",
                "Yuxuan Li",
                "Ganqu Cui",
                "Ning Ding",
                "Xu Han",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "MiniCPM-V Team, OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18154.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "MiniCPM-V 4.5: Компактность и эффективность в мультимодальных ИИ-моделях",
                    "desc": "MiniCPM-V 4.5 - это мультимодальная большая языковая модель с 8 миллиардами параметров, которая достигает высокой производительности и эффективности. Модель использует унифицированную архитектуру 3D-Resampler, единую парадигму обучения и гибридную стратегию обучения с подкреплением. MiniCPM-V 4.5 превосходит более крупные модели, такие как GPT-4 и Qwen2.5-VL 72B, при значительно меньших затратах памяти и времени вывода. Модель демонстрирует передовые результаты на бенчмарке VideoMME среди моделей до 30 миллиардов параметров."
                },
                "en": {
                    "title": "Efficiency Meets Performance in Multimodal AI",
                    "desc": "MiniCPM-V 4.5 is an advanced multimodal large language model with 8 billion parameters, designed to enhance both performance and efficiency. It utilizes a novel 3D-Resampler architecture that allows for compact encoding of images and videos, streamlining the processing of multimodal data. The model also incorporates a unified learning paradigm that simplifies document knowledge and text recognition, reducing the need for extensive data engineering. Additionally, a hybrid reinforcement learning strategy enables the model to excel in both short and long reasoning tasks, achieving superior results while using significantly less computational resources compared to larger models."
                },
                "zh": {
                    "title": "高效多模态语言模型的未来",
                    "desc": "MiniCPM-V 4.5 是一个拥有 80 亿参数的多模态大型语言模型，采用统一的 3D-重采样架构，旨在提高效率和性能。该模型通过统一学习范式和混合强化学习策略，解决了多模态模型在训练和推理中的效率瓶颈。实验结果表明，MiniCPM-V 4.5 在 OpenCompass 评估中超越了许多知名模型，展现出卓越的性能和效率。尤其是在 VideoMME 基准测试中，该模型在 30B 以下的模型中实现了最先进的性能，显著降低了 GPU 内存和推理时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19296",
            "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
            "url": "https://huggingface.co/papers/2509.19296",
            "abstract": "A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "763d3ecf06625fdf",
            "authors": [
                "Sherwin Bahmani",
                "Tianchang Shen",
                "Jiawei Ren",
                "Jiahui Huang",
                "Yifeng Jiang",
                "Haithem Turki",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Zan Gojcic",
                "Sanja Fidler",
                "Huan Ling",
                "Jun Gao",
                "Xuanchi Ren"
            ],
            "affiliations": [
                "NVIDIA",
                "Simon Fraser University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19296.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#games",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "3D-миры из текста и изображений: новый подход к генерации виртуальных сред",
                    "desc": "Статья представляет фреймворк самодистилляции, который преобразует неявные 3D-знания из моделей видеодиффузии в явное 3D-представление с использованием метода Gaussian Splatting. Этот подход позволяет генерировать трехмерные сцены на основе текста или изображений без необходимости в многоракурсных обучающих данных. Модель включает в себя RGB-декодер и 3DGS-декодер, который обучается на синтетических данных, созданных моделями видеодиффузии. Фреймворк также поддерживает генерацию динамических 3D-сцен из монокулярного видео."
                },
                "en": {
                    "title": "Transforming 2D Imagination into 3D Reality",
                    "desc": "This paper introduces a self-distillation framework that transforms implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation. This method allows for the generation of 3D scenes from text or images without needing extensive multi-view training data. By enhancing the RGB decoder with a 3DGS decoder, the framework can be trained solely on synthetic data produced by video diffusion models. The results demonstrate superior performance in generating both static and dynamic 3D scenes, making it valuable for applications in gaming and robotics."
                },
                "zh": {
                    "title": "自蒸馏框架：从视频生成3D场景的创新方法",
                    "desc": "本文提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识转化为显式的3D高斯点云表示。这种方法使得从文本或图像生成3D场景成为可能，避免了对多视角训练数据的依赖。我们通过增强典型的RGB解码器，加入3D高斯点云解码器，并利用RGB解码器的输出进行监督训练。实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19284",
            "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
            "url": "https://huggingface.co/papers/2509.19284",
            "abstract": "Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "4e8ddb8ed978283e",
            "authors": [
                "Yunzhen Feng",
                "Julia Kempe",
                "Cheng Zhang",
                "Parag Jain",
                "Anthony Hartshorn"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19284.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Качество, а не количество: ключ к эффективным цепочкам рассуждений в ИИ",
                    "desc": "Исследование показывает, что эффективность цепочек рассуждений (Chain-of-Thought, CoT) в крупных моделях рассуждений (Large Reasoning Models, LRM) определяется не их длиной или степенью пересмотра, а меньшим количеством неудачных шагов и лучшим структурным качеством. Введен новый показатель - доля неудачных шагов (Failed-Step Fraction, FSF), который лучше предсказывает корректность рассуждений, чем длина CoT. Эксперименты с ранжированием и редактированием CoT подтверждают, что удаление неудачных ветвей улучшает точность. Результаты указывают на важность структурного подхода к масштабированию CoT вместо простого увеличения их длины."
                },
                "en": {
                    "title": "Less Failure, More Structure: The Key to Effective Reasoning in AI",
                    "desc": "This paper investigates what makes chain-of-thought (CoT) reasoning effective in large reasoning models (LRMs). It challenges the idea that longer CoTs are always better, showing that both longer CoTs and increased review can lead to lower accuracy. The authors introduce a new metric called the Failed-Step Fraction (FSF), which measures the proportion of steps in abandoned reasoning paths, and find it to be a better predictor of correctness than length or review. Their findings suggest that effective CoTs are characterized by fewer failures and emphasize the importance of structure in reasoning processes."
                },
                "zh": {
                    "title": "有效思维链：减少失败步骤，提升推理质量",
                    "desc": "这篇论文探讨了大型推理模型（LRMs）中有效的思维链（CoT）的特征。研究发现，思维链的有效性与失败步骤的数量和结构质量有关，而不是简单的长度或复审次数。通过对十个LRMs进行系统评估，结果表明，简单地延长思维链或增加复审会导致准确率降低。论文提出了一种新的图形视角来提取思维链的结构，并引入了一个统计量——失败步骤比例（FSF），该比例能够更好地预测模型的正确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18824",
            "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2509.18824",
            "abstract": "Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "6de809558bad8c90",
            "authors": [
                "Yanzuo Lu",
                "Xin Xia",
                "Manlin Zhang",
                "Huafeng Kuang",
                "Jianbin Zheng",
                "Yuxi Ren",
                "Xuefeng Xiao"
            ],
            "affiliations": [
                "ByteDance AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Hyper-Bagel: Сверхбыстрое мультимодальное ИИ без потери качества",
                    "desc": "Статья представляет Hyper-Bagel - унифицированную систему для ускорения мультимодальных задач понимания и генерации контента. Она использует спекулятивное декодирование для предсказания следующих токенов и многоэтапную дистилляцию для шумоподавления диффузии. Hyper-Bagel достигает значительного ускорения: более чем в 2 раза для задач понимания и до 22 раз для задач генерации. Разработанная 1-NFE модель обеспечивает практически мгновенное редактирование и генерацию мультимодального контента."
                },
                "en": {
                    "title": "Hyper-Bagel: Speeding Up Multimodal Tasks with Smart Techniques",
                    "desc": "Hyper-Bagel is a new framework that speeds up tasks involving multiple types of data, like text and images, by using advanced techniques. It combines speculative decoding, which predicts the next piece of data quickly, with a multi-stage distillation process to reduce noise in the data. This approach allows for more than double the speed in understanding multimodal content and significantly faster generation of images from text. The framework also includes a model that can edit and generate content in real-time, making it efficient and responsive while maintaining high-quality results."
                },
                "zh": {
                    "title": "Hyper-Bagel：加速多模态任务的创新框架",
                    "desc": "Hyper-Bagel 是一个加速多模态理解和生成任务的框架，采用了推测解码和多阶段蒸馏的方法。它通过分而治之的策略，显著减少了计算开销，同时保持了高质量的输出。该框架在多模态理解任务中实现了超过2倍的加速，而在生成任务中，文本到图像生成的速度提升达到了16.67倍，图像编辑的速度提升达到了22倍。通过结合对抗蒸馏和人类反馈学习，Hyper-Bagel 使得复杂的多模态交互变得无缝且即时。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19297",
            "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
            "url": "https://huggingface.co/papers/2509.19297",
            "abstract": "VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
            "score": 2,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "708db1d702c58d65",
            "authors": [
                "Weijie Wang",
                "Yeqing Chen",
                "Zeyu Zhang",
                "Hengyu Liu",
                "Haoxiao Wang",
                "Zhiyuan Feng",
                "Wenkang Qin",
                "Zheng Zhu",
                "Donny Y. Chen",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "GigaAI",
                "Monash University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19297.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "VolSplat: революция в синтезе новых ракурсов через воксельное предсказание гауссианов",
                    "desc": "VolSplat - это новый метод синтеза новых ракурсов, использующий воксельно-выровненное предсказание гауссианов вместо попиксельного. Он преодолевает ограничения пиксельного выравнивания, улучшая качество 3D-реконструкции и обеспечивая более надежную согласованность между ракурсами. VolSplat позволяет адаптивно контролировать плотность гауссианов на основе сложности 3D-сцены. Эксперименты показывают, что метод достигает современного уровня производительности, создавая более правдоподобные и согласованные гауссовы реконструкции."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Voxel-Aligned Gaussians",
                    "desc": "VolSplat is a novel method for synthesizing new views in 3D reconstruction by using voxel-aligned Gaussian predictions instead of the traditional pixel-aligned approach. This new technique addresses limitations such as dependency on input views, view-biased density distributions, and alignment errors caused by occlusions or low texture in source views. By predicting Gaussians directly from a 3D voxel grid, VolSplat enhances multi-view consistency and adapts Gaussian density based on scene complexity. Experiments show that it outperforms existing methods, providing more accurate and consistent 3D reconstructions, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "体素对齐，重塑3D重建的未来",
                    "desc": "VolSplat是一种基于体素对齐的高斯预测方法，旨在改善新视角合成，克服像素对齐的局限性，并提升3D重建质量。传统方法依赖于像素对齐的高斯预测，这导致重建的3D模型对输入视图数量高度依赖，并且在视图偏差和遮挡情况下容易出现对齐错误。VolSplat通过直接从预测的3D体素网格中预测高斯，避免了对错误易感的2D特征匹配，从而确保了多视图的一致性。实验结果表明，VolSplat在多个基准测试中表现出色，提供了更真实和一致的高斯重建。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19087",
            "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
            "url": "https://huggingface.co/papers/2509.19087",
            "abstract": "A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.   To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
            "score": 1,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "fbe226390b6ea231",
            "authors": [
                "Ganesh Mallya",
                "Yotam Gigi",
                "Dahun Kim",
                "Maxim Neumann",
                "Genady Beryozkin",
                "Tomer Shekel",
                "Anelia Angelova"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19087.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Мультиспектральное зрение для ИИ без переобучения",
                    "desc": "Статья представляет метод, позволяющий мультимодальным моделям обрабатывать мультиспектральные изображения в режиме zero-shot, без дополнительного обучения. Этот подход адаптирует специализированные мультиспектральные входные данные для использования в генералистических моделях, обученных только на RGB-изображениях. Метод был протестирован с моделью Gemini2.5 и показал значительное улучшение производительности в задачах дистанционного зондирования, таких как классификация землепользования. Предложенный подход открывает возможности для специалистов в области геопространственных данных использовать мощные мультимодальные модели для работы со специализированными сенсорными данными."
                },
                "en": {
                    "title": "Unlocking Multimodal Models for Multi-Spectral Imagery Without Training",
                    "desc": "This paper presents a novel training-free method that allows generalist multimodal models to process multi-spectral imagery without prior training, enhancing their performance in remote sensing tasks. Multi-spectral images, which contain additional spectral bands, are crucial for accurately identifying physical materials on the ground. The proposed approach enables these models, typically trained only on RGB images, to adapt to multi-spectral data in a zero-shot manner by injecting domain-specific instructions. The results demonstrate significant performance improvements on remote sensing benchmarks, showcasing the potential for geospatial professionals to utilize advanced multimodal models effectively."
                },
                "zh": {
                    "title": "无训练的多模态模型，轻松处理多光谱图像",
                    "desc": "本论文提出了一种无训练的方法，使通用多模态模型能够以零样本的方式处理多光谱图像，从而提高遥感任务的性能。多光谱图像在土地利用分类、环境监测和城市规划等遥感应用中发挥着重要作用。传统上，这些图像需要专门训练的机器学习模型进行自动分析，但这种方法成本高且不够灵活。我们的方法利用通用多模态模型的视觉理解能力，能够轻松适应新的多光谱输入，展示了在遥感基准测试中的显著性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17083",
            "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
            "url": "https://huggingface.co/papers/2509.17083",
            "abstract": "Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
            "score": 1,
            "issue_id": 6053,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 сентября",
                "en": "September 21",
                "zh": "9月21日"
            },
            "hash": "4a06acbb1d75ae4a",
            "authors": [
                "Zipeng Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17083.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🌈",
                "ru": {
                    "title": "Гибридные радиационные поля: высокое качество и эффективность в 3D рендеринге",
                    "desc": "Гибридные радиационные поля (HyRF) объединяют явные гауссианы и нейронные поля для высококачественного рендеринга с уменьшенным использованием памяти и производительностью в реальном времени. Этот метод разбивает сцену на компактный набор явных гауссианов, хранящих только критические высокочастотные параметры, и сеточные нейронные поля, предсказывающие остальные свойства. HyRF использует раздельную архитектуру нейронного поля для моделирования геометрии и цвета, зависящего от точки обзора. Эксперименты показывают, что HyRF достигает лучшего качества рендеринга, уменьшая размер модели более чем в 20 раз по сравнению с 3DGS, сохраняя при этом производительность в реальном времени."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with Hybrid Radiance Fields",
                    "desc": "Hybrid Radiance Fields (HyRF) introduce a new way to represent 3D scenes by merging explicit Gaussians with neural fields. This approach allows for high-quality rendering while significantly reducing memory usage and enabling real-time performance. HyRF uses a compact set of explicit Gaussians to capture essential high-frequency details and employs grid-based neural fields for other properties. The innovative architecture separates geometry and color modeling, leading to improved scene representation and rendering quality compared to previous methods."
                },
                "zh": {
                    "title": "混合辐射场：高效渲染的新方法",
                    "desc": "混合辐射场（HyRF）是一种新颖的场景表示方法，结合了显式高斯和神经场的优点，以实现高质量渲染，同时减少内存使用并保持实时性能。HyRF将场景分解为一组紧凑的显式高斯，仅存储关键的高频参数，以及基于网格的神经场，用于预测其余属性。我们引入了一种解耦的神经场架构，分别建模几何形状（尺度、不透明度、旋转）和视角依赖的颜色。实验表明，HyRF在渲染质量上达到了最先进的水平，同时模型大小比3D高斯点云减少了20倍以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18849",
            "title": "MAPO: Mixed Advantage Policy Optimization",
            "url": "https://huggingface.co/papers/2509.18849",
            "abstract": "Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
            "score": 0,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "9416ec88d3b85956",
            "authors": [
                "Wenke Huang",
                "Quan Zhang",
                "Yiyang Fang",
                "Jian Liang",
                "Xuankun Rong",
                "Huanjin Yao",
                "Guancheng Wan",
                "Ke Liang",
                "Wenwen He",
                "Mingjun Li",
                "Leszek Rutkowski",
                "Mang Ye",
                "Bo Du",
                "Dacheng Tao"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Defense Technology",
                "The AGH University of Krakow",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Адаптивная оптимизация преимущества для улучшения обучения с подкреплением",
                    "desc": "MAPO - это новая стратегия обучения с подкреплением для фундаментальных моделей. Она динамически перевзвешивает функцию преимущества для улучшения ранжирования траекторий. MAPO решает проблемы реверсии и зеркальности преимущества, возникающие в существующих подходах. Метод адаптивно настраивает функцию преимущества с учетом особенностей каждого образца, что повышает эффективность обучения."
                },
                "en": {
                    "title": "Dynamic Advantage Reweighting for Enhanced Trajectory Ranking",
                    "desc": "Mixed Advantage Policy Optimization (MAPO) enhances reinforcement learning by dynamically adjusting the advantage function to better rank trajectories in foundation models. It addresses issues like advantage reversion and advantage mirror problems that affect how advantages are distributed among different query samples. By introducing the concept of advantage percent deviation, MAPO focuses on samples with high-certainty trajectories, allowing for a more tailored advantage allocation. The effectiveness of MAPO is demonstrated through comparisons with existing methods and detailed ablation studies."
                },
                "zh": {
                    "title": "动态调整优势，提升强化学习效果",
                    "desc": "混合优势策略优化（MAPO）是一种在强化学习中动态调整优势函数的方法，旨在改善基础模型的轨迹排名。该方法解决了现有技术中存在的优势反转和优势镜像问题，从而实现更合理的优势分配。MAPO通过引入高确定性轨迹的优势百分比偏差，来适应不同样本的特性。实验结果表明，MAPO在与其他先进方法的比较中表现出色，验证了其有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-09-23.html",
    "link_next": "2025-09-25.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9月23日"
    },
    "short_date_next": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9月25日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}