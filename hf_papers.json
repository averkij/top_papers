{
    "date": {
        "ru": "31 января",
        "en": "January 31",
        "zh": "1月31日"
    },
    "time_utc": "2025-01-31 04:12",
    "weekday": 4,
    "issue_id": 1964,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.18492",
            "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
            "url": "https://huggingface.co/papers/2501.18492",
            "abstract": "As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
            "score": 3,
            "issue_id": 1964,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "2ea0bf2a655fc703",
            "authors": [
                "Yue Liu",
                "Hongcheng Gao",
                "Shengfang Zhai",
                "Jun Xia",
                "Tianyi Wu",
                "Zhiwei Xue",
                "Yulin Chen",
                "Kenji Kawaguchi",
                "Jiaheng Zhang",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Chinese"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18492.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Разумная защита: обучение ИИ безопасному мышлению",
                    "desc": "Статья представляет GuardReasoner - новый метод обеспечения безопасности больших языковых моделей (LLM) путем обучения их рассуждению. Авторы создали датасет GuardReasonerTrain с 127 тысячами образцов и 460 тысячами шагов рассуждений. Они применили методы обучения с подкреплением для развития способности моделей к рассуждению. Эксперименты на 13 бенчмарках показали превосходство GuardReasoner над другими методами, включая GPT-4 и LLaMA Guard."
                },
                "en": {
                    "title": "GuardReasoner: Enhancing LLM Safety through Advanced Reasoning Techniques",
                    "desc": "This paper introduces GuardReasoner, a novel safeguard designed to enhance the safety of large language models (LLMs) in critical applications. It presents a new dataset, GuardReasonerTrain, containing 127,000 samples with 460,000 reasoning steps to train guard models effectively. The authors implement reasoning Supervised Fine-Tuning (SFT) and hard sample Direct Preference Optimization (DPO) to improve the reasoning capabilities of these models. Experimental results show that GuardReasoner outperforms existing models like GPT-4o+CoT and LLaMA Guard 3 in terms of performance, explainability, and generalizability across multiple benchmarks."
                },
                "zh": {
                    "title": "GuardReasoner：提升大型语言模型的安全性与推理能力",
                    "desc": "随着大型语言模型（LLMs）在安全关键应用中的影响日益增加，确保其安全性成为一项重要挑战。本文提出了一种新的保护机制GuardReasoner，通过引导保护模型学习推理来增强安全性。我们首先创建了GuardReasonerTrain数据集，包含127K样本和460K详细推理步骤，并引入推理SFT以解锁保护模型的推理能力。此外，我们还提出了困难样本DPO，以进一步增强其推理能力。"
                }
            }
        }
    ],
    "link_prev": "2025-01-30.html",
    "link_next": "2025-02-03.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1月30日"
    },
    "short_date_next": {
        "ru": "03.02",
        "en": "02/03",
        "zh": "2月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "pinyin": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。\n\nZhè piān wénzhāng tiǎozhàn le jiàndū wēitiáo (SFT) de fànshì, tíchū le pīpíng wēitiáo (CFT) cèlüè. CFT ràng móxíng xuéxí pīpíng yǒu zàoyīn de xiǎngyìng, ér bùshì jiǎndān de mófǎng zhèngquè de xiǎngyìng. Shòu qiángdiào pīpàn xìng sīwéi de rénlèi xuéxí guòchéng qǐfā, CFT gǔlì gèng shēnrù de fēnxi hé xìzhì de lǐjiě. Zuòzhě tōngguò shìyàn yànzhèng le CFT de yǒuxiàoxìng, jiéguǒ xiǎnshì CFT zài duōgè shùxué jīzhǔn shàng yōuyú SFT.",
        "vocab": "[{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervise'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '批评', 'pinyin': 'pī píng', 'trans': 'criticize'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '噪音', 'pinyin': 'zào yīn', 'trans': 'noise'},\n{'word': '响应', 'pinyin': 'xiǎng yìng', 'trans': 'response'},\n{'word': '模仿', 'pinyin': 'mó fǎng', 'trans': 'imitate'},\n{'word': '启发', 'pinyin': 'qǐ fā', 'trans': 'inspire'},\n{'word': '批判性', 'pinyin': 'pī pàn xìng', 'trans': 'critical'},\n{'word': '思维', 'pinyin': 'sī wéi', 'trans': 'thinking'},\n{'word': '鼓励', 'pinyin': 'gǔ lì', 'trans': 'encourage'},\n{'word': '深入', 'pinyin': 'shēn rù', 'trans': 'in-depth'},\n{'word': '细致', 'pinyin': 'xì zhì', 'trans': 'detailed'},\n{'word': '验证', 'pinyin': 'yàn zhèng', 'trans': 'verify'},\n{'word': '有效性', 'pinyin': 'yǒu xiào xìng', 'trans': 'effectiveness'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}]",
        "trans": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.",
        "update_ts": "2025-01-30 09:10"
    }
}