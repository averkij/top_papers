{
    "date": {
        "ru": "9 декабря",
        "en": "December 9",
        "zh": "12月9日"
    },
    "time_utc": "2024-12-09 04:13",
    "weekday": 0,
    "issue_id": 1012,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 11,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение роботов движению через предобучение на видеоданных",
                    "desc": "В статье представлен новый подход к обучению роботов на основе предобученных языковых моделей. Авторы предлагают метод Moto, который преобразует видеоконтент в последовательности латентных токенов движения с помощью Latent Motion Tokenizer. Moto-GPT обучается на основе авторегрессии токенов движения, что позволяет ей захватывать разнообразные знания о визуальном движении. После предобучения и дообучения Moto-GPT демонстрирует высокую эффективность в задачах манипуляции роботов."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "利用视频数据提升机器人学习能力",
                    "desc": "这篇论文探讨了如何利用大规模视频数据来提升机器人学习能力。研究者提出了一种名为Moto的模型，通过将视频内容转换为潜在的运动标记序列，来学习运动知识。Moto-GPT经过预训练后，能够生成可解释的运动标记，并预测合理的运动轨迹。最终，研究表明，经过微调的Moto-GPT在机器人操作基准测试中表现出色，证明了从视频数据转移知识的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 8,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Коллективный интеллект агентов для создания сложных видео по тексту",
                    "desc": "Статья представляет GenMAC - итеративную мультиагентную систему для генерации видео по текстовому описанию. Система использует несколько специализированных агентов на основе мультимодальных языковых моделей (MLLM) для декомпозиции сложных задач. Процесс включает этапы проектирования, генерации и перепроектирования, с итеративным циклом между генерацией и перепроектированием для постепенного улучшения результата. GenMAC демонстрирует передовые результаты в генерации композиционных видео по текстовому описанию."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMAC：协作生成复杂视频的智能框架",
                    "desc": "文本到视频生成模型近年来取得了显著进展，但在生成复杂动态场景时仍面临挑战。我们提出了一种名为GenMAC的多代理框架，通过将复杂任务分解为简单任务来实现协作生成。该框架包括设计、生成和重新设计三个阶段，生成和重新设计之间存在迭代循环，以逐步验证和优化生成的视频。通过自适应选择专门化的修正代理，GenMAC能够有效应对多样化的文本到视频生成场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05237",
            "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale",
            "url": "https://huggingface.co/papers/2412.05237",
            "abstract": "Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.",
            "score": 7,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "1e3d5645afd61cf2",
            "authors": [
                "Jarvis Guo",
                "Tuney Zheng",
                "Yuelin Bai",
                "Bo Li",
                "Yubo Wang",
                "King Zhu",
                "Yizhi Li",
                "Graham Neubig",
                "Wenhu Chen",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanyang Technological University",
                "The University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05237.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений мультимодальных ИИ через обучение на масштабном наборе данных с промежуточными выводами",
                    "desc": "Статья представляет новый метод создания масштабного набора данных для обучения мультимодальных языковых моделей с открытым исходным кодом. В отличие от существующих наборов данных, новый подход включает подробные промежуточные рассуждения, что способствует развитию навыков рассуждения у моделей. Авторы создали набор данных из 12 миллионов пар инструкций и ответов, охватывающих разнообразные задачи, требующие интенсивных рассуждений. Эксперименты показали значительное улучшение способностей моделей к рассуждению, достигая лучших результатов на нескольких бенчмарках."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Rich Rationales",
                    "desc": "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."
                },
                "zh": {
                    "title": "提升多模态模型推理能力的新方法",
                    "desc": "这篇论文介绍了一种构建大规模多模态指令调优数据集的方法，以提高多模态大语言模型（MLLMs）的推理能力。现有的数据集主要来自学术研究，任务简单，缺乏中间推理过程的详细解释。我们的方法创建了一个包含1200万对指令和响应的数据集，涵盖了多样化且需要推理的任务，并提供了丰富的推理依据。实验结果表明，使用该数据集训练的MLLMs在多个基准测试中显著提高了推理能力，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05270",
            "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
            "url": "https://huggingface.co/papers/2412.05270",
            "abstract": "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.",
            "score": 4,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "14bb480f5fe29bae",
            "authors": [
                "Hanqing Zhu",
                "Zhenyu Zhang",
                "Wenyan Cong",
                "Xi Liu",
                "Sem Park",
                "Vikas Chandra",
                "Bo Long",
                "David Z. Pan",
                "Zhangyang Wang",
                "Jinwon Lee"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.05270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "APOLLO: эффективное обучение LLM с минимальными затратами памяти",
                    "desc": "Статья представляет новый оптимизатор APOLLO для обучения больших языковых моделей (LLM). APOLLO использует структурированное обновление скорости обучения, основанное на случайном проецировании, что позволяет значительно снизить потребление памяти по сравнению с популярным оптимизатором AdamW. Эксперименты показывают, что APOLLO достигает сопоставимой или лучшей производительности, чем AdamW, при существенной экономии памяти. Это позволяет увеличить пропускную способность, масштабируемость модели и делает возможным обучение LLM даже на GPU среднего уровня."
                },
                "en": {
                    "title": "APOLLO: Memory-Efficient Optimization for Large Language Models",
                    "desc": "This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs."
                },
                "zh": {
                    "title": "APOLLO：高效内存优化的未来",
                    "desc": "大型语言模型（LLMs）在训练过程中对内存的需求非常高，尤其是使用流行的AdamW优化器时。为了解决这个问题，研究者们提出了多种内存高效的优化器，但它们面临着依赖昂贵的SVD操作和性能折衷等挑战。本文提出了一种名为APOLLO的优化方法，通过近似学习率缩放来减少内存使用，同时保持与AdamW相当的预训练性能。实验结果表明，APOLLO系列在内存节省方面表现优异，能够在较低的内存成本下实现更高的训练吞吐量和模型可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 2,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "EXAONE 3.5: Новое слово в языковых моделях от LG AI Research",
                    "desc": "Компания LG AI Research представила языковые модели EXAONE 3.5, обученные выполнению инструкций. Модели доступны в трех конфигурациях: 32B, 7.8B и 2.4B параметров. Они демонстрируют исключительные способности в следовании инструкциям, понимании длинного контекста и показывают конкурентоспособные результаты по сравнению с современными открытыми моделями аналогичного размера. Модели EXAONE 3.5 доступны для исследовательских целей и могут быть загружены с платформы Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5：指令跟随与长上下文理解的先锋",
                    "desc": "EXAONE 3.5 是由 LG AI 研究所开发的指令调优语言模型，提供三种配置：32B、7.8B 和 2.4B。这些模型在实际场景中具有卓越的指令跟随能力，在七个基准测试中取得了最高分。它们在长上下文理解方面表现出色，在四个基准测试中也达到了最佳性能。此外，与同类开源模型相比，EXAONE 3.5 在九个通用基准测试中也展现了竞争力的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 1,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Мгновенное редактирование изображений текстом",
                    "desc": "SwiftEdit - это новый инструмент для быстрого редактирования изображений с помощью текстовых запросов. В отличие от существующих многошаговых методов, SwiftEdit использует одношаговую инверсию и механизм масштабирования внимания для локального редактирования. Это позволяет выполнять изменения изображений практически мгновенно - за 0.23 секунды. SwiftEdit работает как минимум в 50 раз быстрее аналогов, сохраняя при этом высокое качество результатов."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEdit：瞬时文本引导图像编辑的革命",
                    "desc": "最近的文本引导图像编辑技术使用户能够通过简单的文本输入进行图像编辑，利用了多步扩散模型的丰富先验知识。然而，这些方法在实际应用中往往速度较慢，无法满足实时和设备端的需求。为此，我们提出了SwiftEdit，这是一种简单而高效的编辑工具，可以实现瞬时的文本引导图像编辑，速度达到0.23秒。SwiftEdit的创新在于其一体化的反演框架和基于掩码的编辑技术，能够快速进行局部图像编辑，同时保持与传统多步方法相当的编辑效果。"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
        "pinyin": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。\n\nZuìjìn de shìjué-yǔyán móxíng tōngguò zēngjiā shìjué biāojì de chángdù tígāo le xìngnéng, dàn yě zēngjiā le jìsuàn chéngběn. Wǒmen fāxiàn, liúxíng de shìjué biānmǎqì shēngchéng de shìjué biāojì cúnzài dàliàng rǒngyù. Wèi jiějué zhègè wèntí, wǒmen yǐnrù le VisionZip, yīzhǒng xuǎnzé xìnxī fēngfù de shìjué biāojì de fāngfǎ, jiǎnshǎo rǒngyù bìng tígāo xiàolǜ. VisionZip kě yìngyòng yú túxiàng hé shìpǐn lǐjiě rènwù, shìyòng yú duōlún duìhuà. Shíyàn jiéguǒ xiǎnshì, VisionZip zài jīhuā suǒyǒu shèzhì zhōng dōu yōu zhīqián de zuìjiā fāngfǎ, bìng xiǎnzhù tígāo le móxíng tuīlǐ sùdù.",
        "vocab": "[{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '语言', 'pinyin': 'yǔyán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '增加', 'pinyin': 'zēngjiā', 'trans': 'increase'}, {'word': '长度', 'pinyin': 'chángdù', 'trans': 'length'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'calculation'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '发现', 'pinyin': 'fāxiàn', 'trans': 'discover'}, {'word': '流行', 'pinyin': 'liúxíng', 'trans': 'popular'}, {'word': '编码器', 'pinyin': 'biānmǎqì', 'trans': 'encoder'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'}, {'word': '存在', 'pinyin': 'cúnzài', 'trans': 'exist'}, {'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundancy'}, {'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '选择', 'pinyin': 'xuǎnzé', 'trans': 'select'}, {'word': '丰富', 'pinyin': 'fēngfù', 'trans': 'rich'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '应用', 'pinyin': 'yìngyòng', 'trans': 'apply'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '理解', 'pinyin': 'lǐjiě', 'trans': 'understanding'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '适用', 'pinyin': 'shìyòng', 'trans': 'applicable'}, {'word': '多轮', 'pinyin': 'duōlún', 'trans': 'multi-turn'}, {'word': '对话', 'pinyin': 'duìhuà', 'trans': 'dialogue'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}, {'word': '之前', 'pinyin': 'zhīqián', 'trans': 'previous'}, {'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'}, {'word': '设置', 'pinyin': 'shèzhì', 'trans': 'setting'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'inference'}, {'word': '速度', 'pinyin': 'sùdù', 'trans': 'speed'}]",
        "trans": "Recent vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. We have discovered that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a method for selecting information-rich visual tokens to reduce redundancy and enhance efficiency. VisionZip can be applied to both image and video understanding tasks and is suitable for multi-turn conversations. Experimental results show that VisionZip outperforms previous best methods in almost all settings and significantly speeds up model inference.",
        "update_ts": "2024-12-08 12:42"
    }
}