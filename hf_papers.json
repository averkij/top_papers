{
    "date": {
        "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 9",
        "zh": "4æœˆ9æ—¥"
    },
    "time_utc": "2025-04-09 05:11",
    "weekday": 2,
    "issue_id": 3140,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.06263",
            "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
            "url": "https://huggingface.co/papers/2504.06263",
            "abstract": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.",
            "score": 26,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "3b3365aa60717b2a",
            "authors": [
                "Yiying Yang",
                "Wei Cheng",
                "Sijin Chen",
                "Xianfang Zeng",
                "Jiaxu Zhang",
                "Liao Wang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06263.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OmniSVG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "OmniSVG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ SVG Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ SVG Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMSVG-2M Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… SVG-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OmniSVG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° SVG."
                },
                "en": {
                    "title": "OmniSVG: Revolutionizing SVG Generation with Vision-Language Models",
                    "desc": "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."
                },
                "zh": {
                    "title": "OmniSVGï¼šé«˜æ•ˆç”Ÿæˆå¤æ‚SVGçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºOmniSVGçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡å’Œå¤æ‚çš„å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡å°†SVGå‘½ä»¤å’Œåæ ‡å‚æ•°åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€SVGç”Ÿæˆã€‚OmniSVGå°†ç»“æ„é€»è¾‘ä¸ä½çº§å‡ ä½•è§£è€¦ï¼Œä»è€Œåœ¨ä¿æŒå¤æ‚SVGç»“æ„è¡¨ç°åŠ›çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†MMSVG-2Mæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤ç™¾ä¸‡ä¸ªä¸°å¯Œæ³¨é‡Šçš„SVGèµ„äº§ï¼Œä»¥æ¨åŠ¨SVGåˆæˆçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05979",
            "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
            "url": "https://huggingface.co/papers/2504.05979",
            "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
            "score": 23,
            "issue_id": 3137,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "f1195a87ec5b86f1",
            "authors": [
                "Sixiang Chen",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Tian Ye",
                "Qingyu Shi",
                "Donghao Zhou",
                "Wenhao Chai",
                "Xin Lin",
                "Jianzong Wu",
                "Chao Tang",
                "Shilin Xu",
                "Tao Zhang",
                "Haobo Yuan",
                "Yikang Zhou",
                "Wei Chow",
                "Linfeng Li",
                "Xiangtai Li",
                "Lei Zhu",
                "Lu Qi"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Washington",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05979.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ GPT-4o. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ GPT-4o Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-3D Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-X. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ GPT-4o Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unifying Image and Text Generation with GPT-4o",
                    "desc": "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."
                },
                "zh": {
                    "title": "æ¢ç´¢ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœªæ¥æ–¹å‘",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«ã€‚åˆ†æç»“æœæ­ç¤ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å°†å…¶ç½®äºç”Ÿæˆå»ºæ¨¡çš„æ›´å¹¿æ³›æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœªæ¥ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®æ‰©å±•çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02160",
            "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
            "url": "https://huggingface.co/papers/2504.02160",
            "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
            "score": 9,
            "issue_id": 3139,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "511e3ea71050e14e",
            "authors": [
                "Shaojin Wu",
                "Mengqi Huang",
                "Wenxu Wu",
                "Yufeng Cheng",
                "Fei Ding",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02160.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#synthetic",
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UNO Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Image Generation with Consistent Data Synthesis",
                    "desc": "This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks."
                },
                "zh": {
                    "title": "é«˜ä¸€è‡´æ€§å¤šä¸»é¢˜ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œå¦‚ä½•è§£å†³ä»¥ä¸»é¢˜ä¸ºé©±åŠ¨çš„ç”Ÿæˆé¢ä¸´çš„æ•°æ®å¯æ‰©å±•æ€§å’Œä¸»é¢˜æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜ä¸€è‡´æ€§çš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨çš„å†…åœ¨ç”Ÿæˆèƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¸€è‡´æ€§çš„å¤šä¸»é¢˜é…å¯¹æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†UNOæ¨¡å‹ï¼Œç»“åˆäº†æ¸è¿›çš„è·¨æ¨¡æ€å¯¹é½å’Œé€šç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸»é¢˜å’Œå¤šä¸»é¢˜ç”Ÿæˆä¸­ä¿æŒé«˜ä¸€è‡´æ€§å’Œå¯æ§æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œæ§åˆ¶èƒ½åŠ›ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02810",
            "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2504.02810",
            "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
            "score": 6,
            "issue_id": 3138,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "5a9b5f817a1c09b6",
            "authors": [
                "Haowei Lin",
                "Xiangyu Wang",
                "Ruilin Yan",
                "Baizhou Huang",
                "Haotian Ye",
                "Jianhua Zhu",
                "Zihao Wang",
                "James Zou",
                "Jianzhu Ma",
                "Yitao Liang"
            ],
            "affiliations": [
                "Computer Science Department, Stanford University, California, United States",
                "Department of Electronic Engineering, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Peking University, Beijing, China",
                "Wangxuan institute of computer technology, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02810.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "KUMO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ KUMO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. KUMO Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ LLM Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 23 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° 5000 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "KUMO: Unveiling True Reasoning in LLMs",
                    "desc": "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."
                },
                "zh": {
                    "title": "KUMOï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å·¥å…·",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…ä»…ä»å…¶åºå¤§çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†KUMOï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMsæ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOç»“åˆäº†LLMså’Œç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ï¼Œä¿ƒè¿›æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹23ä¸ªæœ€å…ˆè¿›çš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè®¸å¤šæ¨¡å‹åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¤§å­¦ç”Ÿçš„è¡¨ç°ï¼Œè€Œåœ¨å¤æ‚æ¨ç†æŒ‘æˆ˜ä¸­ä¹Ÿè¾¾åˆ°äº†å¤§å­¦æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05594",
            "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
            "url": "https://huggingface.co/papers/2504.05594",
            "abstract": "Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.",
            "score": 5,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "7da2f86ad5e0bfc2",
            "authors": [
                "Qi Mao",
                "Lan Chen",
                "Yuchao Gu",
                "Mike Zheng Shou",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "State Key Laboratory of Media Convergence and Communication, Communication University of China",
                "University of California at Merced",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05594.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifyEdit - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit",
                    "desc": "This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks."
                },
                "zh": {
                    "title": "å¹³è¡¡ä¿çœŸåº¦ä¸å¯ç¼–è¾‘æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘ä¸­ï¼Œå¹³è¡¡ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§éå¸¸é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿æŒç»“æ„ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æœºåˆ¶æ¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†UnifyEditï¼Œè¿™æ˜¯ä¸€ç§æ— è°ƒä¼˜çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ½œåœ¨ä¼˜åŒ–å®ç°ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§çš„å¹³è¡¡ã€‚æˆ‘ä»¬å¼€å‘äº†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çº¦æŸï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥è°ƒåº¦å™¨ï¼Œä»¥åŠ¨æ€è°ƒæ•´è¿™äº›çº¦æŸçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00043",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
            "url": "https://huggingface.co/papers/2504.00043",
            "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
            "score": 4,
            "issue_id": 3137,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "2b2bfdd590c5394d",
            "authors": [
                "Jixuan Leng",
                "Chengsong Huang",
                "Langlin Huang",
                "Bill Yuchen Lin",
                "William W. Cohen",
                "Haohan Wang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "CMU",
                "UIUC",
                "UW",
                "WUSTL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00043.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞšÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "CrossWordBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ ÑĞµÑ‚ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ LLM Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ LVLM Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ° ÑĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles",
                    "desc": "This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†ï¼šCrossWordBench",
                    "desc": "ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ¨ç†è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æ¨ç†æˆ–è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ä¸Šï¼Œç¼ºä¹æ–‡æœ¬ä¸è§†è§‰ä¹‹é—´çš„åŠ¨æ€äº’åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossWordBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¡«å­—æ¸¸æˆè¯„ä¼°LLMså’ŒLVLMsæ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œè¦æ±‚åœ¨æ–‡æœ¬çº¿ç´¢å’Œè§†è§‰ç½‘æ ¼ç»“æ„çš„è¯­ä¹‰çº¦æŸä¸‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚CrossWordBenchåˆ©ç”¨å¯æ§çš„æ‹¼å›¾ç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆå¤šç§æ ¼å¼çš„æ‹¼å›¾ï¼Œå¹¶æä¾›ä»ç›´æ¥è§£è°œåˆ°äº’åŠ¨æ¨¡å¼çš„ä¸åŒè¯„ä¼°ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMsåœ¨åˆ©ç”¨äº¤å‰å­—æ¯çº¦æŸæ–¹é¢æ˜¾è‘—ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œè€ŒLVLMsåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå…¶è§£è°œè¡¨ç°ä¸ç½‘æ ¼è§£æå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-08.html",
    "link_next": "2025-04-10.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å˜å‹å™¨åœ¨ç”Ÿæˆä¸€åˆ†é’Ÿè§†é¢‘æ—¶çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›å±‚åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ•ˆç‡ä½ä¸‹ã€‚ä½œè€…å°è¯•ä½¿ç”¨æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰å±‚ï¼Œå…¶éšè—çŠ¶æ€æœ¬èº«å¯ä»¥æ˜¯ç¥ç»ç½‘ç»œï¼Œä»è€Œæ›´å…·è¡¨ç°åŠ›ã€‚å°†TTTå±‚åŠ å…¥é¢„è®­ç»ƒçš„å˜å‹å™¨åï¼Œå¯ä»¥ä»æ–‡å­—æ•…äº‹æ¿ç”Ÿæˆä¸€åˆ†é’Ÿè§†é¢‘ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€æ¦‚å¿µï¼Œä½œè€…æ ¹æ®ã€ŠçŒ«å’Œè€é¼ ã€‹åŠ¨ç”»ç‰‡ç¼–åˆ¶äº†ä¸€ä¸ªæ•°æ®é›†ã€‚ä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTTTå±‚ç”Ÿæˆçš„è§†é¢‘æ›´è¿è´¯ï¼Œèƒ½å¤Ÿè®²è¿°å¤æ‚çš„æ•…äº‹ã€‚",
        "title": "One-Minute Video Generation with Test-Time Training",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å˜å‹å™¨åœ¨ç”Ÿæˆä¸€åˆ†é’Ÿè§†é¢‘æ—¶çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›å±‚åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ•ˆç‡ä½ä¸‹ã€‚ä½œè€…å°è¯•ä½¿ç”¨æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰å±‚ï¼Œå…¶éšè—çŠ¶æ€æœ¬èº«å¯ä»¥æ˜¯ç¥ç»ç½‘ç»œï¼Œä»è€Œæ›´å…·è¡¨ç°åŠ›ã€‚å°†TTTå±‚åŠ å…¥é¢„è®­ç»ƒçš„å˜å‹å™¨åï¼Œå¯ä»¥ä»æ–‡å­—æ•…äº‹æ¿ç”Ÿæˆä¸€åˆ†é’Ÿè§†é¢‘ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€æ¦‚å¿µï¼Œä½œè€…æ ¹æ®ã€ŠçŒ«å’Œè€é¼ ã€‹åŠ¨ç”»ç‰‡ç¼–åˆ¶äº†ä¸€ä¸ªæ•°æ®é›†ã€‚ä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTTTå±‚ç”Ÿæˆçš„è§†é¢‘æ›´è¿è´¯ï¼Œèƒ½å¤Ÿè®²è¿°å¤æ‚çš„æ•…äº‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le biÃ n yÄ qÃ¬ zÃ i shÄ“ng chÃ©ng yÄ« fÄ“n zhÅng shÃ¬ pÇn de chÃ¡o zhÃ n, yÄ«n wÃ¨i zÃ¬ zhÃ¹ yÃ¬ lÃ¬ cÃ©ng zÃ i chÃ¡ng shÃ ng xiÃ  wÃ©n zhÅng xiÃ o lÇœ dÄ« xiÃ . zuÃ² zhÄ› chÃ¡ng shÃ¬ shÇ yÃ²ng cÃ¨ shÃ¬ shÃ­ xÃ¹n liÃ n (TTT) cÃ©ng, qÃ­ yÇn cÃ¡ng zhuÃ ng tÃ i bÄ›n shÄ“n kÄ› yÇ shÃ¬ shÃ©n jÄ«ng wÇng luÃ², cÃ³ng Ã©r gÃ¨ng jÃ¹ biÇo xiÃ n lÃ¬. jiÄng TTT cÃ©ng jiÄ rÃ¹ yÃ¹ xÃ¹n liÃ n de biÃ n yÄ qÃ¬ hÃ²u, kÄ› yÇ cÃ³ng wÃ©n zÃ¬ gÃ¹ shÃ¬ bÇn shÄ“ng chÃ©ng yÄ« fÄ“n zhÅng shÃ¬ pÇn. wÃ¨i le yÃ n zhÃ¨ng zhÃ¨ yÄ« gÃ i niÃ n, zuÃ² zhÄ› gÄ“n jÃ¹ ã€ŠmÄo hÃ© lÇo shÇ”ã€‹ dÃ²ng huÃ  piÄn biÄn zhÃ¬ le yÄ« gÃ¨ shÃ¹ jÃ¹ jÃ­. yÇ” qÃ­ tÄ jÄ« xiÃ n fÄng fÇ biÇo bÇ, TTT cÃ©ng shÄ“ng chÃ©ng de shÃ¬ pÇn gÃ¨ng liÃ¡n gÇ”, nÃ©ng gÃ²u jiÇng shÃ¹ fÃº zÃ  de gÃ¹ shÃ¬.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å˜å‹å™¨\", \"pinyin\": \"biÃ n yÄ qÃ¬\", \"trans\": \"transformer\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"è‡ªæ³¨æ„åŠ›\", \"pinyin\": \"zÃ¬ zhÃ¹ yÃ¬ lÃ¬\", \"trans\": \"self-attention\"},\n    {\"word\": \"å±‚\", \"pinyin\": \"cÃ©ng\", \"trans\": \"layer\"},\n    {\"word\": \"é•¿\", \"pinyin\": \"chÃ¡ng\", \"trans\": \"long\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n\", \"trans\": \"context\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ o lÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"ä½ä¸‹\", \"pinyin\": \"dÄ« xiÃ \", \"trans\": \"low\"},\n    {\"word\": \"å°è¯•\", \"pinyin\": \"chÃ¡ng shÃ¬\", \"trans\": \"attempt\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"æµ‹è¯•æ—¶è®­ç»ƒ\", \"pinyin\": \"cÃ¨ shÃ¬ shÃ­ xÃ¹n liÃ n\", \"trans\": \"test-time training\"},\n    {\"word\": \"éšè—\", \"pinyin\": \"yÇn cÃ¡ng\", \"trans\": \"hidden\"},\n    {\"word\": \"çŠ¶æ€\", \"pinyin\": \"zhuÃ ng tÃ i\", \"trans\": \"state\"},\n    {\"word\": \"æœ¬èº«\", \"pinyin\": \"bÄ›n shÄ“n\", \"trans\": \"itself\"},\n    {\"word\": \"ç¥ç»ç½‘ç»œ\", \"pinyin\": \"shÃ©n jÄ«ng wÇng luÃ²\", \"trans\": \"neural network\"},\n    {\"word\": \"è¡¨ç°åŠ›\", \"pinyin\": \"biÇo xiÃ n lÃ¬\", \"trans\": \"expressiveness\"},\n    {\"word\": \"åŠ å…¥\", \"pinyin\": \"jiÄ rÃ¹\", \"trans\": \"add\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-trained\"},\n    {\"word\": \"æ–‡å­—\", \"pinyin\": \"wÃ©n zÃ¬\", \"trans\": \"text\"},\n    {\"word\": \"æ•…äº‹æ¿\", \"pinyin\": \"gÃ¹ shÃ¬ bÇn\", \"trans\": \"storyboard\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ n zhÃ¨ng\", \"trans\": \"validate\"},\n    {\"word\": \"æ¦‚å¿µ\", \"pinyin\": \"gÃ i niÃ n\", \"trans\": \"concept\"},\n    {\"word\": \"æ ¹æ®\", \"pinyin\": \"gÄ“n jÃ¹\", \"trans\": \"based on\"},\n    {\"word\": \"ã€ŠçŒ«å’Œè€é¼ ã€‹\", \"pinyin\": \"ã€ŠmÄo hÃ© lÇo shÇ”ã€‹\", \"trans\": \"Tom and Jerry\"},\n    {\"word\": \"åŠ¨ç”»ç‰‡\", \"pinyin\": \"dÃ²ng huÃ  piÃ n\", \"trans\": \"cartoon\"},\n    {\"word\": \"ç¼–åˆ¶\", \"pinyin\": \"biÄn zhÃ¬\", \"trans\": \"compile\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"åŸºçº¿\", \"pinyin\": \"jÄ« xiÃ n\", \"trans\": \"baseline\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç›¸æ¯”\", \"pinyin\": \"xiÄng bÇ\", \"trans\": \"compared to\"},\n    {\"word\": \"è¿è´¯\", \"pinyin\": \"liÃ¡n guÃ n\", \"trans\": \"coherent\"},\n    {\"word\": \"è®²è¿°\", \"pinyin\": \"jiÇng shÃ¹\", \"trans\": \"narrate\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"}\n]",
        "trans": "This article discusses the challenges faced by transformers in generating a one-minute video due to the inefficiency of self-attention layers in long contexts. The authors attempt to use Test-Time Training (TTT) layers, whose hidden states can themselves be neural networks, making them more expressive. By incorporating TTT layers into pre-trained transformers, it is possible to generate a one-minute video from a textual storyboard. To validate this concept, the authors created a dataset based on the \"Tom and Jerry\" cartoon. Compared to other baseline methods, the videos generated by TTT layers are more coherent and capable of telling complex stories.",
        "update_ts": "2025-04-08 09:12"
    }
}