{
    "date": {
        "ru": "16 января",
        "en": "January 16",
        "zh": "1月16日"
    },
    "time_utc": "2025-01-16 05:10",
    "weekday": 3,
    "issue_id": 1697,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08994",
            "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
            "url": "https://huggingface.co/papers/2501.08994",
            "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "0d164d45ba2a5c71",
            "authors": [
                "Chenyang Si",
                "Weichen Fan",
                "Zhengyao Lv",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore, 639798",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08994.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "RepVideo: стабильные представления для качественной генерации видео",
                    "desc": "Статья представляет RepVideo - улучшенную систему представлений для диффузионных моделей генерации видео на основе текста. Авторы обнаружили, что вариации в картах внимания между слоями приводят к нестабильным семантическим представлениям и снижают согласованность соседних кадров. RepVideo решает эту проблему путем накопления признаков из соседних слоев для создания обогащенных представлений. Эксперименты показывают, что RepVideo значительно улучшает способность генерировать точные пространственные образы и повышает временную согласованность при генерации видео."
                },
                "en": {
                    "title": "Enhancing Video Generation with Stable Representations",
                    "desc": "This paper presents RepVideo, a new framework designed to improve video generation using text-to-video diffusion models. It identifies issues with unstable semantic representations caused by variations in attention maps across different layers of the model. By accumulating features from neighboring layers, RepVideo creates more stable and enriched representations that enhance the model's ability to maintain consistency between adjacent frames. The results show that RepVideo significantly improves both the spatial accuracy of generated videos and their temporal coherence, leading to more realistic video outputs."
                },
                "zh": {
                    "title": "提升视频生成质量的RepVideo框架",
                    "desc": "本论文探讨了扩散模型在视频生成中的应用，提出了RepVideo框架以改善视频生成的质量。研究发现中间层特征的注意力图存在显著差异，这导致语义表示的不稳定性，进而影响相邻帧之间的相似性和时间一致性。RepVideo通过从相邻层累积特征，形成更丰富的表示，从而捕捉更稳定的语义信息。实验结果表明，RepVideo显著提高了生成视频的空间表现能力和时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09019",
            "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
            "url": "https://huggingface.co/papers/2501.09019",
            "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "c4c991699f684865",
            "authors": [
                "Jingyuan Chen",
                "Fuchen Long",
                "Jie An",
                "Zhaofan Qiu",
                "Ting Yao",
                "Jiebo Luo",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Rochester, Rochester, NY USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09019.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "🐍",
                "ru": {
                    "title": "Бесконечное видео: Ouroboros-Diffusion для непрерывной генерации согласованного контента",
                    "desc": "Эта статья представляет новый метод генерации видео произвольной длины под названием Ouroboros-Diffusion. Метод улучшает структурную и сюжетную согласованность видео с помощью нового подхода к выборке латентного пространства и механизма Subject-Aware Cross-Frame Attention. Авторы также вводят самоповторяющееся руководство, использующее информацию из предыдущих очищенных кадров для улучшения шумных кадров. Эксперименты на бенчмарке VBench показывают превосходство Ouroboros-Diffusion в сохранении согласованности субъектов, плавности движения и временной согласованности."
                },
                "en": {
                    "title": "Ouroboros-Diffusion: Enhancing Long Video Consistency and Coherence",
                    "desc": "The paper introduces Ouroboros-Diffusion, a new framework for improving long video generation using a pre-trained text-to-video model. It addresses the limitations of FIFO-Diffusion, particularly in maintaining long-range temporal consistency across video frames. The proposed method enhances structural consistency through a novel latent sampling technique and improves subject consistency with a Subject-Aware Cross-Frame Attention mechanism. Additionally, self-recurrent guidance is implemented to utilize information from previous frames, resulting in videos with better visual coherence and smoother transitions."
                },
                "zh": {
                    "title": "Ouroboros-Diffusion：提升视频生成一致性的创新框架",
                    "desc": "FIFO视频扩散是一种基于预训练文本到视频模型的长视频生成方法，但在生成视频时常常缺乏长时间的一致性。本文提出了Ouroboros-Diffusion框架，通过引入新的潜在采样技术和主题感知跨帧注意机制，增强了视频的结构和内容一致性。该方法确保了帧之间的平滑过渡，并通过自递归引导技术利用前面清晰帧的信息来改善后面噪声帧的去噪效果。实验结果表明，Ouroboros-Diffusion在主题一致性、运动平滑性和时间一致性方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08809",
            "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
            "url": "https://huggingface.co/papers/2501.08809",
            "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "d4d018c9adb2579c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#audio",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "XMusic: ИИ-композитор нового поколения с управляемыми эмоциями",
                    "desc": "Статья представляет XMusic - генерализованный фреймворк для генерации символической музыки, поддерживающий различные типы промптов. XMusic состоит из двух ключевых компонентов: XProjector для обработки промптов и XComposer для генерации музыки. Авторы также создали датасет XMIDI, содержащий более 100 тысяч MIDI-файлов с аннотациями эмоций и жанров. Согласно оценкам, XMusic значительно превосходит современные методы по качеству генерируемой музыки."
                },
                "en": {
                    "title": "XMusic: Emotionally Controlled Music Generation Made Easy!",
                    "desc": "This paper introduces XMusic, a new framework for generating symbolic music that can be controlled by emotional prompts. It includes two main components: XProjector, which converts various input types into musical elements, and XComposer, which generates and selects high-quality music. The framework uses a multi-task learning approach to ensure the generated music meets quality, emotional, and genre standards. Additionally, the authors created a large dataset, XMIDI, to support their research and demonstrate that XMusic outperforms existing methods in music generation."
                },
                "zh": {
                    "title": "XMusic：情感可控的高质量音乐生成",
                    "desc": "近年来，人工智能生成内容（AIGC）在图像合成和文本生成领域取得了显著进展，但在音乐生成方面仍面临挑战。本文提出了一种通用的符号音乐生成框架XMusic，能够通过灵活的提示生成可控情感和高质量的符号音乐。XMusic由两个核心组件组成：XProjector和XComposer，前者将多种模态的提示解析为音乐元素，后者则生成和选择高质量的音乐。通过构建大规模的XMIDI数据集和多任务学习方案，XMusic在音乐质量上显著优于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-01-15.html",
    "link_next": "2025-01-17.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1月15日"
    },
    "short_date_next": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 MiniMax-01 系列，包括 MiniMax-Text-01 和 MiniMax-VL-01。这些模型在处理长上下文方面具有卓越能力。核心在于闪电注意力和其高效扩展。我们将其与混合专家模型（MoE）集成，创建了一个具有 32 个专家和 4560 亿总参数的模型。我们开发了优化的并行策略和高效的计算通信重叠技术。这使我们能够在数百亿参数的模型上进行高效训练和推理。MiniMax-Text-01 的上下文窗口在训练期间可达到 100 万个标记，并在推理期间扩展到 400 万个标记。MiniMax-VL-01 通过使用 5120 亿视觉语言标记进行持续训练。实验表明，我们的模型在标准和内部基准上的性能与 GPT-4o 和 Claude-3.5-Sonnet 相当，同时提供 20-32 倍的上下文窗口。我们在 https://github.com/MiniMax-AI 公开发布了 MiniMax-01。",
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
        "pinyin": "Wǒmen jièshào le MiniMax-01 xìliè, bāokuò MiniMax-Text-01 hé MiniMax-VL-01. Zhèxiē móxíng zài chǔlǐ cháng shàngxìawén fāngmiàn jùyǒu zhuóyuè nénglì. Héxīn zàiyú shǎndiǎn zhùyìlì hé qí gāoxiào kuòzhǎn. Wǒmen jiāng qí yǔ hùn hé zhuānjiā móxíng (MoE) jíchéng, chuàngjiàn le yīgè jùyǒu 32 gè zhuānjiā hé 4560 yì zǒng cānshù de móxíng. Wǒmen kāifā le yōuhuà de bìngxíng cèlüè hé gāoxiào de jìsuàn tōngxìn zhòngdié jìshù. Zhè shǐ wǒmen nénggòu zài shùbǎiyì cānshù de móxíng shàng jìnxíng gāoxiào xùnliàn hé tuìlǐ. MiniMax-Text-01 de shàngxìawén chuāngkǒu zài xùnliàn qījiān kě dádào 100 wàn gè biāojì, bìng zài tuìlǐ qījiān kuòzhǎn dào 400 wàn gè biāojì. MiniMax-VL-01 tōngguò shǐyòng 5120 yì shìjué yǔyán biāojì jìnxíng chíxù xùnliàn. Shìyàn biǎomíng, wǒmen de móxíng zài biāozhǔn hé nèibù jīzhǔn shàng de xiàonénglì yǔ GPT-4o hé Claude-3.5-Sonnet xiāngdāng, tóngshí tígōng 20-32 bèi de shàngxìawén chuāngkǒu. Wǒmen zài https://github.com/MiniMax-AI gōngkāi fābù le MiniMax-01.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"系列\", \"pinyin\": \"xì liè\", \"trans\": \"series\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"上下文\", \"pinyin\": \"shàng xià wén\", \"trans\": \"context\"},\n    {\"word\": \"卓越\", \"pinyin\": \"zhuó yuè\", \"trans\": \"outstanding\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"核心\", \"pinyin\": \"hé xīn\", \"trans\": \"core\"},\n    {\"word\": \"闪电\", \"pinyin\": \"shǎn diàn\", \"trans\": \"lightning\"},\n    {\"word\": \"注意力\", \"pinyin\": \"zhù yì lì\", \"trans\": \"attention\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"expand\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"集成\", \"pinyin\": \"jí chéng\", \"trans\": \"integrate\"},\n    {\"word\": \"并行\", \"pinyin\": \"bìng xíng\", \"trans\": \"parallel\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"通信\", \"pinyin\": \"tōng xìn\", \"trans\": \"communication\"},\n    {\"word\": \"重叠\", \"pinyin\": \"chóng dié\", \"trans\": \"overlap\"},\n    {\"word\": \"技术\", \"pinyin\": \"jì shù\", \"trans\": \"technology\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"窗口\", \"pinyin\": \"chuāng kǒu\", \"trans\": \"window\"},\n    {\"word\": \"标记\", \"pinyin\": \"biāo jì\", \"trans\": \"token\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"持续\", \"pinyin\": \"chí xù\", \"trans\": \"continuous\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"}\n]",
        "trans": "We introduced the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01. These models excel in handling long contexts, with a core focus on flash attention and its efficient scaling. We integrated them with a Mixture of Experts (MoE) model, creating a model with 32 experts and a total of 4560 billion parameters. We developed optimized parallel strategies and efficient computation-communication overlap techniques. This enables us to perform efficient training and inference on models with hundreds of billions of parameters. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and expands to 4 million tokens during inference. MiniMax-VL-01 undergoes continuous training using 5120 billion vision-language tokens. Experiments show that our models perform comparably to GPT-4o and Claude-3.5-Sonnet on standard and internal benchmarks while providing a 20-32 times larger context window. We have made MiniMax-01 publicly available at https://github.com/MiniMax-AI.",
        "update_ts": "2025-01-15 09:11"
    }
}