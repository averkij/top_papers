{
    "date": {
        "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 31",
        "zh": "3æœˆ31æ—¥"
    },
    "time_utc": "2025-03-31 07:11",
    "weekday": 0,
    "issue_id": 2976,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.22675",
            "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
            "url": "https://huggingface.co/papers/2503.22675",
            "abstract": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.",
            "score": 18,
            "issue_id": 2971,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "092ab2fa75277891",
            "authors": [
                "Jiakai Tang",
                "Sunhao Dai",
                "Teng Shi",
                "Jun Xu",
                "Xu Chen",
                "Wen Chen",
                "Wu Jian",
                "Yuning Jiang"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22675.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "ReaRec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ensemble Reasoning Learning (ERL) Ğ¸ Progressive Reasoning Learning (PRL). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReaRec Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning",
                    "desc": "This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems."
                },
                "zh": {
                    "title": "ReaRecï¼šæå‡é¡ºåºæ¨èçš„æ¨ç†èƒ½åŠ›",
                    "desc": "é¡ºåºæ¨èï¼ˆSeqRecï¼‰æ—¨åœ¨é€šè¿‡æ•æ‰ç”¨æˆ·å†å²äº¤äº’ä¸­çš„é¡ºåºæ¨¡å¼æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼Œè¿™åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„æ¨èç³»ç»Ÿä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨ç›´æ¥çš„å‰å‘è®¡ç®—èŒƒå¼ï¼Œæœ€ç»ˆçš„éšè—çŠ¶æ€ä½œä¸ºç”¨æˆ·è¡¨ç¤ºï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å»ºæ¨¡ç”¨æˆ·åå¥½çš„å¤æ‚æ¼”å˜æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRecï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ¨èç³»ç»Ÿçš„æ¨ç†æ—¶è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡éšå¼å¤šæ­¥æ¨ç†å¢å¼ºç”¨æˆ·è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReaRecæ˜¾è‘—æé«˜äº†å¤šä¸ªé¡ºåºæ¨èæ¨¡å‹çš„æ€§èƒ½ï¼Œå¼€è¾Ÿäº†æ¨ç†æ—¶è®¡ç®—çš„æ–°ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22230",
            "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
            "url": "https://huggingface.co/papers/2503.22230",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.",
            "score": 13,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "a994668c51ac51c4",
            "authors": [
                "Wei Shen",
                "Guanlin Liu",
                "Zheng Wu",
                "Ruofei Zhu",
                "Qingping Yang",
                "Chao Xin",
                "Yu Yue",
                "Lin Yan"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22230.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#data",
                    "#reasoning",
                    "#alignment",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RLHF: Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RTV) Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (GenRM), Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pre-PPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RLHF, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing RLHF: Bridging Data Gaps for Better AI Alignment",
                    "desc": "This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance."
                },
                "zh": {
                    "title": "ä¼˜åŒ–äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶é›†ä¸­åœ¨ç®—æ³•æ”¹è¿›ä¸Šï¼Œä½†æç¤ºæ•°æ®æ„å»ºçš„é‡è¦æ€§å´è¢«å¿½è§†ã€‚æœ¬æ–‡æ¢è®¨äº†RLHFæ€§èƒ½æ‰©å±•ä¸­çš„æ•°æ®é©±åŠ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¥–åŠ±é»‘å®¢å’Œå“åº”å¤šæ ·æ€§ä¸‹é™çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆå¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ¨ç†ä»»åŠ¡éªŒè¯å™¨ï¼ˆRTVï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä»¥å‡è½»å¥–åŠ±é»‘å®¢ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œä»¥ä¿æŒå“åº”å¤šæ ·æ€§å¹¶å¢å¼ºå­¦ä¹ æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22194",
            "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2503.22194",
            "abstract": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.",
            "score": 12,
            "issue_id": 2971,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "6d93cc886c16f9b0",
            "authors": [
                "Yunhong Min",
                "Daehyeon Choi",
                "Kyeongmin Yeo",
                "Jihyun Lee",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22194.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ORIGEN - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ›Ğ°Ğ½Ğ¶ĞµĞ²ĞµĞ½Ğ° Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ORIGEN Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN",
                    "desc": "ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations."
                },
                "zh": {
                    "title": "ORIGENï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„3Dæ–¹å‘å®šä½æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ORIGENï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°3Dæ–¹å‘å®šä½çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Då®šä½ä¸Šï¼Œç¼ºä¹å¯¹3Dæ–¹å‘çš„æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±å¼•å¯¼çš„é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹è¿›è¡Œ3Dæ–¹å‘ä¼°è®¡ï¼Œå¹¶ç»“åˆä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæµæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORIGENåœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºåŸºäºè®­ç»ƒå’Œæµ‹è¯•æ—¶å¼•å¯¼çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21614",
            "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
            "url": "https://huggingface.co/papers/2503.21614",
            "abstract": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.",
            "score": 10,
            "issue_id": 2976,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "805b7cd4ec307c34",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° LRM - Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Efficiency in Large Reasoning Models",
                    "desc": "This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ•ˆç‡ï¼ŒåŠ©åŠ›å¤§å‹æ¨¡å‹å‘å±•",
                    "desc": "æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1ï¼Œé€šè¿‡æ‰©å±•æ¨ç†é“¾ï¼ˆCoTï¼‰çš„é•¿åº¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ç”Ÿæˆè¿‡é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå†…å®¹å†—ä½™ï¼ˆä¾‹å¦‚ï¼Œé‡å¤å®šä¹‰ï¼‰ã€å¯¹ç®€å•é—®é¢˜çš„è¿‡åº¦åˆ†æï¼Œä»¥åŠå¯¹å¤æ‚ä»»åŠ¡çš„å¤šæ¡æ¨ç†è·¯å¾„çš„è¡¨é¢æ¢ç´¢ã€‚è¿™ç§ä½æ•ˆæ€§åœ¨è®­ç»ƒã€æ¨ç†å’Œå®é™…åº”ç”¨ä¸­å¼•å‘äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¤ç‰Œç»æµè‡³å…³é‡è¦ã€‚æœ¬æ–‡ç»¼è¿°äº†æ”¹å–„LRMsæ¨ç†æ•ˆç‡çš„æœ€æ–°åŠªåŠ›ï¼Œè¯†åˆ«äº†å¸¸è§çš„ä½æ•ˆæ¨¡å¼ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21821",
            "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
            "url": "https://huggingface.co/papers/2503.21821",
            "abstract": "We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.",
            "score": 9,
            "issue_id": 2972,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "5254bbde255c8669",
            "authors": [
                "Kaiyue Feng",
                "Yilun Zhao",
                "Yixin Liu",
                "Tianyu Yang",
                "Chen Zhao",
                "John Sous",
                "Arman Cohan"
            ],
            "affiliations": [
                "New York University",
                "Notre Dame University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21821.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ",
                    "desc": "PHYSICS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1297 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RAG Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Benchmarking Physics Problem Solving with PHYSICS",
                    "desc": "The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges."
                },
                "zh": {
                    "title": "ç‰©ç†é—®é¢˜è§£å†³çš„æ–°åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†PHYSICSï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤§å­¦ç‰©ç†é—®é¢˜è§£å†³åŸºå‡†ã€‚å®ƒåŒ…å«1297ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ç»å…¸åŠ›å­¦ã€é‡å­åŠ›å­¦ã€çƒ­åŠ›å­¦ä¸ç»Ÿè®¡åŠ›å­¦ã€ç”µç£å­¦ã€åŸå­ç‰©ç†å’Œå…‰å­¦å…­ä¸ªæ ¸å¿ƒé¢†åŸŸã€‚æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦é«˜çº§ç‰©ç†çŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹o3-miniçš„å‡†ç¡®ç‡ä»…ä¸º59.9%ï¼Œè¿™çªæ˜¾äº†è§£å†³é«˜æ°´å¹³ç§‘å­¦é—®é¢˜çš„é‡å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19693",
            "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
            "url": "https://huggingface.co/papers/2503.19693",
            "abstract": "Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance",
            "score": 9,
            "issue_id": 2976,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "2c907e98a0aafb46",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#low_resource",
                    "#training",
                    "#optimization",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…",
                    "desc": "AdaptiVocab - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° n-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…. AdaptiVocab Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ n-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdaptiVocab ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 25% Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Efficiency with Domain-Specific Vocabulary",
                    "desc": "This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance."
                },
                "zh": {
                    "title": "æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨æ¨¡å‹ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šåŠŸèƒ½æ€§ï¼Œä½†å…¶å¹¿æ³›åº”ç”¨ä¼´éšç€é«˜æ˜‚çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦è¿›è¡Œå‰å‘ä¼ æ’­ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ï¼Œé€šç”¨èƒ½åŠ›å¹¶éå¿…è¦ï¼Œå¯ä»¥é€šè¿‡æé«˜æ•ˆç‡æ¥è¿›è¡Œäº¤æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸé€‚åº”æ–¹æ³•â€”â€”AdaptiVocabï¼Œé€šè¿‡è°ƒæ•´è¯æ±‡è¡¨æ¥é™ä½å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ï¼Œæ—¨åœ¨æé«˜LLMåœ¨ä½èµ„æºé¢†åŸŸçš„æ•ˆç‡ã€‚AdaptiVocabå¯ä»¥åº”ç”¨äºä»»ä½•åˆ†è¯å™¨å’Œæ¶æ„ï¼Œé€šè¿‡ç”¨ç‰¹å®šé¢†åŸŸçš„n-gramä»£æ›¿åŸæœ‰çš„tokensï¼Œå‡å°‘è¾“å…¥å¤„ç†å’Œè¾“å‡ºç”Ÿæˆæ‰€éœ€çš„tokensæ•°é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22236",
            "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
            "url": "https://huggingface.co/papers/2503.22236",
            "abstract": "With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.",
            "score": 7,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "5024619189472d8c",
            "authors": [
                "Chongjie Ye",
                "Yushuang Wu",
                "Ziteng Lu",
                "Jiahao Chang",
                "Xiaoyang Guo",
                "Jiaqing Zhou",
                "Hao Zhao",
                "Xiaoguang Han"
            ],
            "affiliations": [
                "ByteDance",
                "The Chinese University of Hong Kong, Shenzhen",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22236.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ»Ğ¾ÑĞºĞ¾Ğ³Ğ¾ Ğº Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ¼Ñƒ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Hi3DGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Hi3DGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging Normals for High-Fidelity 3D Generation",
                    "desc": "The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸ3Då‡ ä½•ä½“ç”Ÿæˆçš„æ–°æ–¹å‘",
                    "desc": "éšç€å¯¹ä»2Då›¾åƒç”Ÿæˆé«˜ä¿çœŸ3Dæ¨¡å‹çš„éœ€æ±‚å¢åŠ ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®å†ç°ç»†è‡´å‡ ä½•ç»†èŠ‚æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Hi3DGenï¼Œä¸€ä¸ªé€šè¿‡æ³•çº¿æ¡¥æ¥ç”Ÿæˆé«˜ä¿çœŸ3Då‡ ä½•ä½“çš„æ–°æ¡†æ¶ã€‚Hi3DGenåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾åƒåˆ°æ³•çº¿ä¼°è®¡å™¨ã€æ³•çº¿åˆ°å‡ ä½•ä½“å­¦ä¹ æ–¹æ³•å’Œ3Dæ•°æ®åˆæˆç®¡é“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆä¸°å¯Œå‡ ä½•ç»†èŠ‚æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20785",
            "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
            "url": "https://huggingface.co/papers/2503.20785",
            "abstract": "We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.",
            "score": 6,
            "issue_id": 2974,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "1a3973da9e51afd7",
            "authors": [
                "Tianqi Liu",
                "Zihao Huang",
                "Zhaoxi Chen",
                "Guangcong Wang",
                "Shoukang Hu",
                "Liao Shen",
                "Huiqiang Sun",
                "Zhiguo Cao",
                "Wei Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Great Bay University",
                "Huazhong University of Science and Technology",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20785.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ 4D-ÑÑ†ĞµĞ½Ğµ",
                    "desc": "Free4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Free4D ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Generation from a Single Image",
                    "desc": "Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology."
                },
                "zh": {
                    "title": "ä»å•å›¾åƒç”Ÿæˆ4Dåœºæ™¯çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†Free4Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è°ƒä¼˜çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆ4Dåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºç‰©ä½“çº§ç”Ÿæˆï¼Œå¯¼è‡´åœºæ™¯çº§ç”Ÿæˆä¸å¯è¡Œï¼Œæˆ–è€…ä¾èµ–äºå¤§è§„æ¨¡å¤šè§†è§’è§†é¢‘æ•°æ®é›†è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒï¼Œä¸”ç”±äº4Dåœºæ™¯æ•°æ®ç¨€ç¼ºï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯æç‚¼é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä»¥å®ç°ä¸€è‡´çš„4Dåœºæ™¯è¡¨ç¤ºï¼Œè¿™æä¾›äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„ä¼˜åŠ¿ã€‚é€šè¿‡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œç„¶ååˆå§‹åŒ–4Då‡ ä½•ç»“æ„ï¼Œæœ€ç»ˆå®ç°å®æ—¶ã€å¯æ§çš„4Dåœºæ™¯ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22268",
            "title": "Segment Any Motion in Videos",
            "url": "https://huggingface.co/papers/2503.22268",
            "abstract": "Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.",
            "score": 5,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "084606e82bff72ff",
            "authors": [
                "Nan Huang",
                "Wenzhao Zheng",
                "Chenfeng Xu",
                "Kurt Keutzer",
                "Shanghang Zhang",
                "Angjoo Kanazawa",
                "Qianqian Wang"
            ],
            "affiliations": [
                "Peking University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22268.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ DINO Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ SAM2 Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Moving Object Segmentation with Motion-Semantic Integration",
                    "desc": "This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects."
                },
                "zh": {
                    "title": "åˆ›æ–°ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œæå‡è§†è§‰ç†è§£èƒ½åŠ›",
                    "desc": "ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ˜¯ç†è§£è§†è§‰åœºæ™¯çš„é‡è¦ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å…‰æµæ¥æä¾›è¿åŠ¨çº¿ç´¢ï¼Œä½†åœ¨å¤„ç†éƒ¨åˆ†è¿åŠ¨ã€å¤æ‚å˜å½¢ã€è¿åŠ¨æ¨¡ç³Šå’ŒèƒŒæ™¯å¹²æ‰°æ—¶å¸¸å¸¸æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆé•¿è·ç¦»è½¨è¿¹è¿åŠ¨çº¿ç´¢ä¸åŸºäºDINOçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¿­ä»£æç¤ºç­–ç•¥åˆ©ç”¨SAM2è¿›è¡Œåƒç´ çº§æ©è†œç»†åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨æ—¶ç©ºè½¨è¿¹æ³¨æ„åŠ›å’Œè¿åŠ¨-è¯­ä¹‰è§£è€¦åµŒå…¥ï¼Œä¼˜å…ˆè€ƒè™‘è¿åŠ¨ï¼ŒåŒæ—¶æ•´åˆè¯­ä¹‰æ”¯æŒï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22329",
            "title": "A Refined Analysis of Massive Activations in LLMs",
            "url": "https://huggingface.co/papers/2503.22329",
            "abstract": "Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.",
            "score": 4,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "025b5484847cd3d9",
            "authors": [
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Abhay Kumar",
                "Fabian GÃ¼ra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22329.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#long_context",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² LLM: Ğ½Ğµ Ğ²ÑĞµ Ñ‚Ğ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GLU Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ²ÑĞµ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹, Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Target Variance Rescaling Ñ Attention KV bias Ğ¸Ğ»Ğ¸ Dynamic Tanh, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Balancing Act: Mitigating Massive Activations in LLMs",
                    "desc": "This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques."
                },
                "zh": {
                    "title": "å¤§æ¿€æ´»çš„æŒ‘æˆ˜ä¸æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¤§æ¿€æ´»ç°è±¡ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨ä½ç²¾åº¦è®­ç»ƒå’Œé‡åŒ–ä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†å¤šç§LLMæ¶æ„ï¼ŒåŒ…æ‹¬åŸºäºGLUå’ŒéGLUçš„æ¨¡å‹ï¼Œå‘ç°å¹¶éæ‰€æœ‰å¤§æ¿€æ´»éƒ½æ˜¯æœ‰å®³çš„ï¼ŒæŠ‘åˆ¶å®ƒä»¬å¹¶ä¸ä¼šå¯¼è‡´å›°æƒ‘åº¦çš„çˆ†ç‚¸æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å´©æºƒã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç°æœ‰çš„ç¼“è§£ç­–ç•¥å¦‚æ³¨æ„åŠ›KVåç½®åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯æ¨¡å‹ç‰¹å®šçš„ä¸”æ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æ··åˆç¼“è§£ç­–ç•¥ï¼Œç»“åˆç›®æ ‡æ–¹å·®é‡æ ‡å®šï¼ˆTVRï¼‰ä¸æ³¨æ„åŠ›KVåç½®æˆ–åŠ¨æ€Tanhï¼ˆDyTï¼‰ï¼ŒæˆåŠŸå¹³è¡¡äº†å¤§æ¿€æ´»çš„ç¼“è§£ä¸ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„ä¿æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21779",
            "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
            "url": "https://huggingface.co/papers/2503.21779",
            "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
            "score": 2,
            "issue_id": 2976,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "628d77c9c1bdcd9e",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#healthcare",
                    "#cv"
                ],
                "emoji": "ğŸ«",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ 4D-ĞšĞ¢ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X^2-Gaussian - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ĞšĞ¢ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑĞ¿Ğ¸Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. X^2-Gaussian Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 9.93 Ğ´Ğ‘ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PSNR, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 4D CT with Continuous Motion Modeling",
                    "desc": "This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods."
                },
                "zh": {
                    "title": "X^2-Gaussianï¼šæ— ç¡¬ä»¶çš„é«˜ä¿çœŸ4D CTé‡å»ºæ–°æ–¹æ³•",
                    "desc": "å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4D CTï¼‰é‡å»ºå¯¹äºæ•æ‰åŠ¨æ€è§£å‰–å˜åŒ–è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„ç›¸ä½åˆ†ç®±å·¥ä½œæµç¨‹å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å°†æ—¶é—´åˆ†è¾¨ç‡ç¦»æ•£åŒ–ä¸ºå›ºå®šç›¸ä½ï¼Œä½¿ç”¨å‘¼å¸é—¨æ§è®¾å¤‡ï¼Œå¯¼è‡´è¿åŠ¨é”™ä½å¹¶é™åˆ¶ä¸´åºŠå®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶X^2-Gaussianï¼Œé€šè¿‡ç»“åˆåŠ¨æ€è¾å°„é«˜æ–¯ç‚¹äº‘å’Œè‡ªç›‘ç£å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œå®ç°è¿ç»­æ—¶é—´çš„4D CTé‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„é¢„æµ‹æ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ï¼Œæ¶ˆé™¤äº†ç›¸ä½ç¦»æ•£åŒ–çš„éœ€æ±‚ï¼Œå¹¶å¼•å…¥ç”Ÿç†é©±åŠ¨çš„å‘¨æœŸä¸€è‡´æ€§æŸå¤±ï¼Œç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21732",
            "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
            "url": "https://huggingface.co/papers/2503.21732",
            "abstract": "Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.",
            "score": 2,
            "issue_id": 2972,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "f17ea311cc796683",
            "authors": [
                "Xianglong He",
                "Zi-Xin Zou",
                "Chia-Hao Chen",
                "Yuan-Chen Guo",
                "Ding Liang",
                "Chun Yuan",
                "Wanli Ouyang",
                "Yan-Pei Cao",
                "Yangguang Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21732.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ",
                    "desc": "SparseFlex - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 1024^3 Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Flexicubes Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°ÑÑ‰Ğ¸Ñ… Ğº Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ñ€ÑƒÑÑ‚ÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. SparseFlex Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑĞµÑ‚ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail",
                    "desc": "This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies."
                },
                "zh": {
                    "title": "SparseFlexï¼šé«˜åˆ†è¾¨ç‡3Dç½‘æ ¼é‡å»ºçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSparseFlexçš„æ–°å‹ç¨€ç–ç»“æ„ç­‰å€¼é¢è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ä¿çœŸ3Dç½‘æ ¼é‡å»ºä¸­çš„æŒ‘æˆ˜ã€‚SparseFlexèƒ½å¤Ÿç›´æ¥ä»æ¸²æŸ“æŸå¤±ä¸­è¿›è¡Œå¯å¾®åˆ†çš„ç½‘æ ¼é‡å»ºï¼Œæ”¯æŒé«˜è¾¾1024^3çš„åˆ†è¾¨ç‡ã€‚é€šè¿‡ç»“åˆFlexicubesçš„å‡†ç¡®æ€§å’Œç¨€ç–ä½“ç´ ç»“æ„ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†å¼€æ”¾è¡¨é¢ï¼Œå¹¶å¼•å…¥äº†åŸºäºè§†é”¥çš„åˆ†æ®µä½“ç´ è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseFlexåœ¨é‡å»ºç²¾åº¦ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒæˆåŠŸç”Ÿæˆäº†å…·æœ‰ä»»æ„æ‹“æ‰‘çš„é«˜åˆ†è¾¨ç‡ã€ç»†èŠ‚ä¸°å¯Œçš„3Då½¢çŠ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18968",
            "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
            "url": "https://huggingface.co/papers/2503.18968",
            "abstract": "Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
            "score": 2,
            "issue_id": 2973,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "662e73dea1e23d07",
            "authors": [
                "Ziyue Wang",
                "Junde Wu",
                "Chang Han Low",
                "Yueming Jin"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18968.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#multimodal",
                    "#reasoning",
                    "#interpretability",
                    "#healthcare",
                    "#agents"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "MedAgent-Pro: Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MedAgent-Pro - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. MedAgent-Pro Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MedAgent-Pro Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2D Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis",
                    "desc": "This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings."
                },
                "zh": {
                    "title": "æå‡åŒ»ç–—è¯Šæ–­çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMedAgent-Proçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€åŒ»ç–—è¯Šæ–­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†åŸºäºçŸ¥è¯†çš„æ¨ç†å’Œå¤šå·¥å…·ä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å¹¶ç”Ÿæˆå¯é çš„è¯Šæ–­è®¡åˆ’ã€‚é€šè¿‡å¯¹2Då’Œ3DåŒ»ç–—è¯Šæ–­ä»»åŠ¡çš„å…¨é¢å®éªŒï¼ŒMedAgent-Proå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›äº†ç²¾ç¡®çš„è¯Šæ–­ï¼Œè¿˜å…·å¤‡è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œé€‚åˆä¸´åºŠåº”ç”¨ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-28.html",
    "link_next": "2025-04-01.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "28.03",
        "en": "03/28",
        "zh": "3æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "01.04",
        "en": "04/01",
        "zh": "4æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 5,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVideo-R1çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ¿€å‘è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚å—DeepSeek-R1æˆåŠŸçš„å¯å‘ï¼Œä½œè€…æå‡ºäº†T-GRPOç®—æ³•ï¼Œä»¥è§£å†³ç›´æ¥åº”ç”¨RLè®­ç»ƒä¸­çš„æ—¶é—´å»ºæ¨¡å’Œé«˜è´¨é‡æ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚ä»–ä»¬è¿˜åˆ›å»ºäº†ä¸¤ä¸ªåŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®çš„æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†Video-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒVideo-R1-7Båœ¨VSI-benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†35.8%ï¼Œè¶…è¶Šäº†å•†ä¸šæ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®éƒ½å·²å…¬å¼€ã€‚",
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVideo-R1çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ¿€å‘è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚å—DeepSeek-R1æˆåŠŸçš„å¯å‘ï¼Œä½œè€…æå‡ºäº†T-GRPOç®—æ³•ï¼Œä»¥è§£å†³ç›´æ¥åº”ç”¨RLè®­ç»ƒä¸­çš„æ—¶é—´å»ºæ¨¡å’Œé«˜è´¨é‡æ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚ä»–ä»¬è¿˜åˆ›å»ºäº†ä¸¤ä¸ªåŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®çš„æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†Video-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒVideo-R1-7Båœ¨VSI-benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†35.8%ï¼Œè¶…è¶Šäº†å•†ä¸šæ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®éƒ½å·²å…¬å¼€ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ©i Video-R1 de xÄ«n fÄng fÇ, yÃ²ng yÃº zÃ i duÅ mÃ³ tÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng zhÅng jÄ« fÄ shÃ¬ pÇn tuÄ« lÇ nÃ©ng lÃ¬. shÃ²u DeepSeek-R1 chÃ©ng gÅng de qÇ fÇ, zuÃ² zhÄ› tÃ­ chÅ« le T-GRPO suÃ n fÇ, yÇ jiÄ› juÃ© zhÃ­ jiÄ“ yÃ²ng RL xÃ¹n liÃ n zhÅng de shÃ­ jiÄn jiÃ n mÃ³ hÃ© gÄo zhÃ¬ liÃ ng shÃ¹ juÃ© de quÄ“ fÃ¡ de wÃ¨n tÃ­. tÄ men hÃ¡i chuÃ ng jiÃ n le liÇng gÃ¨ bÄo hÃ¡n tÃº xiÃ ng hÃ© shÃ¬ pÇn shÃ¹ juÃ© de shÃ¹ ju ji, bÃ¬ng zhÃ n shÃ¬ le Video-R1 zÃ i duÅ gÃ¨ shÃ¬ pÇn tuÄ« lÇ jÄ« zhÇ”n shÃ ng de xiÇn zhÃ¹ gÇi jÃ¬n. tÃ¨ biÃ© shÃ¬, Video-R1-7B zÃ i VSI-bench shÃ ng de zhÇ”n quÃ¨ lÇœ dÃ¡ dÃ o le 35.8%, chÄo yuÃ¨ le shÄng yÃ¨ mÃ³ xÃ­ng GPT-4o. suÇ’ yÇ’u dÃ i mÇ, mÃ³ xÃ­ng hÃ© shÃ¹ ju dÅu yÇ gÅng kÄi.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¿€å‘\", \"pinyin\": \"jÄ« fÄ\", \"trans\": \"stimulate\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"å¯å‘\", \"pinyin\": \"qÇ fÄ\", \"trans\": \"inspire\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç®—æ³•\", \"pinyin\": \"suÃ n fÇ\", \"trans\": \"algorithm\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"ç›´æ¥\", \"pinyin\": \"zhÃ­ jiÄ“\", \"trans\": \"direct\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"æ—¶é—´\", \"pinyin\": \"shÃ­ jiÄn\", \"trans\": \"time\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ n mÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"ç¼ºä¹\", \"pinyin\": \"quÄ“ fÃ¡\", \"trans\": \"lack\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improvement\"},\n    {\"word\": \"å‡†ç¡®ç‡\", \"pinyin\": \"zhÇ”n quÃ¨ lÇœ\", \"trans\": \"accuracy\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å•†ä¸š\", \"pinyin\": \"shÄng yÃ¨\", \"trans\": \"commercial\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new method called Video-R1 for eliciting video reasoning capabilities in multimodal large language models. Inspired by the success of DeepSeek-R1, the authors propose the T-GRPO algorithm to address the issues of temporal modeling in direct RL training and the lack of high-quality data. They also created two datasets containing image and video data and demonstrated significant improvements of Video-R1 on multiple video reasoning benchmarks. Notably, Video-R1-7B achieved an accuracy of 35.8% on the VSI-bench, surpassing the commercial model GPT-4o. All code, models, and data have been made publicly available.",
        "update_ts": "2025-03-30 12:41"
    }
}