{
    "date": {
        "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 17",
        "zh": "3æœˆ17æ—¥"
    },
    "time_utc": "2025-03-17 02:21",
    "weekday": 0,
    "issue_id": 2730,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07677",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "url": "https://huggingface.co/papers/2503.07677",
            "abstract": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.",
            "score": 6,
            "issue_id": 2730,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "913b88ac595cc8b6",
            "authors": [
                "Kwanyoung Kim",
                "Byeongsu Sim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PLADIS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PLADIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. PLADIS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ»ÑÑ‡ Ğ² ÑĞ»Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion Models with Sparse Attention",
                    "desc": "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."
                },
                "zh": {
                    "title": "PLADISï¼šé«˜æ•ˆæå‡æ‰©æ•£æ¨¡å‹çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡æ¡ä»¶æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸å¼•å¯¼è’¸é¦æ¨¡å‹ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºPLADISï¼Œé€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æ¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚U-Net/Transformerï¼‰ã€‚PLADISåœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„softmaxå’Œç¨€ç–å¯¹åº”ç‰©ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œæ˜¾è‘—æ”¹å–„æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11514",
            "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2503.11514",
            "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.",
            "score": 4,
            "issue_id": 2730,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "d31bf6f9bd4bc86b",
            "authors": [
                "Pengxin Guo",
                "Runxi Wang",
                "Shuang Zeng",
                "Jinjing Zhu",
                "Haoning Jiang",
                "Yanran Wang",
                "Yuyin Zhou",
                "Feifei Wang",
                "Hui Xiong",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA",
                "Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA",
                "Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China",
                "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
                "Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China",
                "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China",
                "School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China",
                "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11514.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#benchmark",
                    "#security",
                    "#survey",
                    "#healthcare",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° (GIA) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ GIA Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ñ‚Ğ°Ğº Ğ² FL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ GIA ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks",
                    "desc": "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."
                },
                "zh": {
                    "title": "æå‡è”é‚¦å­¦ä¹ éšç§ä¿æŠ¤çš„é˜²å¾¡ç­–ç•¥",
                    "desc": "è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§ä¿æŠ¤éšç§çš„åä½œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œä¸éœ€è¦å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å…±äº«æ¢¯åº¦ä¿¡æ¯ï¼Œç§å¯†ä¿¡æ¯ä»ç„¶å¯èƒ½è¢«æ³„éœ²ï¼Œå¹¶å—åˆ°æ¢¯åº¦åæ¼”æ”»å‡»ï¼ˆGIAï¼‰çš„å¨èƒã€‚æœ¬æ–‡å¯¹ç°æœ‰çš„GIAæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„å›é¡¾å’Œåˆ†ç±»ï¼Œå¹¶åˆ†æäº†ä¸‰ç§ç±»å‹çš„GIAåœ¨FLä¸­çš„è¡¨ç°å’Œå±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„é˜²å¾¡æ–¹æ¡ˆï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨è®¾è®¡FLæ¡†æ¶æ—¶æ›´å¥½åœ°ä¿æŠ¤éšç§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11647",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "url": "https://huggingface.co/papers/2503.11647",
            "abstract": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "7e72838ea84ed904",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xiao Fu",
                "Xintao Wang",
                "Lianrui Mu",
                "Jinwen Cao",
                "Zuozhu Liu",
                "Haoji Hu",
                "Xiang Bai",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "HUST",
                "Kuaishou Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11647.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#video",
                    "#games"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ReCamMaster - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-video Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Unreal Engine 5. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ°ÑƒÑ‚Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "ReCamMaster: Mastering Camera Control in Video Generation",
                    "desc": "This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution."
                },
                "zh": {
                    "title": "é‡å¡‘è§†é¢‘åŠ¨æ€ï¼ŒæŒæ§ç›¸æœºè½¨è¿¹",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†åœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹ç”Ÿæˆè§†é¢‘æ—¶çš„ç›¸æœºæ§åˆ¶é—®é¢˜ã€‚å°½ç®¡æ”¹å˜è§†é¢‘çš„ç›¸æœºè½¨è¿¹å¾ˆé‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ReCamMasterï¼Œä¸€ä¸ªåŸºäºç”Ÿæˆæ¨¡å‹çš„è§†é¢‘é‡æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ–°çš„ç›¸æœºè½¨è¿¹ä¸‹é‡ç°è¾“å…¥è§†é¢‘çš„åŠ¨æ€åœºæ™¯ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤šç›¸æœºåŒæ­¥è§†é¢‘æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¾“å…¥ä¸‹è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11069",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "url": "https://huggingface.co/papers/2503.11069",
            "abstract": "Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "29e714954ed20978",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ API Ğ¸ GUI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM ÑÑ‚Ğ¸Ñ€Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: API and GUI LLM Agents Unite",
                    "desc": "This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents."
                },
                "zh": {
                    "title": "APIä¸GUIä»£ç†çš„æ¯”è¾ƒä¸èåˆä¹‹è·¯",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»ä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå‘å±•åˆ°èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€å‘½ä»¤ç›´æ¥è½¬åŒ–ä¸ºå®é™…æ“ä½œçš„è½¯ä»¶ä»£ç†ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¯”è¾ƒäº†åŸºäºAPIçš„LLMä»£ç†å’ŒåŸºäºGUIçš„LLMä»£ç†ï¼Œåˆ†æäº†å®ƒä»¬åœ¨æ¶æ„å¤æ‚æ€§ã€å¼€å‘å·¥ä½œæµç¨‹å’Œç”¨æˆ·äº¤äº’æ¨¡å‹ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬æ¢è®¨äº†å…³é”®ç»´åº¦ï¼Œå¹¶å¼ºè°ƒäº†æ··åˆæ–¹æ³•åœ¨åˆ©ç”¨ä¸¤è€…äº’è¡¥ä¼˜åŠ¿æ–¹é¢çš„åœºæ™¯ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æŒ‡å‡ºLLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–åˆ›æ–°å°†æ¨¡ç³ŠAPIå’ŒGUIä»£ç†ä¹‹é—´çš„ç•Œé™ï¼Œä¸ºå„ç§å®é™…åº”ç”¨æä¾›æ›´çµæ´»ã€é€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09279",
            "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
            "url": "https://huggingface.co/papers/2503.09279",
            "abstract": "Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.",
            "score": 1,
            "issue_id": 2730,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "edf6712b564fd37a",
            "authors": [
                "Luozheng Qin",
                "Zhiyu Tan",
                "Mengping Yang",
                "Xiaomeng Yang",
                "Hao Li"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#alignment"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "Cockatiel: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ (VDC) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Cockatiel. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cockatiel-13B Ğ¸ ĞµĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Cockatiel-8B. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Cockatiel Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ VDCSCORE Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Bridging Vision and Language with Cockatiel for Enhanced Video Captioning",
                    "desc": "This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘æè¿°çš„æ™ºèƒ½åŒ–ä¸äººæ€§åŒ–",
                    "desc": "è§†é¢‘è¯¦ç»†æè¿°ï¼ˆVDCï¼‰æ˜¯è¿æ¥è§†è§‰å’Œè¯­è¨€çš„é‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¯¹å¤æ‚è§†é¢‘å†…å®¹è¿›è¡Œç»†è‡´çš„æè¿°ã€‚æœ¬æ–‡é¦–å…ˆå¯¹å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶ç³»ç»Ÿåœ°è¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šå¯¹ç‰¹å®šæè¿°æ–¹é¢çš„åè§èƒ½åŠ›å’Œä¸äººç±»åå¥½çš„ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Cockatielï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆäº†åˆæˆå’Œäººç±»å¯¹é½çš„è®­ç»ƒï¼Œä»¥æé«˜VDCæ€§èƒ½ã€‚é€šè¿‡å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VDCSCOREä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨ä¸äººç±»åå¥½çš„æ¯”è¾ƒä¸­å¤§å¹…è¶…è¶Šäº†é¢†å…ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-14.html",
    "link_next": "2025-03-18.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ­£åˆ™åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿ä¸ä½¿ç”¨æ­£åˆ™åŒ–å±‚ï¼ŒTransformersä¹Ÿèƒ½é€šè¿‡ä¸€ç§ç®€å•çš„æŠ€æœ¯è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ç§æŠ€æœ¯å«åšDynamic Tanh (DyT)ï¼Œæ˜¯ä¸€ç§å…ƒç´ çº§æ“ä½œã€‚DyTçš„çµæ„Ÿæ¥è‡ªäºå±‚æ­£åˆ™åŒ–åœ¨Transformersä¸­å¸¸å¸¸äº§ç”Ÿç±»ä¼¼tanhçš„Så½¢è¾“å…¥-è¾“å‡ºæ˜ å°„ã€‚é€šè¿‡ä½¿ç”¨DyTï¼ŒTransformerså¯ä»¥åœ¨æ²¡æœ‰æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹åŒ¹æ•Œæˆ–è¶…è¶Šæœ‰æ­£åˆ™åŒ–çš„æ€§èƒ½ï¼Œé€šå¸¸ä¸éœ€è¦è¶…å‚æ•°è°ƒæ•´ã€‚",
        "title": "Transformers without Normalization",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ­£åˆ™åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿ä¸ä½¿ç”¨æ­£åˆ™åŒ–å±‚ï¼ŒTransformersä¹Ÿèƒ½é€šè¿‡ä¸€ç§ç®€å•çš„æŠ€æœ¯è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ç§æŠ€æœ¯å«åšDynamic Tanh (DyT)ï¼Œæ˜¯ä¸€ç§å…ƒç´ çº§æ“ä½œã€‚DyTçš„çµæ„Ÿæ¥è‡ªäºå±‚æ­£åˆ™åŒ–åœ¨Transformersä¸­å¸¸å¸¸äº§ç”Ÿç±»ä¼¼tanhçš„Så½¢è¾“å…¥-è¾“å‡ºæ˜ å°„ã€‚é€šè¿‡ä½¿ç”¨DyTï¼ŒTransformerså¯ä»¥åœ¨æ²¡æœ‰æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹åŒ¹æ•Œæˆ–è¶…è¶Šæœ‰æ­£åˆ™åŒ–çš„æ€§èƒ½ï¼Œé€šå¸¸ä¸éœ€è¦è¶…å‚æ•°è°ƒæ•´ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le zhÃ¨ngzÃ©huÃ  cÃ©ng zÃ i xiÃ ndÃ i shÃ©njÄ«ng wÇngluÃ² zhÅng de zuÃ²yÃ²ng. YÃ¡njiÅ« fÄxiÃ n, jÃ­shÇ bÃ¹ shÇyÃ²ng zhÃ¨ngzÃ©huÃ  cÃ©ng, Transformers yÄ› nÃ©ng tÅngguÃ² yÄ«zhÇ’ng jiÇndÄn de jÃ¬shÃ¹ dÃ¡ dÃ o xiÄngtÃ³ng huÃ² gÃ¨ng hÇo de xÃ­ngnÃ©ng. ZhÃ¨ zhÇ’ng jÃ¬shÃ¹ jiÃ ozuÃ² Dynamic Tanh (DyT), shÃ¬ yÄ«zhÇ’ng yuÃ¡nsÃ¹ jÃ­ cÇozuÃ². DyT de lÃ­nggÇn lÃ¡izÃ¬yÃº cÃ©ng zhÃ¨ngzÃ©huÃ  zÃ i Transformers zhÅng chÃ¡ngchÃ¡ng chÇnshÄ“ng lÃ¨isÃ¬ tanh de S xÃ­ng shÅ«rÃ¹-shÅ«chÅ« yÃ­ngshÃ¨. TÅngguÃ² shÇyÃ²ng DyT, Transformers kÄ›yÇ zÃ i mÃ©iyÇ’u zhÃ¨ngzÃ©huÃ  de qÃ­ngkuÃ ng xiÃ  pÇdÃ­ huÃ² chÄoyuÃ© yÇ’u zhÃ¨ngzÃ©huÃ  de xÃ­ngnÃ©ng, tÅngchÃ¡ng bÃ¹ xÅ«yÃ o chÄocÄnshÃ¹ tiÃ¡ozhÄ›ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"æ­£åˆ™åŒ–\", \"pinyin\": \"zhÃ¨ng zÃ© huÃ \", \"trans\": \"regularization\"},\n    {\"word\": \"ç°ä»£\", \"pinyin\": \"xiÃ n dÃ i\", \"trans\": \"modern\"},\n    {\"word\": \"ç¥ç»ç½‘ç»œ\", \"pinyin\": \"shÃ©n jÄ«ng wÇng luÃ²\", \"trans\": \"neural network\"},\n    {\"word\": \"ä½œç”¨\", \"pinyin\": \"zuÃ² yÃ²ng\", \"trans\": \"role\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discover\"},\n    {\"word\": \"å³ä½¿\", \"pinyin\": \"jÃ­ shÇ\", \"trans\": \"even if\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"Transformers\", \"pinyin\": \"TÃ¨i huÃ n fÄ mÇ”\", \"trans\": \"Transformers\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technique\"},\n    {\"word\": \"è¾¾åˆ°\", \"pinyin\": \"dÃ¡ dÃ o\", \"trans\": \"achieve\"},\n    {\"word\": \"ç›¸åŒ\", \"pinyin\": \"xiÄng tÃ³ng\", \"trans\": \"same\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å…ƒç´ çº§\", \"pinyin\": \"yuÃ¡n sÃ¹ jÃ­\", \"trans\": \"element-wise\"},\n    {\"word\": \"æ“ä½œ\", \"pinyin\": \"cÄo zuÃ²\", \"trans\": \"operation\"},\n    {\"word\": \"çµæ„Ÿ\", \"pinyin\": \"lÃ­ng gÇn\", \"trans\": \"inspiration\"},\n    {\"word\": \"å±‚\", \"pinyin\": \"cÃ©ng\", \"trans\": \"layer\"},\n    {\"word\": \"å¸¸å¸¸\", \"pinyin\": \"chÃ¡ng chÃ¡ng\", \"trans\": \"often\"},\n    {\"word\": \"äº§ç”Ÿ\", \"pinyin\": \"chÇn shÄ“ng\", \"trans\": \"generate\"},\n    {\"word\": \"ç±»ä¼¼\", \"pinyin\": \"lÃ¨i sÃ¬\", \"trans\": \"similar\"},\n    {\"word\": \"tanh\", \"pinyin\": \"tÇn h\", \"trans\": \"tanh\"},\n    {\"word\": \"Så½¢\", \"pinyin\": \"S xÃ­ng\", \"trans\": \"S-shaped\"},\n    {\"word\": \"è¾“å…¥-è¾“å‡º\", \"pinyin\": \"shÅ« rÃ¹ - shÅ« chÅ«\", \"trans\": \"input-output\"},\n    {\"word\": \"æ˜ å°„\", \"pinyin\": \"yÃ¬ng shÃ¨\", \"trans\": \"mapping\"},\n    {\"word\": \"åŒ¹æ•Œ\", \"pinyin\": \"pÇ dÃ­\", \"trans\": \"match\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ng kuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"è¶…å‚æ•°\", \"pinyin\": \"chÄo cÄn shÃ¹\", \"trans\": \"hyperparameter\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡o zhÄ›ng\", \"trans\": \"adjust\"}\n]",
        "trans": "This article discusses the role of regularization layers in modern neural networks. Research has found that even without using regularization layers, Transformers can achieve the same or better performance through a simple technique called Dynamic Tanh (DyT), which is an element-wise operation. The inspiration for DyT comes from the observation that layer regularization in Transformers often produces tanh-like S-shaped input-output mappings. By using DyT, Transformers can match or exceed the performance of regularized models without the need for hyperparameter tuning.",
        "update_ts": "2025-03-16 12:40"
    }
}