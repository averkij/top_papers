{
    "date": {
        "ru": "18 декабря",
        "en": "December 18",
        "zh": "12月18日"
    },
    "time_utc": "2024-12-18 21:09",
    "weekday": 2,
    "issue_id": 1199,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.13147",
            "title": "Are Your LLMs Capable of Stable Reasoning?",
            "url": "https://huggingface.co/papers/2412.13147",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK.",
            "score": 56,
            "issue_id": 1181,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "a030a3cb6cc36da2",
            "authors": [
                "Junnan Liu",
                "Hongwei Liu",
                "Linchen Xiao",
                "Ziyi Wang",
                "Kuikun Liu",
                "Songyang Gao",
                "Wenwei Zhang",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13147.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#evaluation",
                    "#reasoning",
                    "#leakage"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Новый подход к оценке способностей языковых моделей в сложных математических задачах",
                    "desc": "Статья представляет новый метод оценки больших языковых моделей (LLM) в задачах сложных рассуждений. Авторы вводят метрику G-Pass@k, которая оценивает как пиковую производительность модели, так и её стабильность при многократных попытках. Также представлен динамический бенчмарк LiveMathBench с современными математическими задачами для минимизации утечки данных при оценке. Эксперименты показывают значительный потенциал для улучшения 'реалистичных' способностей LLM к рассуждениям."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing LLM Evaluation for Real-World Reasoning",
                    "desc": "This paper addresses the gap between the performance of Large Language Models (LLMs) on benchmarks and their effectiveness in real-world scenarios, particularly in complex reasoning tasks. The authors propose a new evaluation metric called G-Pass@k, which assesses model performance over multiple attempts, focusing on both peak performance and stability. Additionally, they introduce LiveMathBench, a benchmark of challenging mathematical problems that reduces data leakage during evaluation. The study reveals that current LLMs have significant room for improvement in their reasoning capabilities, emphasizing the need for better evaluation methods."
                },
                "zh": {
                    "title": "提升大型语言模型推理能力的评估新方法",
                    "desc": "本论文探讨了大型语言模型（LLMs）在复杂推理任务中的表现与实际应用之间的差距。我们认为，这一差距主要源于当前的评估协议和指标无法全面反映LLMs的能力，尤其是在准确性和一致性至关重要的复杂推理任务中。为此，我们提出了G-Pass@k这一新评估指标，能够在多次采样中持续评估模型性能，并量化模型的最佳表现潜力和稳定性。此外，我们还推出了LiveMathBench，这是一个动态基准，包含具有挑战性的现代数学问题，旨在减少评估过程中的数据泄露风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13018",
            "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
            "url": "https://huggingface.co/papers/2412.13018",
            "abstract": "As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47\\% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in https://github.com/RUC-NLPIR/OmniEval{https://github.com/RUC-NLPIR/OmniEval}.",
            "score": 28,
            "issue_id": 1184,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "293aa1b03b853973",
            "authors": [
                "Shuting Wang",
                "Jiejun Tan",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13018.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#rag",
                    "#science"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "OmniEval: Всесторонняя оценка RAG-систем в финансовой сфере",
                    "desc": "Статья представляет OmniEval - многомерный бенчмарк для оценки методов Retrieval-Augmented Generation (RAG) в финансовой сфере. Авторы разработали матричную систему оценки сценариев RAG, включающую 5 классов задач и 16 финансовых тем. Бенчмарк использует многоэтапную систему оценки, анализирующую как извлечение информации, так и генерацию текста. OmniEval демонстрирует различия в производительности систем RAG для разных тем и задач, выявляя потенциал для улучшения моделей в узкоспециализированных областях."
                },
                "en": {
                    "title": "OmniEval: Elevating RAG Evaluation in Finance",
                    "desc": "This paper presents OmniEval, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) techniques specifically in the financial domain. It features a multi-dimensional evaluation framework that categorizes queries into various task classes and financial topics, allowing for structured assessments. The benchmark combines automatic data generation using GPT-4 with human annotations to ensure high-quality evaluation instances. The study reveals significant performance variations in RAG systems, highlighting areas for improvement and providing a comprehensive resource for future research in this area."
                },
                "zh": {
                    "title": "OmniEval：金融领域的全方位RAG评估基准",
                    "desc": "本文介绍了一种名为OmniEval的全方位自动化检索增强生成（RAG）基准，专注于金融领域。该基准具有多维评估框架，包括基于矩阵的RAG场景评估系统，能够将查询分类为五个任务类别和16个金融主题。我们还采用了结合GPT-4自动生成和人工标注的多维评估数据生成方法，确保生成实例的高接受率。实验结果表明，OmniEval在评估RAG系统的性能方面具有全面性，揭示了RAG模型在特定领域提升能力的显著机会。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12606",
            "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models",
            "url": "https://huggingface.co/papers/2412.12606",
            "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether LMMs align with the diverse needs of humans in real-world scenarios. To bridge this gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which includes over 500 images covering six common scenarios of human life. Notably, the MDI-Benchmark offers two significant advantages over existing evaluations: (1) Each image is accompanied by two types of questions: simple questions to assess the model's understanding of the image, and complex questions to evaluate the model's ability to analyze and reason beyond basic content. (2) Recognizing that people of different age groups have varying needs and perspectives when faced with the same scenario, our benchmark stratifies questions into three age categories: young people, middle-aged people, and older people. This design allows for a detailed assessment of LMMs' capabilities in meeting the preferences and needs of different age groups. With MDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related tasks, indicating that existing LMMs still have considerable room for improvement in addressing real-world applications. Looking ahead, we anticipate that the MDI-Benchmark will open new pathways for aligning real-world personalization in LMMs. The MDI-Benchmark data and evaluation code are available at https://mdi-benchmark.github.io/",
            "score": 28,
            "issue_id": 1182,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "75fba478b54ada06",
            "authors": [
                "YiFan Zhang",
                "Shanglin Lei",
                "Runqi Qiao",
                "Zhuoma GongQue",
                "Xiaoshuai Song",
                "Guanting Dong",
                "Qiuna Tan",
                "Zhe Wei",
                "Peiqing Yang",
                "Ye Tian",
                "Yadong Xue",
                "Xiaofei Wang",
                "Honggang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12606.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Многомерная оценка мультимодальных моделей для реальных задач",
                    "desc": "Предложен новый бенчмарк Multi-Dimensional Insights (MDI) для оценки мультимодальных моделей. MDI включает более 500 изображений с простыми и сложными вопросами, охватывающими 6 сценариев человеческой жизни. Бенчмарк учитывает потребности людей разных возрастных групп, что позволяет оценить способность моделей адаптироваться к различным пользователям. Даже передовые модели вроде GPT-4 достигают лишь 79% точности на задачах с учетом возраста, что указывает на потенциал для улучшения мультимодальных моделей."
                },
                "en": {
                    "title": "Enhancing LMM Evaluation with Age-Responsive Insights",
                    "desc": "This paper introduces the Multi-Dimensional Insights (MDI) benchmark, designed to evaluate large multimodal models (LMMs) in a more comprehensive way. It includes over 500 images and features two types of questions: simple ones for basic understanding and complex ones for deeper analysis and reasoning. The benchmark also categorizes questions by age groups, recognizing that different ages have unique perspectives and needs. The results show that while models like GPT-4o perform well, there is still significant room for improvement in aligning LMMs with real-world applications."
                },
                "zh": {
                    "title": "多维洞察基准：提升多模态模型的评估标准",
                    "desc": "大型多模态模型（LMMs）正在迅速发展，但现有的评估标准无法全面、客观地评估这些模型是否满足人类在现实场景中的多样化需求。为了解决这个问题，我们提出了多维洞察（MDI）基准，包含500多张图片，涵盖六种常见的人类生活场景。MDI基准的两个主要优势是：每张图片配有简单和复杂两种问题，评估模型对图像的理解和分析推理能力；同时，基准根据不同年龄段的需求，将问题分为年轻人、中年人和老年人三类。通过MDI基准，我们希望推动LMMs在现实应用中的个性化对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13171",
            "title": "Compressed Chain of Thought: Efficient Reasoning Through Dense Representations",
            "url": "https://huggingface.co/papers/2412.13171",
            "abstract": "Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the-shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.",
            "score": 14,
            "issue_id": 1194,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "ae88057a656ae089",
            "authors": [
                "Jeffrey Cheng",
                "Benjamin Van Durme"
            ],
            "affiliations": [
                "Department of Computer Science, Johns Hopkins University, Baltimore, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Сжатые цепочки рассуждений для более эффективных языковых моделей",
                    "desc": "Статья представляет новый метод под названием Compressed Chain-of-Thought (CCoT) для улучшения рассуждений языковых моделей. CCoT генерирует содержательные и непрерывные токены созерцания переменной длины, которые являются сжатыми представлениями цепочек рассуждений. Метод применим к существующим декодерным языковым моделям и позволяет адаптивно улучшать точность рассуждений, контролируя количество генерируемых токенов созерцания. Эксперименты показывают, что CCoT обеспечивает дополнительные рассуждения над плотными содержательными представлениями, что приводит к повышению точности."
                },
                "en": {
                    "title": "Enhancing Reasoning with Compressed Contemplation Tokens",
                    "desc": "This paper introduces Compressed Chain-of-Thought (CCoT), a new framework that enhances reasoning in language models by using variable-length contemplation tokens. These tokens serve as compressed representations of reasoning chains, allowing models to perform additional reasoning without being limited to fixed-length sequences. The method can be applied to existing decoder language models, improving their accuracy by enabling more effective reasoning over dense content. Additionally, the number of contemplation tokens can be adjusted to control the level of reasoning enhancement, providing flexibility in model performance."
                },
                "zh": {
                    "title": "压缩链式思维：提升推理性能的新方法",
                    "desc": "本文提出了一种新的框架，称为压缩链式思维（CCoT），用于生成可变长度的思维令牌，以提高语言模型的推理性能。思维令牌是指在推理过程中使用的特殊令牌，允许进行额外的计算。与之前的固定长度序列不同，CCoT生成的是压缩的推理链表示，能够提供更丰富的内容。通过实验，我们展示了CCoT如何通过密集的内容表示实现推理的改进，并且可以根据需求灵活调整生成的思维令牌数量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12276",
            "title": "Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers",
            "url": "https://huggingface.co/papers/2412.12276",
            "abstract": "Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., ``Finding the first noun in a sentence.\") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
            "score": 6,
            "issue_id": 1184,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "9baa157dac26994a",
            "authors": [
                "Seungwook Han",
                "Jinyeop Song",
                "Jeff Gore",
                "Pulkit Agrawal"
            ],
            "affiliations": [
                "Improbable AI",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12276.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#interpretability",
                    "#transfer_learning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие тайн обучения в контексте: как трансформеры формируют и используют абстракции",
                    "desc": "Статья исследует механизмы обучения в контексте (ICL) у трансформеров, предлагая концепцию кодирования-декодирования. Авторы анализируют, как модели формируют внутренние абстракции в своих представлениях на синтетических задачах ICL. Исследование показывает, что по мере обучения модели кодировать различные латентные концепты, она одновременно улучшает алгоритмы декодирования и производительность ICL. Эти выводы подтверждаются на предобученных моделях разного масштаба и через механистические вмешательства."
                },
                "en": {
                    "title": "Unlocking In-Context Learning: The Power of Concept Encoding in Transformers",
                    "desc": "This paper explores how autoregressive transformers, like those used in natural language processing, learn and adapt through a process called in-context learning (ICL). The authors introduce a concept encoding-decoding mechanism that helps explain how these models form and utilize internal abstractions in their representations. By analyzing a small transformer on synthetic ICL tasks, they observe that as the model encodes different concepts, it simultaneously develops decoding strategies that enhance its performance. The study confirms that the quality of concept encoding is crucial for ICL success, providing insights into the workings of large language models."
                },
                "zh": {
                    "title": "揭示自回归变换器的学习机制",
                    "desc": "本文探讨了自回归变换器如何通过上下文学习（ICL）进行适应性学习。我们提出了一种概念编码-解码机制，以解释变换器如何在其表示中形成和使用内部抽象。通过对合成ICL任务的分析，我们发现模型在学习编码不同潜在概念的同时，构建条件解码算法，从而提高ICL性能。我们的研究结果揭示了概念编码质量与ICL表现之间的因果关系，帮助我们更好地理解大型语言模型的成功与失败模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13180",
            "title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration",
            "url": "https://huggingface.co/papers/2412.13180",
            "abstract": "Recent works on accelerating Vision-Language Models show that strong performance can be maintained across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model and find that its strong performance across many tasks is not due to an exceptional ability to compress visual information, but rather the benchmarks' limited ability to assess fine-grained visual capabilities. Namely, we demonstrate a core issue with the acceleration approach where most tokens towards the top of the image are pruned away. Yet, this issue is only reflected in performance for a small subset of tasks such as localization. For the other evaluated tasks, strong performance is maintained with the flawed pruning strategy. Noting the limited visual capabilities of the studied acceleration technique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), a straightforward approach that (1) resolves the identified issue with early-layer pruning, (2) incorporates uniform sampling to ensure coverage across all image regions, and (3) applies pruning in two stages to allow the criteria to become more effective at a later layer while still achieving significant speedup through early-layer pruning. With comparable computational savings, we find that FEATHER has more than 5times performance improvement on the vision-centric localization benchmarks compared to the original acceleration approach.",
            "score": 5,
            "issue_id": 1194,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "ff9e7c2001c6c5b2",
            "authors": [
                "Mark Endo",
                "Xiaohan Wang",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13180.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🪶",
                "ru": {
                    "title": "Эффективное ускорение мультимодальных моделей без потери точности",
                    "desc": "Исследование посвящено ускорению моделей компьютерного зрения и обработки естественного языка (Vision-Language Models). Авторы обнаружили, что популярный подход раннего отсечения визуальных токенов имеет существенный недостаток - большинство токенов в верхней части изображения отбрасываются. Для решения этой проблемы предложен метод FEATHER, который использует двухэтапное отсечение и равномерную выборку токенов. FEATHER показывает значительное улучшение производительности на задачах локализации по сравнению с оригинальным подходом при сопоставимой вычислительной эффективности."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with FEATHER: Pruning Smartly for Better Performance",
                    "desc": "This paper investigates the effectiveness of early pruning in Vision-Language Models, revealing that the strong performance of these models is not solely due to their ability to compress visual information. The authors identify a significant flaw in the pruning strategy, where important visual tokens, especially those at the top of images, are often discarded, impacting performance on specific tasks like localization. They introduce a new method called FEATHER, which addresses this issue by ensuring better coverage of image regions through uniform sampling and implementing a two-stage pruning process. The results show that FEATHER achieves over five times improvement in localization tasks while maintaining computational efficiency compared to previous methods."
                },
                "zh": {
                    "title": "提升视觉-语言模型性能的新方法",
                    "desc": "最近的研究表明，视觉-语言模型在压缩视觉信息的情况下仍能在多种任务中保持良好表现。本文探讨了在语言模型中早期修剪视觉标记的加速方法，发现其强性能并非源于压缩视觉信息的能力，而是基准测试对细粒度视觉能力评估的局限性。我们提出了FEATHER方法，解决了早期修剪的核心问题，并通过均匀采样确保覆盖所有图像区域。与原始加速方法相比，FEATHER在视觉定位基准上实现了超过5倍的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13194",
            "title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
            "url": "https://huggingface.co/papers/2412.13194",
            "abstract": "The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator, an effective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the environment such as user demos or even just the name of the website itself for Internet-browsing agents. Then, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting trajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager and WebArena.To the best of our knowledge, this work represents the first effective learning system to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks with SOTA performances. Our open-source checkpoints and code can be found in https://yanqval.github.io/PAE/",
            "score": 4,
            "issue_id": 1193,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "de947186dd7a199a",
            "authors": [
                "Yifei Zhou",
                "Qianlan Yang",
                "Kaixiang Lin",
                "Min Bai",
                "Xiong Zhou",
                "Yu-Xiong Wang",
                "Sergey Levine",
                "Erran Li"
            ],
            "affiliations": [
                "Amazon",
                "University of California, Berkeley",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13194.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Автономное обучение агентов: от предложения задач до их выполнения",
                    "desc": "Эта статья представляет новую систему обучения под названием Proposer-Agent-Evaluator (PAE) для агентов на основе фундаментальных моделей. PAE позволяет агентам автономно открывать и практиковать навыки в реальном мире, используя контекстно-зависимый генератор задач, политику агента и автономный оценщик успеха на основе VLM. Система применяет обучение с подкреплением для улучшения политик агента. Авторы демонстрируют эффективность PAE на сложных задачах веб-навигации с использованием зрения, достигая передовых результатов на реальных тестах с аннотациями от людей."
                },
                "en": {
                    "title": "Empowering Agents to Learn Autonomously in the Real World",
                    "desc": "This paper introduces Proposer-Agent-Evaluator (PAE), a novel learning system designed for foundation model agents to autonomously learn and practice diverse skills in real-world environments. PAE features a context-aware task proposer that generates tasks based on environmental cues, allowing agents to engage in practical learning without extensive human-annotated instructions. The agent's performance is evaluated by a vision-language model (VLM) that provides feedback, which is then used as a reward signal for reinforcement learning (RL) to improve the agent's policies. The system demonstrates state-of-the-art performance in vision-based web navigation tasks, showcasing its ability to generalize across various human-annotated benchmarks."
                },
                "zh": {
                    "title": "自主学习与任务提议的智能代理系统",
                    "desc": "本文提出了一种名为Proposer-Agent-Evaluator（PAE）的学习系统，旨在帮助基础模型代理自主发现和练习技能。PAE的核心是一个上下文感知的任务提议者，它根据环境信息自动提出任务供代理练习。代理通过思考和实际操作来尝试这些任务，并由基于视觉语言模型的成功评估器进行评估。该系统在视觉基础的网页导航任务中表现出色，展示了其在真实世界人类标注基准上的广泛适应能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10704",
            "title": "VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2412.10704",
            "abstract": "Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. A key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce a coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, we benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and long-context LLM baselines for end-to-end multimodal document QA by 12-20%.",
            "score": 2,
            "issue_id": 1192,
            "pub_date": "2024-12-14",
            "pub_date_card": {
                "ru": "14 декабря",
                "en": "December 14",
                "zh": "12月14日"
            },
            "hash": "5acee7c37cff2e05",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Franck Dernoncourt",
                "Kanika Goswami",
                "Ryan A. Rossi",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10704.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#rag",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Мультимодальный RAG для вопросно-ответного поиска в документах",
                    "desc": "Статья представляет VisDoMBench - первый комплексный бенчмарк для оценки систем вопросно-ответного поиска в мультимодальных мультидокументных сценариях. Авторы предлагают VisDoMRAG - новый подход к мультимодальной генерации с дополнением извлечённой информацией (RAG), который одновременно использует визуальный и текстовый RAG. VisDoMRAG применяет многоэтапный процесс рассуждения, включающий отбор доказательств и цепочку рассуждений для параллельных текстовых и визуальных RAG-конвейеров. Ключевая особенность VisDoMRAG - механизм слияния модальностей с ограничением согласованности, который улучшает точность в сценариях, где критическая информация распределена между модальностями."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Question Answering with VisDoMRAG",
                    "desc": "This paper presents VisDoMBench, a new benchmark for evaluating question answering (QA) systems that work with multiple documents containing rich visual elements like tables and charts. It introduces VisDoMRAG, a novel approach that combines visual and textual retrieval in a Retrieval Augmented Generation (RAG) framework, enhancing the QA process. VisDoMRAG uses a multi-step reasoning method that integrates evidence curation and chain-of-thought reasoning to improve the accuracy of answers derived from both text and visuals. The paper demonstrates that VisDoMRAG significantly outperforms existing unimodal and long-context language models in multimodal document QA tasks, achieving better accuracy and verifiability of answers."
                },
                "zh": {
                    "title": "多模态问答的新突破",
                    "desc": "本文介绍了VisDoMBench，这是第一个全面的基准，用于评估多文档环境下的问答系统，特别是那些包含丰富视觉元素的文档。我们提出了一种新颖的多模态检索增强生成（RAG）方法，称为VisDoMRAG，它同时利用视觉和文本的RAG，结合强大的视觉检索能力和复杂的语言推理。VisDoMRAG采用多步骤推理过程，包括证据整理和思维链推理，以支持文本和视觉的并行RAG管道。通过一致性约束的模态融合机制，VisDoMRAG在推理时对不同模态的推理过程进行对齐，从而生成一致的最终答案，显著提高了多模态文档问答的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11713",
            "title": "Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework",
            "url": "https://huggingface.co/papers/2412.11713",
            "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open-source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Block, and Distorted Handling Solution. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi-agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices in real development scenarios, providing valuable insights for future improvements in code reliability.",
            "score": 1,
            "issue_id": 1194,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "7957a02cb5f0752d",
            "authors": [
                "Xuanming Zhang",
                "Yuxuan Chen",
                "Yiming Zheng",
                "Zhexin Zhang",
                "Yuan Yuan",
                "Minlie Huang"
            ],
            "affiliations": [
                "Beihang University",
                "ByteDance",
                "Lingxin AI",
                "The CoAI Group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11713.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#open_source",
                    "#science",
                    "#plp"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Повышение надежности кода: LLM на страже обработки исключений",
                    "desc": "Данная статья исследует использование больших языковых моделей (LLM) для улучшения обработки исключений в программном коде. Авторы выявили три ключевые проблемы: нечувствительное обнаружение хрупкого кода, неточный захват блока исключений и искаженное решение для обработки. Для решения этих проблем предложена мультиагентная система Seeker, вдохновленная стратегиями опытных разработчиков. Seeker использует различных агентов для более эффективного обнаружения, захвата и разрешения исключений с помощью LLM."
                },
                "en": {
                    "title": "Enhancing Code Reliability with LLMs for Exception Handling",
                    "desc": "This paper addresses the challenges of exception handling in software development, particularly in open-source projects where improper handling can lead to unreliable code. It identifies three main issues: the insensitivity in detecting fragile code, inaccuracies in capturing exception blocks, and distorted solutions for handling exceptions. To tackle these problems, the authors propose a multi-agent framework called Seeker, which utilizes various agents to assist large language models (LLMs) in improving exception handling practices. This research is significant as it is the first systematic study to explore the application of LLMs in enhancing the robustness of exception handling in real-world coding scenarios."
                },
                "zh": {
                    "title": "利用智能体提升异常处理的可靠性",
                    "desc": "在软件开发中，异常处理不当会严重影响代码的健壮性和可靠性。许多开发者在检测和管理异常时面临困难，导致代码脆弱，尤其是在开源项目中更为明显。为了解决这个问题，我们提出了Seeker，一个多智能体框架，旨在利用大型语言模型（LLMs）来改善异常处理。Seeker通过多个智能体协作，帮助开发者更有效地检测、捕获和解决异常，从而提高代码的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12527",
            "title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention",
            "url": "https://huggingface.co/papers/2412.12527",
            "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge). While substantial efforts have been made to leverage both forms of knowledge, scenarios in which the model lacks any relevant knowledge remain underexplored. Such limitations can result in issues like hallucination, causing reduced reliability and potential risks in high-stakes applications. To address such limitations, this paper extends the task scope to encompass cases where the user's request cannot be fulfilled due to the lack of relevant knowledge. To this end, we introduce Contrastive Decoding with Abstention (CDA), a training-free decoding method that empowers LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA evaluates the relevance of each knowledge for a given query, adaptively determining which knowledge to prioritize or which to completely ignore. Extensive experiments with four LLMs on three question-answering datasets demonstrate that CDA can effectively perform accurate generation and abstention simultaneously. These findings highlight CDA's potential to broaden the applicability of LLMs, enhancing reliability and preserving user trust.",
            "score": 1,
            "issue_id": 1192,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "28e931208632a312",
            "authors": [
                "Hyuhng Joon Kim",
                "Youna Kim",
                "Sang-goo Lee",
                "Taeuk Kim"
            ],
            "affiliations": [
                "Hanyang University",
                "IntelliSys, Korea",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12527.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#alignment",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CDA: умное воздержание для надежных языковых моделей",
                    "desc": "Эта статья представляет новый метод декодирования для больших языковых моделей (LLM) под названием Contrastive Decoding with Abstention (CDA). CDA позволяет моделям генерировать ответы, когда доступны релевантные знания, и воздерживаться от ответа в противном случае. Метод оценивает релевантность каждого элемента знаний для данного запроса, адаптивно определяя, какие знания приоритизировать или игнорировать. Эксперименты показали эффективность CDA в точной генерации и воздержании одновременно, что может повысить надежность и доверие пользователей к LLM."
                },
                "en": {
                    "title": "Empowering LLMs: Generate or Abstain with Contrastive Decoding",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) when they encounter queries without relevant knowledge, which can lead to unreliable outputs or hallucinations. It introduces a novel method called Contrastive Decoding with Abstention (CDA), which allows LLMs to generate responses when they have relevant information and to abstain from answering when they do not. CDA works by evaluating the relevance of available knowledge for each query, enabling the model to prioritize useful information and ignore irrelevant data. The results from experiments on multiple datasets show that CDA improves both the accuracy of responses and the model's reliability, making LLMs more trustworthy in critical applications."
                },
                "zh": {
                    "title": "提升大型语言模型的可靠性与信任",
                    "desc": "大型语言模型（LLMs）在多种任务中表现出色，利用了预训练知识和外部知识。然而，当模型缺乏相关知识时，可能会出现幻觉等问题，影响其可靠性。为了解决这一问题，本文提出了一种新的解码方法——对比解码与放弃（CDA），使模型能够在有相关知识时生成响应，而在缺乏知识时选择放弃。实验结果表明，CDA能够有效地同时进行准确生成和放弃，从而提高LLMs的适用性和用户信任。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12877",
            "title": "MIVE: New Design and Benchmark for Multi-Instance Video Editing",
            "url": "https://huggingface.co/papers/2412.12877",
            "abstract": "Recent AI-based video editing has enabled users to edit videos through simple text prompts, significantly simplifying the editing process. However, recent zero-shot video editing techniques primarily focus on global or single-object edits, which can lead to unintended changes in other parts of the video. When multiple objects require localized edits, existing methods face challenges, such as unfaithful editing, editing leakage, and lack of suitable evaluation datasets and metrics. To overcome these limitations, we propose a zero-shot Multi-Instance Video Editing framework, called MIVE. MIVE is a general-purpose mask-based framework, not dedicated to specific objects (e.g., people). MIVE introduces two key modules: (i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and (ii) Instance-centric Probability Redistribution (IPR) to ensure precise localization and faithful editing. Additionally, we present our new MIVE Dataset featuring diverse video scenarios and introduce the Cross-Instance Accuracy (CIA) Score to evaluate editing leakage in multi-instance video editing tasks. Our extensive qualitative, quantitative, and user study evaluations demonstrate that MIVE significantly outperforms recent state-of-the-art methods in terms of editing faithfulness, accuracy, and leakage prevention, setting a new benchmark for multi-instance video editing. The project page is available at https://kaist-viclab.github.io/mive-site/",
            "score": 1,
            "issue_id": 1189,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "5f31fcc15693d0d3",
            "authors": [
                "Samuel Teodoro",
                "Agus Gunawan",
                "Soo Ye Kim",
                "Jihyong Oh",
                "Munchurl Kim"
            ],
            "affiliations": [
                "Adobe Research",
                "Chung-Ang University",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12877.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#video",
                    "#optimization",
                    "#leakage"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MIVE: Точное редактирование нескольких объектов в видео с помощью ИИ",
                    "desc": "Статья представляет новый подход к редактированию видео с помощью искусственного интеллекта, названный MIVE (Multi-Instance Video Editing). MIVE позволяет редактировать несколько объектов в видео одновременно, избегая нежелательных изменений в других частях кадра. Ключевые компоненты системы включают модуль выборки для предотвращения утечки эффектов редактирования и модуль перераспределения вероятностей для точной локализации изменений. Авторы также представляют новый набор данных и метрику оценки для задач многообъектного редактирования видео."
                },
                "en": {
                    "title": "MIVE: Revolutionizing Multi-Instance Video Editing with Precision and Faithfulness",
                    "desc": "This paper presents a new framework called MIVE for zero-shot multi-instance video editing, which allows users to edit multiple objects in videos using simple text prompts. MIVE addresses common issues in existing methods, such as editing leakage and unfaithful edits, by introducing two innovative modules: Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR). The framework is designed to work with various objects, not just specific ones, making it versatile for different editing scenarios. Additionally, the authors introduce a new dataset and evaluation metric to assess the performance of multi-instance video editing, demonstrating that MIVE outperforms current techniques in accuracy and editing quality."
                },
                "zh": {
                    "title": "MIVE：多实例视频编辑的新标准",
                    "desc": "最近的基于人工智能的视频编辑技术使用户能够通过简单的文本提示来编辑视频，极大地简化了编辑过程。然而，现有的零-shot视频编辑技术主要集中在全局或单一对象的编辑，这可能导致视频其他部分的意外变化。当多个对象需要局部编辑时，现有方法面临诸如编辑不准确、编辑泄漏以及缺乏合适的评估数据集和指标等挑战。为了解决这些问题，我们提出了一种名为MIVE的零-shot多实例视频编辑框架，旨在提高编辑的准确性和保真度。"
                }
            }
        }
    ],
    "link_prev": "2024-12-17.html",
    "link_next": "2024-12-19.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12月17日"
    },
    "short_date_next": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12月19日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 1,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 2,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0,
        "#evaluation": 1
    },
    "zh": {
        "text": "大型语言模型（LLMs）在复杂推理任务中取得了显著进展。然而，基准测试性能与实际应用之间仍存在差距。主要原因是当前的评估方法和指标无法充分捕捉LLMs的全部能力，特别是在需要准确性和一致性的复杂推理任务中。本文提出了G-Pass@k，一个新的评估指标，能够在多次采样中连续评估模型性能，量化模型的最高性能潜力和稳定性。同时，我们推出了LiveMathBench，一个包含具有挑战性、当代数学问题的动态基准，旨在减少评估过程中的数据泄露风险。详细结果和基准可访问：https://github.com/open-compass/GPassK。",
        "title": "Are Your LLMs Capable of Stable Reasoning?",
        "pinyin": "Dàxíng yǔyán móxíng (LLMs) zài fùzá xīnglǐ rènwù zhōng qǔdéle xiǎnzhù jìnbù. Rán'ér, jīzhǔn cèshǐ xìngnéng yǔ shíjì yìngyòng zhījiān réng cúnzài chājù. Zhǔyào yuányīn shì dāngqián de pínggǔ fāngfǎ hé zhǐbiāo wúfǎ chōngfēn bǐngqǔ LLMs de quánbù nénglì, tèbié shì zài xūyào zhǔnquèxìng hé yīzhìxìng de fùzá xīnglǐ rènwù zhōng. Běnwén tíchūle G-Pass@k, yīgè xīn de pínggǔ zhǐbiāo, nénggòu zài duōcì cǎiyàng zhōng liánxù pínggǔ móxíng xìngnéng, liàngzhì móxíng de zuìgāo xìngnéng qiánlì hé wěndíngxìng. Tóngshí, wǒmen tuīchūle LiveMathBench, yīgè bāohán jùyǒu tiǎozhànxìng, dāngdài shùxué wèntí de dòngtài jīzhǔn, zhǐyú jiǎnshǎo pínggǔ guòchéng zhōng de shùjù lòushì fēngxiǎn. Xiángxì jiéguǒ hé jīzhǔn kě fāngwèn: https://github.com/open-compass/GPassK.",
        "vocab": "[{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '实际', 'pinyin': 'shí jì', 'trans': 'actual'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'},\n{'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'},\n{'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '指标', 'pinyin': 'zhǐ biāo', 'trans': 'metric'},\n{'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'capability'},\n{'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'},\n{'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '采样', 'pinyin': 'cǎi yàng', 'trans': 'sampling'},\n{'word': '连续', 'pinyin': 'lián xù', 'trans': 'continuous'},\n{'word': '量化', 'pinyin': 'liàng huà', 'trans': 'quantify'},\n{'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'},\n{'word': '稳定性', 'pinyin': 'wěn dìng xìng', 'trans': 'stability'},\n{'word': '推出', 'pinyin': 'tuī chū', 'trans': 'launch'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '挑战性', 'pinyin': 'tiǎo zhàn xìng', 'trans': 'challenging'},\n{'word': '当代', 'pinyin': 'dāng dài', 'trans': 'contemporary'},\n{'word': '数学', 'pinyin': 'shù xué', 'trans': 'mathematics'},\n{'word': '问题', 'pinyin': 'wèn tí', 'trans': 'problem'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'},\n{'word': '泄露', 'pinyin': 'xiè lòu', 'trans': 'leak'},\n{'word': '风险', 'pinyin': 'fēng xiǎn', 'trans': 'risk'},\n{'word': '详细', 'pinyin': 'xiáng xì', 'trans': 'detailed'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'},\n{'word': '访问', 'pinyin': 'fǎng wèn', 'trans': 'access'}]",
        "trans": "Large language models (LLMs) have made significant strides in complex reasoning tasks. However, there remains a gap between benchmark test performance and real-world applications. The primary reason is that current evaluation methods and metrics fail to fully capture the capabilities of LLMs, especially in complex reasoning tasks that require accuracy and consistency. This paper introduces G-Pass@k, a new evaluation metric that can continuously assess model performance across multiple samples, quantifying the model's peak performance potential and stability. Additionally, we present LiveMathBench, a dynamic benchmark containing challenging, contemporary mathematical problems aimed at reducing the risk of data leakage during the evaluation process. Detailed results and benchmarks are available at: https://github.com/open-compass/GPassK.",
        "update_ts": "2024-12-18 09:11"
    }
}