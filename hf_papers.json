{
    "date": {
        "ru": "29 января",
        "en": "January 29",
        "zh": "1月29日"
    },
    "time_utc": "2025-01-29 04:12",
    "weekday": 2,
    "issue_id": 1919,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.16372",
            "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
            "url": "https://huggingface.co/papers/2501.16372",
            "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant challenges regarding the computational resources required for fine-tuning and deployment. Recent advancements in low-rank adapters have demonstrated their efficacy in parameter-efficient fine-tuning (PEFT) of these models. This retrospective paper comprehensively discusses innovative approaches that synergize low-rank representations with Neural Architecture Search (NAS) techniques, particularly weight-sharing super-networks. Robust solutions for compressing and fine-tuning large pre-trained models are developed by integrating these methodologies. Our analysis highlights the potential of these combined strategies to democratize the use of LLMs, making them more accessible for deployment in resource-constrained environments. The resulting models exhibit reduced memory footprints and faster inference times, paving the way for more practical and scalable applications of LLMs. Models and code are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
            "score": 2,
            "issue_id": 1918,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "f1d43a985dbea0af",
            "authors": [
                "J. Pablo Muñoz",
                "Jinjie Yuan",
                "Nilesh Jain"
            ],
            "affiliations": [
                "Intel Corporation",
                "Intel Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16372.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#low_resource",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная настройка крупных языковых моделей для ограниченных ресурсов",
                    "desc": "Эта статья рассматривает проблему больших вычислительных ресурсов, необходимых для настройки и развертывания крупных языковых моделей (LLM). Авторы предлагают комбинировать низкоранговые адаптеры и методы поиска нейронных архитектур (NAS) для эффективной настройки параметров. Такой подход позволяет сжимать и дообучать большие предобученные модели, делая их более доступными в условиях ограниченных ресурсов. В результате получаются модели с меньшим потреблением памяти и более быстрым выводом, что открывает путь к более практичному применению LLM."
                },
                "en": {
                    "title": "Democratizing Large Language Models with Efficient Fine-Tuning Techniques",
                    "desc": "This paper addresses the challenges of using Large Language Models (LLMs) due to their high computational demands. It explores the use of low-rank adapters for parameter-efficient fine-tuning (PEFT), which helps reduce the resources needed. The authors combine low-rank representations with Neural Architecture Search (NAS) techniques, particularly through weight-sharing super-networks, to create efficient solutions for model compression and fine-tuning. The findings suggest that these strategies can make LLMs more accessible and practical for deployment in environments with limited resources, resulting in models that are faster and require less memory."
                },
                "zh": {
                    "title": "低秩适配器助力大型语言模型的高效微调",
                    "desc": "大型语言模型（LLMs）的快速发展带来了在微调和部署时对计算资源的巨大挑战。最近，低秩适配器在参数高效微调（PEFT）方面显示出了良好的效果。本文回顾了将低秩表示与神经架构搜索（NAS）技术相结合的创新方法，特别是权重共享超网络。通过整合这些方法，开发了压缩和微调大型预训练模型的稳健解决方案，使得LLMs在资源受限的环境中更易于部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15747",
            "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
            "url": "https://huggingface.co/papers/2501.15747",
            "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the intricacies of Indian languages. IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks' design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models.",
            "score": 2,
            "issue_id": 1918,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "4b666d035c5e5c4c",
            "authors": [
                "Sankalp KJ",
                "Ashutosh Kumar",
                "Laxmaan Balaji",
                "Nikunj Kotecha",
                "Vinija Jain",
                "Aman Chadha",
                "Sreyoshi Bhaduri"
            ],
            "affiliations": [
                "Amazon Gen AI",
                "Artificial Intelligence Institute, University of South Carolina",
                "Independent Researcher",
                "Meta AI",
                "Rochester Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15747.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "Новый рубеж в NLP: комплексная оценка языковых моделей для индийских языков",
                    "desc": "IndicMMLU-Pro - это комплексный бенчмарк для оценки языковых моделей в индийских языках. Он охватывает 9 основных языков Индийского субконтинента и включает широкий спектр задач по пониманию языка, рассуждению и генерации текста. Бенчмарк разработан с учетом уникальных особенностей и сложностей индийских языков. IndicMMLU-Pro предоставляет стандартизированную систему оценки для продвижения исследований в области ИИ для индийских языков."
                },
                "en": {
                    "title": "Empowering Indic Languages with Advanced NLP Benchmarks",
                    "desc": "The paper introduces IndicMMLU-Pro, a benchmark specifically designed to assess Large Language Models (LLMs) in the context of Indic languages. It builds on the existing MMLU Pro framework and includes major languages like Hindi, Bengali, and Tamil, addressing the unique linguistic challenges of the Indian subcontinent. The benchmark features a variety of tasks that test language comprehension, reasoning, and generation, ensuring a comprehensive evaluation of models. By providing a standardized framework, IndicMMLU-Pro aims to enhance the development of more accurate and culturally aware AI models for Indic languages."
                },
                "zh": {
                    "title": "推动印度语言AI研究的基准",
                    "desc": "IndicMMLU-Pro是一个专门为印度语言设计的基准，旨在评估大型语言模型（LLMs）的表现。该基准基于MMLU Pro框架，涵盖了印地语、孟加拉语、古吉拉特语等主要语言，解决了印度次大陆语言的多样性带来的挑战。它包括语言理解、推理和生成等多种任务，旨在捕捉印度语言的复杂性。通过提供标准化的评估框架，IndicMMLU-Pro推动了印度语言人工智能的研究，促进了更准确、高效和文化敏感的模型的发展。"
                }
            }
        }
    ],
    "link_prev": "2025-01-28.html",
    "link_next": "2025-01-30.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1月28日"
    },
    "short_date_next": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1月30日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "我们介绍了 Baichuan-Omni-1.5，这是一个具有全模态理解和端到端音频生成能力的模型。为了实现流畅的高质量跨模态交互，我们优化了三个关键方面。首先，我们建立了一个全面的数据清洗和合成管道，获得了大约 500B 的高质量数据（文本、音频和视觉）。其次，我们设计了一个音频分词器，捕捉音频的语义和声学信息。最后，我们设计了一个多阶段训练策略，确保所有模态的有效协同。Baichuan-Omni-1.5 在全模态能力方面领先于当前模型，并在多个多模态医疗基准上取得了可比的结果。",
        "title": "Baichuan-Omni-1.5 Technical Report",
        "pinyin": "Wǒmen jièshào le Baichuan-Omni-1.5, zhè shì yīgè jùyǒu quán móshì lǐjiě hé duān dào duān yīnpiàn shēngchéng nénglì de móxíng. Wèile shíxiàn liúchàng de gāo zhìliàng kuà móshì jiāohù, wǒmen yōuhuà le sān gè guǎnjiàn fāngcè. Shǒuxiān, wǒmen jiànlì le yīgè quánmiàn de shùjù qīngxī hé héchéng guǎndǎo, huòdé le dàyuē 500B de gāo zhìliàng shùjù (wénběn, yīnpiàn hé shìjué). Qícì, wǒmen shèjì le yīgè yīnpiàn fēncíqì, bīngzhuō yīnpiàn de yǔyì hé shēngxué xìnxī. Zuìhòu, wǒmen shèjì le yīgè duō jiēduàn xùnliàn cèlüè, quèbǎo suǒyǒu móshì de yǒuxiào xiétóng. Baichuan-Omni-1.5 zài quán móshì nénglì fāngmiàn lǐngxiān yú dāngqián móxíng, bìng zài duō gè duō móshì yīliáo jīzhǔn shàng qudé le kěbǐ de jiéguǒ.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},\n{'word': 'Baichuan-Omni-1.5', 'pinyin': 'Bài chuān-Ōu mí-1.5', 'trans': 'Baichuan-Omni-1.5'},\n{'word': '具有', 'pinyin': 'jù yǒu', 'trans': 'have'},\n{'word': '全模态', 'pinyin': 'quán mó shì', 'trans': 'full modality'},\n{'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understanding'},\n{'word': '端到端', 'pinyin': 'duān dào duān', 'trans': 'end-to-end'},\n{'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '流畅', 'pinyin': 'liú chàng', 'trans': 'smooth'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '跨模态', 'pinyin': 'kuà mó shì', 'trans': 'cross-modality'},\n{'word': '交互', 'pinyin': 'jiāo hù', 'trans': 'interaction'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'},\n{'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'},\n{'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'},\n{'word': '建立', 'pinyin': 'jiàn lì', 'trans': 'establish'},\n{'word': '全面', 'pinyin': 'quán miàn', 'trans': 'comprehensive'},\n{'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'},\n{'word': '清洗', 'pinyin': 'qīng xǐ', 'trans': 'clean'},\n{'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesize'},\n{'word': '管道', 'pinyin': 'guǎn dào', 'trans': 'pipeline'},\n{'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'},\n{'word': '大约', 'pinyin': 'dà yuē', 'trans': 'about'},\n{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'},\n{'word': '分词器', 'pinyin': 'fēn cí qì', 'trans': 'tokenizer'},\n{'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'},\n{'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantics'},\n{'word': '声学', 'pinyin': 'shēng xué', 'trans': 'acoustics'},\n{'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'},\n{'word': '多阶段', 'pinyin': 'duō jiē duàn', 'trans': 'multi-stage'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '协同', 'pinyin': 'xié tóng', 'trans': 'coordination'},\n{'word': '领先', 'pinyin': 'lǐng xiān', 'trans': 'lead'},\n{'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '可比', 'pinyin': 'kě bǐ', 'trans': 'comparable'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}]",
        "trans": "We introduce Baichuan-Omni-1.5, a model with full-modal understanding and end-to-end audio generation capabilities. To achieve smooth, high-quality cross-modal interaction, we optimized three key aspects. First, we established a comprehensive data cleaning and synthesis pipeline, obtaining approximately 500B high-quality data (text, audio, and visual). Second, we designed an audio tokenizer to capture the semantic and acoustic information of audio. Lastly, we designed a multi-stage training strategy to ensure effective collaboration across all modalities. Baichuan-Omni-1.5 leads in full-modal capabilities and achieves comparable results on multiple multimodal medical benchmarks.",
        "update_ts": "2025-01-28 09:10"
    }
}