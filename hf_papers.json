{
    "date": {
        "ru": "2 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 2",
        "zh": "12Êúà2Êó•"
    },
    "time_utc": "2024-12-02 09:12",
    "weekday": 0,
    "issue_id": 890,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19930",
            "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2411.19930",
            "abstract": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.",
            "score": 11,
            "issue_id": 885,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "5d14749b38f15e60",
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Ziyu Zhu",
                "Xintong Zhang",
                "Wayne Xin Zhao",
                "Zhongzhi Luan",
                "Bo Dai",
                "Zhenliang Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Institute of Technology",
                "Renmin University of China",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19930.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–æ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∑–∞–¥–∞—á –ø—Ä–∏ –¥–æ–º–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏ –ø–∏—â–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö MLLM, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques",
                    "desc": "This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄß",
                    "desc": "ËøëÂπ¥Êù•ÔºåÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøÖÈÄüÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÈÄöÁî®MLLMsÈÄÇÂ∫î‰∫éÁâπÂÆöÈ¢ÜÂüüÔºåÂ¶ÇÁßëÂ≠¶ÂíåÂ∑•‰∏öÂ∫îÁî®Ôºå‰ªçÁÑ∂ËæÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜMLLMsÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄßÔºåÈáçÁÇπÂú®‰∫éÊï∞ÊçÆÂêàÊàê„ÄÅËÆ≠ÁªÉÊµÅÁ®ãÂíå‰ªªÂä°ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçËßÜËßâÊåá‰ª§ÂêàÊàêÂô®ÔºåËÉΩÂ§üÊúâÊïàÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßâÊåá‰ª§‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçáMLLMsÂú®ÁâπÂÆöÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19108",
            "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
            "url": "https://huggingface.co/papers/2411.19108",
            "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
            "score": 6,
            "issue_id": 886,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "02a6c2edf156e9d3",
            "authors": [
                "Feng Liu",
                "Shiwei Zhang",
                "Xiaofeng Wang",
                "Yujie Wei",
                "Haonan Qiu",
                "Yuzhong Zhao",
                "Yingya Zhang",
                "Qixiang Ye",
                "Fang Wan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Institute of Automation, Chinese Academy of Sciences",
                "Nanyang Technological University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19108.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#video",
                    "#inference"
                ],
                "emoji": "‚è±Ô∏è",
                "ru": {
                    "title": "–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TeaCache. –ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤. TeaCache –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ —Ä–∞–∑–ª–∏—á–∏–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TeaCache –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 4,41 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Open-Sora-Plan –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Accelerating Video Generation with Smart Caching",
                    "desc": "This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜÈ¢ëÁîüÊàêÈÄüÂ∫¶ÁöÑÊñ∞ÊñπÊ≥ïÔºöTeaCache",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁºìÂ≠òÊñπÊ≥ïÔºåÁß∞‰∏∫Êó∂Èó¥Ê≠•ÂµåÂÖ•ÊÑüÁü•ÁºìÂ≠òÔºàTeaCacheÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊâ©Êï£Ê®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöËøáÂú®ÂùáÂåÄÈÄâÊã©ÁöÑÊó∂Èó¥Ê≠•ÁºìÂ≠òÊ®°ÂûãËæìÂá∫Ôºå‰ΩÜÂøΩÁï•‰∫Ü‰∏çÂêåÊó∂Èó¥Ê≠•‰πãÈó¥ËæìÂá∫Â∑ÆÂºÇÁöÑ‰∏çÂùáÂåÄÊÄß„ÄÇTeaCacheÈÄöËøáË∞ÉËäÇÂô™Â£∞ËæìÂÖ•ÔºåÂà©Áî®Êó∂Èó¥Ê≠•ÂµåÂÖ•Êù•Êõ¥Â•ΩÂú∞Ëøë‰ººÊ®°ÂûãËæìÂá∫ÁöÑÂ∑ÆÂºÇÔºå‰ªéËÄå‰ºòÂåñÁºìÂ≠òÈÄâÊã©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTeaCacheÂú®‰øùÊåÅËßÜËßâË¥®ÈáèÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü4.41ÂÄç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19460",
            "title": "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing",
            "url": "https://huggingface.co/papers/2411.19460",
            "abstract": "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.",
            "score": 4,
            "issue_id": 886,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "b96751a3db484750",
            "authors": [
                "Hosu Lee",
                "Junho Kim",
                "Hyunjun Kim",
                "Yong Man Ro"
            ],
            "affiliations": [
                "Integrated Vision and Language Lab, KAIST, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19460.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#video",
                    "#optimization"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Video-Ma^2mba",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Video-Ma^2mba - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤–º–µ—Å—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–∏–Ω–µ–π–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LMM) –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ –º—É–ª—å—Ç–∏–æ—Å–µ–≤–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —á–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥–∞ (MA-GC) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Video-Ma^2mba –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º GPU, —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Revolutionizing Long Video Processing with Linear Scalability",
                    "desc": "The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models."
                },
                "zh": {
                    "title": "È´òÊïàÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "ÈöèÁùÄËßÜÈ¢ëÊï∞ÊçÆËßÑÊ®°ÂíåÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÈù¢‰∏¥ÁùÄÊòæËëóÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Êû∂ÊûÑVideo-Ma^2mbaÔºåÂÆÉÂú®Mamba-2Ê°ÜÊû∂‰∏≠ÂºïÂÖ•‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâÔºåÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªéËÄå‰ΩøÂæóÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êó∂Èó¥ÂíåÂÜÖÂ≠òÈúÄÊ±Ç‰∏äÂÆûÁé∞Á∫øÊÄßÊâ©Â±ï„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂ§öËΩ¥Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàMA-GCÔºâÊñπÊ≥ïÔºå‰ºòÂåñÂÜÖÂ≠òÁÆ°ÁêÜÔºå‰ªÖ‰øùÁïôÂøÖË¶ÅÁöÑÊøÄÊ¥ª‰ø°ÊÅØÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideo-Ma^2mbaËÉΩÂ§üÂú®Âçï‰∏™GPU‰∏äÂ§ÑÁêÜÁõ∏ÂΩì‰∫éÊï∞Áôæ‰∏á‰∏™Ê†áËÆ∞ÊàñË∂ÖËøá‰∏§Â∞èÊó∂ÁöÑËøûÁª≠ËßÜÈ¢ëÂ∫èÂàóÔºåÊèêÂçá‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19950",
            "title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
            "url": "https://huggingface.co/papers/2411.19950",
            "abstract": "We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets",
            "score": 3,
            "issue_id": 888,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "9f7d2daec9cb311d",
            "authors": [
                "Yuze He",
                "Wang Zhao",
                "Shaohui Liu",
                "Yubin Hu",
                "Yushi Bai",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ETH Zurich",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19950.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "üìê",
                "ru": {
                    "title": "AlphaTablets: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaTablets - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –≤ –≤–∏–¥–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ —Å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–∞–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—á–µ—Ç–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö 2D –∏ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ç–æ—á–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Ä–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ScanNet –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–µ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π."
                },
                "en": {
                    "title": "AlphaTablets: Revolutionizing 3D Plane Representation",
                    "desc": "AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset."
                },
                "zh": {
                    "title": "AlphaTabletsÔºö3DÂπ≥Èù¢ÈáçÂª∫ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜAlphaTabletsÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñ‰∏îÈÄöÁî®ÁöÑ3DÂπ≥Èù¢Ë°®Á§∫ÊñπÊ≥ïÔºåÂÖ∑ÊúâËøûÁª≠ÁöÑ3DË°®Èù¢ÂíåÁ≤æÁ°ÆÁöÑËæπÁïåÂàíÂàÜ„ÄÇÈÄöËøáÂ∞Ü3DÂπ≥Èù¢Ë°®Á§∫‰∏∫Â∏¶ÊúâalphaÈÄöÈÅìÁöÑÁü©ÂΩ¢ÔºåAlphaTabletsÁªìÂêà‰∫ÜÂΩìÂâç2DÂíå3DÂπ≥Èù¢Ë°®Á§∫ÁöÑ‰ºòÁÇπÔºåÂÆûÁé∞‰∫Ü3DÂπ≥Èù¢ÁöÑÂáÜÁ°Æ„ÄÅ‰∏ÄËá¥ÂíåÁÅµÊ¥ªÂª∫Ê®°„ÄÇÊàë‰ª¨Âú®AlphaTabletsÁöÑÂü∫Á°Ä‰∏äÊé®ÂØºÂá∫ÂèØÂæÆÂàÜÂÖâÊ†ÖÂåñÊäÄÊúØÔºå‰ª•È´òÊïàÂú∞Â∞Ü3DÂπ≥Èù¢Ê∏≤Êüì‰∏∫ÂõæÂÉèÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™‰∏ãËÄå‰∏äÁöÑÂçïÁõÆËßÜÈ¢ë3DÂπ≥Èù¢ÈáçÂª∫ÁÆ°ÈÅì„ÄÇÈÄöËøáËø≠‰ª£‰ºòÂåñÂíåÂêàÂπ∂ÔºåÊàë‰ª¨ËÉΩÂ§üÈáçÂª∫Âá∫ÂÆåÊï¥‰∏îÂáÜÁ°ÆÁöÑ3DÂπ≥Èù¢ÔºåÂÖ∑ÊúâÂùöÂÆûÁöÑË°®Èù¢ÂíåÊ∏ÖÊô∞ÁöÑËæπÁïå„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19146",
            "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
            "url": "https://huggingface.co/papers/2411.19146",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.",
            "score": 3,
            "issue_id": 886,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "b33bb17742a81e99",
            "authors": [
                "Akhiad Bercovich",
                "Tomer Ronen",
                "Talor Abramovich",
                "Nir Ailon",
                "Nave Assaf",
                "Mohammad Dabbah",
                "Ido Galil",
                "Amnon Geifman",
                "Yonatan Geifman",
                "Izhak Golan",
                "Netanel Haber",
                "Ehud Karpas",
                "Itay Levy",
                "Shahar Mor",
                "Zach Moshe",
                "Najeeb Nabwani",
                "Omri Puny",
                "Ran Rubin",
                "Itamar Schen",
                "Ido Shahaf",
                "Oren Tropp",
                "Omer Ullman Argov",
                "Ran Zilberstein",
                "Ran El-Yaniv"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19146.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#inference"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Puzzle –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (NAS) –∏ –±–ª–æ—á–Ω—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π (BLD), Puzzle –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ —Å –¥–µ—Å—è—Ç–∫–∞–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ Nemotron-51B, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–∑ Llama-3.1-70B-Instruct, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è 2.17-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 98.4% –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–æ–ª–∂–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Optimizing Large Language Models for Efficient Inference",
                    "desc": "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."
                },
                "zh": {
                    "title": "È´òÊïàÊé®ÁêÜÔºåÂº∫Â§ßÊ®°ÂûãÁöÑÊñ∞ËåÉÂºè",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÈ´òËÆ°ÁÆóÊàêÊú¨ÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPuzzleÊ°ÜÊû∂ÔºåÈÄöËøáÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢ÔºàNASÔºâÂú®ÁâπÂÆöÁ°¨‰ª∂‰∏äÂä†ÈÄüLLMÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂùóÁä∂Â±ÄÈÉ®Áü•ËØÜËí∏È¶èÔºàBLDÔºâËøõË°åÂπ∂Ë°åÊû∂ÊûÑÊé¢Á¥¢ÔºåÂπ∂ÈááÁî®Ê∑∑ÂêàÊï¥Êï∞ËßÑÂàíËøõË°åÁ≤æÁ°ÆÁ∫¶Êùü‰ºòÂåñ„ÄÇÈÄöËøáNemotron-51BÊ®°ÂûãÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®Âçï‰∏™NVIDIA H100 GPU‰∏äÂÆûÁé∞2.17ÂÄçÊé®ÁêÜÂêûÂêêÈáèÊèêÂçáÁöÑÂÆûÈôÖÊïàÊûúÔºåÂêåÊó∂‰øùÁïô‰∫Ü98.4%ÁöÑÂéüÂßãÊ®°ÂûãËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19324",
            "title": "Trajectory Attention for Fine-grained Video Motion Control",
            "url": "https://huggingface.co/papers/2411.19324",
            "abstract": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.",
            "score": 3,
            "issue_id": 885,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "02a266f597ae69e7",
            "authors": [
                "Zeqi Xiao",
                "Wenqi Ouyang",
                "Yifan Zhou",
                "Shuai Yang",
                "Lei Yang",
                "Jianlou Si",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Sensetime Research",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19324.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é trajectory attention",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'trajectory attention' –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤–¥–æ–ª—å –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø–∏–∫—Å–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –≤–Ω–µ–¥—Ä—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. Trajectory attention —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ç–≤—å –Ω–∞—Ä—è–¥—É —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–∞–∫ —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Enhancing Video Generation with Trajectory Attention",
                    "desc": "This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks."
                },
                "zh": {
                    "title": "ËΩ®ËøπÊ≥®ÊÑèÂäõÔºöÁ≤æÁ°ÆÊéßÂà∂ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩ®ËøπÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Â§ÑÁêÜËøêÂä®ÊéßÂà∂ÔºåÂπ∂ÊúâÊïàÂú∞ÁªìÂêà‰∫ÜËΩ®Ëøπ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜËΩ®ËøπÊ≥®ÊÑèÂäõ‰Ωú‰∏∫ËæÖÂä©ÂàÜÊîØ‰∏é‰º†ÁªüÊó∂Èó¥Ê≥®ÊÑèÂäõÁªìÂêàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêÊñ∞ÂÜÖÂÆπÁöÑÂêåÊó∂ÔºåÁ°Æ‰øù‰∫ÜËøêÂä®ÊéßÂà∂ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≤æÂ∫¶ÂíåÈïøË∑ùÁ¶ª‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18478",
            "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS",
            "url": "https://huggingface.co/papers/2411.18478",
            "abstract": "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).",
            "score": 2,
            "issue_id": 890,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "05890d0739faa85c",
            "authors": [
                "Jinyang Wu",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Feihu Che",
                "Zengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.18478.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "HiAR-ICL: –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "HiAR-ICL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –º—ã—à–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—è—Ç—å –±–∞–∑–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—à–ª–µ–Ω–∏—è. –ü—Ä–∏–º–µ–Ω—è—è –ø–æ–∏—Å–∫ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ, HiAR-ICL —Å–æ–∑–¥–∞–µ—Ç '–∫–∞—Ä—Ç–æ—á–∫–∏ –º—ã—Å–ª–µ–π' –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –≤—ã–≤–æ–¥–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ HiAR-ICL –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è GPT-4 –∏ Claude 3.5."
                },
                "en": {
                    "title": "Revolutionizing Mathematical Reasoning with HiAR-ICL",
                    "desc": "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."
                },
                "zh": {
                    "title": "È´òÂ±ÇÊ¨°Ëá™Âä®Êé®ÁêÜÔºöË∂ÖË∂ä‰º†Áªü‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÂ±ÄÈôêÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ´òÂ±ÇÊ¨°Ëá™Âä®Êé®ÁêÜËåÉÂºèHiAR-ICLÔºåÊó®Âú®Ëß£ÂÜ≥‰º†Áªü‰∏ä‰∏ãÊñáÂ≠¶‰π†Âú®Â§çÊùÇÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇHiAR-ICLÈÄöËøáÂºïÂÖ•‰∫îÁßçÂü∫Êú¨Êé®ÁêÜÂä®‰ΩúÔºåËΩ¨Âèò‰∫ÜÂØπÂÖ∑‰ΩìÁ§∫‰æãÁöÑ‰æùËµñÔºåÂº∫Ë∞ÉÊäΩË±°ÊÄùÁª¥Ê®°ÂºèÁöÑÈáçË¶ÅÊÄß„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Êé¢Á¥¢Êé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÊûÑÂª∫ÊÄùÁª¥Âç°Áâá‰ª•ÊåáÂØºÂêéÁª≠Êé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHiAR-ICLÂú®MATHÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü79.6%ÁöÑÂáÜÁ°ÆÁéáÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂÖàËøõÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19189",
            "title": "Video Depth without Video Models",
            "url": "https://huggingface.co/papers/2411.19189",
            "abstract": "Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.",
            "score": 2,
            "issue_id": 889,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "1fc611a9a44595a1",
            "authors": [
                "Bingxin Ke",
                "Dominik Narnhofer",
                "Shengyu Huang",
                "Lei Ke",
                "Torben Peters",
                "Katerina Fragkiadaki",
                "Anton Obukhov",
                "Konrad Schindler"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19189.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#optimization"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "RollingDepth: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LDM",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RollingDepth. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (LDM) –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –æ—Ü–µ–Ω—â–∏–∫ –≥–ª—É–±–∏–Ω—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–±–æ—Ä–∫–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª–æ—Å—Ç–Ω–æ–µ –≤–∏–¥–µ–æ. RollingDepth —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ, —Ç–∞–∫ –∏ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç–∏ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≥–ª—É–±–∏–Ω—ã –ø—Ä–∏ –¥–≤–∏–∂–µ–Ω–∏–∏ –∫–∞–º–µ—Ä—ã."
                },
                "en": {
                    "title": "Transforming Monocular Videos into Accurate 3D Depth with RollingDepth",
                    "desc": "This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences."
                },
                "zh": {
                    "title": "Â∞ÜÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°ÊèêÂçá‰∏∫ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂàõÊñ∞‰πãË∑Ø",
                    "desc": "ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°ÈÄöËøáÊé®Êñ≠ÊØèÂ∏ßÁöÑÂØÜÈõÜÊ∑±Â∫¶ÔºåÂ∞ÜÂçïÁõÆËßÜÈ¢ëÁâáÊÆµÊèêÂçá‰∏∫3D„ÄÇÊúÄËøëÔºåÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°ÁöÑËøõÂ±ïÊøÄÂèë‰∫ÜÂØπËßÜÈ¢ëÊ∑±Â∫¶ÁöÑÂÖ≥Ê≥®Ôºå‰ΩÜÁÆÄÂçïÂú∞Â∞ÜÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°Âô®Â∫îÁî®‰∫éÊØèÂ∏ß‰ºöÂøΩÁï•Êó∂Èó¥ËøûÁª≠ÊÄßÔºåÂØºËá¥Èó™ÁÉÅÂíåÊ∑±Â∫¶ËåÉÂõ¥ÁöÑÁ™ÅÁÑ∂ÂèòÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RollingDepthÁöÑÊ®°ÂûãÔºåÂÆÉÁªìÂêà‰∫ÜÂ§öÂ∏ßÊ∑±Â∫¶‰º∞ËÆ°Âíå‰ºòÂåñÁöÑÊ≥®ÂÜåÁÆóÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÈïøËßÜÈ¢ëÂπ∂Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÊ∑±Â∫¶ËßÜÈ¢ë„ÄÇËØ•Ê®°ÂûãÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫Ü‰∏ìÁî®ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°Âô®ÂíåÈ´òÊÄßËÉΩÂçïÂ∏ßÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18673",
            "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.18673",
            "abstract": "Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.",
            "score": 2,
            "issue_id": 886,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "1ea35d3552a278a3",
            "authors": [
                "Sherwin Bahmani",
                "Ivan Skorokhodov",
                "Guocheng Qian",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Sergey Tulyakov"
            ],
            "affiliations": [
                "SFU",
                "Snap Inc.",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18673.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#training",
                    "#optimization",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã —Å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –∫–∞–º–µ—Ä—ã –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Advanced 3D Camera Control (AC3D), –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å AC3D –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–∞–º–µ—Ä–æ–π."
                },
                "en": {
                    "title": "Precision in 3D Camera Control for Enhanced Video Generation",
                    "desc": "This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling."
                },
                "zh": {
                    "title": "ÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè",
                    "desc": "Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü3DÁõ∏Êú∫ÊéßÂà∂Âú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®ÔºåÂèëÁé∞Áõ∏Êú∫ËøêÂä®ÂØπËßÜÈ¢ëÁîüÊàêË¥®ÈáèÊúâÊòæËëóÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÂíåÊµãËØïÂßøÊÄÅË∞ÉËäÇÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊî∂ÊïõÈÄüÂ∫¶ÂíåËßÜÈ¢ëÁöÑËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÂØπÊó†Êù°‰ª∂ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÁöÑË°®Á§∫ËøõË°åÊé¢ÊµãÔºåÊàë‰ª¨ÂèëÁé∞Áõ∏Êú∫ÂßøÊÄÅ‰º∞ËÆ°Âú®Ê®°ÂûãÂÜÖÈÉ®ÈöêÂºèÊâßË°åÔºåÂõ†Ê≠§Êàë‰ª¨ÈôêÂà∂‰∫ÜÁõ∏Êú∫Êù°‰ª∂ÁöÑÊ≥®ÂÖ•Ôºå‰ª•ÂáèÂ∞ëÂØπÂÖ∂‰ªñËßÜÈ¢ëÁâπÂæÅÁöÑÂπ≤Êâ∞„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂Êû∂ÊûÑÔºàAC3DÔºâÔºåÊàê‰∏∫ÂÖ∑ÊúâÁõ∏Êú∫ÊéßÂà∂ÁöÑÁîüÊàêËßÜÈ¢ëÂª∫Ê®°ÁöÑÊñ∞‰∏Ä‰ª£Ê®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19638",
            "title": "LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification",
            "url": "https://huggingface.co/papers/2411.19638",
            "abstract": "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.",
            "score": 1,
            "issue_id": 889,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "98bf5f113194343b",
            "authors": [
                "Taja Kuzman",
                "Nikola Ljube≈°iƒá"
            ],
            "affiliations": [
                "Department of Knowledge Technologies, Jo≈æef Stefan Institute, 1000 Ljubljana, Slovenia",
                "Jo≈æef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia",
                "University of Ljubljana, 1000 Ljubljana, Slovenia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19638.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#training",
                    "#low_resource",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "üì∞",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ '—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫'. –ë–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å GPT –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —Ä–æ–ª–∏ —É—á–∏—Ç–µ–ª—è, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–Ω–æ—Ç–∏—Ä—É—è –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö —è–∑—ã–∫–∞—Ö. –ú–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–æ—Å—Ç–∏–≥–∞—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –∏ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π."
                },
                "en": {
                    "title": "Empowering Multilingual News Classification with Teacher-Student LLMs",
                    "desc": "This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages."
                },
                "zh": {
                    "title": "Â§öËØ≠Ë®ÄÊñ∞ÈóªÂàÜÁ±ªÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "ÈöèÁùÄÂú®Á∫øÊñ∞ÈóªÊï∞ÈáèÁöÑ‰∏çÊñ≠Â¢ûÂä†ÔºåÊåâ‰∏ªÈ¢òÂØπÊñ∞ÈóªËøõË°åÂàÜÁ±ªÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïôÂ∏à-Â≠¶ÁîüÊ°ÜÊû∂ÔºåÁî®‰∫éÂºÄÂèëÂ§öËØ≠Ë®ÄÊñ∞ÈóªÂàÜÁ±ªÊ®°ÂûãÔºå‰∏îÊó†ÈúÄÊâãÂä®Êï∞ÊçÆÊ†áÊ≥®„ÄÇÊïôÂ∏àÊ®°Âûã‰ΩøÁî®ÁîüÊàêÈ¢ÑËÆ≠ÁªÉÂèòÊç¢Âô®ÔºàGPTÔºâËá™Âä®Ê†áÊ≥®Êñ∞ÈóªÊñáÁ´†ÔºåÂ±ïÁ§∫Âá∫Âú®Â§öÁßçËØ≠Ë®Ä‰∏äÁöÑÈ´òÈõ∂Ê†∑Êú¨ÊÄßËÉΩ„ÄÇÈÄöËøáÂæÆË∞ÉËæÉÂ∞èÁöÑBERTÁ±ªÂ≠¶ÁîüÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂú®Áõ∏ÂØπËæÉÂ∞ëÁöÑËÆ≠ÁªÉÂÆû‰æã‰∏ã‰πüËÉΩËææÂà∞‰∏éÊïôÂ∏àÊ®°ÂûãÁõ∏ÂΩìÁöÑÈ´òÊÄßËÉΩ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-11-29.html",
    "link_next": "2024-12-03.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11Êúà29Êó•"
    },
    "short_date_next": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12Êúà3Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "ËøëÂπ¥Êù•ÔºåÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂèñÂæó‰∫ÜÂø´ÈÄüÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜËøô‰∫õÈÄöÁî®Ê®°ÂûãÂ∫îÁî®Âà∞ÁâπÂÆöÈ¢ÜÂüüÔºåÂ¶ÇÁßëÂ≠¶ÂíåÂ∑•‰∏öÔºå‰ªçÁÑ∂ËæÉÂ∞ëÊé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÈÄöËøáÂêéËÆ≠ÁªÉËøõË°åMLLMsÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÔºåÈáçÁÇπÊòØÊï∞ÊçÆÂêàÊàê„ÄÅËÆ≠ÁªÉÊµÅÊ∞¥Á∫øÂíå‰ªªÂä°ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ËßÜËßâÊåá‰ª§ÂêàÊàêÂô®ÔºåËÉΩÂ§ü‰ªéÁâπÂÆöÈ¢ÜÂüüÁöÑÂõæÁâá-Ê†áÈ¢òÂØπ‰∏≠ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßâÊåá‰ª§‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÂêàÊàê‰ªªÂä°Âú®Â¢ûÂº∫MLLMsÁöÑÈ¢ÜÂüüÁâπÂÆöÊÄßËÉΩÊñπÈù¢‰ºò‰∫éÊâãÂä®ËßÑÂàô„ÄÅGPT-4ÂíåGPT-4VÁîüÊàêÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨Ëøò‰ΩøÁî®ÂçïÈò∂ÊÆµËÆ≠ÁªÉÊµÅÊ∞¥Á∫øÊù•Â¢ûÂº∫ÁâπÂÆöÈ¢ÜÂüüÁöÑ‰ªªÂä°Â§öÊ†∑ÊÄß„ÄÇÊàë‰ª¨Âú®ÁîüÁâ©ÂåªÂ≠¶ÂíåÈ£üÂìÅÈ¢ÜÂüüËøõË°å‰∫ÜÂÆûÈ™åÔºåÂπ∂ËØÑ‰º∞‰∫Ü‰∏çÂêåÊù•Ê∫êÂíåËßÑÊ®°ÁöÑMLLMsÂú®ÂêÑÁßçÁâπÂÆöÈ¢ÜÂüü‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ‰∏∫ÊîØÊåÅËøõ‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàë‰ª¨Â∞ÜÂºÄÊ∫êÊàë‰ª¨ÁöÑÂÆûÁé∞„ÄÇ",
        "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
        "pinyin": "J√¨n ni√°n l√°i, t≈çngy√≤ng du≈ç m√≥ t√†i d√† y«îy√°n m√≥x√≠ng (MLLMs) q«îd√© le ku√†is√π fƒÅzh«én. R√°n'√©r, jiƒÅng zh√®xiƒì t≈çngy√≤ng m√≥x√≠ng y√¨ngy√≤ng d√†o t√®d√¨ng l«êngy√π, r√∫ kƒìxu√© h√© g≈çngy√®, r√©ngr√°n ji√†osh«éo t√†nsu«í. Bƒõnw√©n x√¨t«íng de y√°nji≈´ le t≈çnggu√≤ h√≤u x√πnli√†n j√¨nx√≠ng MLLMs de l«êngy√π sh√¨y√¨ng, zh√≤ngdi«én sh√¨ sh√πj√π h√©ch√©ng, x√πnli√†n li√∫shu«êxi√†n h√© r√®nw√π p√≠ngg«î. W«ímen kƒÅifƒÅ le yƒ´g√® sh√¨ji√†o zh«êl√¨ng h√©ch√©ngq√¨, n√©ngg√≤u c√≥ng t√®d√¨ng l«êngy√π de t√∫pi√†n-biƒÅot√≠ du√¨ zh≈çng shƒìngch√©ng du≈çy√†nghu√† de sh√¨ji√†o zh«êl√¨ng r√®nw√π. W«ímen de h√©ch√©ng r√®nw√π z√†i zƒìngqi√°ng MLLMs de l«êngy√π t√®d√¨ng x√¨ngn√©ng fƒÅngmi√†n y≈çu sh«íud√≤ng guƒ´z√©, GPT-4 h√© GPT-4V shƒìngch√©ng de r√®nw√π. W«ímen h√°i sh«êy√≤ng dƒÅn jiƒìdu√†n x√πnli√†n li√∫shu«êxi√†n l√°i zƒìngqi√°ng t√®d√¨ng l«êngy√π de r√®nw√π du≈çy√†nghu√†x√¨ng. W«ímen z√†i shƒìngw√π yƒ´xi√†o h√© sh√≠p«ên l«êngy√π j√¨nx√≠ng le sh√≠y√†n, b√¨ng p√≠ngg«î le b√πt√≥ng l√°iyu√°n h√© guƒ´m√≥ de MLLMs z√†i g√®zh«íng t√®d√¨ng l«êngy√π r√®nw√π sh√†ng de x√¨ngn√©ng. W√®i zhƒ´ch√≠ j√¨nfƒÅ y√°nji≈´, w«ímen jiƒÅng kƒÅiyu√°n w«ímen de sh√≠xi√†n.",
        "vocab": "[{'word': 'ËøëÂπ¥Êù•', 'pinyin': 'j√¨n ni√°n l√°i', 'trans': 'in recent years'},\n{'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'universal'},\n{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'},\n{'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'},\n{'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'},\n{'word': 'Âø´ÈÄü', 'pinyin': 'ku√†i s√π', 'trans': 'rapid'},\n{'word': 'ÂèëÂ±ï', 'pinyin': 'fƒÅ zh«én', 'trans': 'development'},\n{'word': 'ÁÑ∂ËÄå', 'pinyin': 'r√°n √©r', 'trans': 'however'},\n{'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'apply'},\n{'word': 'ÁâπÂÆö', 'pinyin': 't√® d√¨ng', 'trans': 'specific'},\n{'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'},\n{'word': 'ÁßëÂ≠¶', 'pinyin': 'kƒì xu√©', 'trans': 'science'},\n{'word': 'Â∑•‰∏ö', 'pinyin': 'g≈çng y√®', 'trans': 'industry'},\n{'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'explore'},\n{'word': 'Á≥ªÁªüÂú∞', 'pinyin': 'x√¨ t«íng de', 'trans': 'systematically'},\n{'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'study'},\n{'word': 'ÂêéËÆ≠ÁªÉ', 'pinyin': 'h√≤u x√πn li√†n', 'trans': 'post-training'},\n{'word': 'ËøõË°å', 'pinyin': 'j√¨n x√≠ng', 'trans': 'conduct'},\n{'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adaptation'},\n{'word': 'ÈáçÁÇπ', 'pinyin': 'zh√≤ng di«én', 'trans': 'focus'},\n{'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'},\n{'word': 'ÂêàÊàê', 'pinyin': 'h√© ch√©ng', 'trans': 'synthesis'},\n{'word': 'ÊµÅÊ∞¥Á∫ø', 'pinyin': 'li√∫ shu«ê xi√†n', 'trans': 'pipeline'},\n{'word': '‰ªªÂä°', 'pinyin': 'r√®n w√π', 'trans': 'task'},\n{'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'},\n{'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅi fƒÅ', 'trans': 'develop'},\n{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'},\n{'word': 'Êåá‰ª§', 'pinyin': 'zh«ê l√¨ng', 'trans': 'instruction'},\n{'word': 'ÂêàÊàêÂô®', 'pinyin': 'h√© ch√©ng q√¨', 'trans': 'synthesizer'},\n{'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'},\n{'word': 'ÂõæÁâá', 'pinyin': 't√∫ pi√†n', 'trans': 'image'},\n{'word': 'Ê†áÈ¢ò', 'pinyin': 'biƒÅo t√≠', 'trans': 'title'},\n{'word': 'ÂØπ', 'pinyin': 'du√¨', 'trans': 'pair'},\n{'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diversify'},\n{'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhance'},\n{'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'},\n{'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'},\n{'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'},\n{'word': 'ÊâãÂä®', 'pinyin': 'sh«íu d√≤ng', 'trans': 'manual'},\n{'word': 'ËßÑÂàô', 'pinyin': 'guƒ´ z√©', 'trans': 'rule'},\n{'word': 'ÂçïÈò∂ÊÆµ', 'pinyin': 'dƒÅn jiƒì du√†n', 'trans': 'single-stage'},\n{'word': 'ÁîüÁâ©ÂåªÂ≠¶', 'pinyin': 'shƒìng w√π yƒ´ xu√©', 'trans': 'biomedical'},\n{'word': 'È£üÂìÅ', 'pinyin': 'sh√≠ p«ên', 'trans': 'food'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'},\n{'word': 'Êù•Ê∫ê', 'pinyin': 'l√°i yu√°n', 'trans': 'source'},\n{'word': 'ËßÑÊ®°', 'pinyin': 'guƒ´ m√≥', 'trans': 'scale'},\n{'word': 'ÊîØÊåÅ', 'pinyin': 'zhƒ´ ch√≠', 'trans': 'support'},\n{'word': 'Ëøõ‰∏ÄÊ≠•', 'pinyin': 'j√¨n yƒ´ b√π', 'trans': 'further'},\n{'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open-source'},\n{'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'implementation'}]",
        "trans": "In recent years, general-purpose multimodal large language models (MLLMs) have made rapid progress. However, the application of these general models to specific domains, such as science and industry, remains relatively unexplored. This paper systematically investigates the domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. We developed a visual instruction synthesizer capable of generating diverse visual instruction tasks from domain-specific image-caption pairs. Our synthesized tasks outperform those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. We also employed a single-stage training pipeline to enhance the diversity of domain-specific tasks. We conducted experiments in the biomedical and food domains and evaluated the performance of MLLMs from different sources and scales on various domain-specific tasks. To support further research, we will open-source our implementation.",
        "update_ts": "2024-12-02 09:12"
    }
}