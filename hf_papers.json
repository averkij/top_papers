{
    "date": {
        "ru": "26 мая",
        "en": "May 26",
        "zh": "5月26日"
    },
    "time_utc": "2025-05-26 07:14",
    "weekday": 0,
    "issue_id": 3950,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.17612",
            "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
            "url": "https://huggingface.co/papers/2505.17612",
            "abstract": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
            "score": 28,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "258cfb9a5b51fa42",
            "authors": [
                "Minki Kang",
                "Jongwon Jeong",
                "Seanie Lee",
                "Jaewoong Cho",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "KRAFTON"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17612.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#small_models",
                    "#agents",
                    "#transfer_learning",
                    "#training",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Передача навыков агента: от больших моделей к малым",
                    "desc": "Метод Agent Distillation позволяет передавать навыки рассуждения и решения задач от больших языковых моделей (LLM) к меньшим моделям. Этот подход использует улучшенные промпты и самосогласованную генерацию действий. Agent Distillation позволяет маленьким моделям достигать производительности, сравнимой с более крупными моделями, на различных задачах, требующих рассуждений. Метод был протестирован на восьми задачах в фактологических и математических областях."
                },
                "en": {
                    "title": "Empowering Small Models with Big Model Intelligence",
                    "desc": "Agent Distillation is a method that helps smaller language models (sLMs) learn reasoning and task-solving skills from larger language models (LLMs). It uses improved prompts and self-consistent actions to enhance the performance of sLMs on reasoning tasks, making them competitive with larger models. The approach addresses challenges like hallucination in sLMs when faced with rare facts or complex computations. By evaluating on various reasoning tasks, the study shows that even small models can perform well, paving the way for more efficient AI applications."
                },
                "zh": {
                    "title": "代理蒸馏：小型模型的推理能力提升",
                    "desc": "本论文提出了一种名为代理蒸馏的框架，旨在将大型语言模型（LLM）的推理和任务解决能力转移到较小的语言模型（sLM）中。通过使用增强的提示和自一致性动作，代理蒸馏能够在多个推理任务上实现与大型模型相当的性能。我们的方法包括引入一种新的提示方法和改进小型代理在测试时的鲁棒性。实验结果表明，参数量为0.5B到3B的小型模型可以在事实和数学领域的推理任务中与更大的模型竞争。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15929",
            "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
            "url": "https://huggingface.co/papers/2505.15929",
            "abstract": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy respectively-performance gaps exceeding 29\\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation.",
            "score": 28,
            "issue_id": 3950,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 мая",
                "en": "May 21",
                "zh": "5月21日"
            },
            "hash": "995e1fe73d1e1cef",
            "authors": [
                "Hui Shen",
                "Taiqiang Wu",
                "Qi Han",
                "Yunta Hsieh",
                "Jizhou Wang",
                "Yuyue Zhang",
                "Yuxin Cheng",
                "Zijian Hao",
                "Yuansheng Ni",
                "Xin Wang",
                "Zhongwei Wan",
                "Kai Zhang",
                "Wendong Xu",
                "Jing Xiong",
                "Ping Luo",
                "Wenhu Chen",
                "Chaofan Tao",
                "Zhuoqing Mao",
                "Ngai Wong"
            ],
            "affiliations": [
                "Independent",
                "The Ohio State University",
                "The University of Hong Kong",
                "University of Michigan",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15929.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PhyX: новый рубеж в оценке физического мышления ИИ",
                    "desc": "Новый эталонный тест PhyX оценивает способность моделей к физически обоснованным рассуждениям в визуальных сценариях. Тест включает 3000 тщательно подобранных мультимодальных вопросов по 6 типам рассуждений в 25 поддоменах и 6 основных областях физики. Даже современные модели искусственного интеллекта, такие как GPT-4 и Claude, демонстрируют значительно более низкую точность по сравнению с экспертами-людьми. Анализ выявил критические ограничения моделей, включая чрезмерную зависимость от запомненных знаний и поверхностное сопоставление визуальных паттернов вместо истинного физического понимания."
                },
                "en": {
                    "title": "PhyX: Bridging the Gap in Physics-Grounded Reasoning for AI",
                    "desc": "The paper introduces PhyX, a new benchmark for evaluating models' abilities in physics-grounded reasoning within visual contexts. It highlights that existing benchmarks do not adequately assess this crucial aspect of intelligence, which combines domain knowledge and real-world constraints. PhyX consists of 3,000 carefully curated multimodal questions across various physics domains, revealing that even advanced models like GPT-4o and Claude3.7-Sonnet perform poorly compared to human experts. The study identifies key limitations in current models, such as reliance on memorized knowledge and superficial visual pattern recognition, and provides a robust evaluation framework for future research."
                },
                "zh": {
                    "title": "PhyX：评估物理推理能力的新基准",
                    "desc": "PhyX是一个新的基准测试，旨在评估模型在视觉场景中的物理推理能力。研究表明，当前的模型在物理理解方面存在显著局限，远不及人类专家。PhyX包含3000个精心策划的多模态问题，涵盖25个子领域和6个核心物理领域。通过全面评估，发现即使是最先进的模型在物理推理上也面临重大挑战，准确率远低于人类专家。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17667",
            "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17667",
            "abstract": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.",
            "score": 26,
            "issue_id": 3948,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "943935a610a2c31e",
            "authors": [
                "Fanqi Wan",
                "Weizhou Shen",
                "Shengyi Liao",
                "Yingcheng Shi",
                "Chenliang Li",
                "Ziyi Yang",
                "Ji Zhang",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Qwen-Doc Team, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17667.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "QwenLong-L1: Прорыв в обработке длинного контекста для моделей рассуждения",
                    "desc": "QwenLong-L1 - это фреймворк, улучшающий модели крупномасштабного рассуждения (LRM) для работы с длинным контекстом с помощью обучения с подкреплением. Он использует поэтапный подход, включающий предварительное обучение с учителем и курируемое обучение с подкреплением. QwenLong-L1 решает проблемы неэффективности обучения и нестабильной оптимизации при работе с длинным контекстом. Эксперименты показывают, что QwenLong-L1-32B превосходит ведущие LRM в задачах ответов на вопросы по документам с длинным контекстом."
                },
                "en": {
                    "title": "Empowering Long-Context Reasoning with QwenLong-L1",
                    "desc": "The paper introduces QwenLong-L1, a framework designed to enhance large reasoning models (LRMs) for long-context reasoning tasks using reinforcement learning (RL). It addresses the challenges of training efficiency and optimization stability that arise when adapting LRMs from short-context to long-context scenarios. The framework employs a warm-up supervised fine-tuning stage to create a strong initial policy, followed by a curriculum-guided RL approach to ensure stable policy updates. Experimental results show that QwenLong-L1 significantly outperforms existing LRMs on long-context document question-answering benchmarks, marking a significant advancement in the field."
                },
                "zh": {
                    "title": "QwenLong-L1：长上下文推理的新突破",
                    "desc": "QwenLong-L1是一个增强大型推理模型的框架，旨在通过强化学习提高长上下文推理能力。该框架解决了在长上下文输入中进行有效推理的关键挑战，包括训练效率低下和优化过程不稳定。通过逐步上下文扩展和温暖启动的监督微调阶段，QwenLong-L1建立了稳健的初始策略，并采用课程引导的阶段性强化学习技术来稳定策略演变。实验结果表明，QwenLong-L1在长上下文文档问答基准测试中表现优异，超越了其他领先的推理模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18092",
            "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
            "url": "https://huggingface.co/papers/2505.18092",
            "abstract": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the \"lost in the middle\" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance.   Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference.   Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59times context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance.",
            "score": 23,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "59626d27805a1427",
            "authors": [
                "Weizhou Shen",
                "Chenliang Li",
                "Fanqi Wan",
                "Shengyi Liao",
                "Shaopeng Lai",
                "Bo Zhang",
                "Yingcheng Shi",
                "Yuning Wu",
                "Gang Fu",
                "Zhansheng Li",
                "Bin Yang",
                "Ji Zhang",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18092.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революционное улучшение обработки длинного контекста в нейросетях",
                    "desc": "QwenLong-CPRS - это фреймворк для оптимизации контекста в больших языковых моделях. Он использует многоуровневое сжатие контекста, динамическую оптимизацию на основе естественного языка и эффективное двунаправленное рассуждение. QwenLong-CPRS решает проблемы высоких вычислительных затрат и деградации производительности при обработке длинных последовательностей. Тесты показали превосходство QwenLong-CPRS над другими методами управления контекстом как по точности, так и по эффективности."
                },
                "en": {
                    "title": "Revolutionizing Long Context Management in LLMs",
                    "desc": "QwenLong-CPRS is a framework that improves large language models (LLMs) by optimizing how they handle long contexts. It uses a dynamic optimization method that is guided by natural language, allowing for better context compression at multiple levels. This approach not only enhances the efficiency of processing long sequences but also improves the overall performance of the models. The framework has been tested against various benchmarks, showing significant gains in accuracy and efficiency compared to existing context management techniques."
                },
                "zh": {
                    "title": "QwenLong-CPRS：高效的上下文压缩与优化",
                    "desc": "QwenLong-CPRS 是一种用于长上下文优化的框架，旨在解决大型语言模型在处理长序列时的计算开销和性能下降问题。它通过自然语言指导的动态上下文优化机制，实现了多粒度的上下文压缩，从而提高了效率和性能。该框架引入了四项关键创新，包括双向推理层和基于语言建模的令牌评估机制。综合评估显示，QwenLong-CPRS 在多个基准测试中表现优异，显著超越了其他上下文管理方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17225",
            "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
            "url": "https://huggingface.co/papers/2505.17225",
            "abstract": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, . Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.",
            "score": 22,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "41036303d3b75082",
            "authors": [
                "Doohyuk Jang",
                "Yoonjeon Kim",
                "Chanjae Park",
                "Hyun Ryu",
                "Eunho Yang"
            ],
            "affiliations": [
                "AITRICS",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17225.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#dataset",
                    "#data",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Преодоление жесткости мышления в ИИ: новый подход к диагностике языковых моделей",
                    "desc": "Статья представляет диагностический набор для анализа и категоризации жесткости рассуждений в больших языковых моделях. Исследователи выявили тенденцию моделей игнорировать инструкции и использовать знакомые паттерны рассуждений. Были определены три режима контаминации: перегрузка интерпретации, недоверие к входным данным и частичное внимание к инструкциям. Набор данных включает модифицированные варианты математических тестов и головоломок, требующих отклонения от привычных стратегий рассуждения."
                },
                "en": {
                    "title": "Unraveling Reasoning Rigidity in Language Models",
                    "desc": "This paper investigates a phenomenon called reasoning rigidity in large language models, where these models often ignore user instructions and revert to familiar reasoning patterns. The authors introduce a diagnostic set designed to identify and categorize this behavior, which can lead to incorrect conclusions in tasks requiring precise adherence to instructions. They highlight three specific modes of contamination: Interpretation Overload, Input Distrust, and Partial Instruction Attention, which describe how models distort or overlook given instructions. By releasing this diagnostic set, the authors aim to support further research aimed at reducing reasoning rigidity in language models."
                },
                "zh": {
                    "title": "揭示语言模型的推理僵化现象",
                    "desc": "本文探讨了大型语言模型中的推理僵化现象，即模型在面对明确指令时，仍然倾向于使用熟悉的推理模式。我们引入了一个专家策划的诊断集，以系统地研究这一行为，特别是在数学和逻辑难题等领域。该数据集包含经过修改的数学基准和重新设计的难题，旨在促使模型偏离常规推理策略。通过分析，我们识别出三种主要的推理僵化模式，帮助未来的研究更好地解决这一问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17941",
            "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
            "url": "https://huggingface.co/papers/2505.17941",
            "abstract": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker",
            "score": 19,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "cfc0e5dae345ea81",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Ruonan Yu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17941.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное сжатие цепочек рассуждений без потери точности",
                    "desc": "VeriThinker - это новый подход к сжатию цепочек рассуждений в крупных моделях рассуждений (LRM). Метод использует дообучение LRM на вспомогательной задаче верификации, что позволяет моделям лучше оценивать необходимость дальнейших шагов самоанализа. Эксперименты показывают, что VeriThinker значительно сокращает длину цепочек рассуждений, сохраняя или даже немного улучшая точность. Подход также демонстрирует возможность обобщения на спекулятивные рассуждения без дополнительного обучения."
                },
                "en": {
                    "title": "Streamlining Reasoning with VeriThinker",
                    "desc": "VeriThinker is a method designed to enhance Large Reasoning Models (LRMs) by reducing the length of their reasoning chains. It achieves this by fine-tuning the models on a verification task instead of directly on the original reasoning tasks. This approach helps the models become more efficient by minimizing unnecessary steps in their reasoning process, which lowers inference costs. Experimental results show that VeriThinker not only shortens reasoning chains but also improves accuracy in various tasks, demonstrating its effectiveness in optimizing LRM performance."
                },
                "zh": {
                    "title": "VeriThinker：优化推理链，提升效率与准确性",
                    "desc": "VeriThinker是一种新方法，通过在验证任务上微调大型推理模型（LRMs），减少复杂推理链的长度，从而降低推理成本而不显著牺牲准确性。传统方法直接在原始推理任务上微调模型，而VeriThinker则创新性地仅通过辅助验证任务进行微调。通过训练LRMs准确验证推理解决方案的正确性，模型能够更好地判断后续自我反思步骤的必要性，有效抑制过度思考。实验结果表明，VeriThinker显著减少了推理链的长度，同时保持或略微提高了准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18125",
            "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
            "url": "https://huggingface.co/papers/2505.18125",
            "abstract": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.",
            "score": 16,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "e6debd33931f5a78",
            "authors": [
                "Alan Arazi",
                "Eilam Shapira",
                "Roi Reichart"
            ],
            "affiliations": [
                "Technion - IIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#transfer_learning",
                    "#architecture",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "TabSTAR: Умное представление текста для табличных данных",
                    "desc": "TabSTAR - это новая модель машинного обучения для табличных данных с текстовыми признаками. Она использует трансферное обучение и не требует настройки параметров под конкретный набор данных. TabSTAR превосходит существующие методы в задачах классификации благодаря семантически целевым представлениям текста. Модель демонстрирует масштабируемость при предобучении на большом количестве наборов данных, что открывает возможности для дальнейшего улучшения производительности."
                },
                "en": {
                    "title": "TabSTAR: Revolutionizing Tabular Learning with Contextual Text Understanding",
                    "desc": "TabSTAR is a new model designed to improve classification tasks that involve tabular data with text features. It uses a unique approach called semantically target-aware representations, which helps the model understand the context of the data better. Unlike previous methods, TabSTAR does not rely on dataset-specific parameters, allowing it to generalize across different datasets effectively. This model achieves top performance on various benchmarks, demonstrating its potential for enhancing tabular learning tasks."
                },
                "zh": {
                    "title": "TabSTAR：表格数据的新突破",
                    "desc": "TabSTAR是一种新的表格基础模型，专注于处理带有文本特征的分类任务。它通过转移学习实现了无数据集特定参数的高效性能，克服了传统方法的局限。该模型利用语义目标感知的表示，能够更好地理解任务上下文，从而生成更有效的特征嵌入。TabSTAR在多个基准测试中表现出色，展示了在中型和大型数据集上的最佳性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16211",
            "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.16211",
            "abstract": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.",
            "score": 14,
            "issue_id": 3946,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "1849951d3588375e",
            "authors": [
                "Kai Li",
                "Can Shen",
                "Yile Liu",
                "Jirui Han",
                "Kelong Zheng",
                "Xuechao Zou",
                "Zhe Wang",
                "Xingjian Du",
                "Shun Zhang",
                "Hanjun Luo",
                "Yingbin Jin",
                "Xinxin Xing",
                "Ziyang Ma",
                "Yue Liu",
                "Xiaojun Jia",
                "Yifan Zhang",
                "Junfeng Fang",
                "Kun Wang",
                "Yibo Yan",
                "Haoyang Li",
                "Yiming Li",
                "Xiaobin Zhuang",
                "Yang Liu",
                "Haibo Hu",
                "Zhuo Chen",
                "Zhizheng Wu",
                "Xiaolin Hu",
                "Eng-Siong Chng",
                "XiaoFeng Wang",
                "Wenyuan Xu",
                "Wei Dong",
                "Xinfeng Li"
            ],
            "affiliations": [
                "ACM Member",
                "BJTU",
                "BNBU",
                "Bytedance",
                "CAS",
                "HUST",
                "Hong Kong Polytechnic University",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Independent Researcher",
                "Nanyang Technological University",
                "National University of Singapore",
                "QHU",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong (Shenzhen)",
                "Tsinghua University",
                "University of Rochester",
                "Waseda University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16211.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#security",
                    "#ethics",
                    "#open_source",
                    "#dataset",
                    "#audio"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "AudioTrust: Комплексная оценка надежности аудио ИИ",
                    "desc": "AudioTrust - это первая многогранная система оценки надежности Аудио Больших Языковых Моделей (АБЛМ). Она оценивает АБЛМ по шести ключевым параметрам: справедливость, галлюцинации, безопасность, конфиденциальность, устойчивость и аутентификация. Система использует набор данных из более чем 4420 аудио/текстовых образцов из реальных сценариев и 9 специфических метрик оценки. Результаты экспериментов выявляют границы надежности и ограничения современных АБЛМ в различных высокорисковых аудиосценариях."
                },
                "en": {
                    "title": "Evaluating Trust in Audio Large Language Models with AudioTrust",
                    "desc": "AudioTrust is a novel framework designed to evaluate the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions. It addresses the unique challenges and risks associated with audio data, which are often overlooked in existing evaluation methods that focus primarily on text. The framework includes a comprehensive dataset of over 4,420 audio/text samples and employs 18 experimental setups to assess six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. By utilizing nine audio-specific metrics and an automated scoring pipeline, AudioTrust provides insights into the limitations of current ALLMs, guiding their secure deployment in real-world applications."
                },
                "zh": {
                    "title": "音频模型信任评估新标准",
                    "desc": "AudioTrust是一个专门为音频大型语言模型（ALLMs）设计的多维信任评估框架。它通过一个包含4420多个音频/文本样本的数据集，评估模型在公平性、幻觉、安全性、隐私、鲁棒性和认证等六个关键维度的表现。该框架采用了18种不同的实验设置和9个音频特定的评估指标，以确保全面评估ALLMs的信任worthiness。实验结果揭示了当前最先进的开源和闭源ALLMs在高风险音频场景下的信任边界和局限性，为未来音频模型的安全和可信部署提供了重要见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17561",
            "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
            "url": "https://huggingface.co/papers/2505.17561",
            "abstract": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/",
            "score": 13,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "224b15e182587a84",
            "authors": [
                "Kwanyoung Kim",
                "Sanghyun Kim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17561.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умный выбор шума для лучшего видео-синтеза",
                    "desc": "Статья представляет ANSE - метод улучшения видео-диффузионных моделей путем выбора начальных шумовых сидов на основе уверенности модели. В основе лежит функция BANSA, оценивающая энтропию разногласий между стохастическими выборками внимания. Для эффективного применения во время инференса предложена аппроксимация BANSA с использованием маскирования по Бернулли. Эксперименты показали улучшение качества видео и временной согласованности при минимальном увеличении времени инференса."
                },
                "en": {
                    "title": "Smart Noise Selection for Better Video Generation",
                    "desc": "The paper introduces ANSE, a method that enhances video diffusion models by intelligently selecting noise seeds based on the model's confidence. It highlights the importance of initial noise in generating high-quality videos, as different seeds can lead to varying results. ANSE utilizes an acquisition function called BANSA, which measures uncertainty through attention-based entropy to identify the best noise seeds. This approach improves video quality and temporal coherence while only slightly increasing the time needed for inference."
                },
                "zh": {
                    "title": "主动选择噪声，提升视频生成质量",
                    "desc": "ANSE（主动噪声选择生成）通过基于模型信心选择噪声种子，增强了视频扩散模型的性能。该方法利用注意力机制量化不确定性，从而选择高质量的噪声种子，显著提高视频质量和时间一致性。核心算法BANSA（基于注意力的贝叶斯主动噪声选择）通过测量多个随机注意力样本之间的熵不一致性来估计模型的信心和一致性。实验结果表明，ANSE在推理时间仅增加8%和13%的情况下，显著改善了视频生成的质量和一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18129",
            "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.18129",
            "abstract": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.",
            "score": 12,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "e690c5668b7f4cd0",
            "authors": [
                "Yan Ma",
                "Linge Du",
                "Xuyang Shen",
                "Shaoxiang Chen",
                "Pengfei Li",
                "Qibing Ren",
                "Lizhuang Ma",
                "Yuchao Dai",
                "Pengfei Liu",
                "Junjie Yan"
            ],
            "affiliations": [
                "Google DeepMind",
                "MiniMax-AI",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18129.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единая система обучения с подкреплением для визуального ИИ",
                    "desc": "V-Triune - это система обучения с подкреплением, объединяющая задачи визуального рассуждения и восприятия в визуально-языковых моделях через единый процесс обучения. Система включает три компонента: форматирование данных на уровне выборки, вычисление наград на уровне верификатора и мониторинг метрик на уровне источника. Введена новая динамическая награда IoU для задач восприятия. Результирующая модель Orsta демонстрирует значительные улучшения как в задачах рассуждения, так и в задачах восприятия."
                },
                "en": {
                    "title": "Unifying Visual Reasoning and Perception in One RL System",
                    "desc": "The paper introduces V-Triune, a unified reinforcement learning system designed to enhance vision-language models (VLMs) by integrating visual reasoning and perception tasks into a single training framework. It features three key components: Sample-Level Data Formatting for input unification, Verifier-Level Reward Computation for tailored reward systems, and Source-Level Metric Monitoring for data diagnostics. A novel Dynamic IoU reward mechanism is also proposed, providing adaptive feedback for perception tasks. The resulting model, Orsta, shows significant performance improvements across various reasoning and perception benchmarks, demonstrating the effectiveness of this unified approach."
                },
                "zh": {
                    "title": "统一强化学习，提升视觉推理与感知能力",
                    "desc": "V-Triune是一个统一的强化学习系统，旨在通过单一的训练流程结合视觉推理和感知任务。该系统包含三个互补的组件，分别是样本级数据格式化、验证器级奖励计算和源级指标监控，以支持多样化的任务输入和定制化的奖励反馈。我们还引入了一种新的动态IoU奖励，为感知任务提供适应性和渐进性的反馈。通过在多样化数据集上训练，V-Triune显著提升了视觉语言模型在推理和感知任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15692",
            "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
            "url": "https://huggingface.co/papers/2505.15692",
            "abstract": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (\"thought patterns\"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.",
            "score": 11,
            "issue_id": 3946,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 мая",
                "en": "May 21",
                "zh": "5月21日"
            },
            "hash": "5b1bedaf6be49ffa",
            "authors": [
                "Jinyang Wu",
                "Chonghua Liao",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Zhengqi Wen",
                "Pengpeng Shao",
                "Huazhe Xu",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing National Research Center for Information Science and Technology",
                "Department of Automation, Tsinghua University",
                "Institution for Interdisciplinary Information Sciences, Tsinghua University",
                "Shanghai AI Lab",
                "Shanghai Qi Zhi Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15692.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "TAPO: Усиление обучения с подкреплением внешними мыслительными паттернами",
                    "desc": "TAPO - это новая система обучения с подкреплением, которая включает внешние подсказки для улучшения производительности и исследовательской способности моделей. В отличие от существующих методов, TAPO интегрирует структурированные мыслительные паттерны во время обучения, эффективно балансируя между внутренним исследованием модели и использованием внешних указаний. Эксперименты показывают значительное превосходство TAPO над GRPO на различных математических задачах. Анализ также выявил, что внедрение внешних указаний приводит к созданию мощных моделей рассуждения с улучшенной объяснимостью и читаемостью выводов."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Thought Patterns",
                    "desc": "The paper introduces TAPO (Thought-Augmented Policy Optimization), a new reinforcement learning framework that enhances model performance by integrating external guidance. Traditional RL methods often limit exploration by focusing solely on reward-maximizing paths, which restricts the reasoning capabilities of the models. TAPO addresses this issue by incorporating structured thought patterns during training, allowing for a better balance between internal exploration and external guidance. Experimental results demonstrate that TAPO significantly outperforms existing methods, leading to improved reasoning models that are more explainable and readable."
                },
                "zh": {
                    "title": "TAPO：增强探索与推理的新框架",
                    "desc": "本文提出了一种新的强化学习框架TAPO，通过整合外部指导来提升模型性能和探索能力。传统的强化学习方法往往只关注最大化奖励路径，缺乏外部知识的引入，限制了模型的探索能力。TAPO通过在训练过程中适应性地整合结构化思维，平衡了模型内部的探索与外部指导的利用。实验结果表明，TAPO在多个任务上显著优于现有方法，展示了其在广泛应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16479",
            "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
            "url": "https://huggingface.co/papers/2505.16479",
            "abstract": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. This paper first explores the challenging multi-weather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects. To support the research, we contribute the AllWeatherNight dataset, featuring large-scale high-quality nighttime images with diverse compositional degradations, synthesized using our introduced illumination-aware degradation generation. Moreover, we present ClearNight, a unified nighttime image restoration framework, which effectively removes complex degradations in one go. Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to focus on uneven illumination regions and intrinsic texture contents respectively, thereby enhancing restoration effectiveness in nighttime scenarios. In order to better represent the common and unique characters of multiple weather degradations, we introduce a weather-aware dynamic specific-commonality collaboration method, which identifies weather degradations and adaptively selects optimal candidate units associated with specific weather types. Our ClearNight achieves state-of-the-art performance on both synthetic and real-world images. Comprehensive ablation experiments validate the necessity of AllWeatherNight dataset as well as the effectiveness of ClearNight. Project page: https://henlyta.github.io/ClearNight/mainpage.html",
            "score": 9,
            "issue_id": 3950,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "e02bace9da76256b",
            "authors": [
                "Yuetong Liu",
                "Yunqiu Xu",
                "Yang Wei",
                "Xiuli Bi",
                "Bin Xiao"
            ],
            "affiliations": [
                "Chongqing University of Posts and Telecommunications",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16479.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🌙",
                "ru": {
                    "title": "Единый подход к восстановлению ночных изображений в сложных погодных условиях",
                    "desc": "Статья представляет новый подход к восстановлению ночных изображений в различных погодных условиях. Авторы разработали датасет AllWeatherNight с высококачественными ночными снимками, содержащими разнообразные искажения. Предложенная модель ClearNight использует двойные априорные данные на основе теории Ретинекса и адаптивное сотрудничество для эффективного удаления сложных искажений. Метод достигает наилучших результатов как на синтетических, так и на реальных изображениях."
                },
                "en": {
                    "title": "ClearNight: Mastering Nighttime Image Restoration Across Weather Conditions",
                    "desc": "This paper addresses the challenge of restoring nighttime images that are affected by various weather conditions and lighting effects. It introduces the AllWeatherNight dataset, which contains high-quality nighttime images with different types of weather degradations. The authors propose a framework called ClearNight that utilizes Retinex-based dual priors to enhance image restoration by focusing on illumination and texture. Additionally, a weather-aware collaboration method is introduced to adaptively handle different weather conditions, resulting in state-of-the-art performance in image restoration tasks."
                },
                "zh": {
                    "title": "统一框架，清晰夜景",
                    "desc": "本论文探讨了在多种恶劣天气条件下恢复夜间图像的挑战性任务。我们提出了AllWeatherNight数据集，包含多样化的夜间图像，帮助研究不同天气退化的影响。ClearNight是我们提出的统一夜间图像恢复框架，能够有效去除复杂的退化现象。通过引入天气感知的动态特定-共性协作方法，ClearNight在合成和真实图像上均实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17558",
            "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
            "url": "https://huggingface.co/papers/2505.17558",
            "abstract": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.",
            "score": 8,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "9faa21418742a88c",
            "authors": [
                "Shrey Pandit",
                "Ashwin Vinod",
                "Liu Leqi",
                "Ying Ding"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17558.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Обучение LLM распознавать галлюцинации с помощью самих галлюцинаций",
                    "desc": "Статья описывает новый метод улучшения способности больших языковых моделей (LLM) обнаруживать галлюцинации. Авторы используют тщательно сконструированные галлюцинации в качестве негативных примеров в процедуре DPO-выравнивания. Метод включает стратегию обучения с учебным планом, постепенно переходя от легких образцов к более сложным. Эксперименты показывают, что модели HaluCheck, обученные этим методом, значительно превосходят существующие модели в обнаружении галлюцинаций."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection in LLMs through Curriculum Learning",
                    "desc": "This paper presents a novel approach to improve large language models' (LLMs) ability to detect hallucinations by using specially designed negative examples in a curriculum learning framework. The authors recognize that hallucinated texts are often more deceptive than standard negative samples, and they leverage this insight in the DPO alignment procedure. By gradually increasing the difficulty of training samples, the method ensures that LLMs learn to identify hallucinations more effectively over time. Experimental results show that the proposed HaluCheck models achieve significant performance gains, particularly on challenging benchmarks, and demonstrate strong performance even in zero-shot scenarios."
                },
                "zh": {
                    "title": "利用课程学习提升幻觉检测能力",
                    "desc": "本文提出了一种在DPO对齐过程中使用精心设计的幻觉样本的课程学习方法，以提高大型语言模型（LLMs）对幻觉的检测能力。我们认识到，幻觉样本通常比传统的负样本具有更高的欺骗性，因此将这些幻觉样本作为负例使用。通过逐步引入更难的样本，我们的课程学习策略确保了稳定的增量学习。实验结果表明，使用课程DPO方法和高质量负样本训练的HaluCheck模型在多个指标上显著提高了模型性能，尤其在MedHallu和HaluEval等困难基准上提升了多达24%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16483",
            "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16483",
            "abstract": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
            "score": 7,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "3bd78fedbb109d9a",
            "authors": [
                "Shuzheng Si",
                "Haozhe Zhao",
                "Cheng Gao",
                "Yuzhuo Bai",
                "Zhitong Wang",
                "Bofei Gao",
                "Kangyang Luo",
                "Wenhao Li",
                "Yufei Huang",
                "Gang Chen",
                "Fanchao Qi",
                "Minjia Zhang",
                "Baobao Chang",
                "Maosong Sun"
            ],
            "affiliations": [
                "DeepLang AI",
                "Peking University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16483.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🛶",
                "ru": {
                    "title": "Достоверность без разметки: CANOE улучшает генерацию текста языковыми моделями",
                    "desc": "CANOE - это новый подход к улучшению достоверности генерации текста языковыми моделями без использования размеченных данных. Метод основан на синтетических вопросно-ответных парах и обучении с подкреплением по алгоритму Dual-GRPO. CANOE применяет три специальных правила для вознаграждения на основе синтетических данных, оптимизируя одновременно генерацию коротких и длинных ответов. Эксперименты показали, что CANOE значительно повышает достоверность языковых моделей на 11 различных задачах, превосходя даже самые передовые модели вроде GPT-4."
                },
                "en": {
                    "title": "Enhancing LLM Faithfulness with CANOE and Synthetic Data",
                    "desc": "The paper presents CANOE, a framework designed to enhance the faithfulness of large language models (LLMs) in generating text. It achieves this by creating synthetic question-answering (QA) data, which serves as high-quality training material without requiring human annotations. The framework employs a novel reinforcement learning approach called Dual-GRPO, which uses rule-based rewards to optimize both short-form and long-form text generation. Experimental results demonstrate that CANOE significantly improves LLM performance across various tasks, surpassing even state-of-the-art models like GPT-4o."
                },
                "zh": {
                    "title": "CANOE：提升大型语言模型的可信度",
                    "desc": "CANOE是一个系统框架，旨在提高大型语言模型（LLMs）在生成任务中的可信度，而无需人工标注。该方法通过合成短形式问答（QA）数据，构建高质量的训练数据，并使用双重GRPO强化学习方法来优化生成过程。双重GRPO结合了基于规则的奖励机制，确保在短形式和长形式生成任务中都能有效提升模型的表现。实验结果表明，CANOE在11个不同的下游任务中显著提高了LLMs的可信度，甚至超越了最先进的模型，如GPT-4o和OpenAI o1。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15389",
            "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
            "url": "https://huggingface.co/papers/2505.15389",
            "abstract": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.",
            "score": 6,
            "issue_id": 3945,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 мая",
                "en": "May 21",
                "zh": "5月21日"
            },
            "hash": "102a2cdaf1ccf7d5",
            "authors": [
                "DongGeon Lee",
                "Joonwon Jang",
                "Jihae Jeong",
                "Hwanjo Yu"
            ],
            "affiliations": [
                "LG AI Research",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15389.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#ethics",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Мемы vs ИИ: неожиданная угроза безопасности визуально-языковых моделей",
                    "desc": "Исследование показывает, что визуально-языковые модели (VLM) более уязвимы к вредоносным мемам, чем к синтетическим изображениям. Авторы представили MemeSafetyBench - набор данных из 50 430 мемов с вредными и безобидными инструкциями для оценки безопасности VLM. Многоэтапные взаимодействия частично снижают риски, но значительная уязвимость сохраняется. Результаты подчеркивают необходимость экологически валидных оценок и усиления механизмов безопасности для VLM."
                },
                "en": {
                    "title": "Meme Vulnerability: A Call for Safer VLMs",
                    "desc": "This paper investigates the safety of vision-language models (VLMs) when exposed to real-world meme images, which are often shared by users. The authors introduce a benchmark called MemeSafetyBench, consisting of over 50,000 instances of meme images paired with harmful and benign instructions. The study finds that VLMs are more susceptible to harmful prompts from memes compared to synthetic images, and while multi-turn interactions can offer some protection, vulnerabilities remain significant. The results emphasize the importance of realistic evaluations and the need for improved safety measures in VLMs."
                },
                "zh": {
                    "title": "恶搞图像对视觉语言模型的安全威胁",
                    "desc": "本研究探讨了视觉语言模型（VLMs）在面对用户分享的恶搞图像时的安全性。我们引入了MemeSafetyBench，这是一个包含50,430个实例的基准，结合了真实的恶搞图像和有害与无害的指令。研究发现，VLMs对恶搞图像的有害提示比对合成图像更脆弱，且多轮对话虽然提供了一定的保护，但仍然存在显著的脆弱性。我们的结果强调了需要进行生态有效的评估和更强的安全机制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17826",
            "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2505.17826",
            "abstract": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "6f6fdf1b20859c44",
            "authors": [
                "Xuchen Pan",
                "Yanxi Chen",
                "Yushuo Chen",
                "Yuchang Sun",
                "Daoyuan Chen",
                "Wenhao Zhang",
                "Yuexiang Xie",
                "Yilun Huang",
                "Yilei Zhang",
                "Dawei Gao",
                "Yaliang Li",
                "Bolin Ding",
                "Jingren Zhou"
            ],
            "affiliations": [
                "alibaba-inc.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17826.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#agi",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Trinity-RFT: универсальная платформа для дообучения языковых моделей",
                    "desc": "Trinity-RFT - это гибкая и масштабируемая платформа для дообучения больших языковых моделей с помощью обучения с подкреплением. Она поддерживает различные режимы взаимодействия и обработки данных, включая синхронные/асинхронные, on-policy/off-policy и онлайн/офлайн подходы. Архитектура Trinity-RFT состоит из универсального ядра RFT, модуля интеграции агента и окружения, а также оптимизированных конвейеров данных. Платформа легко адаптируется под различные сценарии применения и позволяет исследовать продвинутые парадигмы обучения с подкреплением."
                },
                "en": {
                    "title": "Empowering Language Models with Flexible Reinforcement Fine-Tuning",
                    "desc": "Trinity-RFT is a versatile framework designed for reinforcement fine-tuning (RFT) of large language models. It features a decoupled architecture that supports various modes of RFT, including synchronous and asynchronous, as well as on-policy and off-policy approaches. The framework ensures efficient and robust interactions between agents and environments, while also providing optimized data pipelines for RFT tasks. This makes Trinity-RFT adaptable to a wide range of applications, serving as a comprehensive platform for exploring advanced reinforcement learning techniques."
                },
                "zh": {
                    "title": "Trinity-RFT：灵活的强化微调框架",
                    "desc": "Trinity-RFT是一个灵活且可扩展的框架，专门用于大型语言模型的强化微调。它采用解耦设计，包含一个RFT核心，能够统一和概括同步/异步、在线/离线等多种强化微调模式。该框架支持高效且稳健的智能体与环境的交互，并优化了数据管道以适应强化微调的需求。Trinity-RFT易于适应不同的应用场景，是探索先进强化学习范式的统一平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17417",
            "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
            "url": "https://huggingface.co/papers/2505.17417",
            "abstract": "The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "9d72b20aca0789ee",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Huy Hoang Ha",
                "Tuan Le Duc Anh",
                "Shreyas Gopal",
                "Yue Heng Yeo",
                "Warren Keng Hoong Low",
                "Eng Siong Chng",
                "Jia Qi Yip"
            ],
            "affiliations": [
                "CCDS, Nanyang Technological University, Singapore",
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17417.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#synthetic",
                    "#training",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Голосовые ассистенты для редких языков: обучение без TTS",
                    "desc": "Статья описывает новый подход к обучению голосовых ассистентов для языков с ограниченными ресурсами. Авторы предлагают метод, позволяющий обойти необходимость в качественной системе text-to-speech, останавливая синтез на уровне семантического представления. Это достигается путем выравнивания синтетических семантических представлений с предобученным энкодером Whisper. Такой подход позволяет обучать языковую модель на текстовых инструкциях, сохраняя способность понимать устные команды при инференсе."
                },
                "en": {
                    "title": "Empowering Voice Assistants for Low-Resource Languages",
                    "desc": "This paper addresses the challenge of training voice assistants in low-resource languages, where there is a lack of speech instruction data. It proposes a novel method that generates synthetic speech by stopping at the semantic representation level, eliminating the need for a text-to-speech (TTS) model. By aligning these semantic representations with the pre-trained Whisper encoder, the approach allows for fine-tuning large language models (LLMs) on text instructions while still being able to process spoken commands. This method simplifies the training process and enhances the development of voice assistants for languages with limited resources."
                },
                "zh": {
                    "title": "为低资源语言构建语音助手的新方法",
                    "desc": "本论文探讨了为语音助手训练所需的语音指令数据的不足问题。尽管语音识别数据丰富，但语音指令数据却相对稀缺，这对模型理解和执行口头命令至关重要。我们提出了一种新方法，通过在语义表示层面停止合成，避免了对文本到语音（TTS）模型的依赖。该方法通过将合成的语义表示与预训练的Whisper编码器对齐，使得大型语言模型（LLM）能够在文本指令上进行微调，同时在推理过程中理解口头指令。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17508",
            "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.17508",
            "abstract": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.",
            "score": 4,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "6ae63edcb7127847",
            "authors": [
                "Yifan Zhang",
                "Yifeng Liu",
                "Huizhuo Yuan",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew C Yao"
            ],
            "affiliations": [
                "IIIS, Tsinghua University",
                "Shanghai Qi Zhi Institute",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17508.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Регуляризация градиента политики для улучшения рассуждений языковых моделей",
                    "desc": "Статья представляет новый фреймворк под названием RPG (regularized policy gradient) для улучшения способностей больших языковых моделей к рассуждению в онлайн-обучении с подкреплением. Авторы исследуют различные формулировки KL-дивергенции для регуляризации градиента политики. Эксперименты показывают, что предложенный метод повышает стабильность обучения и производительность по сравнению с сильными базовыми алгоритмами. Фреймворк RPG предоставляет систематический подход к анализу и разработке KL-регуляризованных методов градиента политики."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Regularized Policy Gradients",
                    "desc": "This paper introduces a regularized policy gradient framework that utilizes Kullback-Leibler (KL) divergence to improve the reasoning abilities of large language models (LLMs) in online reinforcement learning (RL). It systematically explores various KL divergence formulations to enhance training stability and performance through surrogate loss functions. The authors derive policy gradients for both forward and reverse KL divergences, accommodating different types of policy distributions. Extensive experiments demonstrate that their proposed methods achieve better or comparable results against established baselines in RL tasks involving LLMs."
                },
                "zh": {
                    "title": "正则化策略梯度：提升LLM推理能力的关键",
                    "desc": "本文提出了一种正则化策略梯度框架，用于探索KL散度的不同形式，以增强大型语言模型（LLMs）在在线强化学习中的推理能力。我们系统地分析了如何将不同的KL散度估计整合到替代损失函数中，从而提高训练的稳定性和性能。通过对正向和反向KL散度的正则化目标，我们推导了相应的策略梯度和损失函数，并考虑了标准化和非标准化的策略分布。实验结果表明，与现有的强基线算法相比，我们的方法在训练稳定性和性能上都有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17412",
            "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
            "url": "https://huggingface.co/papers/2505.17412",
            "abstract": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high resolution 3D shapes using volumetric representations such as Signed Distance Functions presents substantial computational and memory challenges. We introduce Direct3D S2, a scalable 3D generation framework based on sparse volumes that achieves superior output quality with dramatically reduced training costs. Our key innovation is the Spatial Sparse Attention mechanism, which greatly enhances the efficiency of Diffusion Transformer computations on sparse volumetric data. SSA allows the model to effectively process large token sets within sparse volumes, significantly reducing computational overhead and achieving a 3.9x speedup in the forward pass and a 9.6x speedup in the backward pass. Our framework also includes a variational autoencoder that maintains a consistent sparse volumetric format across input, latent, and output stages. Compared to previous methods with heterogeneous representations in 3D VAE, this unified design significantly improves training efficiency and stability. Our model is trained on public available datasets, and experiments demonstrate that Direct3D S2 not only surpasses state-of-the-art methods in generation quality and efficiency, but also enables training at 1024 resolution using only 8 GPUs, a task typically requiring at least 32 GPUs for volumetric representations at 256 resolution, thus making gigascale 3D generation both practical and accessible. Project page: https://nju3dv.github.io/projects/Direct3D-S2/.",
            "score": 3,
            "issue_id": 3948,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "e0f74abb880208f9",
            "authors": [
                "Shuang Wu",
                "Youtian Lin",
                "Feihu Zhang",
                "Yifei Zeng",
                "Yikang Yang",
                "Yajie Bao",
                "Jiachen Qian",
                "Siyu Zhu",
                "Philip Torr",
                "Xun Cao",
                "Yao Yao"
            ],
            "affiliations": [
                "DreamTech",
                "Fudan University",
                "Nanjing University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17412.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#training",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Революция в генерации 3D-объектов: эффективность и качество на новом уровне",
                    "desc": "Статья представляет Direct3D S2 - масштабируемую систему для генерации 3D-объектов, использующую разреженные объемы и пространственное разреженное внимание. Ключевой инновацией является механизм Spatial Sparse Attention, который значительно повышает эффективность вычислений диффузионного трансформера на разреженных объемных данных. Система включает вариационный автоэнкодер, поддерживающий единый разреженный объемный формат на всех этапах. Эксперименты показывают, что Direct3D S2 превосходит современные методы по качеству и эффективности генерации, позволяя обучать модели с разрешением 1024 на всего 8 GPU."
                },
                "en": {
                    "title": "Efficient High-Resolution 3D Shape Generation with Sparse Volumes",
                    "desc": "This paper presents Direct3D S2, a framework for generating high-resolution 3D shapes using sparse volumetric representations. It introduces a Spatial Sparse Attention mechanism that enhances the efficiency of computations in Diffusion Transformers, allowing for significant reductions in training time and resource usage. The framework employs a variational autoencoder to maintain a consistent format across different stages of processing, improving training stability. Overall, Direct3D S2 achieves superior generation quality while drastically lowering the computational requirements, making high-resolution 3D shape generation more accessible."
                },
                "zh": {
                    "title": "高效生成高分辨率3D形状的创新框架",
                    "desc": "本文介绍了一种可扩展的3D形状生成框架，名为Direct3D S2，利用稀疏体积和空间稀疏注意力机制，能够以较低的计算需求生成高分辨率的3D形状。该框架通过稀疏体积的设计，显著提高了生成质量并降低了训练成本。空间稀疏注意力机制提升了扩散变换器在稀疏体积数据上的计算效率，实现了前向传播速度提高3.9倍和反向传播速度提高9.6倍。与传统的3D变分自编码器相比，Direct3D S2在训练效率和稳定性上有了显著改善，使得在仅使用8个GPU的情况下实现1024分辨率的训练成为可能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16270",
            "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
            "url": "https://huggingface.co/papers/2505.16270",
            "abstract": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.",
            "score": 3,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "242b4420fd3d9c4f",
            "authors": [
                "Jiaru Zou",
                "Yikun Ban",
                "Zihao Li",
                "Yunzhe Qi",
                "Ruizhong Qiu",
                "Ling Yang",
                "Jingrui He"
            ],
            "affiliations": [
                "Princeton University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16270.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Transformer Copilot: Учимся на ошибках для повышения эффективности ИИ",
                    "desc": "Представлена новая архитектура Transformer Copilot, которая улучшает работу больших языковых моделей. Основная идея заключается в использовании модели-второго пилота (Copilot), которая корректирует логиты основной модели (Pilot) на основе журнала ошибок (Mistake Log). Эта система позволяет модели учиться на своих прошлых ошибках, аналогично тому, как учатся люди. Эксперименты на 12 тестовых наборах показали значительное улучшение производительности до 34.5% при минимальных вычислительных затратах."
                },
                "en": {
                    "title": "Enhancing Language Models with Reflective Learning",
                    "desc": "The Transformer Copilot framework improves the performance of large language models by using a Copilot model that refines the Pilot's outputs based on a Mistake Log. This Mistake Log tracks the model's errors during fine-tuning, allowing the Copilot to learn from these mistakes, similar to how humans learn. The framework includes a novel design for the Copilot, a joint training approach where both models learn together, and a fused inference method that enhances the Pilot's predictions. Experiments show that this approach can boost performance by up to 34.5% across various tasks with minimal additional computational cost."
                },
                "zh": {
                    "title": "提升语言模型性能的副驾驶框架",
                    "desc": "Transformer Copilot框架通过一个副驾驶模型来提升大型语言模型的性能。这个副驾驶模型根据错误日志来优化主模型的输出，从而在多个基准测试中实现了持续的性能提升。我们引入了错误日志的概念，以系统地跟踪模型的学习行为和重复错误。通过这种方式，副驾驶模型能够在训练过程中不断学习，从而提高生成的准确性和质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17091",
            "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
            "url": "https://huggingface.co/papers/2505.17091",
            "abstract": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",
            "score": 3,
            "issue_id": 3945,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "7bb07d5dfb6680e6",
            "authors": [
                "Prateek Verma",
                "Mert Pilanci"
            ],
            "affiliations": [
                "Department of Electrical Engineering Stanford University Stanford, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17091.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#audio"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Языковые модели обретают зрение и слух через чтение",
                    "desc": "Исследование показывает, что авторегрессионные языковые модели, обученные на текстовых данных, способны развивать внутренние механизмы для понимания изображений и аудио. Это позволяет им выполнять задачи классификации в разных модальностях без дополнительной настройки. Авторы демонстрируют, что текстовые веса модели могут использоваться для классификации аудио и изображений на различных датасетах. Данное открытие расширяет представление о мощных внутренних схемах, формируемых языковыми моделями, и их потенциальном применении в различных задачах без необходимости обучения с нуля."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Understanding with Text LLMs",
                    "desc": "This paper explores how auto-regressive language models (LLMs) trained solely on text can develop the ability to understand and classify images and audio without needing additional fine-tuning. The authors demonstrate that these text-based models can process inputs like image patches and audio waveforms, producing embeddings or category labels similar to those used in traditional classification tasks. They validate their findings by applying the model to audio classification tasks on datasets like FSD-50K and GTZAN, as well as image classification on CIFAR-10 and Fashion-MNIST. This research highlights the potential of leveraging text LLMs' internal capabilities for multi-modal applications, reducing the need for training separate models for each modality."
                },
                "zh": {
                    "title": "文本模型的跨模态理解能力",
                    "desc": "这篇论文展示了一个有趣的发现：通过对文本进行训练的自回归语言模型，能够内在地发展出理解图像和音频的能力。这样，模型在没有微调的情况下，就能进行跨模态的分类任务。我们的方法通过输入图像块、音频波形或标记，生成典型的分类管道所需的嵌入或类别标签。研究表明，文本模型的权重在音频和图像分类任务中具有广泛的适用性，推动了文本语言模型学习强大内部电路的概念。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17063",
            "title": "Synthetic Data RL: Task Definition Is All You Need",
            "url": "https://huggingface.co/papers/2505.17063",
            "abstract": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
            "score": 3,
            "issue_id": 3947,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "a46b0456dc458ebe",
            "authors": [
                "Yiduo Guo",
                "Zhen Guo",
                "Chuanwei Huang",
                "Zi-Ang Wang",
                "Zekai Zhang",
                "Haofei Yu",
                "Huishuai Zhang",
                "Yikang Shen"
            ],
            "affiliations": [
                "MIT",
                "MIT-IBM",
                "Peking University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17063.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Усиление языковых моделей без человеческих данных",
                    "desc": "Статья представляет новый метод под названием Synthetic Data RL, который улучшает языковые модели с помощью обучения с подкреплением, используя только синтетические данные. Этот подход генерирует пары вопросов и ответов на основе определения задачи и извлеченных документов, адаптирует сложность вопросов и выбирает их для обучения. Метод показывает значительные улучшения производительности на различных задачах по сравнению с базовыми моделями и другими методами. Synthetic Data RL позволяет масштабировать адаптацию моделей без необходимости в больших объемах размеченных человеком данных."
                },
                "en": {
                    "title": "Reinforcement Learning with Synthetic Data: A Game Changer for Model Training!",
                    "desc": "Synthetic Data RL is a novel approach that enhances foundation models using reinforcement learning (RL) without the need for extensive human-labeled data. It generates synthetic question and answer pairs based on task definitions, allowing for effective model fine-tuning. The method adapts question difficulty according to the model's performance, optimizing the training process. Results show significant performance improvements on various benchmarks, demonstrating that this approach can achieve results comparable to traditional RL methods that rely on human data."
                },
                "zh": {
                    "title": "合成数据强化学习：高效提升模型性能的创新方法",
                    "desc": "本文介绍了一种名为合成数据强化学习（Synthetic Data RL）的方法，该方法通过使用合成数据来增强基础模型的性能。该方法生成与任务定义相关的问题和答案对，并根据模型的解决能力调整问题的难度。通过这种方式，Synthetic Data RL在多个数据集上实现了显著的性能提升，甚至接近于使用全人类标注数据的强化学习模型。此方法减少了对人类数据标注的依赖，使得模型适应变得更加高效和可扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17618",
            "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
            "url": "https://huggingface.co/papers/2505.17618",
            "abstract": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.",
            "score": 2,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "dfe3bcf9e6ec80f9",
            "authors": [
                "Haoran He",
                "Jiajun Liang",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Ling Pan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17618.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Эволюционный поиск для улучшения генеративных моделей",
                    "desc": "EvoSearch - это новый метод тест-тайм скейлинга для генеративных моделей на основе диффузии и потоков. Он использует принципы эволюционного поиска для улучшения качества и разнообразия генерируемых изображений и видео. EvoSearch не требует дополнительного обучения или расширения модели, а работает на этапе вывода. Метод превосходит существующие подходы и демонстрирует хорошую обобщаемость на различных метриках оценки."
                },
                "en": {
                    "title": "EvoSearch: Evolving Image and Video Generation at Test Time",
                    "desc": "EvoSearch is an innovative evolutionary search method designed to improve test-time scaling (TTS) for generative models, specifically diffusion and flow-based models. It addresses the limitations of existing TTS strategies by reformulating the scaling process as an evolutionary problem, allowing for efficient exploration of denoising trajectories. By utilizing selection and mutation mechanisms inspired by biological evolution, EvoSearch enhances the quality and diversity of generated images and videos without the need for additional training. Our extensive evaluations show that EvoSearch consistently outperforms current methods, demonstrating superior generalizability and diversity in generative tasks."
                },
                "zh": {
                    "title": "进化搜索：提升生成模型的测试时扩展能力",
                    "desc": "EvoSearch是一种进化搜索方法，旨在提高扩散和流式生成模型在测试时的扩展能力，从而改善图像和视频生成的质量、多样性和泛化能力。随着模型预训练期间计算成本的显著增加，测试时扩展（TTS）成为提升生成模型性能的一个有前景的方向。尽管现有方法在语言任务上取得了成功，但在图像和视频生成模型的测试时扩展行为上仍存在显著差距。EvoSearch通过将测试时扩展重新构造成一个进化搜索问题，利用生物进化的原理高效探索和优化去噪轨迹，最终实现了更高质量的生成结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17016",
            "title": "Interactive Post-Training for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2505.17016",
            "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.",
            "score": 1,
            "issue_id": 3948,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "ab53333533d30d3f",
            "authors": [
                "Shuhan Tan",
                "Kairan Dou",
                "Yue Zhao",
                "Philipp Krähenbühl"
            ],
            "affiliations": [
                "Nankai University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17016.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#multimodal",
                    "#rl",
                    "#transfer_learning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Эффективное пост-обучение VLA моделей с минимальным надзором",
                    "desc": "RIPT-VLA - это новая парадигма пост-обучения моделей зрения-языка-действия (VLA) с использованием обучения с подкреплением. Она позволяет адаптировать предобученные модели к новым задачам, используя только бинарные сигналы успеха. RIPT-VLA применима к различным VLA моделям и показывает значительное улучшение их производительности. Метод эффективен с точки зрения вычислений и данных, позволяя достичь высокой точности за небольшое количество итераций."
                },
                "en": {
                    "title": "Reinforcement Learning for Efficient Vision-Language-Action Adaptation",
                    "desc": "RIPT-VLA is a new method that enhances Vision-Language-Action (VLA) models using reinforcement learning with minimal supervision. It allows these models to learn from sparse binary rewards instead of relying on extensive expert demonstrations. This approach improves the performance of various VLA models significantly, achieving a 97.5% success rate with the OpenVLA-OFT model. Additionally, RIPT-VLA is efficient in both computation and data usage, enabling models to adapt quickly to new tasks with just one demonstration."
                },
                "zh": {
                    "title": "RIPT-VLA：高效的视觉-语言-动作模型后训练范式",
                    "desc": "我们介绍了RIPT-VLA，这是一种简单且可扩展的基于强化学习的交互式后训练范式，旨在使用稀疏的二元成功奖励来微调预训练的视觉-语言-动作（VLA）模型。现有的VLA训练流程过于依赖离线专家示范数据和监督模仿，限制了它们在低数据环境下适应新任务和新环境的能力。RIPT-VLA通过动态回放采样和留一优势估计的稳定策略优化算法，实现了交互式后训练。实验结果表明，RIPT-VLA在不同任务和场景中具有良好的泛化能力，并且对初始状态上下文具有鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12891",
            "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2505.12891",
            "abstract": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .",
            "score": 1,
            "issue_id": 3949,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "3f60161242cb3f4c",
            "authors": [
                "Shaohang Wei",
                "Wei Li",
                "Feifan Song",
                "Wen Luo",
                "Tianyi Zhuang",
                "Haochen Tan",
                "Zhijiang Guo",
                "Houfeng Wang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12891.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#survey",
                    "#reasoning",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "TIME: Комплексная оценка темпорального рассуждения в LLM",
                    "desc": "Статья представляет новый бенчмарк TIME для оценки темпорального рассуждения в больших языковых моделях (LLM). Бенчмарк охватывает реальные сценарии с интенсивной временной информацией, быстро меняющейся динамикой событий и сложными социальными взаимодействиями. TIME включает 38,522 пары вопросов-ответов, охватывающих 3 уровня и 11 подзадач. Исследователи провели эксперименты на моделях с рассуждением и без, а также проанализировали влияние масштабирования во время тестирования на способности к темпоральному рассуждению."
                },
                "en": {
                    "title": "TIME: Benchmarking Temporal Reasoning in LLMs",
                    "desc": "The paper introduces a benchmark called TIME, which evaluates temporal reasoning capabilities in Large Language Models (LLMs) across real-world scenarios. It addresses three main challenges: handling intensive temporal information, adapting to fast-changing events, and understanding complex social interactions. TIME includes 38,522 question-answer pairs organized into three levels and 11 sub-tasks, with datasets like TIME-Wiki, TIME-News, and TIME-Dial. The authors also provide a human-annotated subset, TIME-Lite, to support future research and standardized evaluation in this area."
                },
                "zh": {
                    "title": "TIME基准：提升时间推理能力的关键",
                    "desc": "本论文提出了一个名为TIME的基准，用于评估大型语言模型（LLMs）在现实世界中进行时间推理的能力。TIME基准涵盖了38,522个问答对，分为三个层次和11个细分任务，旨在应对密集的时间信息、快速变化的事件动态和复杂的社会互动中的时间依赖性。我们对推理模型和非推理模型进行了广泛的实验，并分析了不同现实场景和任务中的时间推理表现。为了促进未来的研究和标准化评估，我们还发布了TIME-Lite，一个经过人工标注的子集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17540",
            "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17540",
            "abstract": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.",
            "score": 0,
            "issue_id": 3950,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "0b4459388e7a7ca1",
            "authors": [
                "Mingrui Wu",
                "Lu Wang",
                "Pu Zhao",
                "Fangkai Yang",
                "Jianjin Zhang",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Weihao Han",
                "Hao Sun",
                "Jiayi Ji",
                "Xiaoshuai Sun",
                "Qingwei Lin",
                "Weiwei Deng",
                "Dongmei Zhang",
                "Feng Sun",
                "Qi Zhang",
                "Rongrong Ji"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17540.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#rag",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Умное улучшение промптов для более точной генерации изображений",
                    "desc": "RePrompt - это новая система для улучшения генерации изображений по текстовому описанию, использующая обучение с подкреплением. Она оптимизирует промпты, анализируя результирующие изображения с помощью специальных моделей оценки. RePrompt значительно улучшает пространственную компоновку и композиционную генерализацию по сравнению с существующими методами. Система не требует размеченных данных и может работать с различными моделями генерации изображений."
                },
                "en": {
                    "title": "RePrompt: Enhancing Text-to-Image Generation with Reinforcement Learning",
                    "desc": "RePrompt is a new framework that uses reinforcement learning to improve text-to-image generation by focusing on the quality of the generated images. It addresses the common issue where existing models fail to accurately interpret short prompts, often leading to unrealistic outputs. By incorporating explicit reasoning into the prompt enhancement process, RePrompt generates structured prompts that are better aligned with user intentions. The framework trains a language model to optimize prompts based on image-level outcomes, achieving significant improvements in spatial layout and compositional generalization without needing human-annotated data."
                },
                "zh": {
                    "title": "RePrompt：优化文本到图像生成的新方法",
                    "desc": "RePrompt是一个使用强化学习的重新提示框架，旨在提升文本到图像生成的效果。它通过优化图像级结果，显著改善了空间布局和组合泛化能力。与传统方法不同，RePrompt不依赖手工规则，而是训练语言模型生成结构化的自反提示。实验结果表明，RePrompt在多个文本到图像生成基准上取得了新的最先进成果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16293",
            "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
            "url": "https://huggingface.co/papers/2505.16293",
            "abstract": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.  \t\t\t\t\tAI-generated summary \t\t\t\t Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant information. This hinders a model's capacity to process and reason over retrieved content and limits performance. While recent methods focus on compressing retrieved information, they are either restricted to single-round RAG, require finetuning or lack scalability in iterative RAG. To address these challenges, we propose Notes Writing, a method that generates concise and relevant notes from retrieved documents at each step, thereby reducing noise and retaining only essential information. This indirectly increases the effective context length of Large Language Models (LLMs), enabling them to reason and plan more effectively while processing larger volumes of input text. Notes Writing is framework agnostic and can be integrated with different iterative RAG methods. We demonstrate its effectiveness with three iterative RAG methods, across two models and four evaluation datasets. Notes writing yields an average improvement of 15.6 percentage points overall, with minimal increase in output tokens.",
            "score": 0,
            "issue_id": 3949,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "c597f26ca16d4748",
            "authors": [
                "Rishabh Maheshwary",
                "Masoud Hashemi",
                "Khyati Mahajan",
                "Shiva Krishna Reddy Malay",
                "Sai Rajeswar",
                "Sathwik Tejaswi Madhusudhan",
                "Spandana Gella",
                "Vikas Yadav"
            ],
            "affiliations": [
                "ServiceNow",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16293.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#rag",
                    "#long_context"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "Усиление итеративного RAG через генерацию кратких заметок",
                    "desc": "Статья предлагает метод Notes Writing для улучшения итеративного RAG в задачах ответов на многоэтапные вопросы. Этот подход генерирует краткие заметки из извлеченных документов на каждом шаге, уменьшая шум и сохраняя только важную информацию. Notes Writing косвенно увеличивает эффективную длину контекста для больших языковых моделей, позволяя им лучше рассуждать и планировать. Метод показал среднее улучшение на 15.6 процентных пунктов при минимальном увеличении выходных токенов."
                },
                "en": {
                    "title": "Enhancing Iterative RAG with Concise Notes for Better Reasoning",
                    "desc": "The paper introduces a method called Notes Writing, which enhances iterative Retrieval-Augmented Generation (RAG) by creating concise notes from retrieved documents at each step. This approach helps to reduce irrelevant information and noise, allowing models to focus on essential content, which improves reasoning and performance. Unlike previous methods that struggle with lengthy contexts or require fine-tuning, Notes Writing is scalable and can be applied across various RAG frameworks. The results show an average performance improvement of 15.6 percentage points with minimal increase in output tokens, demonstrating its effectiveness in multi-hop question answering tasks."
                },
                "zh": {
                    "title": "笔记写作：提升迭代RAG的有效性",
                    "desc": "本文提出了一种名为“笔记写作”的方法，旨在改善多跳问答中的迭代检索增强生成（RAG）过程。该方法通过在每个步骤生成简洁的笔记，减少了冗余信息的干扰，从而提高了模型的推理能力和性能。与现有方法相比，笔记写作不需要微调，且具有更好的可扩展性，适用于不同的迭代RAG方法。实验结果表明，笔记写作在多个模型和评估数据集上平均提高了15.6个百分点，同时输出令牌的增加非常有限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11881",
            "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
            "url": "https://huggingface.co/papers/2505.11881",
            "abstract": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.  \t\t\t\t\tAI-generated summary \t\t\t\t Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy gain for ViT-B on ImageNet-1k.",
            "score": 0,
            "issue_id": 3948,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "b5265bcccaafd719",
            "authors": [
                "Giyeong Oh",
                "Woohyun Cho",
                "Siyeol Kim",
                "Suhwan Choi",
                "Younjae Yu"
            ],
            "affiliations": [
                "Maum.AI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11881.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Ортогональность для инноваций: новый подход к остаточным связям",
                    "desc": "Статья представляет новый метод под названием 'Ортогональное Остаточное Обновление' для улучшения обучения глубоких нейронных сетей. Этот подход разлагает выход модуля относительно входного потока и добавляет только компонент, ортогональный этому потоку. Метод направлен на стимулирование модулей вносить преимущественно новые репрезентативные направления, способствуя более богатому обучению признаков. Эксперименты показывают улучшение точности обобщения и стабильности обучения для различных архитектур и наборов данных."
                },
                "en": {
                    "title": "Unlocking New Features with Orthogonal Residual Updates",
                    "desc": "This paper introduces Orthogonal Residual Updates, a novel approach to enhance feature learning in deep neural networks. By decomposing the output of a module relative to the input stream, it ensures that only the orthogonal component is added, allowing the model to learn new features rather than just modifying existing ones. This method addresses the limitations of traditional residual connections, which can lead to underutilization of the model's capacity. The authors demonstrate that this technique improves generalization accuracy and training stability across various architectures and datasets, achieving significant performance gains."
                },
                "zh": {
                    "title": "正交残差更新：提升特征学习与训练稳定性",
                    "desc": "这篇论文提出了一种新的更新方法，称为正交残差更新，旨在增强特征学习和训练稳定性。传统的残差连接直接将模块输出添加到输入流中，这可能导致学习新特征的能力被低估。正交残差更新通过将模块输出相对于输入流进行分解，只添加与输入流正交的部分，从而引导模块主要贡献新的表示方向。实验表明，这种方法在多种架构和数据集上提高了泛化准确性和训练稳定性。"
                }
            }
        }
    ],
    "link_prev": "2025-05-23.html",
    "link_next": "2025-05-27.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "23.05",
        "en": "05/23",
        "zh": "5月23日"
    },
    "short_date_next": {
        "ru": "27.05",
        "en": "05/27",
        "zh": "5月27日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 1,
        "#cv": 4,
        "#rl": 9,
        "#rlhf": 7,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 3,
        "#video": 2,
        "#multimodal": 6,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 19,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 11,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 18,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了人工智能（AI）如何加速科学研究范式的转变，提高研究效率并推动创新。作者提出了NovelSeek，一个统一的闭环多智能体框架，用于在各种科学研究领域中进行自主科学研究（ASR）。NovelSeek具有三个关键优势：可扩展性、互动性和高效性。例如，在反应收率预测中，它在12小时内提高了7.8%；在增强活性预测中，准确性在4小时内提高了0.27；在2D语义分割中，精度在30小时内提高了2.2%。",
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
        "pinyin": "Zhè piān wénzhāng jièshào le réngōng zhìnéng (AI) rúhé jiāsù kēxué yánjiū fànshì de zhuǎnbiàn, tígāo yánjiū xiàolǜ bìng tuīdòng chuàngxīn. Zuòzhě tíchū le NovelSeek, yīgè tǒngyī de bìhuán duō zhìnéngtǐ kuàngjià, yòngyú zài gèzhǒng kēxué yánjiū lǐngyù zhōng jìnxíng zìzhǔ kēxué yánjiū (ASR). NovelSeek jùyǒu sān gè guǎnjiàn yōushì: kě kuòzhǎn xìng, hùdòng xìng hé gāoxiào xìng. Lìrú, zài fǎnyìng shōulǜ yùcè zhōng, tā zài 12 xiǎoshí nèi tígāo le 7.8%; zài zēngqiáng huóxìng yùcè zhōng, zhǔnquè xìng zài 4 xiǎoshí nèi tígāo le 0.27; zài 2D yǔyán fēn'gé zhōng, jīngdù zài 30 xiǎoshí nèi tígāo le 2.2%.",
        "vocab": "[\n    {\"word\": \"人工智能\", \"pinyin\": \"réngōng zhìnéng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"范式\", \"pinyin\": \"fànshì\", \"trans\": \"paradigm\"},\n    {\"word\": \"转变\", \"pinyin\": \"zhuǎnbiàn\", \"trans\": \"transformation\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuīdòng\", \"trans\": \"promote\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovation\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒngyī\", \"trans\": \"unified\"},\n    {\"word\": \"闭环\", \"pinyin\": \"bìhuán\", \"trans\": \"closed-loop\"},\n    {\"word\": \"多智能体\", \"pinyin\": \"duō zhìnéngtǐ\", \"trans\": \"multi-agent\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomous\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐngyù\", \"trans\": \"field\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōushì\", \"trans\": \"advantage\"},\n    {\"word\": \"可扩展性\", \"pinyin\": \"kě kuòzhān xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"互动性\", \"pinyin\": \"hùdòng xìng\", \"trans\": \"interactivity\"},\n    {\"word\": \"高效性\", \"pinyin\": \"gāoxiào xìng\", \"trans\": \"efficiency\"},\n    {\"word\": \"反应\", \"pinyin\": \"fǎnyìng\", \"trans\": \"reaction\"},\n    {\"word\": \"收率\", \"pinyin\": \"shōulǜ\", \"trans\": \"yield\"},\n    {\"word\": \"预测\", \"pinyin\": \"yùcè\", \"trans\": \"prediction\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēngqiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"活性\", \"pinyin\": \"huóxìng\", \"trans\": \"activity\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔnquè xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"2D\", \"pinyin\": \"èr wéi\", \"trans\": \"2D\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"分割\", \"pinyin\": \"fēngē\", \"trans\": \"segmentation\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"precision\"}\n]",
        "trans": "This article discusses how artificial intelligence (AI) is accelerating the transformation of scientific research paradigms, enhancing research efficiency, and driving innovation. The authors introduce NovelSeek, a unified closed-loop multi-agent framework for autonomous scientific research (ASR) across various scientific domains. NovelSeek offers three key advantages: scalability, interactivity, and efficiency. For instance, in reaction yield prediction, it improved accuracy by 7.8% within 12 hours; in enhanced activity prediction, accuracy increased by 0.27 within 4 hours; and in 2D semantic segmentation, precision improved by 2.2% within 30 hours.",
        "update_ts": "2025-05-25 12:45"
    }
}