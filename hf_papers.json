{
    "date": {
        "ru": "12 ноября",
        "en": "November 12",
        "zh": "11月12日"
    },
    "time_utc": "2024-11-12 08:16",
    "weekday": 1,
    "issue_id": 523,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07140",
            "title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models",
            "url": "https://huggingface.co/papers/2411.07140",
            "abstract": "New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.",
            "score": 17,
            "issue_id": 522,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "ffca97b13123516b",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "🇨🇳",
                "ru": {
                    "title": "Новый китайский бенчмарк для оценки фактологических способностей языковых моделей",
                    "desc": "Статья представляет новый бенчмарк Chinese SimpleQA для оценки фактологических способностей языковых моделей на китайском языке. Этот бенчмарк охватывает 6 основных тем и 99 подтем, с коротким форматом вопросов и ответов. Авторы провели тщательный контроль качества для обеспечения высокого уровня вопросов и ответов. Chinese SimpleQA позволяет легко оценивать модели с помощью API OpenAI и призван помочь разработчикам лучше понять возможности своих моделей в работе с китайским языком."
                },
                "en": {
                    "title": "Empowering Chinese LLMs with SimpleQA Factuality Benchmark",
                    "desc": "This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context."
                },
                "zh": {
                    "title": "中文SimpleQA：提升语言模型事实能力的基准",
                    "desc": "本文介绍了中文SimpleQA，这是第一个全面评估语言模型回答短问题的事实能力的基准。该基准专注于中文，涵盖六个主要主题和99个多样化的子主题。我们通过严格的质量控制过程，确保问题和答案的高质量，并且参考答案是静态的，不会随时间变化。希望中文SimpleQA能够帮助开发者更好地理解其模型的中文事实能力，促进基础模型的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07232",
            "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
            "url": "https://huggingface.co/papers/2411.07232",
            "abstract": "Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed \"Additing Affordance Benchmark\" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.",
            "score": 9,
            "issue_id": 523,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "5e344b551de578a9",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Add-it: Умное добавление объектов в изображения без дополнительного обучения",
                    "desc": "В статье представлен метод Add-it для добавления объектов в изображения на основе текстовых инструкций. Он использует расширенный механизм внимания в диффузионных моделях, учитывая информацию из сцены, текстового запроса и генерируемого изображения. Add-it превосходит современные методы на реальных и сгенерированных бенчмарках без специального обучения. Метод предпочтителен в более чем 80% случаев по оценкам людей."
                },
                "en": {
                    "title": "Seamless Object Insertion with Add-it: No Fine-Tuning Needed!",
                    "desc": "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."
                },
                "zh": {
                    "title": "无缝图像编辑的新突破",
                    "desc": "这篇论文介绍了一种名为Add-it的方法，用于根据文本指令将物体添加到图像中。该方法利用扩散模型的注意力机制，结合场景图像、文本提示和生成图像的信息，以实现自然的物体放置。Add-it在不进行特定任务微调的情况下，达到了图像插入基准测试的最先进结果，并在80%以上的情况下被人类评估者所偏好。该方法保持了结构一致性和细节，同时确保了物体的自然位置。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07199",
            "title": "OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision",
            "url": "https://huggingface.co/papers/2411.07199",
            "abstract": "Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/",
            "score": 9,
            "issue_id": 522,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "89d7bedc1b5241ac",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "OmniEdit: универсальный редактор изображений",
                    "desc": "В статье рассматриваются методы редактирования изображений с использованием моделей диффузии, которые обучаются на автоматически синтезированных или вручную аннотированных парах изображений. Основные проблемы существующих моделей включают ограниченные навыки редактирования, шумные и артефактные данные, а также ограничение по разрешению и соотношению сторон изображений. Представлена новая модель OmniEdit, способная выполнять семь различных задач редактирования изображений с любым соотношением сторон. OmniEdit использует обучение от семи специализированных моделей, улучшает качество данных с помощью важностной выборки и предлагает новую архитектуру EditNet для повышения успешности редактирования."
                },
                "en": {
                    "title": "OmniEdit: The All-in-One Image Editing Solution",
                    "desc": "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."
                },
                "zh": {
                    "title": "全能图像编辑，打破现实应用的限制",
                    "desc": "本文介绍了一种名为\textit{omniedit}的全能图像编辑器，旨在解决现有图像编辑方法在实际应用中的局限性。我们识别出三个主要挑战，包括偏见合成过程导致的编辑能力有限、训练数据集中的噪声和伪影问题，以及数据集的低分辨率和固定宽高比限制。通过利用七个不同专业模型的监督，\textit{omniedit}能够处理七种不同的图像编辑任务，并且支持任意宽高比。我们的实验结果表明，\textit{omniedit}在自动评估和人工评估中均显著优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07126",
            "title": "Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models",
            "url": "https://huggingface.co/papers/2411.07126",
            "abstract": "We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.",
            "score": 9,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "a7486a925b416669",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Фотореалистичная генерация изображений с пиксельной точностью",
                    "desc": "Edify Image - это семейство диффузионных моделей, способных генерировать фотореалистичный контент с пиксельной точностью. Модель использует каскадные диффузионные модели в пространстве пикселей, обученные с помощью нового процесса лапласовской диффузии. В этом процессе сигналы изображения на разных частотных диапазонах затухают с разной скоростью. Edify Image поддерживает широкий спектр приложений, включая синтез изображений по тексту, апскейлинг до 4K, ControlNets и генерацию панорам HDR 360°. Модель также позволяет осуществлять тонкую настройку для кастомизации изображений."
                },
                "en": {
                    "title": "Edify Image: Revolutionizing Photorealistic Image Generation with Precision",
                    "desc": "Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks."
                },
                "zh": {
                    "title": "Edify Image：生成真实感图像的新突破",
                    "desc": "Edify Image是一种扩散模型，能够生成像素级精确的真实感图像内容。它采用级联像素空间扩散模型，并使用新颖的拉普拉斯扩散过程进行训练，能够以不同的速率衰减不同频率带的图像信号。该模型支持多种应用，包括文本到图像合成、4K超分辨率、ControlNets、360 HDR全景生成以及图像定制的微调。Edify Image在图像生成领域展现了强大的灵活性和高质量的输出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06176",
            "title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
            "url": "https://huggingface.co/papers/2411.06176",
            "abstract": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.",
            "score": 7,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 ноября",
                "en": "November 9",
                "zh": "11月9日"
            },
            "hash": "950719af940fd8d0",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#training"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "M-LongDoc: прорыв в понимании длинных мультимодальных документов",
                    "desc": "Статья представляет M-LongDoc - новый набор данных и фреймворк для оценки мультимодальных моделей в задаче понимания длинных документов. Авторы предлагают метод дообучения, учитывающий особенности поиска в мультимодальных документах. Набор данных включает 851 образец современных многостраничных документов, требующих генеративных, а не только экстрактивных ответов. Эксперименты показывают, что предложенный подход улучшает корректность ответов модели на 4.6% по сравнению с базовыми моделями с открытым исходным кодом."
                },
                "en": {
                    "title": "Enhancing Multimodal Document Understanding with M-LongDoc",
                    "desc": "This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models."
                },
                "zh": {
                    "title": "提升多模态文档理解的效率与效果",
                    "desc": "本文介绍了一种名为M-LongDoc的基准数据集，包含851个样本，旨在评估大型多模态模型在文档理解和问答任务中的表现。由于文档通常包含文本、图形和表格等多种内容，人工阅读耗时较长，因此需要开发有效的自动化方法来辅助人类。我们提出了一种检索感知的调优方法，以提高多模态文档阅读的效率和效果。实验结果表明，该方法在模型响应的正确性上相较于基线开源模型有4.6%的相对提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06208",
            "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
            "url": "https://huggingface.co/papers/2411.06208",
            "abstract": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
            "score": 5,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 ноября",
                "en": "November 9",
                "zh": "11月9日"
            },
            "hash": "de83b5a8e14da36e",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "TRACE и IOPO: новый подход к обучению языковых моделей сложным инструкциям",
                    "desc": "Эта статья представляет TRACE - новый эталонный тест для улучшения и оценки способности языковых моделей следовать сложным инструкциям. Авторы также предлагают метод IOPO (оптимизация предпочтений ввода-вывода) для более эффективного обучения моделей. TRACE включает 120 тысяч тренировочных и 1000 оценочных примеров. Эксперименты показывают значительное улучшение результатов по сравнению с существующими методами как на целевых, так и на сторонних данных."
                },
                "en": {
                    "title": "Enhancing LLMs with TRACE and IOPO for Complex Instructions",
                    "desc": "This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO."
                },
                "zh": {
                    "title": "提升复杂指令跟随能力的创新方法",
                    "desc": "在大型语言模型（LLMs）领域，模型准确遵循指令的能力至关重要，尤其是在指令复杂性迅速增加的情况下。本文提出了TRACE，一个用于提高和评估复杂指令跟随能力的基准，包含12万条训练数据和1000条评估数据。我们还提出了IOPO（输入-输出偏好优化）对齐方法，考虑了输入和输出偏好对，帮助LLMs快速对齐响应偏好并深入探索指令偏好。通过在领域内和领域外数据集上的广泛实验，验证了IOPO的有效性，显示出在领域内数据上分别提高了8.15%和2.18%，在领域外数据上提高了6.29%和3.13%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07180",
            "title": "Counterfactual Generation from Language Models",
            "url": "https://huggingface.co/papers/2411.07180",
            "abstract": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.",
            "score": 2,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "6b57fa07bdf242ce",
            "data": {
                "categories": [
                    "#math",
                    "#interpretability",
                    "#reasoning",
                    "#training",
                    "#data"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Контрфактическое рассуждение в языковых моделях: новый взгляд на причинно-следственные связи",
                    "desc": "Данная статья представляет новый подход к генерации контрфактических примеров в языковых моделях. Авторы предлагают рассматривать языковые модели как обобщенные структурные уравнения, используя трюк Гумбеля-макса. Это позволяет моделировать совместное распределение оригинальных строк и их контрфактических вариантов. Разработанный алгоритм, основанный на выборке Гумбеля с учетом последствий, позволяет выводить скрытые переменные шума и генерировать контрфактические примеры для наблюдаемых строк."
                },
                "en": {
                    "title": "Harnessing Counterfactuals for Better Control of Language Models",
                    "desc": "This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques."
                },
                "zh": {
                    "title": "掌握语言模型的因果生成机制",
                    "desc": "本文探讨了在语言模型中理解和操控因果生成机制的重要性。以往的研究主要依赖于表示手术等技术来干预模型，但我们强调反事实推理与干预是不同的概念。我们提出了一种框架，通过将语言模型重构为广义结构方程模型，生成真实的字符串反事实。实验表明，该方法能够生成有意义的反事实，同时揭示了常用干预技术的显著副作用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05990",
            "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games",
            "url": "https://huggingface.co/papers/2411.05990",
            "abstract": "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 ноября",
                "en": "November 8",
                "zh": "11月8日"
            },
            "hash": "aae23469f2886f4c",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#rl",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🎲",
                "ru": {
                    "title": "Повышение рациональности языковых моделей в теории игр",
                    "desc": "Статья исследует рациональность больших языковых моделей (LLM) в контексте теории игр. Авторы оценивают современные LLM в различных играх с полной и неполной информацией, обнаруживая, что модели часто отклоняются от рациональных стратегий. Для решения этой проблемы предложены специальные рабочие процессы, улучшающие способность моделей вычислять равновесия Нэша и принимать рациональные решения. Результаты показывают значительное повышение рациональности и устойчивости LLM в игровых задачах при использовании этих методов."
                },
                "en": {
                    "title": "Enhancing LLM Rationality in Strategic Decision-Making",
                    "desc": "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."
                },
                "zh": {
                    "title": "提升大型语言模型的博弈理性",
                    "desc": "本文研究了大型语言模型（LLMs）在战略决策中的理性，特别是在博弈论框架下。我们评估了多种先进的LLMs在完全信息和不完全信息博弈中的表现，发现随着博弈复杂性的增加，LLMs常常偏离理性策略。为了解决这些问题，我们设计了多种博弈论工作流程，以指导LLMs的推理和决策过程，从而提高其计算纳什均衡和在不确定条件下做出理性选择的能力。实验结果表明，采用这些工作流程显著提高了LLMs在博弈任务中的理性和稳健性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06272",
            "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
            "url": "https://huggingface.co/papers/2411.06272",
            "abstract": "As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.",
            "score": 1,
            "issue_id": 520,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 ноября",
                "en": "November 9",
                "zh": "11月9日"
            },
            "hash": "2559c023f673c9b4",
            "data": {
                "categories": [
                    "#low_resource",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "Эталон для оценки финансовых языковых моделей",
                    "desc": "В статье обсуждается необходимость создания стандартизированного метода оценки производительности больших языковых моделей (LLM) в финансовом секторе. Авторы предлагают \"Golden Touchstone\", первый двуязычный эталон для финансовых LLM, который включает в себя наборы данных на китайском и английском языках для восьми основных финансовых задач NLP. Этот эталон позволяет более полно оценивать способности моделей в понимании и генерации финансовой информации. Исследование также включает открытый доступ к коду и весам модели Touchstone-GPT, что способствует дальнейшему развитию и оптимизации финансовых LLM."
                },
                "en": {
                    "title": "Golden Touchstone: Elevating Financial LLM Evaluation",
                    "desc": "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."
                },
                "zh": {
                    "title": "金融领域的标准化评估工具——金色基准",
                    "desc": "随着大型语言模型在金融领域的广泛应用，评估其性能的标准化方法变得尤为重要。现有的金融基准测试存在语言和任务覆盖面有限、数据集质量低以及适应性不足等问题。为了解决这些问题，我们提出了“金色基准”，这是第一个全面的双语金融基准，涵盖了中英文的八个核心金融自然语言处理任务。通过对主要模型的比较分析，我们揭示了它们在处理复杂金融信息时的优缺点，并开源了Touchstone-GPT模型，以促进未来的研究和优化。"
                }
            }
        }
    ],
    "link_prev": "2024-11-11.html",
    "link_next": "2024-11-13.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11月11日"
    },
    "short_date_next": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11月13日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。\n\nzhè piān wén zhāng jiè shào le StdGEN, yī zhǒng chuàng xīn de cóng dān zhāng tú xiàng shēng chéng gāo zhì liàng 3D jué sè de fāng fǎ. tā néng zài sān fēn zhōng nèi shēng chéng jù yǒu fēn lì de yǔ yì zǔ jìn (rú shēn tǐ, yī fú hé tóu fà) de xiáng xì 3D jué sè. StdGEN de hé xīn shì tí chū de yǔ yì gǎn jué dà xíng chóng jiàn mó xíng (S-LRM), néng gòu cóng duō shì jì tú xiàng zhōng chóng jiàn jǐ hé, yán sè hé yǔ yì. shí yàn zhèng míng, StdGEN zài 3D dòng màn jué sè shēng chéng fāng miàn biǎo xiǎn chū sè, yōu yú xiàn yǒu fāng fǎ. tā wèi xū nǐ xiàn shí, yóu xì hé diàn yǐng zhì zuò děng ti gōng gěi le línghuó de dìng zhì huà 3D jué sè.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sītīdī jīn\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovative\"},\n    {\"word\": \"角色\", \"pinyin\": \"juésè\", \"trans\": \"character\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔjiàn\", \"trans\": \"component\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐhé\", \"trans\": \"geometry\"},\n    {\"word\": \"重建\", \"pinyin\": \"chóngjiàn\", \"trans\": \"reconstruct\"},\n    {\"word\": \"多视图\", \"pinyin\": \"duōshìtú\", \"trans\": \"multi-view\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"灵活\", \"pinyin\": \"línghuó\", \"trans\": \"flexible\"},\n    {\"word\": \"定制化\", \"pinyin\": \"dìngzhìhuà\", \"trans\": \"customized\"},\n    {\"word\": \"虚拟现实\", \"pinyin\": \"xūnǐ xiànshí\", \"trans\": \"virtual reality\"},\n    {\"word\": \"游戏\", \"pinyin\": \"yóuxì\", \"trans\": \"game\"},\n    {\"word\": \"电影制作\", \"pinyin\": \"diànyǐng zhìzuò\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    }
}