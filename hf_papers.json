{
    "date": {
        "ru": "30 июля",
        "en": "July 30",
        "zh": "7月30日"
    },
    "time_utc": "2025-07-30 03:03",
    "weekday": 2,
    "issue_id": 5080,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.22058",
            "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again",
            "url": "https://huggingface.co/papers/2507.22058",
            "abstract": "Reinforcement learning enhances discrete autoregressive modeling for image and language generation, achieving high-quality image generation and instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts.",
            "score": 6,
            "issue_id": 5080,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "33e74a909f1e3165",
            "authors": [
                "Zigang Geng",
                "Yibing Wang",
                "Yeyao Ma",
                "Chen Li",
                "Yongming Rao",
                "Shuyang Gu",
                "Zhao Zhong",
                "Qinglin Lu",
                "Han Hu",
                "Xiaosong Zhang",
                "Linus",
                "Di Wang",
                "Jie Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22058.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#cv",
                    "#diffusion",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Единая модель для высококачественной генерации изображений и текста",
                    "desc": "Данная статья представляет новый подход к генерации изображений и текста с использованием дискретного авторегрессионного моделирования, усиленного обучением с подкреплением. Авторы разработали фреймворк X-Omni, включающий семантический токенизатор изображений, единую авторегрессионную модель для языка и изображений, и офлайн-декодер диффузии для генерации изображений. X-Omni достигает высокого качества генерации изображений, используя языковую модель размером 7 миллиардов параметров. Модель демонстрирует способность следовать сложным инструкциям и отображать длинные тексты, преодолевая ограничения предыдущих подходов."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts Image and Language Generation Quality",
                    "desc": "This paper presents a novel approach that combines reinforcement learning with discrete autoregressive modeling to improve image and language generation. The authors address common issues in generating images, such as low quality and failure to follow complex instructions, which arise from errors in autoregressive inference and discretization. By introducing a framework called X-Omni, which includes a semantic image tokenizer and a unified model for both images and language, they enhance the quality of generated outputs. The results show that X-Omni achieves state-of-the-art performance in image generation, producing aesthetically pleasing images while effectively following detailed instructions."
                },
                "zh": {
                    "title": "强化学习提升图像与语言生成的质量",
                    "desc": "这篇论文探讨了如何通过强化学习来提升离散自回归模型在图像和语言生成中的表现。研究表明，传统的自回归建模在生成图像时常常面临视觉质量低、输出失真和复杂指令执行不佳等问题。通过引入强化学习，作者提出了一种名为X-Omni的框架，能够有效减少生成过程中的伪影，提高生成质量。X-Omni结合了语义图像标记器、统一的自回归模型和离线扩散解码器，达到了图像生成任务的最先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21183",
            "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
            "url": "https://huggingface.co/papers/2507.21183",
            "abstract": "MaPPO, a framework for preference optimization, enhances alignment of large language models with human preferences by integrating prior reward knowledge into a Maximum a Posteriori objective, improving performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.",
            "score": 4,
            "issue_id": 5080,
            "pub_date": "2025-07-27",
            "pub_date_card": {
                "ru": "27 июля",
                "en": "July 27",
                "zh": "7月27日"
            },
            "hash": "e1328e80ce51ef74",
            "authors": [
                "Guangchen Lan",
                "Sipeng Zhang",
                "Tianle Wang",
                "Yuwei Zhang",
                "Daoan Zhang",
                "Xinpeng Wei",
                "Xiaoman Pan",
                "Hongming Zhang",
                "Dong-Jun Han",
                "Christopher G. Brinton"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Purdue University",
                "Tencent AI Lab",
                "University of California, San Diego",
                "University of Rochester",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21183.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MaPPO: Улучшение согласованности LLM с человеческими предпочтениями",
                    "desc": "MaPPO - это новый фреймворк для оптимизации предпочтений в больших языковых моделях (LLM). Он использует байесовский подход, включая априорные знания о вознаграждении в целевую функцию максимального апостериорного оценивания. MaPPO обобщает и улучшает существующие методы, такие как DPO, не добавляя новых гиперпараметров. Эмпирические тесты на стандартных бенчмарках показывают стабильное улучшение производительности без потери вычислительной эффективности."
                },
                "en": {
                    "title": "Enhancing Language Model Alignment with MaPPO",
                    "desc": "MaPPO is a new framework designed to improve how large language models (LLMs) align with human preferences. It does this by using a Maximum a Posteriori (MaP) approach that incorporates prior reward knowledge into the optimization process. Unlike traditional methods that rely solely on Maximum Likelihood Estimation (MLE), MaPPO enhances preference learning by addressing the limitations of binary classification in response evaluation. The framework is flexible, requiring no extra hyperparameters, and shows significant performance improvements across various benchmarks while maintaining computational efficiency."
                },
                "zh": {
                    "title": "MaPPO：优化人类偏好的新框架",
                    "desc": "MaPPO是一种偏好优化框架，旨在增强大型语言模型与人类偏好的对齐。它通过将先前的奖励知识整合到最大后验目标中，来提高模型在各种基准测试中的表现。与现有的直接偏好优化方法不同，MaPPO通过引入先前的奖励估计，扩展了偏好学习的范式。该方法无需额外的超参数，并支持离线和在线的偏好优化，能够与多种DPO变体兼容使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22061",
            "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
            "url": "https://huggingface.co/papers/2507.22061",
            "abstract": "A new dataset and baseline method for motion-guided few-shot video object segmentation are introduced, addressing challenges in motion understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction.",
            "score": 0,
            "issue_id": 5080,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "d205fb0c00f383de",
            "authors": [
                "Kaining Ying",
                "Hengrui Hu",
                "Henghui Ding"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый подход к сегментации движущихся объектов в видео при малом количестве данных",
                    "desc": "Представлен новый набор данных MOVE и базовый метод для сегментации объектов в видео с учетом движения при малом количестве примеров. Существующие методы фокусируются на статических атрибутах объектов, игнорируя динамику в видео. Авторы провели оценку 6 современных методов в 2 экспериментальных условиях, выявив их ограничения. Предложенная сеть DMA показала превосходные результаты в понимании движения при малом числе примеров."
                },
                "en": {
                    "title": "Enhancing Video Segmentation with Motion Understanding",
                    "desc": "This paper presents a new dataset called MOVE, aimed at improving motion-guided few-shot video object segmentation (FSVOS). Unlike previous datasets that focus on static object categories, MOVE emphasizes the importance of understanding motion dynamics in videos. The authors evaluate existing state-of-the-art methods and find that they struggle with motion-guided tasks, highlighting the need for better solutions. To address this, they propose a new baseline method called Decoupled Motion Appearance Network (DMA), which shows improved performance in segmenting dynamic objects with limited annotated examples."
                },
                "zh": {
                    "title": "运动引导的少样本视频分割新突破",
                    "desc": "本文介绍了一种新的数据集和基线方法，用于运动引导的少样本视频目标分割（FSVOS）。该方法旨在根据少量带注释的示例，分割视频中的动态物体，特别关注运动模式。现有的FSVOS数据集和方法通常忽视视频中的时间动态，限制了其在需要运动理解的场景中的应用。我们提出的MOVE数据集和解耦运动外观网络（DMA）基线方法，显著提高了少样本运动理解的性能，为未来的研究奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21364",
            "title": "Evaluating Deep Learning Models for African Wildlife Image\n  Classification: From DenseNet to Vision Transformers",
            "url": "https://huggingface.co/papers/2507.21364",
            "abstract": "A comparative study of deep learning models for wildlife image classification highlights trade-offs between accuracy, resource requirements, and deployability, with DenseNet-201 and Vision Transformer ViT-H/14 performing best among evaluated models.  \t\t\t\t\tAI-generated summary \t\t\t\t Wildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as a promising tool for biodiversity monitoring and conservation. This paper presents a comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using a public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to African-grounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation.",
            "score": 0,
            "issue_id": 5080,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "906cbe0bf6d751e4",
            "authors": [
                "Lukman Jibril Aliyu",
                "Umar Sani Muhammad",
                "Bilqisu Ismail",
                "Nasiru Muhammad",
                "Almustapha A Wakili",
                "Seid Muhie Yimam",
                "Shamsuddeen Hassan Muhammad",
                "Mustapha Abdullahi"
            ],
            "affiliations": [
                "Arewa Data Science Academy Kano, Nigeria",
                "Azman University Kano, Nigeria",
                "Imperial College London London, United Kingdom",
                "Towson University Maryland, USA",
                "Universitat Hamburg Hamburg, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21364.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#cv",
                    "#transfer_learning",
                    "#science"
                ],
                "emoji": "🦏",
                "ru": {
                    "title": "Глубокое обучение на страже африканской фауны",
                    "desc": "Исследование сравнивает модели глубокого обучения для классификации изображений дикой природы в Африке. Оценивались DenseNet-201, ResNet-152, EfficientNet-B4 и Vision Transformer ViT-H/14 на датасете из четырех видов животных. ViT-H/14 показал наилучшую точность (99%), но с высокими вычислительными затратами, в то время как DenseNet-201 достиг лучшего баланса между точностью и ресурсоемкостью. Работа демонстрирует возможности применения легковесных моделей глубокого обучения для задач сохранения дикой природы."
                },
                "en": {
                    "title": "Balancing Accuracy and Deployability in Wildlife Image Classification",
                    "desc": "This paper compares different deep learning models for classifying images of African wildlife, focusing on their accuracy, resource needs, and how easily they can be deployed. The study evaluates models like DenseNet-201 and Vision Transformer ViT-H/14, finding that DenseNet-201 offers good accuracy with lower resource requirements, while ViT-H/14 achieves higher accuracy but is more resource-intensive. The research emphasizes the importance of balancing model performance with practical deployment considerations in conservation efforts. Ultimately, the findings aim to guide the selection and use of deep learning models for effective wildlife monitoring and protection."
                },
                "zh": {
                    "title": "深度学习助力野生动物保护的最佳选择",
                    "desc": "本研究比较了多种深度学习模型在野生动物图像分类中的表现，重点关注准确性、资源需求和可部署性之间的权衡。我们使用了一个包含水牛、大象、犀牛和斑马的公共数据集，评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14的性能。结果显示，DenseNet-201在卷积网络中表现最佳，准确率为67%，而ViT-H/14的整体准确率最高，达到99%，但计算成本显著更高，影响了其部署。该研究为野生动物保护提供了深度学习工具的模型选择、数据集准备和负责任部署的实用见解。"
                }
            }
        }
    ],
    "link_prev": "2025-07-29.html",
    "link_next": "2025-07-31.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "29.07",
        "en": "07/29",
        "zh": "7月29日"
    },
    "short_date_next": {
        "ru": "31.07",
        "en": "07/31",
        "zh": "7月31日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}