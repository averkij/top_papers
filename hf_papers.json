{
    "date": {
        "ru": "15 июля",
        "en": "July 15",
        "zh": "7月15日"
    },
    "time_utc": "2025-07-15 02:59",
    "weekday": 1,
    "issue_id": 4814,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.09862",
            "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation",
            "url": "https://huggingface.co/papers/2507.09862",
            "abstract": "A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
            "score": 0,
            "issue_id": 4814,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 июля",
                "en": "July 14",
                "zh": "7月14日"
            },
            "hash": "67b82ab227be6ce9",
            "authors": [
                "Youliang Zhang",
                "Zhaoyang Li",
                "Duomin Wang",
                "Jiahe Zhang",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Gang Yu",
                "Xiu Li"
            ],
            "affiliations": [
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09862.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SpeakerVid-5M: Революция в создании интерактивных виртуальных людей",
                    "desc": "Представлен крупномасштабный набор данных SpeakerVid-5M для генерации аудиовизуальных диадических интерактивных виртуальных людей. Датасет содержит более 5,2 миллиона видеоклипов с портретами людей, охватывающих различные типы взаимодействий. Структура набора данных организована по двум ключевым измерениям: типу взаимодействия и качеству данных. Авторы также предоставляют базовую модель видеочата на основе авторегрессии и набор метрик для оценки будущих работ в этой области."
                },
                "en": {
                    "title": "Unlocking Interactive Virtual Humans with SpeakerVid-5M",
                    "desc": "The paper introduces the SpeakerVid-5M dataset, a large-scale resource designed for generating audio-visual dyadic interactive virtual humans. It consists of over 8,743 hours of video data, featuring more than 5.2 million clips that capture various interaction types, such as monadic and dyadic conversations. The dataset is organized into two main categories: interaction type and data quality, allowing for effective pre-training and fine-tuning of models. Additionally, the authors present a baseline video chat model and a benchmark called VidChatBench to facilitate future research in this area."
                },
                "zh": {
                    "title": "SpeakerVid-5M：音视频互动虚拟人的新里程碑",
                    "desc": "本文介绍了一个名为SpeakerVid-5M的大规模数据集，旨在生成音视频双向互动的虚拟人。该数据集包含超过8743小时的高质量视频片段，涵盖多种互动类型，如单向对话和双向对话。数据集按照互动类型和数据质量两个维度进行结构化，支持多种2D虚拟人任务。我们还提供了基于自回归模型的视频聊天基线，并设立了VidChatBench作为未来研究的基准。"
                }
            }
        }
    ],
    "link_prev": "2025-07-14.html",
    "link_next": "2025-07-16.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7月14日"
    },
    "short_date_next": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}