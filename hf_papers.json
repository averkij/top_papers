{
    "date": {
        "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 19",
        "zh": "1æœˆ19æ—¥"
    },
    "time_utc": "2026-01-19 07:30",
    "weekday": 0,
    "issue_id": 644,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.08521",
            "title": "Your Group-Relative Advantage Is Biased",
            "url": "https://huggingface.co/papers/2601.08521",
            "abstract": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.",
            "score": 79,
            "issue_id": 640,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "caf215355050af49",
            "authors": [
                "Fengkai Yang",
                "Zherui Chen",
                "Xiaohan Wang",
                "Xiaodong Lu",
                "Jiajun Chai",
                "Guojun Yin",
                "Wei Lin",
                "Shuai Ma",
                "Fuzhen Zhuang",
                "Deqing Wang",
                "Yaodong Yang",
                "Jianxin Li",
                "Yikun Ban"
            ],
            "affiliations": [
                "Beihang University",
                "Meituan",
                "Peking University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08521.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#rlhf",
                    "#math"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ´Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑÑƒ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ History-Aware Adaptive Difficulty Weighting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞºĞ¾Ñ€Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Correcting Bias in Advantage Estimation for Better Learning",
                    "desc": "This paper addresses a problem in group-based reinforcement learning from verifier rewards, where the advantage estimation is biased. Specifically, it underestimates the difficulty of hard prompts while overestimating the ease of simple prompts, which can lead to ineffective learning. The authors introduce a new method called History-Aware Adaptive Difficulty Weighting (HA-DW) that adjusts the advantage estimates based on the difficulty of tasks and the training process. Their experiments show that incorporating HA-DW into existing models significantly enhances performance on reasoning tasks."
                },
                "zh": {
                    "title": "çº æ­£åå·®ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åŸºäºéªŒè¯è€…å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­å­˜åœ¨çš„åå·®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¾¤ä½“åŸºç¡€æ–¹æ³•ä¸­ã€‚ç ”ç©¶å‘ç°ï¼Œç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨åœ¨é¢å¯¹å›°éš¾æç¤ºæ—¶ä¼šä½ä¼°ä¼˜åŠ¿ï¼Œè€Œåœ¨é¢å¯¹ç®€å•æç¤ºæ—¶åˆ™ä¼šé«˜ä¼°ä¼˜åŠ¿ï¼Œå¯¼è‡´æ¢ç´¢å’Œåˆ©ç”¨çš„ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å†å²æ„ŸçŸ¥è‡ªé€‚åº”éš¾åº¦åŠ æƒæ–¹æ³•ï¼ˆHA-DWï¼‰ï¼Œè¯¥æ–¹æ³•æ ¹æ®ä¸æ–­å˜åŒ–çš„éš¾åº¦é”šç‚¹å’Œè®­ç»ƒåŠ¨æ€è°ƒæ•´ä¼˜åŠ¿ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†HA-DWé›†æˆåˆ°GRPOåŠå…¶å˜ä½“ä¸­å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10355",
            "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
            "url": "https://huggingface.co/papers/2601.10355",
            "abstract": "A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on Ï„ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
            "score": 22,
            "issue_id": 640,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "561cb5995d29e4d4",
            "authors": [
                "Zhihao Xu",
                "Rumei Li",
                "Jiahuan Li",
                "Rongxiang Weng",
                "Jingang Wang",
                "Xunliang Cai",
                "Xiting Wang"
            ],
            "affiliations": [
                "Meituan",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10355.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GEM â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Trajectory Synthesizer Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ end-to-end Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ GEM-32B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 16.5% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BFCL V3 Multi-turn Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° in-domain Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Trajectory Synthesizer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Harnessing Text for Efficient Multi-Turn Tool Use in LLMs",
                    "desc": "This paper presents a new method for generating multi-turn tool-use trajectories for large language models (LLMs) using text-based data synthesis. The proposed GEM pipeline extracts and refines problem-solving experiences from existing text corpora, allowing for the creation of realistic multi-turn interactions. By training a specialized Trajectory Synthesizer, the authors reduce computational costs while maintaining high performance in generating these trajectories. Experimental results show significant improvements in benchmark tests, demonstrating the effectiveness and efficiency of this approach compared to traditional data collection methods."
                },
                "zh": {
                    "title": "æ–‡æœ¬åˆæˆåŠ©åŠ›å¤šè½®å·¥å…·ä½¿ç”¨çš„æ™ºèƒ½åŒ–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šè½®å·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å››ä¸ªé˜¶æ®µçš„å¤„ç†æµç¨‹ï¼Œä»æ–‡æœ¬è¯­æ–™åº“ä¸­æå–å’Œç”Ÿæˆå¤šè½®å·¥å…·ä½¿ç”¨æ•°æ®ã€‚æˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨çš„è½¨è¿¹åˆæˆå™¨ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜ç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè½®åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08430",
            "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
            "url": "https://huggingface.co/papers/2601.08430",
            "abstract": "RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.",
            "score": 21,
            "issue_id": 640,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "9d3d0f8e4f3e3b69",
            "authors": [
                "Sunzhu Li",
                "Jiale Zhao",
                "Miteto Wei",
                "Huimin Ren",
                "Yang Zhou",
                "Jingwen Yang",
                "Shunyu Liu",
                "Kaike Zhang",
                "Wei Chen"
            ],
            "affiliations": [
                "Li Auto Inc.",
                "Nanyang Technological University",
                "The Chinese University of Hong Kong, Shenzhen",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08430.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#healthcare",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ², Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RubricHub Ñ 110 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ Ñ‡ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing AI Reasoning with Automated Rubric Generation",
                    "desc": "This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in open-ended generation tasks, particularly in health reasoning. It introduces an automated framework for generating evaluation rubrics that enhances the performance of AI models by providing structured criteria for assessment. The proposed Coarse-to-Fine Rubric Generation framework combines various techniques to create detailed and effective evaluation metrics. The authors validate their approach with a new dataset, RubricHub, which significantly improves the performance of AI models on health reasoning benchmarks, achieving state-of-the-art results."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–è¯„åˆ†æ ‡å‡†æå‡å¥åº·æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„è¯„åˆ†æ ‡å‡†ç”Ÿæˆæ¡†æ¶ï¼Œä»¥æé«˜åœ¨å¥åº·æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ¨ç†å¯†é›†å‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸåˆ™å¼•å¯¼åˆæˆã€å¤šæ¨¡å‹èšåˆå’Œéš¾åº¦æ¼”å˜ï¼Œç”Ÿæˆå…¨é¢ä¸”é«˜åº¦åŒºåˆ†çš„è¯„åˆ†æ ‡å‡†ã€‚é€šè¿‡RubricHubæ•°æ®é›†çš„éªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¥åº·åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‰æ²¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11000",
            "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
            "url": "https://huggingface.co/papers/2601.11000",
            "abstract": "Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  \t\t\t\t\tAI-generated summary \t\t\t\t Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.",
            "score": 15,
            "issue_id": 644,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "891d8c6f80792efc",
            "authors": [
                "Zhongxiang Sun",
                "Yi Zhan",
                "Chenglei Shen",
                "Weijie Yu",
                "Xiao Zhang",
                "Ming He",
                "Jun Xu"
            ],
            "affiliations": [
                "AI Lab at Lenovo Research",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "School of Artificial Intelligence and Data Science, University of International Business and Economics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11000.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#hallucinations"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹, Ğ¸Ğ·-Ğ·Ğ° Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ FPPS (Factuality-Preserving Personalized Steering), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PFQABench, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FPPS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Balancing Personalization and Factual Accuracy in LLMs",
                    "desc": "This paper addresses the challenge of personalized large language models (LLMs) generating inaccurate information based on user history instead of factual accuracy. It identifies a problem where these models produce answers that reflect a user's past interactions, leading to 'hallucinations' that can spread misinformation. To solve this, the authors introduce a method called Factuality-Preserving Personalized Steering (FPPS), which helps maintain factual correctness while still providing personalized responses. Additionally, they present PFQABench, a benchmark for evaluating both factual accuracy and personalization in question answering."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–ä¸äº‹å®å‡†ç¡®æ€§çš„å¹³è¡¡ä¹‹é“",
                    "desc": "ä¸ªæ€§åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ ¹æ®ç”¨æˆ·çš„å†å²ç”Ÿæˆä¿¡æ¯ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„å†…å®¹ä¸äº‹å®ä¸ç¬¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºäº‹å®ä¿æŒä¸ªæ€§åŒ–å¼•å¯¼ï¼ˆFPPSï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ä¿æŒä¸ªæ€§åŒ–å“åº”çš„åŒæ—¶ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„äº‹å®å‡†ç¡®æ€§ã€‚FPPSé€šè¿‡è½»é‡çº§çš„æ¨ç†æ–¹æ³•ï¼Œå‡å°‘ä¸ªæ€§åŒ–å¼•èµ·çš„äº‹å®æ‰­æ›²ï¼Œé¿å…äº†é”™è¯¯ä¿¡å¿µçš„ä¼ æ’­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†PFQABenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨å…±åŒè¯„ä¼°ä¸ªæ€§åŒ–é—®ç­”å’Œäº‹å®å‡†ç¡®æ€§çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11037",
            "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
            "url": "https://huggingface.co/papers/2601.11037",
            "abstract": "Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  \t\t\t\t\tAI-generated summary \t\t\t\t RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.",
            "score": 12,
            "issue_id": 640,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "872fa7ac4160c51e",
            "authors": [
                "Shiyu Liu",
                "Yongjing Yin",
                "Jianhao Yan",
                "Yunbo Tang",
                "Qinggang Zhang",
                "Bei Li",
                "Xin Chen",
                "Jingang Wang",
                "Xunliang Cai",
                "Jinsong Su"
            ],
            "affiliations": [
                "Institute of Artificial Intelligence, Xiamen University",
                "Meituan Inc.",
                "School of Informatics, Xiamen University",
                "The Hong Kong Polytechnic University",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11037.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ÑÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Boundary-Aware Policy Optimization (BAPO) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Enhancing Reliability in Agentic Search with Boundary Awareness",
                    "desc": "This paper presents a new reinforcement learning framework called Boundary-Aware Policy Optimization (BAPO) aimed at improving the reliability of agents in complex search tasks. The framework teaches agents to recognize their reasoning limits and encourages them to respond with 'I DON'T KNOW' when evidence is insufficient. By incorporating a boundary-aware reward system, BAPO ensures that agents only admit uncertainty when necessary, thus reducing the risk of providing misleading answers. Experimental results show that BAPO significantly enhances the reliability of agentic search while maintaining high accuracy in responses."
                },
                "zh": {
                    "title": "æå‡æ™ºèƒ½ä½“æœç´¢å¯é æ€§çš„è¾¹ç•Œæ„è¯†ä¼˜åŒ–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“æœç´¢çš„å¯é æ€§ã€‚é€šè¿‡æ•™å¯¼æ™ºèƒ½ä½“è¯†åˆ«æ¨ç†çš„å±€é™æ€§ï¼Œå½“è¯æ®ä¸è¶³æ—¶èƒ½å¤Ÿé€‚å½“åœ°å›åº”ã€‚æˆ‘ä»¬å¼•å…¥äº†è¾¹ç•Œæ„è¯†ç­–ç•¥ä¼˜åŒ–ï¼ˆBAPOï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¥–åŠ±æœºåˆ¶é¼“åŠ±æ™ºèƒ½ä½“åœ¨æ¨ç†è¾¾åˆ°æé™æ—¶ç»™å‡ºâ€œæˆ‘ä¸çŸ¥é“â€çš„å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBAPOæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“æœç´¢çš„æ•´ä½“å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09195",
            "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
            "url": "https://huggingface.co/papers/2601.09195",
            "abstract": "Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.",
            "score": 8,
            "issue_id": 640,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "687e04f44bf56850",
            "authors": [
                "Tao Liu",
                "Taiqiang Wu",
                "Runming Yang",
                "Shaoning Sun",
                "Junjie Wang",
                "Yujiu Yang"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09195.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¦ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ProFit Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ SFT Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ ĞµĞ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½ĞµÑÑƒÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ â€” ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Fine-Tuning by Focusing on Core Concepts",
                    "desc": "This paper introduces ProFit, a method for improving supervised fine-tuning (SFT) of Large Language Models (LLMs) by addressing the issue of overfitting to non-core expressions. Traditional SFT often relies on a single reference answer, which can lead to models learning irrelevant details instead of core concepts. ProFit strategically masks low-probability tokens, which are less semantically important, allowing the model to focus on high-probability tokens that represent the main ideas. The results show that ProFit outperforms standard SFT methods in tasks requiring general reasoning and mathematical skills."
                },
                "zh": {
                    "title": "ProFitï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒè¡¨è¾¾èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ProFitï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒä¸­å¯¹éæ ¸å¿ƒè¡¨è¾¾çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒé€šå¸¸åªä¾èµ–å•ä¸€å‚è€ƒç­”æ¡ˆï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹ŸåˆäºæŸäº›è¡¨é¢è¡¨è¾¾ã€‚é€šè¿‡å¼•å…¥å¤šä¸ªå‚è€ƒç­”æ¡ˆï¼ŒProFitèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è¿™ç§è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶é€šè¿‡æ©è”½ä½æ¦‚ç‡æ ‡è®°æ¥ä¿æŒæ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProFitåœ¨ä¸€èˆ¬æ¨ç†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10825",
            "title": "Reasoning Models Generate Societies of Thought",
            "url": "https://huggingface.co/papers/2601.10825",
            "abstract": "Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.",
            "score": 4,
            "issue_id": 640,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "7dcd7d60c59e0050",
            "authors": [
                "Junsol Kim",
                "Shiyang Lai",
                "Nino Scherrer",
                "Blaise AgÃ¼era y Arcas",
                "James Evans"
            ],
            "affiliations": [
                "Google, Paradigms of Intelligence Team",
                "Santa Fe Institute",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10825.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#training",
                    "#agents",
                    "#architecture",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ»Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº DeepSeek-R1 Ğ¸ QwQ-32B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ - Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ğ¾Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…."
                },
                "en": {
                    "title": "Harnessing Collective Intelligence for Enhanced Reasoning",
                    "desc": "This paper explores how reasoning models can improve their performance by simulating interactions similar to those found in multi-agent systems. It highlights that the success of these models is not just due to longer computation times, but also from the diversity of perspectives created through internal cognitive debates. By analyzing reasoning processes, the authors demonstrate that models like DeepSeek-R1 and QwQ-32B show greater diversity in thought, leading to better problem-solving outcomes. The study suggests that organizing these cognitive perspectives in a structured way can mimic collective intelligence, enhancing the models' ability to tackle complex tasks."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“äº’åŠ¨æå‡æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ¨ç†æ¨¡å‹å¦‚ä½•é€šè¿‡æ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“çš„äº’åŠ¨æ¥æå‡æ€§èƒ½ã€‚è¿™ç§äº’åŠ¨åˆ›é€ äº†å¤šæ ·åŒ–çš„è®¤çŸ¥è§†è§’ï¼Œä½¿å¾—é—®é¢˜è§£å†³æ›´åŠ é«˜æ•ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¸ä»…æ¥è‡ªäºå»¶é•¿çš„è®¡ç®—é“¾ï¼Œè¿˜æºäºå†…éƒ¨è®¤çŸ¥è§†è§’ä¹‹é—´çš„è¾©è®ºå’Œå¤šæ ·æ€§ã€‚é€šè¿‡é‡åŒ–åˆ†æï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„è§†è§’å¤šæ ·æ€§ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11516",
            "title": "Building Production-Ready Probes For Gemini",
            "url": "https://huggingface.co/papers/2601.11516",
            "abstract": "Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
            "score": 2,
            "issue_id": 640,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "87ddd4cbadd8d1f3",
            "authors": [
                "JÃ¡nos KramÃ¡r",
                "Joshua Engels",
                "Zheng Wang",
                "Bilal Chughtai",
                "Rohin Shah",
                "Neel Nanda",
                "Arthur Conmy"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11516.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#security",
                    "#long_context"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹: Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¸ ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² Ğ±Ğ¾ĞµĞ²Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… Gemini."
                },
                "en": {
                    "title": "Enhancing Misuse Mitigation in Language Models with Robust Probes",
                    "desc": "This paper discusses the challenges of using activation probes to prevent misuse of language models, especially when dealing with long-context inputs. The authors highlight that existing probe architectures struggle to generalize effectively when the input context length increases. They propose new architectures and emphasize the importance of training on diverse data distributions to improve robustness against various production shifts. The study also shows that combining probes with prompted classifiers can enhance accuracy while maintaining computational efficiency, leading to successful implementations in Google's Gemini model."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ»¥ç”¨ç¼“è§£çš„æ¢é’ˆæ¶æ„",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¿€æ´»æ¢é’ˆåœ¨è¯­è¨€æ¨¡å‹æ»¥ç”¨ç¼“è§£ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡æ³›åŒ–æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ¢é’ˆæ¶æ„åœ¨å¤„ç†ä»çŸ­ä¸Šä¸‹æ–‡åˆ°é•¿ä¸Šä¸‹æ–‡çš„è¾“å…¥æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å‡ ç§æ–°çš„æ¢é’ˆæ¶æ„ï¼Œä»¥åº”å¯¹è¿™ç§é•¿ä¸Šä¸‹æ–‡åˆ†å¸ƒçš„å˜åŒ–ã€‚é€šè¿‡åœ¨ç½‘ç»œæ”»å‡»é¢†åŸŸè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œç»“åˆæ¶æ„é€‰æ‹©å’Œå¤šæ ·åŒ–è®­ç»ƒæ˜¯å®ç°å¹¿æ³›æ³›åŒ–çš„å…³é”®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11087",
            "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
            "url": "https://huggingface.co/papers/2601.11087",
            "abstract": "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.",
            "score": 2,
            "issue_id": 640,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "7de1634e59778ee2",
            "authors": [
                "Qiyuan Zhang",
                "Biao Gong",
                "Shuai Tan",
                "Zheng Zhang",
                "Yujun Shen",
                "Xing Zhu",
                "Yuyuan Li",
                "Kelu Yao",
                "Chunhua Shen",
                "Changqing Zou"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11087.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#video",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ĞºĞ°Ğº Ğ½ĞµĞ¾Ñ‚ÑŠĞµĞ¼Ğ»ĞµĞ¼Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ, Ğ° Ğ½Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Mimicry-Discovery Cycle (MDcycle), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PhysRVGBench Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Physics-Aware Learning",
                    "desc": "This paper presents a new approach to video generation using reinforcement learning that incorporates physical laws, specifically collision rules, directly into the model. Unlike traditional methods that treat physics as optional constraints, this paradigm ensures that physical principles are strictly followed, enhancing the realism of generated videos. The authors introduce a framework called Mimicry-Discovery Cycle (MDcycle) that allows for fine-tuning while maintaining the benefits of physics-based feedback. To demonstrate the effectiveness of their method, they create a new benchmark dataset, PhysRVGBench, and conduct comprehensive experiments to evaluate performance."
                },
                "zh": {
                    "title": "ç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‰©ç†æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆï¼Œç›´æ¥åœ¨é«˜ç»´ç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡Œç‰©ç†ç¢°æ’è§„åˆ™ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç‰©ç†çŸ¥è¯†çš„ä¸¥æ ¼åº”ç”¨ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºæ¡ä»¶çº¦æŸã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹åœ¨åƒç´ çº§å…¨å±€å»å™ªæ—¶å¿½è§†äº†ç‰©ä½“çš„åˆšæ€§ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘çš„ç‰©ç†çœŸå®æ„Ÿå—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†è¿™ä¸€èŒƒå¼ï¼Œå½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç§°ä¸ºæ¨¡ä»¿å‘ç°å¾ªç¯ï¼ˆMDcycleï¼‰ï¼Œä»¥ä¾¿åœ¨å……åˆ†ä¿ç•™æ¨¡å‹åˆ©ç”¨ç‰©ç†åé¦ˆèƒ½åŠ›çš„åŒæ—¶è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10909",
            "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
            "url": "https://huggingface.co/papers/2601.10909",
            "abstract": "A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.",
            "score": 2,
            "issue_id": 640,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "a6f74fdfe5df1e23",
            "authors": [
                "Chuqiao Li",
                "Xianghui Xie",
                "Yong Cao",
                "Andreas Geiger",
                "Gerard Pons-Moll"
            ],
            "affiliations": [
                "Max Planck Institute for Informatics, Saarland Informatics Campus, Germany",
                "Tubingen AI Center, University of Tubingen, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10909.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ÑŒÑ Ñ‚ĞµĞ»Ğ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ‚ĞµĞ»Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FrankenMotion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ÑŒÑ Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸ (Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»Ğ°), Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FrankenMotion: Fine-Grained Control of Human Motion Generation from Text",
                    "desc": "This paper presents a novel framework called FrankenMotion for generating human motion from text prompts with detailed control over individual body parts. It addresses the limitations of previous methods by introducing a new dataset that includes atomic, temporally-aware annotations for part-level movements. By utilizing large language models, the framework allows for precise guidance of each body part through its own structured textual prompt. The results show that FrankenMotion significantly outperforms existing models, demonstrating its ability to create complex motions that were not part of the training data."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦æ§åˆ¶çš„äººä½“è¿åŠ¨ç”Ÿæˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬æç¤ºç”Ÿæˆäººä½“è¿åŠ¨ï¼Œå¹¶å®ç°ç»†ç²’åº¦çš„éƒ¨ä½çº§æ§åˆ¶ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„è¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«åŸå­çº§ã€æ—¶é—´æ„ŸçŸ¥çš„éƒ¨ä½æ–‡æœ¬æ³¨é‡Šï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶ä¸ªåˆ«èº«ä½“éƒ¨ä½æ—¶çš„å±€é™æ€§ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ•æ‰äº†ä¸åŒæ­¥ä¸”è¯­ä¹‰æ˜ç¡®çš„éƒ¨ä½è¿åŠ¨ï¼Œå…·æœ‰ç²¾ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFrankenMotionåœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢ä¼˜äºæ‰€æœ‰ä¹‹å‰çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆè®­ç»ƒæœŸé—´æœªè§è¿‡çš„è¿åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11354",
            "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
            "url": "https://huggingface.co/papers/2601.11354",
            "abstract": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
            "score": 0,
            "issue_id": 640,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "91ba6b67dda55368",
            "authors": [
                "Weiyi Wang",
                "Xinchi Chen",
                "Jingjing Gong",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "OpenMOSS Team",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11354.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#benchmark",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ ĞºĞ¾ÑĞ¼Ğ¾ÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° AstroReason-Bench â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾ÑĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ‚Ñ€Ğ°ÑĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ½Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ—ĞµĞ¼Ğ»Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "AstroReason-Bench: Testing LLMs in Real-World Space Planning",
                    "desc": "This paper presents AstroReason-Bench, a new benchmark designed to evaluate the performance of agentic Large Language Models (LLMs) in Space Planning Problems (SPP). Unlike previous benchmarks that focus on simpler environments, AstroReason-Bench addresses complex real-world scenarios with strict physical constraints and diverse objectives. The study reveals that current LLMs struggle to compete with specialized solvers in these challenging tasks, indicating limitations in their generalist planning capabilities. AstroReason-Bench aims to provide a rigorous testing ground for future research in agentic planning and improve the development of more effective agents."
                },
                "zh": {
                    "title": "AstroReason-Benchï¼šæ™ºèƒ½è§„åˆ’çš„æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†AstroReason-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½è§„åˆ’çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´è§„åˆ’é—®é¢˜ï¼ˆSPPï¼‰ä¸­ã€‚ç°æœ‰çš„æ™ºèƒ½ä½“åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç¬¦å·æˆ–å¼±åŸºç¡€ç¯å¢ƒä¸­ï¼Œç¼ºä¹å¯¹ç‰©ç†çº¦æŸçš„çœŸå®ä¸–ç•Œé¢†åŸŸçš„ç ”ç©¶ã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›çš„æ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å½“å‰çš„æ™ºèƒ½ä½“åœ¨å¤æ‚çš„ç°å®çº¦æŸä¸‹è¡¨ç°ä¸ä½³ï¼Œè¿œä¸å¦‚ä¸“é—¨çš„æ±‚è§£å™¨ã€‚AstroReason-Benchä¸ºæœªæ¥çš„æ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè¯Šæ–­æ€§çš„æµ‹è¯•å¹³å°ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-16.html",
    "link_next": "2026-01-20.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}