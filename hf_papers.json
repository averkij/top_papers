{
    "date": {
        "ru": "24 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 24",
        "zh": "12Êúà24Êó•"
    },
    "time_utc": "2024-12-24 16:11",
    "weekday": 1,
    "issue_id": 1294,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.14922",
            "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
            "url": "https://huggingface.co/papers/2412.14922",
            "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
            "score": 29,
            "issue_id": 1281,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 19",
                "zh": "12Êúà19Êó•"
            },
            "hash": "9cc4a703e686ea87",
            "authors": [
                "Junyu Luo",
                "Xiao Luo",
                "Kaize Ding",
                "Jingyang Yuan",
                "Zhiping Xiao",
                "Ming Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Peking University",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14922.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üßº",
                "ru": {
                    "title": "–ß–∏—Å—Ç–∞—è –¥–æ–æ–±—É—á–∫–∞: RobustFT –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ RobustFT –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–π –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. RobustFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º—É —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —à—É–º–∞ –∏ –ø–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥–µ–∂–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –ø—è—Ç–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å RobustFT –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å —à—É–º–æ–º."
                },
                "en": {
                    "title": "Enhancing Language Models with Robust Fine-Tuning",
                    "desc": "This paper presents a new framework called RobustFT for supervised fine-tuning (SFT) of large language models (LLMs) that addresses the issue of noisy data in training. The framework includes a multi-expert system for detecting noise in the data, which helps improve the quality of the training process. It also features a context-enhanced strategy for relabeling data, ensuring that only the most relevant and reliable information is used for fine-tuning. Experiments show that RobustFT significantly enhances model performance on downstream tasks, even in the presence of noise."
                },
                "zh": {
                    "title": "ÊûÑÂª∫ÊäóÂô™Â£∞ÁöÑÂæÆË∞ÉÊ°ÜÊû∂ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ",
                    "desc": "ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂú®Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄÇÂ∫îÁâπÂÆöÈ¢ÜÂüüÊàñ‰ªªÂä°‰∏≠Ëµ∑ÁùÄÈáçË¶Å‰ΩúÁî®„ÄÇÁÑ∂ËÄåÔºåÂÆûÈôÖÂ∫îÁî®‰∏≠Êî∂ÈõÜÁöÑÊï∞ÊçÆ‰∏çÂèØÈÅøÂÖçÂú∞ÂåÖÂê´Âô™Â£∞ÔºåËøôÂØπÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏äÁöÑË°®Áé∞ÈÄ†Êàê‰∫ÜÈáçÂ§ßÊåëÊàò„ÄÇÂõ†Ê≠§ÔºåËø´ÂàáÈúÄË¶Å‰∏ÄÁßçÊäóÂô™Â£∞ÁöÑSFTÊ°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫Ê®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ®≥ÂÅ•ÁöÑSFTÊ°ÜÊû∂ÔºàRobustFTÔºâÔºåËØ•Ê°ÜÊû∂Âú®‰∏ãÊ∏∏‰ªªÂä°Êï∞ÊçÆ‰∏äÊâßË°åÂô™Â£∞Ê£ÄÊµãÂíåÈáçÊñ∞Ê†áÊ≥®„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17256",
            "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
            "url": "https://huggingface.co/papers/2412.17256",
            "abstract": "In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.",
            "score": 28,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "1a1ee4818597feae",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Lulu Zhao",
                "Yijun Wang",
                "Zifei Shan",
                "Junxian He"
            ],
            "affiliations": [
                "BAAI",
                "Tencent",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17256.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–ë–∞–ª–∞–Ω—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ò–ò",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞ –≤ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è: —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ) –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–Ω–µ—à–Ω–∏—Ö –Ω–∞–≥—Ä–∞–¥ –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è). –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –æ–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ B-STaR, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ B-STaR —É–ª—É—á—à–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞, –ø—Ä–∏–≤–æ–¥—è –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞."
                },
                "en": {
                    "title": "Balancing Exploration and Exploitation for Self-Improvement in ML Models",
                    "desc": "This paper addresses the challenge of improving machine learning models in the absence of large human-annotated datasets, focusing on self-improvement through iterative training on their own outputs. It identifies two key factors that influence this process: the model's ability to explore diverse responses and the effectiveness of external rewards in selecting high-quality outputs. The authors introduce B-STaR, a framework that dynamically adjusts training configurations to maintain a balance between exploration and exploitation. Experiments show that B-STaR enhances exploratory capabilities and improves overall model performance in reasoning tasks."
                },
                "zh": {
                    "title": "Ëá™ÊàëÊîπËøõÔºöÂπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®ÁöÑÂÖ≥ÈîÆ",
                    "desc": "Âú®Áº∫‰πèÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåËá™ÊàëÊîπËøõÊàê‰∏∫ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑ‰∏ªË¶ÅÊñπÊ≥ï„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜËá™ÊàëÊîπËøõËøáÁ®ã‰∏≠ÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÂìçÂ∫îÁöÑËÉΩÂäõÔºàÊé¢Á¥¢ÔºâÂíåÂ§ñÈÉ®Â•ñÂä±Âú®Âå∫ÂàÜÈ´òË¥®ÈáèÂÄôÈÄâÈ°π‰∏é‰ΩéË¥®ÈáèÂÄôÈÄâÈ°π‰∏≠ÁöÑÊúâÊïàÊÄßÔºàÂà©Áî®Ôºâ„ÄÇÈÄöËøáÂØπÊï∞Â≠¶Êé®ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÊàë‰ª¨ÂèëÁé∞Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂú®Ëø≠‰ª£ËøáÁ®ã‰∏≠ËøÖÈÄü‰∏ãÈôçÔºåËÄåÂ§ñÈÉ®Â•ñÂä±ÁöÑÊúâÊïàÊÄß‰πüÈöè‰πãÂáèÂº±„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜB-STaRÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®Ëø≠‰ª£‰∏≠Ëá™ÊàëË∞ÉÊï¥ÈÖçÁΩÆÔºå‰ª•Âπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®Ôºå‰ªéËÄå‰ºòÂåñËá™ÊàëÊîπËøõÁöÑÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17451",
            "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2412.17451",
            "abstract": "Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.",
            "score": 21,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "2866001b23a585e1",
            "authors": [
                "Wei Liu",
                "Junlong Li",
                "Xiwen Zhang",
                "Fan Zhou",
                "Yu Cheng",
                "Junxian He"
            ],
            "affiliations": [
                "Helixon Research",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17451.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞: –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MSTaR –¥–ª—è —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with Self-Evolving Training",
                    "desc": "This paper focuses on improving reasoning abilities in Large Multimodal Models (LMMs) through a method called self-evolving training, which allows models to learn from their own outputs. The authors identify three critical factors that influence the effectiveness of this training: the Training Method, Reward Model, and Prompt Variation. They provide a detailed analysis of how different configurations of these factors can optimize multimodal reasoning performance. The study culminates in the development of a framework named MSTaR, which demonstrates significant improvements in reasoning tasks across various model sizes and benchmarks without requiring additional human annotations."
                },
                "zh": {
                    "title": "Ëá™ÊàëËøõÂåñËÆ≠ÁªÉÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜËá™ÊàëËøõÂåñËÆ≠ÁªÉÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑÂ∫îÁî®ÔºåÂº∫Ë∞É‰∫ÜÊé®ÁêÜËÉΩÂäõÂØπÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ËØÜÂà´‰∫ÜÂΩ±ÂìçËÆ≠ÁªÉÊïàÊûúÁöÑ‰∏â‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöËÆ≠ÁªÉÊñπÊ≥ï„ÄÅÂ•ñÂä±Ê®°ÂûãÂíåÊèêÁ§∫Âèò‰ΩìÔºåÂπ∂Á≥ªÁªüÂú∞ÂàÜÊûê‰∫ÜËøô‰∫õÂõ†Á¥†ÁöÑ‰∏çÂêåÈÖçÁΩÆ„ÄÇÁ†îÁ©∂ÁªìÊûúÊèê‰æõ‰∫Ü‰∏ÄÂ•óÊúÄ‰Ω≥ÂÆûË∑µÔºåÊó®Âú®‰ºòÂåñÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMSTaRÊ°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫ÜËá™ÊàëËøõÂåñËÆ≠ÁªÉÂú®‰∏çÂêåËßÑÊ®°Ê®°Âûã‰∏äÁöÑÊôÆÈÅçÊúâÊïàÊÄßÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÈ¢ÑÂÖàËøõÂåñÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17153",
            "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
            "url": "https://huggingface.co/papers/2412.17153",
            "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3times speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.",
            "score": 17,
            "issue_id": 1283,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 22",
                "zh": "12Êúà22Êó•"
            },
            "hash": "b1968a6263a19386",
            "authors": [
                "Enshu Liu",
                "Xuefei Ning",
                "Yu Wang",
                "Zinan Lin"
            ],
            "affiliations": [
                "Department of EE, Tsinghua University",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17153.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Distilled Decoding (DD) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö (AR) –º–æ–¥–µ–ª–µ–π. DD –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É flow matching –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π AR –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –æ–¥–∏–Ω –∏–ª–∏ –¥–≤–∞ —à–∞–≥–∞ –≤–º–µ—Å—Ç–æ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è—è —Ä–∞–±–æ—Ç—É AR –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DD –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 6-217 —Ä–∞–∑ —Å –ø—Ä–∏–µ–º–ª–µ–º—ã–º —É—Ö—É–¥—à–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID."
                },
                "en": {
                    "title": "Revolutionizing Autoregressive Generation: Fast and Efficient with Distilled Decoding!",
                    "desc": "This paper introduces Distilled Decoding (DD), a novel approach to enhance the efficiency of autoregressive (AR) models in generating text and images. Traditional AR models generate outputs token-by-token, which can be slow, but DD aims to enable generation in just one or two steps by creating a deterministic mapping from a Gaussian distribution to the AR model's output distribution. By training a network to distill this mapping, DD allows for rapid generation without requiring the original training data, making it more practical for real-world applications. The results show significant speed-ups in generation times while maintaining acceptable quality, challenging the belief that AR models are inherently slow."
                },
                "zh": {
                    "title": "Ëí∏È¶èËß£Á†ÅÔºöÂä†ÈÄüËá™ÂõûÂΩíÊ®°ÂûãÁîüÊàêÁöÑÈù©ÂëΩÊÄßÊñπÊ≥ï",
                    "desc": "Ëá™ÂõûÂΩíÔºàARÔºâÊ®°ÂûãÂú®ÊñáÊú¨ÂíåÂõæÂÉèÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÈÄê‰∏™ÁîüÊàêÁöÑËøáÁ®ãÔºåÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ëí∏È¶èËß£Á†ÅÔºàDDÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑARÊ®°ÂûãÈÄÇÂ∫î‰∏∫‰ªÖÈúÄ‰∏ÄÊ≠•Êàñ‰∏§Ê≠•ÁîüÊàêËæìÂá∫„ÄÇDDÈÄöËøáÊµÅÂåπÈÖçÂàõÂª∫‰ªéÈ´òÊñØÂàÜÂ∏ÉÂà∞ARÊ®°ÂûãËæìÂá∫ÂàÜÂ∏ÉÁöÑÁ°ÆÂÆöÊÄßÊò†Â∞ÑÔºå‰ªéËÄåÂÆûÁé∞Âø´ÈÄüÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDDÂú®Â§ö‰∏™ÂõæÂÉèARÊ®°Âûã‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂèØÊé•ÂèóÁöÑÁîüÊàêË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17747",
            "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
            "url": "https://huggingface.co/papers/2412.17747",
            "abstract": "Techniques enabling large language models (LLMs) to \"think more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",
            "score": 14,
            "issue_id": 1283,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "b3ee8264ebbee41e",
            "authors": [
                "Luyang Liu",
                "Jonas Pfeiffer",
                "Jiaxing Wu",
                "Jun Xie",
                "Arthur Szlam"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17747.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ñ–ª–∞–π–Ω-—Å–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: –£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –æ—Ñ–ª–∞–π–Ω-—Å–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞. –≠—Ç–æ—Ç —Å–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫—ç—à–µ–º –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –¥–æ–±–∞–≤–ª—è—è –≤ –Ω–µ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –û–±—É—á–µ–Ω–∏–µ —Å–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏ –ø–æ–¥ –∑–∞–¥–∞—á—É."
                },
                "en": {
                    "title": "Enhancing LLMs with Offline Cache Augmentation for Better Reasoning",
                    "desc": "This paper presents a method to enhance large language models (LLMs) by using an offline coprocessor that improves the model's key-value (kv) cache. The coprocessor adds latent embeddings to the cache, which helps the model generate better responses by refining its reasoning process. By training the coprocessor with language modeling loss while keeping the main decoder unchanged, the system can learn to optimize its computations without increasing latency. Experimental results show that this cache augmentation leads to lower perplexity and better performance on various reasoning tasks, even without specific training for those tasks."
                },
                "zh": {
                    "title": "Â¢ûÂº∫ÁºìÂ≠òÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™Á¶ªÁ∫øÂçèÂ§ÑÁêÜÂô®ÔºåËØ•ÂçèÂ§ÑÁêÜÂô®Âú®Ê®°ÂûãÁöÑÈîÆÂÄºÁºìÂ≠ò‰∏äÊìç‰ΩúÔºå‰ªéËÄåÊèêÈ´òÂêéÁª≠Ëß£Á†ÅÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÖÅËÆ∏Ê®°Âûã‰ª•Á´ØÂà∞Á´ØÂèØÂæÆÂàÜÁöÑÊñπÂºèÂ≠¶‰π†Â¶Ç‰ΩïÂ∞ÜÈ¢ùÂ§ñÁöÑËÆ°ÁÆóÊèêÁÇºÂà∞ÂÖ∂ÁºìÂ≠ò‰∏≠„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ¢ûÂº∫ÁºìÂ≠òÂêéÔºåËß£Á†ÅÂô®Âú®Â§ö‰∏™ÂêéÁª≠Ê†áËÆ∞‰∏äË°®Áé∞Âá∫Êõ¥‰ΩéÁöÑÂõ∞ÊÉëÂ∫¶Ôºå‰∏îÂú®Êé®ÁêÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÊÄßËÉΩÊòæËëóÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17805",
            "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
            "url": "https://huggingface.co/papers/2412.17805",
            "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~https://yzxing87.github.io/vae/{https://yzxing87.github.io/vae/}.",
            "score": 13,
            "issue_id": 1285,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "58490715e8055e65",
            "authors": [
                "Yazhou Xing",
                "Yang Fei",
                "Yingqing He",
                "Jingye Chen",
                "Jiaxin Xie",
                "Xiaowei Chi",
                "Qifeng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.17805.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–æ—â–Ω—ã–π –≤–∏–¥–µ–æ-–∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ —Ä–∞–∑–º—ã—Ç–∏—è –¥–≤–∏–∂–µ–Ω–∏—è. –û–Ω–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –µ—ë —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Enhancing Video Generation with Temporal-Aware Compression and Text Guidance",
                    "desc": "This paper introduces a new video Variational Autoencoder (VAE) that improves video generation by addressing the limitations of existing models. It focuses on enhancing both spatial and temporal compression to avoid issues like motion blur and detail loss. The model incorporates text guidance from text-to-video datasets, which boosts the quality of video reconstruction. Additionally, it is trained on both images and videos, allowing it to effectively encode and decode both types of data, leading to superior performance compared to previous methods."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜÈ¢ëÁºñÁ†ÅË¥®ÈáèÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑËßÜÈ¢ëÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁºñÁ†ÅÁöÑË¥®ÈáèÂíåÊïàÁéá„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•Êó∂Â∫èÊÑüÁü•ÁöÑÁ©∫Èó¥ÂéãÁº©ÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫Ü‰º†Áªü3D VAEÂú®ËøêÂä®Ê®°Á≥äÂíåÁªÜËäÇÂ§±ÁúüÁöÑÈóÆÈ¢ò„ÄÇÂêåÊó∂ÔºåÁªìÂêàËΩªÈáèÁ∫ßÁöÑËøêÂä®ÂéãÁº©Ê®°ÂûãÔºåËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÊó∂Â∫èÂéãÁº©ÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÂà©Áî®ÊñáÊú¨Âà∞ËßÜÈ¢ëÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊñáÊú¨‰ø°ÊÅØÔºåÊèêÂçá‰∫ÜÈáçÂª∫Ë¥®ÈáèÔºåÁâπÂà´ÊòØÂú®ÁªÜËäÇ‰øùÁïôÂíåÊó∂Â∫èÁ®≥ÂÆöÊÄßÊñπÈù¢„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15118",
            "title": "Outcome-Refining Process Supervision for Code Generation",
            "url": "https://huggingface.co/papers/2412.15118",
            "abstract": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS",
            "score": 11,
            "issue_id": 1284,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 19",
                "zh": "12Êúà19Êó•"
            },
            "hash": "88c43fc4e946d78c",
            "authors": [
                "Zhuohao Yu",
                "Weizheng Gu",
                "Yidong Wang",
                "Zhengran Zeng",
                "Jindong Wang",
                "Wei Ye",
                "Shikun Zhang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15118.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#plp",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ 'Outcome-Refining Process Supervision', –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–µ—à–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing Code Generation with Outcome-Refining Supervision",
                    "desc": "This paper introduces Outcome-Refining Process Supervision (ORPS), a new method to improve code generation in large language models, especially for complex programming tasks. Instead of relying on traditional reward models, ORPS supervises the reasoning process by refining outcomes using concrete execution signals. This approach allows models to explore multiple solution paths simultaneously, enhancing their ability to solve challenging problems. The results show that ORPS significantly boosts the accuracy and efficiency of various models on competitive programming tasks, demonstrating the importance of structured reasoning and reliable verification."
                },
                "zh": {
                    "title": "ÁªìÊûúÁ≤æÁÇºÔºöÊèêÂçáÁºñÁ®ã‰ªªÂä°ÁöÑÊô∫ËÉΩÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰ª£Á†ÅÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅÊ∑±Â∫¶ÁÆóÊ≥ïÊé®ÁêÜÁöÑÂ§çÊùÇÁºñÁ®ã‰ªªÂä°‰∏≠Â∏∏Â∏∏ÈÅáÂà∞Âõ∞Èöæ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõëÁù£Â≠¶‰π†ËåÉÂºè‚Äî‚ÄîÁªìÊûúÁ≤æÁÇºËøáÁ®ãÁõëÁù£ÔºåÊó®Âú®Â∞ÜÁªìÊûúÁ≤æÁÇºÊú¨Ë∫´‰Ωú‰∏∫ÈúÄË¶ÅÁõëÁù£ÁöÑËøáÁ®ã„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂÖ∑‰ΩìÁöÑÊâßË°å‰ø°Âè∑Êù•ÊåáÂØºÊé®ÁêÜÊ≠•È™§ÁöÑÁõëÁù£ÔºåÂêåÊó∂ÈááÁî®Ê†ëÁä∂ÁªìÊûÑÊé¢Á¥¢Êù•ÂêåÊó∂Áª¥Êä§Â§ö‰∏™Ëß£ÂÜ≥ÊñπÊ°àËΩ®Ëøπ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÂæóÂç≥‰ΩøÊòØËæÉÂ∞èÁöÑÊ®°Âûã‰πüËÉΩÂú®Á´û‰∫âÊÄßÁºñÁ®ã‰ªªÂä°‰∏≠ÂÆûÁé∞È´òÊàêÂäüÁéáÂíåÊÄßËÉΩÊåáÊ†áÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ≠£Á°ÆÊÄßÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16926",
            "title": "Revisiting In-Context Learning with Long Context Language Models",
            "url": "https://huggingface.co/papers/2412.16926",
            "abstract": "In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.",
            "score": 11,
            "issue_id": 1281,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 22",
                "zh": "12Êúà22Êó•"
            },
            "hash": "764c013157322e0d",
            "authors": [
                "Jinheon Baek",
                "Sun Jae Lee",
                "Prakhar Gupta",
                "Geunseob",
                "Oh",
                "Siddharth Dalmia",
                "Prateek Kolhar"
            ],
            "affiliations": [
                "Google DeepMind",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16926.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "üìè",
                "ru": {
                    "title": "–ë–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ - –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏–∫ –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LCLM) –º–µ—Ç–æ–¥–æ–º –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 18 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ –¥–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—Å—Ç—ã–º —Å–ª—É—á–∞–π–Ω—ã–º –≤—ã–±–æ—Ä–æ–º. –û—Å–Ω–æ–≤–Ω–æ–π –≤—ã–∑–æ–≤ —Ç–µ–ø–µ—Ä—å –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–µ –≤ –≤—ã–±–æ—Ä–µ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ –≤ —Å–±–æ—Ä–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞. –ü—Ä–æ—Å—Ç–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª–∏–ª–∞ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å ICL –Ω–∞ 5% –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Maximizing ICL Performance with Long Contexts: Simplicity Over Sophistication",
                    "desc": "In-Context Learning (ICL) allows language models to make predictions based on examples in their input. With the introduction of Long Context Language Models (LCLMs), the number of examples that can be included has increased, prompting a reevaluation of example selection methods. The study finds that complex selection techniques do not significantly outperform simple random sampling in many-shot scenarios. Instead, the focus shifts to ensuring enough examples fill the context window, and using data augmentation can enhance ICL performance by 5%."
                },
                "zh": {
                    "title": "Èïø‰∏ä‰∏ãÊñáÊ®°Âûã‰∏ãÁöÑÁ§∫‰æãÈÄâÊã©Êñ∞ÊåëÊàò",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®Èïø‰∏ä‰∏ãÊñáËØ≠Ë®ÄÊ®°ÂûãÔºàLCLMsÔºâ‰∏≠ÔºåÁ§∫‰æãÈÄâÊã©ÂØπ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàICLÔºâÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Â§çÊùÇÁöÑÁ§∫‰æãÈÄâÊã©ÊäÄÊúØÂπ∂Êú™ÊòæËëóÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÁÆÄÂçïÁöÑÈöèÊú∫ÈÄâÊã©ÊñπÊ≥ïÂú®ËÆ∏Â§öÊÉÖÂÜµ‰∏ãË°®Áé∞ËâØÂ•Ω„ÄÇÈöèÁùÄLCLMsÁöÑÂá∫Áé∞ÔºåICLÁöÑÊåëÊàòÂ∑≤‰ªéÈÄâÊã©ÊúÄÊúâÊïàÁöÑÁ§∫‰æãËΩ¨Âèò‰∏∫Êî∂ÈõÜË∂≥Â§üÁöÑÁ§∫‰æã‰ª•Â°´ÂÖÖ‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇÈÄöËøáÁÆÄÂçïÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÊàë‰ª¨Âú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫ÜICLÊÄßËÉΩÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞5%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16429",
            "title": "LearnLM: Improving Gemini for Learning",
            "url": "https://huggingface.co/papers/2412.16429",
            "abstract": "Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of pedagogical instruction following, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\\% over GPT-4o, 11\\% over Claude 3.5, and 13\\% over the Gemini 1.5 Pro model LearnLM was based on.",
            "score": 9,
            "issue_id": 1286,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 21",
                "zh": "12Êúà21Êó•"
            },
            "hash": "6cae08cebf8bc6ae",
            "authors": [
                "LearnLM Team",
                "Abhinit Modi",
                "Aditya Srikanth Veerubhotla",
                "Aliya Rysbek",
                "Andrea Huber",
                "Brett Wiltshire",
                "Brian Veprek",
                "Daniel Gillick",
                "Daniel Kasenberg",
                "Derek Ahmed",
                "Irina Jurenka",
                "James Cohan",
                "Jennifer She",
                "Julia Wilkowski",
                "Kaiz Alarakyia",
                "Kevin McKee",
                "Lisa Wang",
                "Markus Kunesch",
                "Mike Schaekermann",
                "Miruna P√Æslar",
                "Nikhil Joshi",
                "Parsa Mahmoudieh",
                "Paul Jhun",
                "Sara Wiltberger",
                "Shakir Mohamed",
                "Shashank Agarwal",
                "Shubham Milind Phal",
                "Sun Jae Lee",
                "Theofilos Strinopoulos",
                "Wei-Jen Ko",
                "Amy Wang",
                "Ankit Anand",
                "Avishkar Bhoopchand",
                "Dan Wild",
                "Divya Pandya",
                "Filip Bar",
                "Garth Graham",
                "Holger Winnemoeller",
                "Mahvish Nagda",
                "Prateek Kolhar",
                "Renee Schneider",
                "Shaojian Zhu",
                "Stephanie Chan",
                "Steve Yadlowsky",
                "Viknesh Sounderajah",
                "Yannis Assael"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16429.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#alignment",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "üéì",
                "ru": {
                    "title": "–ò–ò-—É—á–∏—Ç–µ–ª—å: –≥–∏–±–∫–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ '—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º', –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å LearnLM, –æ–±—É—á–µ–Ω–Ω–∞—è —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ò–ò-—Å–∏—Å—Ç–µ–º –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Å—Ñ–µ—Ä–µ."
                },
                "en": {
                    "title": "Empowering AI with Human-Like Teaching Skills",
                    "desc": "This paper discusses how current generative AI systems primarily provide information instead of facilitating learning like a human tutor. The authors propose a new approach called pedagogical instruction following, which allows for the inclusion of specific teaching behaviors in the training and evaluation of AI models. This method gives educators the flexibility to define desired pedagogical attributes without being tied to a single definition of pedagogy. The results show that the LearnLM model, trained with this approach, significantly outperforms other models in various learning scenarios, indicating its effectiveness in educational applications."
                },
                "zh": {
                    "title": "ÊïôÂ≠¶Êåá‰ª§Ë∑üÈöèÔºöÊèêÂçáÁîüÊàêÂºèAIÁöÑÂ≠¶‰π†ËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂú®ÊïôËÇ≤‰∏≠ÁöÑÂ∫îÁî®ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂç≥ÊïôÂ≠¶Êåá‰ª§Ë∑üÈöè„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊïôÂ∏àÊàñÂºÄÂèëËÄÖÂèØ‰ª•ÊåáÂÆöÊúüÊúõÁöÑÊïôÂ≠¶Ë°å‰∏∫ÔºåËÄå‰∏çÈúÄË¶ÅÂõ∫ÂÆöÁöÑÊïôÂ≠¶ÂÆö‰πâ„ÄÇËøôÁßçÁÅµÊ¥ªÊÄß‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÂ≠¶‰π†Âú∫ÊôØÔºåÂπ∂ÊèêÂçáÂÖ∂Â≠¶‰π†ËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®ÊïôÂ≠¶Êåá‰ª§Ë∑üÈöèËÆ≠ÁªÉÁöÑLearnLMÊ®°ÂûãÂú®Â§öÁßçÂ≠¶‰π†Âú∫ÊôØ‰∏≠ÂæóÂà∞‰∫Ü‰∏ìÂÆ∂ËØÑÂÆ°ÁöÑÈ´òÂ∫¶ËÆ§ÂèØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16720",
            "title": "OpenAI o1 System Card",
            "url": "https://huggingface.co/papers/2412.16720",
            "abstract": "The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",
            "score": 8,
            "issue_id": 1286,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 21",
                "zh": "12Êúà21Êó•"
            },
            "hash": "5eed348dd7fb2826",
            "authors": [
                "OpenAI",
                ":",
                "Aaron Jaech",
                "Adam Kalai",
                "Adam Lerer",
                "Adam Richardson",
                "Ahmed El-Kishky",
                "Aiden Low",
                "Alec Helyar",
                "Aleksander Madry",
                "Alex Beutel",
                "Alex Carney",
                "Alex Iftimie",
                "Alex Karpenko",
                "Alex Tachard Passos",
                "Alexander Neitz",
                "Alexander Prokofiev",
                "Alexander Wei",
                "Allison Tam",
                "Ally Bennett",
                "Ananya Kumar",
                "Andre Saraiva",
                "Andrea Vallone",
                "Andrew Duberstein",
                "Andrew Kondrich",
                "Andrey Mishchenko",
                "Andy Applebaum",
                "Angela Jiang",
                "Ashvin Nair",
                "Barret Zoph",
                "Behrooz Ghorbani",
                "Ben Rossen",
                "Benjamin Sokolowsky",
                "Boaz Barak",
                "Bob McGrew",
                "Borys Minaiev",
                "Botao Hao",
                "Bowen Baker",
                "Brandon Houghton",
                "Brandon McKinzie",
                "Brydon Eastman",
                "Camillo Lugaresi",
                "Cary Bassin",
                "Cary Hudson",
                "Chak Ming Li",
                "Charles de Bourcy",
                "Chelsea Voss",
                "Chen Shen",
                "Chong Zhang",
                "Chris Koch",
                "Chris Orsinger",
                "Christopher Hesse",
                "Claudia Fischer",
                "Clive Chan",
                "Dan Roberts",
                "Daniel Kappler",
                "Daniel Levy",
                "Daniel Selsam",
                "David Dohan",
                "David Farhi",
                "David Mely",
                "David Robinson",
                "Dimitris Tsipras",
                "Doug Li",
                "Dragos Oprica",
                "Eben Freeman",
                "Eddie Zhang",
                "Edmund Wong",
                "Elizabeth Proehl",
                "Enoch Cheung",
                "Eric Mitchell",
                "Eric Wallace",
                "Erik Ritter",
                "Evan Mays",
                "Fan Wang",
                "Felipe Petroski Such",
                "Filippo Raso",
                "Florencia Leoni",
                "Foivos Tsimpourlas",
                "Francis Song",
                "Fred von Lohmann",
                "Freddie Sulit",
                "Geoff Salmon",
                "Giambattista Parascandolo",
                "Gildas Chabot",
                "Grace Zhao",
                "Greg Brockman",
                "Guillaume Leclerc",
                "Hadi Salman",
                "Haiming Bao",
                "Hao Sheng",
                "Hart Andrin",
                "Hessam Bagherinezhad",
                "Hongyu Ren",
                "Hunter Lightman",
                "Hyung Won Chung",
                "Ian Kivlichan",
                "Ian O'Connell",
                "Ian Osband",
                "Ignasi Clavera Gilaberte",
                "Ilge Akkaya",
                "Ilya Kostrikov",
                "Ilya Sutskever",
                "Irina Kofman",
                "Jakub Pachocki",
                "James Lennon",
                "Jason Wei",
                "Jean Harb",
                "Jerry Twore",
                "Jiacheng Feng",
                "Jiahui Yu",
                "Jiayi Weng",
                "Jie Tang",
                "Jieqi Yu",
                "Joaquin Qui√±onero Candela",
                "Joe Palermo",
                "Joel Parish",
                "Johannes Heidecke",
                "John Hallman",
                "John Rizzo",
                "Jonathan Gordon",
                "Jonathan Uesato",
                "Jonathan Uesato",
                "Jonathan Ward",
                "Joost Huizinga",
                "Julie Wang",
                "Kai Chen",
                "Kai Xiao",
                "Karan Singhal",
                "Karina Nguyen",
                "Karl Cobbe",
                "Katy Shi",
                "Kayla Wood",
                "Kendra Rimbach",
                "Keren Gu-Lemberg",
                "Keren GuLemberg",
                "Kevin Liu",
                "Kevin Lu",
                "Kevin Stone",
                "Kevin Yu",
                "Lama Ahmad",
                "Lauren Yang",
                "Leo Liu",
                "Leon Maksin",
                "Leyton Ho",
                "Liam Fedus",
                "Lilian Weng",
                "Linden Li",
                "Lindsay McCallum",
                "Lindsey Held",
                "Lorenz Kuhn",
                "Lukas Kondraciuk",
                "Lukasz Kaiser",
                "Luke Metz",
                "Madelaine Boyd",
                "Maja Trebacz",
                "Manas Joglekar",
                "Mark Chen",
                "Marko Tintor",
                "Mason Meyer",
                "Matt Jones",
                "Matt Kaufer",
                "Max Schwarzer",
                "Meghan Shah",
                "Mehmet Yatbaz",
                "Melody Guan",
                "Mengyuan Xu",
                "Mengyuan Yan",
                "Mia Glaese",
                "Mianna Chen",
                "Mianna Chen",
                "Michael Lampe",
                "Michael Malek",
                "Michele Wang",
                "Michelle Fradin",
                "Mike McClay",
                "Mikhail Pavlov",
                "Miles Wang",
                "Mingxuan Wang",
                "Mira Murati",
                "Mo Bavarian",
                "Mostafa Rohaninejad",
                "Nat McAleese",
                "Neil Chowdhury",
                "Neil Chowdhury",
                "Nick Ryder",
                "Nikolas Tezak",
                "Noam Brown",
                "Ofir Nachum",
                "Oleg Boiko",
                "Oleg Murk",
                "Olivia Watkins",
                "Patrick Chao",
                "Paul Ashbourne",
                "Pavel Izmailov",
                "Peter Zhokhov",
                "Rachel Dias",
                "Rahul Arora",
                "Randall Lin",
                "Rapha Gontijo Lopes",
                "Raz Gaon",
                "Reah Miyara",
                "Reimar Leike",
                "Renny Hwang",
                "Rhythm Garg",
                "Robin Brown",
                "Roshan James",
                "Rui Shu",
                "Ryan Cheu",
                "Ryan Greene",
                "Saachi Jain",
                "Sam Altman",
                "Sam Toizer",
                "Sam Toyer",
                "Samuel Miserendino",
                "Sandhini Agarwal",
                "Santiago Hernandez",
                "Sasha Baker",
                "Scott McKinney",
                "Scottie Yan",
                "Shengjia Zhao",
                "Shengli Hu",
                "Shibani Santurkar",
                "Shraman Ray Chaudhuri",
                "Shuyuan Zhang",
                "Siyuan Fu",
                "Spencer Papay",
                "Steph Lin",
                "Suchir Balaji",
                "Suvansh Sanjeev",
                "Szymon Sidor",
                "Tal Broda",
                "Aidan Clark",
                "Tao Wang",
                "Taylor Gordon",
                "Ted Sanders",
                "Tejal Patwardhan",
                "Thibault Sottiaux",
                "Thomas Degry",
                "Thomas Dimson",
                "Tianhao Zheng",
                "Timur Garipov",
                "Tom Stasi",
                "Trapit Bansal",
                "Trevor Creech",
                "Troy Peterson",
                "Tyna Eloundou",
                "Valerie Qi",
                "Vineet Kosaraju",
                "Vinnie Monaco",
                "Vitchyr Pong",
                "Vlad Fomenko",
                "Weiyi Zheng",
                "Wenda Zhou",
                "Wes McCabe",
                "Wojciech Zaremba",
                "Yann Dubois",
                "Yinghai Lu",
                "Yining Chen",
                "Young Cha",
                "Yu Bai",
                "Yuchen He",
                "Yuchen Zhang",
                "Yunyun Wang",
                "Zheng Shao",
                "Zhuohan Li"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16720.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#alignment",
                    "#benchmark",
                    "#healthcare",
                    "#rl",
                    "#training",
                    "#ethics"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£—Å–∏–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò —á–µ—Ä–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑",
                    "desc": "–ú–æ–¥–µ–ª–∏ —Å–µ—Ä–∏–∏ o1 –æ–±—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π. –≠—Ç–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –ø—É—Ç–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –ú–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –ø–æ–ª–∏—Ç–∏–∫–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ –¥–µ–ª–∏–±–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ —Ç–∞–∫–∏–º —Ä–∏—Å–∫–∞–º, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã—Ö —Å–æ–≤–µ—Ç–æ–≤, –≤—ã–±–æ—Ä —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ –ø–æ–¥–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ–±—Ö–æ–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Enhancing AI Safety through Chain of Thought Reasoning",
                    "desc": "The o1 model series utilizes large-scale reinforcement learning to enhance reasoning through a chain of thought approach. This method improves the models' ability to align with safety policies when faced with potentially harmful prompts, thereby increasing their robustness. The models demonstrate superior performance on benchmarks related to generating unsafe content and responding to biased queries. The findings highlight the importance of developing strong alignment techniques and rigorous risk management strategies to ensure the safe deployment of advanced AI systems."
                },
                "zh": {
                    "title": "ÊÄùÁª¥ÈìæÊé®ÁêÜÔºöÊèêÂçáÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÂÖ≥ÈîÆ",
                    "desc": "o1Ê®°ÂûãÁ≥ªÂàóÈÄöËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§ü‰ΩøÁî®ÊÄùÁª¥ÈìæËøõË°åÊé®ÁêÜ„ÄÇËøôÁßçÂÖàËøõÁöÑÊé®ÁêÜËÉΩÂäõ‰∏∫ÊèêÈ´òÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÁ®≥ÂÅ•ÊÄßÊèê‰æõ‰∫ÜÊñ∞ÁöÑÈÄîÂæÑ„ÄÇÁâπÂà´ÊòØÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãËÉΩÂ§üÂú®ÂìçÂ∫îÊΩúÂú®‰∏çÂÆâÂÖ®ÁöÑÊèêÁ§∫Êó∂ÔºåËÄÉËôëÂÆâÂÖ®ÊîøÁ≠ñÁöÑ‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÂÆûÁé∞Ê∑±ÊÄùÁÜüËôëÁöÑÂØπÈΩê„ÄÇËøôÈ°πÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÊûÑÂª∫Á®≥ÂÅ•ÁöÑÂØπÈΩêÊñπÊ≥ïÂíåËøõË°åÂÖ®Èù¢ÁöÑÈ£éÈô©ÁÆ°ÁêÜÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17498",
            "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
            "url": "https://huggingface.co/papers/2412.17498",
            "abstract": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at https://github.com/krystalan/DRT-o1",
            "score": 7,
            "issue_id": 1286,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "10ee2563b13fe4ff",
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Yunlong Liang",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17498.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#reasoning",
                    "#machine_translation",
                    "#multilingual",
                    "#long_context"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–î–ª–∏–Ω–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DRT-o1 - –º–æ–¥–µ–ª—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–ª–∏–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞, —Å–æ–≤–µ—Ç–Ω–∏–∫–∞ –∏ –æ—Ü–µ–Ω—â–∏–∫–∞, –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å DRT-o1, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º BLEU –∏ CometScore. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–ª–æ–∂–Ω—ã—Ö –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."
                },
                "en": {
                    "title": "Enhancing Literary Translation with Long Chain-of-Thought Reasoning",
                    "desc": "This paper presents DRT-o1, a novel approach to enhance neural machine translation (MT) by leveraging long chain-of-thought (CoT) reasoning. The method specifically addresses the challenges of translating literature that contains similes and metaphors, which often require deeper semantic understanding due to cultural nuances. DRT-o1 employs a multi-agent framework where a translator iteratively refines translations with guidance from an advisor and feedback from an evaluator. Experimental results indicate that DRT-o1 significantly improves translation quality, as evidenced by higher BLEU and CometScore metrics compared to existing models."
                },
                "zh": {
                    "title": "ÈïøÈìæÊÄùÁª¥Âä©ÂäõÊñáÂ≠¶ÁøªËØëÁöÑÁ™ÅÁ†¥",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ•ûÁªèÊú∫Âô®ÁøªËØëÊ®°ÂûãDRT-o1ÔºåÊó®Âú®Â∞ÜÈïøÈìæÊÄùÁª¥ÔºàCoTÔºâÁöÑÊàêÂäüÂ∫îÁî®‰∫éÊñáÂ≠¶ÁøªËØë„ÄÇÁî±‰∫éÊñáÂåñÂ∑ÆÂºÇÔºåÁøªËØëÂåÖÂê´ÊØîÂñªÂíåÈöêÂñªÁöÑÊñáÊú¨ÈùûÂ∏∏Âõ∞ÈöæÔºå‰º†ÁªüÁöÑÈÄêÂ≠óÁøªËØëÂæÄÂæÄÊó†Ê≥ïÊúâÊïà‰º†ËææÂéüÊÑè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÈÄöËøáËø≠‰ª£ÁøªËØëÂíåËØÑ‰º∞Êù•‰ºòÂåñÁøªËØëÁªìÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDRT-o1Âú®ÊñáÂ≠¶ÁøªËØë‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁøªËØëË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17589",
            "title": "PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World",
            "url": "https://huggingface.co/papers/2412.17589",
            "abstract": "Imagine a world where AI can handle your work while you sleep - organizing your research materials, drafting a report, or creating a presentation you need for tomorrow. However, while current digital agents can perform simple tasks, they are far from capable of handling the complex real-world work that humans routinely perform. We present PC Agent, an AI system that demonstrates a crucial step toward this vision through human cognition transfer. Our key insight is that the path from executing simple \"tasks\" to handling complex \"work\" lies in efficiently capturing and learning from human cognitive processes during computer use. To validate this hypothesis, we introduce three key innovations: (1) PC Tracker, a lightweight infrastructure that efficiently collects high-quality human-computer interaction trajectories with complete cognitive context; (2) a two-stage cognition completion pipeline that transforms raw interaction data into rich cognitive trajectories by completing action semantics and thought processes; and (3) a multi-agent system combining a planning agent for decision-making with a grounding agent for robust visual grounding. Our preliminary experiments in PowerPoint presentation creation reveal that complex digital work capabilities can be achieved with a small amount of high-quality cognitive data - PC Agent, trained on just 133 cognitive trajectories, can handle sophisticated work scenarios involving up to 50 steps across multiple applications. This demonstrates the data efficiency of our approach, highlighting that the key to training capable digital agents lies in collecting human cognitive data. By open-sourcing our complete framework, including the data collection infrastructure and cognition completion methods, we aim to lower the barriers for the research community to develop truly capable digital agents.",
            "score": 5,
            "issue_id": 1288,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "03c22a2ede40011d",
            "authors": [
                "Yanheng He",
                "Jiahe Jin",
                "Shijie Xia",
                "Jiadi Su",
                "Runze Fan",
                "Haoyang Zou",
                "Xiangkun Hu",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "SII",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17589.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#transfer_learning",
                    "#open_source",
                    "#dataset",
                    "#data"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–µ—Ä–µ–Ω–æ—Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –ò–ò –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ PC Agent, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ò–ò, —Å–ø–æ—Å–æ–±–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—É—é —Ä–∞–±–æ—Ç—É –∑–∞ —á–µ–ª–æ–≤–µ–∫–∞. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –∑–∞—Ö–≤–∞—Ç–µ –∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è PC Tracker –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º, –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ PC Agent, –æ–±—É—á–µ–Ω–Ω—ã–π –≤—Å–µ–≥–æ –Ω–∞ 133 –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –º–æ–∂–µ—Ç —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏ —Ä–∞–±–æ—Ç—ã, –≤–∫–ª—é—á–∞—é—â–∏–º–∏ –¥–æ 50 —à–∞–≥–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö."
                },
                "en": {
                    "title": "Empowering AI with Human Cognition for Complex Task Management",
                    "desc": "The paper introduces PC Agent, an AI system designed to enhance digital agents' ability to perform complex tasks by learning from human cognitive processes. It utilizes a lightweight infrastructure called PC Tracker to gather high-quality human-computer interaction data, capturing the cognitive context of users. The system employs a two-stage cognition completion pipeline to transform raw data into detailed cognitive trajectories, enabling the agent to understand both actions and underlying thought processes. Preliminary results show that PC Agent can effectively manage intricate tasks with minimal cognitive data, suggesting that efficient data collection is key to developing advanced digital agents."
                },
                "zh": {
                    "title": "ËÆ©AIÂú®‰Ω†Áù°ËßâÊó∂ÂÆåÊàêÂ∑•‰ΩúÔºÅ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫PC AgentÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÔºåÊó®Âú®ÈÄöËøá‰∫∫Á±ªËÆ§Áü•ËΩ¨ÁßªÊù•Â§ÑÁêÜÂ§çÊùÇÁöÑÂ∑•‰Ωú‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏âÈ°πÂÖ≥ÈîÆÂàõÊñ∞ÔºöPC TrackerÁî®‰∫éÈ´òÊïàÊî∂ÈõÜ‰∫∫Êú∫‰∫§‰∫íËΩ®ËøπÔºåËÆ§Áü•ÂÆåÊàêÁÆ°ÈÅìÂ∞ÜÂéüÂßãÊï∞ÊçÆËΩ¨Âåñ‰∏∫‰∏∞ÂØåÁöÑËÆ§Áü•ËΩ®ËøπÔºå‰ª•ÂèäÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁªìÂêàÂÜ≥Á≠ñËßÑÂàíÂíåËßÜËßâÂü∫Á°Ä„ÄÇÂàùÊ≠•ÂÆûÈ™åË°®ÊòéÔºåPC AgentËÉΩÂ§üÂú®‰ªÖ‰ΩøÁî®133‰∏™ËÆ§Áü•ËΩ®ËøπÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ§ÑÁêÜÂ§öËææ50‰∏™Ê≠•È™§ÁöÑÂ§çÊùÇÂ∑•‰ΩúÂú∫ÊôØÔºåÂ±ïÁ§∫‰∫ÜÊï∞ÊçÆÈ´òÊïàÊÄß„ÄÇÈÄöËøáÂºÄÊ∫êÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÔºåÊàë‰ª¨Â∏åÊúõÈôç‰ΩéÁ†îÁ©∂Á§æÂå∫ÂºÄÂèëÁúüÊ≠£Âº∫Â§ßÊï∞Â≠ó‰ª£ÁêÜÁöÑÈó®Êßõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14470",
            "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents",
            "url": "https://huggingface.co/papers/2412.14470",
            "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at https://github.com/thu-coai/Agent-SafetyBench to facilitate further research and innovation in agent safety evaluation and improvement.",
            "score": 5,
            "issue_id": 1287,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 19",
                "zh": "12Êúà19Êó•"
            },
            "hash": "81f372388a7d55e3",
            "authors": [
                "Zhexin Zhang",
                "Shiyao Cui",
                "Yida Lu",
                "Jingzhuo Zhou",
                "Junxiao Yang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "The Conversational AI (CoAI) group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14470.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#open_source",
                    "#security"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–ª—è–µ—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-SafetyBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 349 –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥ –∏ 2000 —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö 8 –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ä–∏—Å–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ 10 —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –æ—Ç–∫–∞–∑–∞. –û—Ü–µ–Ω–∫–∞ 16 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –Ω–∏ –æ–¥–∏–Ω –∏–∑ –Ω–∏—Ö –Ω–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤—ã—à–µ 60%. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –≤ —Ç–µ–∫—É—â–∏—Ö LLM-–∞–≥–µ–Ω—Ç–∞—Ö: –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤."
                },
                "en": {
                    "title": "Enhancing Safety in LLM Agents with Agent-SafetyBench",
                    "desc": "This paper addresses the safety challenges posed by large language models (LLMs) when they are used as agents in interactive environments. It introduces Agent-SafetyBench, a new benchmark that evaluates the safety of these agents across 349 environments and 2,000 test cases, focusing on 8 categories of safety risks. The study reveals that none of the 16 evaluated LLM agents scored above 60% in safety, indicating significant vulnerabilities. The authors highlight critical issues such as lack of robustness and risk awareness, suggesting that current methods like defense prompts are inadequate for ensuring agent safety."
                },
                "zh": {
                    "title": "ÊèêÂçáLLMÊô∫ËÉΩ‰ΩìÂÆâÂÖ®ÊÄßÁöÑÂÖ≥ÈîÆÊåëÊàò",
                    "desc": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰Ωú‰∏∫Êô∫ËÉΩ‰ΩìÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåÂÆÉ‰ª¨Âú®‰∫íÂä®ÁéØÂ¢É‰∏≠ÁöÑÊï¥ÂêàÂ∏¶Êù•‰∫ÜÊñ∞ÁöÑÂÆâÂÖ®ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜAgent-SafetyBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÔºåÁî®‰∫éËØÑ‰º∞LLMÊô∫ËÉΩ‰ΩìÁöÑÂÆâÂÖ®ÊÄß„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫Ü349‰∏™‰∫íÂä®ÁéØÂ¢ÉÂíå2000‰∏™ÊµãËØïÊ°à‰æãÔºåËØÑ‰º∞8Á±ªÂÆâÂÖ®È£éÈô©ÔºåÂπ∂Ë¶ÜÁõñ10ÁßçÂ∏∏ËßÅÁöÑÂ§±Ë¥•Ê®°Âºè„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºå16‰∏™ÊµÅË°åÁöÑLLMÊô∫ËÉΩ‰ΩìÁöÑÂÆâÂÖ®ËØÑÂàÜÂùáÊú™Ë∂ÖËøá60%ÔºåËøôÂá∏Êòæ‰∫ÜÂΩìÂâçLLMÊô∫ËÉΩ‰ΩìÂú®ÂÆâÂÖ®ÊÄßÊñπÈù¢ÁöÑÈáçÂ§ßÊåëÊàò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16686",
            "title": "NILE: Internal Consistency Alignment in Large Language Models",
            "url": "https://huggingface.co/papers/2412.16686",
            "abstract": "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.",
            "score": 5,
            "issue_id": 1282,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 21",
                "zh": "12Êúà21Êó•"
            },
            "hash": "0d1e640729e131e5",
            "authors": [
                "Minda Hu",
                "Qiyuan Zhang",
                "Yufei Wang",
                "Bowei He",
                "Hongru Wang",
                "Jingyan Zhou",
                "Liangyou Li",
                "Yasheng Wang",
                "Chen Ma",
                "Irwin King"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16686.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#data",
                    "#alignment"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ NILE –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (IFT) —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. NILE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö IFT. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM."
                },
                "en": {
                    "title": "Aligning IFT Datasets for Enhanced LLM Performance",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human intentions through Instruction Fine-Tuning (IFT). It highlights the problem of existing IFT datasets containing inconsistent knowledge that conflicts with the LLMs' pre-trained knowledge, which can hinder their performance. To solve this, the authors introduce the NILE framework, which optimizes IFT datasets by aligning them with the internal knowledge of the LLMs. The framework includes a method called Internal Consistency Filtering (ICF) to ensure high consistency, leading to significant performance improvements in LLM evaluations."
                },
                "zh": {
                    "title": "‰ºòÂåñÊï∞ÊçÆÈõÜÔºåÊèêÂçáLLMÊÄßËÉΩÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫NILEÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ºòÂåñÊåá‰ª§ÂæÆË∞ÉÔºàIFTÔºâÊï∞ÊçÆÈõÜÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏é‰∫∫Á±ªÊÑèÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁé∞ÊúâÁöÑIFTÊï∞ÊçÆÈõÜÂ∏∏Â∏∏ÂåÖÂê´‰∏éLLMsÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÂ≠¶‰π†ÁöÑÂÜÖÈÉ®Áü•ËØÜ‰∏ç‰∏ÄËá¥ÁöÑ‰ø°ÊÅØÔºåËøô‰ºöÂΩ±ÂìçIFTÁöÑÊïàÊûú„ÄÇNILEÈÄöËøáÊèêÂèñÁõÆÊ†áÈ¢ÑËÆ≠ÁªÉLLMÁöÑÂÜÖÈÉ®Áü•ËØÜÊù•‰øÆÊ≠£IFTÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÁ≠îÊ°àÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÜÖÈÉ®‰∏ÄËá¥ÊÄßËøáÊª§ÔºàICFÔºâÊñπÊ≥ïÔºå‰ª•Á°Æ‰øùËÆ≠ÁªÉÊ†∑Êú¨‰∏éLLMÁöÑÂÜÖÈÉ®Áü•ËØÜÈ´òÂ∫¶‰∏ÄËá¥„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNILEÂØπIFTÊï∞ÊçÆÈõÜÁöÑ‰ºòÂåñÊòæËëóÊèêÂçá‰∫ÜLLMÂú®Â§ö‰∏™ËØÑ‰º∞Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÔºåËØÅÊòé‰∫ÜÊï∞ÊçÆÈõÜ‰∏éÈ¢ÑËÆ≠ÁªÉÂÜÖÈÉ®Áü•ËØÜ‰∏ÄËá¥ÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17767",
            "title": "ResearchTown: Simulator of Human Research Community",
            "url": "https://huggingface.co/papers/2412.17767",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified and modeled as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire novel research directions.",
            "score": 4,
            "issue_id": 1291,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "c558325e0c6522a3",
            "authors": [
                "Haofei Yu",
                "Zhaochen Hong",
                "Zirui Cheng",
                "Kunlun Zhu",
                "Keyang Xuan",
                "Jinwei Yao",
                "Tao Feng",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17767.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#graphs",
                    "#multimodal",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ: —Å–∏–º—É–ª—è—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ResearchTown - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –º–æ–¥–µ–ª–∏—Ä—É—é—Ç –Ω–∞—É—á–Ω–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –∫–∞–∫ –≥—Ä–∞—Ñ, –≥–¥–µ —É–∑–ª–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏ —Å—Ç–∞—Ç—å–∏, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç TextGNN –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫ ResearchBench, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∑–∞–¥–∞—á–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ResearchTown —Å–ø–æ—Å–æ–±–µ–Ω —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–µ –∏–¥–µ–∏."
                },
                "en": {
                    "title": "Simulating Research Communities with AI: Introducing ResearchTown",
                    "desc": "This paper explores the use of Large Language Models (LLMs) to simulate human research communities through a framework called ResearchTown. It models researchers and academic papers as nodes in an agent-data graph, capturing their collaboration dynamics. The authors introduce TextGNN, a message-passing framework that simulates various research activities like reading and writing papers. The evaluation tool, ResearchBench, assesses the simulation's effectiveness, revealing that ResearchTown can realistically mimic collaborative research and generate innovative interdisciplinary ideas."
                },
                "zh": {
                    "title": "Ê®°ÊãüÁ†îÁ©∂Á§æÂå∫ÔºåÂêØÂèëÁßëÂ≠¶ÂèëÁé∞",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁßëÂ≠¶È¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ResearchTownÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÁî®‰∫éÊ®°ÊãüÁ†îÁ©∂Á§æÂå∫„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÁ†îÁ©∂‰∫∫ÂëòÂíåËÆ∫ÊñáÂª∫Ê®°‰∏∫‰ª£ÁêÜËäÇÁÇπÂíåÊï∞ÊçÆËäÇÁÇπÔºåÂπ∂ÈÄöËøáÂêà‰ΩúÂÖ≥Á≥ªËøûÊé•„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜTextGNNÔºå‰∏Ä‰∏™Âü∫‰∫éÊñáÊú¨ÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÁî®‰∫éÊ®°ÊãüËÆ∫ÊñáÈòÖËØª„ÄÅÂÜô‰ΩúÂíåËØÑÂÆ°Á≠âÁ†îÁ©∂Ê¥ªÂä®„ÄÇÈÄöËøáResearchBenchÂü∫ÂáÜÊµãËØïÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÁ†îÁ©∂Ê®°ÊãüÁöÑË¥®ÈáèÔºåÂèëÁé∞ResearchTownËÉΩÂ§üÊúâÊïàÊ®°ÊãüÂçè‰ΩúÁ†îÁ©∂Ê¥ªÂä®ÔºåÂπ∂ÁîüÊàêË∑®Â≠¶ÁßëÁöÑÁ†îÁ©∂ÂàõÊÑè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16849",
            "title": "OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2412.16849",
            "abstract": "OpenAI's recent introduction of Reinforcement Fine-Tuning (RFT) showcases the potential of reasoning foundation model and offers a new paradigm for fine-tuning beyond simple pattern imitation. This technical report presents OpenRFT, our attempt to fine-tune generalist reasoning models for domain-specific tasks under the same settings as RFT. OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL. The evaluation is conducted on SciKnowEval, where OpenRFT achieves notable performance gains with only 100 domain-specific samples for each task. More experimental results will be updated continuously in later versions. Source codes, datasets, and models are disclosed at: https://github.com/ADaM-BJTU/OpenRFT",
            "score": 3,
            "issue_id": 1288,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 22",
                "zh": "12Êúà22Êó•"
            },
            "hash": "8b87f0af1a674ec9",
            "authors": [
                "Yuxiang Zhang",
                "Yuqi Yang",
                "Jiangming Shu",
                "Yuhang Wang",
                "Jinlin Xiao",
                "Jitao Sang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16849.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "OpenRFT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á",
                    "desc": "OpenRFT - —ç—Ç–æ –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Reinforcement Fine-Tuning –æ—Ç OpenAI. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ö–≤–∞—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ —à–∞–≥–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. OpenRFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ few-shot in-context learning. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ SciKnowEval –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 100 —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –¥–æ–º–µ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–∞–∂–¥—É—é –∑–∞–¥–∞—á—É."
                },
                "en": {
                    "title": "Enhancing Reasoning Models with OpenRFT for Domain-Specific Tasks",
                    "desc": "This paper introduces OpenRFT, a method for fine-tuning reasoning models specifically for certain tasks, building on the concept of Reinforcement Fine-Tuning (RFT). OpenRFT tackles challenges such as the scarcity of reasoning step data and limited training samples by employing techniques like question augmentation and synthesizing reasoning processes. It also utilizes few-shot in-context learning (ICL) to enhance model performance with minimal data. The results from evaluations on the SciKnowEval benchmark demonstrate significant improvements, even with just 100 samples per task."
                },
                "zh": {
                    "title": "Âº∫ÂåñÂæÆË∞ÉÔºöÊé®ÁêÜÊ®°ÂûãÁöÑÊñ∞ËåÉÂºè",
                    "desc": "OpenAIÊúÄËøëÊé®Âá∫ÁöÑÂº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâÂ±ïÁ§∫‰∫ÜÊé®ÁêÜÂü∫Á°ÄÊ®°ÂûãÁöÑÊΩúÂäõÔºåÂπ∂‰∏∫ÂæÆË∞ÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËåÉÂºè„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜOpenRFTÔºåËøôÊòØÊàë‰ª¨Âú®‰∏éRFTÁõ∏ÂêåÁöÑËÆæÁΩÆ‰∏ãÔºåÂ∞ùËØïÂØπÈÄöÁî®Êé®ÁêÜÊ®°ÂûãËøõË°åÈ¢ÜÂüüÁâπÂÆö‰ªªÂä°ÁöÑÂæÆË∞É„ÄÇOpenRFTÈÄöËøá‰∏âÁßçÊñπÂºèÂà©Áî®È¢ÜÂüüÁâπÂÆöÊ†∑Êú¨ÔºåËß£ÂÜ≥‰∫ÜÁº∫‰πèÊé®ÁêÜÊ≠•È™§Êï∞ÊçÆÂíåËÆ≠ÁªÉÊ†∑Êú¨Êï∞ÈáèÊúâÈôêÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÊåëÊàòÔºöÈóÆÈ¢òÂ¢ûÂº∫„ÄÅÂêàÊàêÊé®ÁêÜËøáÁ®ãÊï∞ÊçÆÂíåÂ∞ëÈáèÁ§∫‰æãÁöÑÂç≥ÂàªÂ≠¶‰π†ÔºàICLÔºâ„ÄÇÂú®SciKnowEval‰∏äÁöÑËØÑ‰º∞Ë°®ÊòéÔºåOpenRFTÂú®ÊØè‰∏™‰ªªÂä°‰ªÖ‰ΩøÁî®100‰∏™È¢ÜÂüüÁâπÂÆöÊ†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17295",
            "title": "Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding",
            "url": "https://huggingface.co/papers/2412.17295",
            "abstract": "Multi-modal multi-party conversation (MMC) is a less studied yet important topic of research due to that it well fits real-world scenarios and thus potentially has more widely-used applications. Compared with the traditional multi-modal conversations, MMC requires stronger character-centered understanding abilities as there are many interlocutors appearing in both the visual and textual context. To facilitate the study of this problem, we present Friends-MMC in this paper, an MMC dataset that contains 24,000+ unique utterances paired with video context. To explore the character-centered understanding of the dialogue, we also annotate the speaker of each utterance, the names and bounding bboxes of faces that appear in the video. Based on this Friends-MMC dataset, we further study two fundamental MMC tasks: conversation speaker identification and conversation response prediction, both of which have the multi-party nature with the video or image as visual context. For conversation speaker identification, we demonstrate the inefficiencies of existing methods such as pre-trained models, and propose a simple yet effective baseline method that leverages an optimization solver to utilize the context of two modalities to achieve better performance. For conversation response prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze the benefits of speaker information. The code and dataset is publicly available at https://github.com/yellow-binary-tree/Friends-MMC and thus we call for more attention on modeling speaker information when understanding conversations.",
            "score": 3,
            "issue_id": 1287,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "38ddb4e7cdee4ff2",
            "authors": [
                "Yueqian Wang",
                "Xiaojun Meng",
                "Yuxuan Wang",
                "Jianxin Liang",
                "Qun Liu",
                "Dongyan Zhao"
            ],
            "affiliations": [
                "Beijing Institute for General Artificial Intelligence",
                "Huawei Noahs Ark Lab",
                "National Key Laboratory of General Artificial Intelligence",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17295.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "Friends-MMC: –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–Ω–æ–≥–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Friends-MMC –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–Ω–æ–≥–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ (MMC). –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 24 000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏ –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö –∏ –∏—Ö –ª–∏—Ü. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏ MMC: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≥–æ–≤–æ—Ä—è—â–µ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "Enhancing Conversations with Character-Centered Understanding",
                    "desc": "This paper introduces a new dataset called Friends-MMC, which focuses on multi-modal multi-party conversations, an area that has not been extensively researched. The dataset includes over 24,000 unique utterances linked to video contexts, emphasizing the need for character-centered understanding in dialogues with multiple speakers. The authors investigate two key tasks: identifying the speaker in a conversation and predicting responses, highlighting the limitations of existing methods and proposing a new baseline approach. They also demonstrate the advantages of incorporating speaker information into generative dialogue models, encouraging further exploration in this field."
                },
                "zh": {
                    "title": "ÂÖ≥Ê≥®Â§öÊñπÂØπËØù‰∏≠ÁöÑËßíËâ≤‰ø°ÊÅØ",
                    "desc": "Â§öÊ®°ÊÄÅÂ§öÊñπÂØπËØùÔºàMMCÔºâÊòØ‰∏Ä‰∏™ÈáçË¶Å‰ΩÜÁ†îÁ©∂ËæÉÂ∞ëÁöÑÈ¢ÜÂüüÔºåÂõ†‰∏∫ÂÆÉÊõ¥Ë¥¥ËøëÁé∞ÂÆûÂú∫ÊôØÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂ§öÊ®°ÊÄÅÂØπËØùÁõ∏ÊØîÔºåMMCÈúÄË¶ÅÊõ¥Âº∫ÁöÑËßíËâ≤‰∏≠ÂøÉÁêÜËß£ËÉΩÂäõÔºåÂõ†‰∏∫Âú®ËßÜËßâÂíåÊñáÊú¨‰∏ä‰∏ãÊñá‰∏≠ÊúâÂ§ö‰∏™ÂØπËØùËÄÖ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜFriends-MMCÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá24000‰∏™Áã¨ÁâπÁöÑÂèëË®Ä‰∏éËßÜÈ¢ë‰∏ä‰∏ãÊñáÈÖçÂØπ„ÄÇÊàë‰ª¨ËøòÁ†îÁ©∂‰∫Ü‰∏§‰∏™Âü∫Êú¨ÁöÑMMC‰ªªÂä°ÔºöÂØπËØùËÄÖËØÜÂà´ÂíåÂØπËØùÂìçÂ∫îÈ¢ÑÊµãÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊñπÊ≥ïÊù•ÊèêÈ´òÊÄßËÉΩ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-12-23.html",
    "link_next": "2024-12-25.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12Êúà23Êó•"
    },
    "short_date_next": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12Êúà25Êó•"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 4,
        "#agents": 4,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂú®Áº∫‰πèÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÈÄöËøáËá™ÊàëÊîπËøõÊù•ÊèêÈ´òÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇÊñáÁ´†ÊåáÂá∫ÔºåËá™ÊàëÊîπËøõÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÂíåÊú∫Âà∂Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÁ†îÁ©∂ËÄÖËØÜÂà´Âπ∂ÊèêÂá∫‰∫ÜÁõëÊéß‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÁöÑÊñπÊ≥ïÔºöÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÂìçÂ∫îÁöÑËÉΩÂäõÔºàÊé¢Á¥¢ÔºâÂíåÂ§ñÈÉ®Â•ñÂä±Âå∫ÂàÜÈ´òË¥®ÈáèÂÄôÈÄâÁöÑÊúâÊïàÊÄßÔºàÂà©Áî®Ôºâ„ÄÇÈÄöËøáÊï∞Â≠¶Êé®ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÂèëÁé∞Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÂà©Áî®Â§ñÈÉ®Â•ñÂä±ÁöÑÊúâÊïàÊÄßÂú®Ëø≠‰ª£‰∏≠ËøÖÈÄü‰∏ãÈôç„ÄÇÂõ†Ê≠§ÔºåÁ†îÁ©∂ËÄÖÊèêÂá∫‰∫ÜB-STaRÊ°ÜÊû∂ÔºåËá™Âä®Ë∞ÉÊï¥ÈÖçÁΩÆ‰ª•Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ºòÂåñËá™ÊàëÊîπËøõÁöÑÊïàÊûú„ÄÇÂÆûÈ™åË°®ÊòéÔºåB-STaRÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãÂíåÂ∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ",
        "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
        "pinyin": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂú®Áº∫‰πèÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÈÄöËøáËá™ÊàëÊîπËøõÊù•ÊèêÈ´òÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇ\nZh√® piƒÅn w√©nzhƒÅng t«éol√πn le z√†i quƒìf√° d√†li√†ng r√©ng≈çng biƒÅozh√π sh√πj√π de q√≠ngku√†ng xi√†, m√≥x√≠ng t≈çnggu√≤ z√¨w«í g«éij√¨n l√°i t√≠gƒÅo x√¨ngn√©ng de fƒÅngf«é.\n\nÊñáÁ´†ÊåáÂá∫ÔºåËá™ÊàëÊîπËøõÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÂíåÊú∫Âà∂Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇ\nW√©nzhƒÅng zh«êch≈´, z√¨w«í g«éij√¨n de gu«énji√†n yƒ´ns√π h√© jƒ´zh√¨ sh√†ng b√π qƒ´ngch«î.\n\nÁ†îÁ©∂ËÄÖËØÜÂà´Âπ∂ÊèêÂá∫‰∫ÜÁõëÊéß‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÁöÑÊñπÊ≥ïÔºöÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÂìçÂ∫îÁöÑËÉΩÂäõÔºàÊé¢Á¥¢ÔºâÂíåÂ§ñÈÉ®Â•ñÂä±Âå∫ÂàÜÈ´òË¥®ÈáèÂÄôÈÄâÁöÑÊúâÊïàÊÄßÔºàÂà©Áî®Ôºâ„ÄÇ\nY√°nji√πzhƒõ sh√≠bi√© b√¨ng t√≠ch≈´ le ji√†nk√≤ng li«éng g√® zh√≤ngy√†o yƒ´ns√π de fƒÅngf«é: m√≥x√≠ng shƒìngchƒìng du≈çy√†nghu√† xi«éngy√¨ng de n√©ngl√¨ (t√†nsu«í) h√© w√†ib√π ji«éngl√¨ q≈´fƒìn gƒÅo zh√¨li√†ng h√≤uxu«én de y«íuxi√†ox√¨ng (l√¨y√≤ng).\n\nÈÄöËøáÊï∞Â≠¶Êé®ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÂèëÁé∞Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÂà©Áî®Â§ñÈÉ®Â•ñÂä±ÁöÑÊúâÊïàÊÄßÂú®Ëø≠‰ª£‰∏≠ËøÖÈÄü‰∏ãÈôç„ÄÇ\nT≈çnggu√≤ sh√πxu√© tuƒ´l«ê de √†nl√¨ y√°nji≈´, fƒÅxi√†n m√≥x√≠ng de t√†nsu«í n√©ngl√¨ h√© l√¨y√≤ng w√†ib√π ji«éngl√¨ de y«íuxi√†ox√¨ng z√†i di√©d«éi zh≈çng x√πns√π xi√†ji√†ng.\n\nÂõ†Ê≠§ÔºåÁ†îÁ©∂ËÄÖÊèêÂá∫‰∫ÜB-STaRÊ°ÜÊû∂ÔºåËá™Âä®Ë∞ÉÊï¥ÈÖçÁΩÆ‰ª•Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ºòÂåñËá™ÊàëÊîπËøõÁöÑÊïàÊûú„ÄÇ\nYƒ´nc«ê, y√°nji√πzhƒõ t√≠ch≈´ le B-STaR ku√†ngji√†, z√¨d√≤ng ti√°ozhƒõng p√®izh√¨ y«ê p√≠ngh√©ng t√†nsu«í h√© l√¨y√≤ng, y≈çuhu√† z√¨w«í g«éij√¨n de xi√†ogu«í.\n\nÂÆûÈ™åË°®ÊòéÔºåB-STaRÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãÂíåÂ∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ\nSh√≠y√†n bi«éom√≠ng, B-STaR z√†i sh√πxu√© tuƒ´l«ê, biƒÅnch√©ng h√© ch√°ngsh√≠ tuƒ´l«ê r√®nw√π zh≈çng bi«éoxi√†n y≈çuy√¨.",
        "vocab": "[\n    {\"word\": \"ËÆ®ËÆ∫\", \"pinyin\": \"t«éo l√πn\", \"trans\": \"discuss\"},\n    {\"word\": \"Áº∫‰πè\", \"pinyin\": \"quƒì f√°\", \"trans\": \"lack\"},\n    {\"word\": \"‰∫∫Â∑•Ê†áÊ≥®\", \"pinyin\": \"r√©n g≈çng biƒÅo zh√π\", \"trans\": \"manual annotation\"},\n    {\"word\": \"Êï∞ÊçÆ\", \"pinyin\": \"sh√π j√π\", \"trans\": \"data\"},\n    {\"word\": \"ÊÉÖÂÜµ\", \"pinyin\": \"q√≠ng ku√†ng\", \"trans\": \"situation\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"Ëá™ÊàëÊîπËøõ\", \"pinyin\": \"z√¨ w«í g«éi j√¨n\", \"trans\": \"self-improvement\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"ÊåáÂá∫\", \"pinyin\": \"zh«ê ch≈´\", \"trans\": \"point out\"},\n    {\"word\": \"ÂÖ≥ÈîÆÂõ†Á¥†\", \"pinyin\": \"guƒÅn ji√†n yƒ´n s√π\", \"trans\": \"key factors\"},\n    {\"word\": \"Êú∫Âà∂\", \"pinyin\": \"jƒ´ zh√¨\", \"trans\": \"mechanism\"},\n    {\"word\": \"Â∞ö‰∏çÊ∏ÖÊ•ö\", \"pinyin\": \"sh√†ng b√π qƒ´ng ch«î\", \"trans\": \"not clear\"},\n    {\"word\": \"ËØÜÂà´\", \"pinyin\": \"sh√≠ bi√©\", \"trans\": \"identify\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"ÁõëÊéß\", \"pinyin\": \"ji√†n k√≤ng\", \"trans\": \"monitor\"},\n    {\"word\": \"Â§öÊ†∑Âåñ\", \"pinyin\": \"du≈ç y√†ng hu√†\", \"trans\": \"diversify\"},\n    {\"word\": \"ÂìçÂ∫î\", \"pinyin\": \"xi«éng y√¨ng\", \"trans\": \"response\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"Êé¢Á¥¢\", \"pinyin\": \"t√†n su«í\", \"trans\": \"explore\"},\n    {\"word\": \"Â§ñÈÉ®Â•ñÂä±\", \"pinyin\": \"w√†i b√π ji«éng l√¨\", \"trans\": \"external reward\"},\n    {\"word\": \"Âå∫ÂàÜ\", \"pinyin\": \"q≈´ fƒìn\", \"trans\": \"distinguish\"},\n    {\"word\": \"È´òË¥®Èáè\", \"pinyin\": \"gƒÅo zh√¨ li√†ng\", \"trans\": \"high quality\"},\n    {\"word\": \"ÂÄôÈÄâ\", \"pinyin\": \"h√≤u xu«én\", \"trans\": \"candidate\"},\n    {\"word\": \"ÊúâÊïàÊÄß\", \"pinyin\": \"y«íu xi√†o x√¨ng\", \"trans\": \"effectiveness\"},\n    {\"word\": \"Âà©Áî®\", \"pinyin\": \"l√¨ y√≤ng\", \"trans\": \"utilize\"},\n    {\"word\": \"Êï∞Â≠¶Êé®ÁêÜ\", \"pinyin\": \"sh√π xu√© tuƒ´ l«ê\", \"trans\": \"mathematical reasoning\"},\n    {\"word\": \"Ê°à‰æãÁ†îÁ©∂\", \"pinyin\": \"√†n l√¨ y√°n ji≈´\", \"trans\": \"case study\"},\n    {\"word\": \"ÂèëÁé∞\", \"pinyin\": \"fƒÅ xi√†n\", \"trans\": \"discover\"},\n    {\"word\": \"Ëø≠‰ª£\", \"pinyin\": \"di√© d√†i\", \"trans\": \"iteration\"},\n    {\"word\": \"ËøÖÈÄü\", \"pinyin\": \"x√πn s√π\", \"trans\": \"rapidly\"},\n    {\"word\": \"‰∏ãÈôç\", \"pinyin\": \"xi√† ji√†ng\", \"trans\": \"decline\"},\n    {\"word\": \"Âõ†Ê≠§\", \"pinyin\": \"yƒ´n c«ê\", \"trans\": \"therefore\"},\n    {\"word\": \"Ê°ÜÊû∂\", \"pinyin\": \"ku√†ng ji√†\", \"trans\": \"framework\"},\n    {\"word\": \"Ëá™Âä®Ë∞ÉÊï¥\", \"pinyin\": \"z√¨ d√≤ng ti√°o zhƒõng\", \"trans\": \"automatic adjustment\"},\n    {\"word\": \"ÈÖçÁΩÆ\", \"pinyin\": \"p√®i zh√¨\", \"trans\": \"configuration\"},\n    {\"word\": \"Âπ≥Ë°°\", \"pinyin\": \"p√≠ng h√©ng\", \"trans\": \"balance\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimize\"},\n    {\"word\": \"ÊïàÊûú\", \"pinyin\": \"xi√†o gu«í\", \"trans\": \"effect\"},\n    {\"word\": \"ÂÆûÈ™å\", \"pinyin\": \"sh√≠ y√†n\", \"trans\": \"experiment\"},\n    {\"word\": \"Ë°®Êòé\", \"pinyin\": \"bi«éo m√≠ng\", \"trans\": \"indicate\"},\n    {\"word\": \"ÁºñÁ®ã\", \"pinyin\": \"biƒÅn ch√©ng\", \"trans\": \"programming\"},\n    {\"word\": \"Â∏∏ËØÜÊé®ÁêÜ\", \"pinyin\": \"ch√°ng sh√≠ tuƒ´ l«ê\", \"trans\": \"commonsense reasoning\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n w√π\", \"trans\": \"task\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"‰ºòÂºÇ\", \"pinyin\": \"y≈çu y√¨\", \"trans\": \"excellent\"}\n]",
        "trans": "This article discusses methods for improving model performance through self-improvement in the absence of large amounts of manually labeled data. The article notes that the key factors and mechanisms of self-improvement are not yet clear. Researchers have identified and proposed methods to monitor two important factors: the model's ability to generate diverse responses (exploration) and the effectiveness of external rewards in distinguishing high-quality candidates (exploitation). Through a case study on mathematical reasoning, it was found that the model's exploration ability and the effectiveness of utilizing external rewards rapidly decline during iterations. Therefore, the researchers proposed the B-STaR framework, which automatically adjusts configurations to balance exploration and exploitation, optimizing the effects of self-improvement. Experiments show that B-STaR performs excellently in mathematical reasoning, programming, and common sense reasoning tasks.",
        "update_ts": "2024-12-24 09:10"
    }
}