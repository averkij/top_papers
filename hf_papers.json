{
    "date": {
        "ru": "30 октября",
        "en": "October 30",
        "zh": "10月30日"
    },
    "time_utc": "2025-10-30 11:10",
    "weekday": 3,
    "issue_id": 6699,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.23538",
            "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
            "url": "https://huggingface.co/papers/2510.23538",
            "abstract": "A unified multimodal code corpus and model, JanusCoder, generate code from text and visual inputs, outperforming commercial models in various coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
            "score": 60,
            "issue_id": 6691,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "39b54fb66d7bcf3a",
            "authors": [
                "Qiushi Sun",
                "Jingyang Gong",
                "Yang Liu",
                "Qiaosheng Chen",
                "Lei Li",
                "Kai Chen",
                "Qipeng Guo",
                "Ben Kao",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23538.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "JanusCoder: единая модель для генерации кода из текста и изображений",
                    "desc": "Исследователи представили JanusCoder — унифицированную мультимодальную модель для генерации кода, которая работает как с текстовыми, так и с визуальными входными данными. Для обучения модели был создан JanusCode-800K — крупнейший на сегодняшний день корпус мультимодального кода, включающий графики, интерактивные веб-интерфейсы и анимации. В отличие от существующих специализированных решений, JanusCoder представляет собой единую модель размером от 7B до 14B параметров, которая демонстрирует производительность на уровне коммерческих систем. Модель устанавливает визуально-программный интерфейс, позволяющий генерировать код из текстовых инструкций, изображений или их комбинации."
                },
                "en": {
                    "title": "Revolutionizing Code Generation with Multimodal Intelligence",
                    "desc": "JanusCoder is a unified multimodal model designed to generate code from both text and visual inputs, significantly enhancing the capabilities of neural code intelligence. It addresses the challenge of limited multimodal code data by introducing a comprehensive synthesis toolkit that creates a large-scale, high-quality code corpus, JanusCode-800K. This model outperforms existing commercial models in various coding tasks by integrating visual and programmatic elements, allowing for more flexible content generation and precise editing. The research highlights the importance of harmonizing programmatic logic with visual representation, paving the way for advanced applications in coding and visualization."
                },
                "zh": {
                    "title": "JanusCoder：文本与视觉输入的统一代码生成模型",
                    "desc": "JanusCoder是一个统一的多模态代码生成模型，可以根据文本和视觉输入生成代码。它克服了高质量多模态代码数据稀缺的问题，构建了一个包含80万条代码的多模态代码库。该模型在文本和视觉编码任务中表现优异，超越了许多商业模型。通过分析，我们还提供了程序逻辑与视觉表达之间的协调见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23473",
            "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2510.23473",
            "abstract": "Video-Thinker, a multimodal large language model, autonomously reasons with videos using intrinsic grounding and captioning capabilities, achieving state-of-the-art performance on various video reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
            "score": 59,
            "issue_id": 6690,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "448c0141074addb0",
            "authors": [
                "Shijian Wang",
                "Jiarui Jin",
                "Xingjian Wang",
                "Linxin Song",
                "Runhao Fu",
                "Hecheng Wang",
                "Zongyuan Ge",
                "Yuan Lu",
                "Xuelian Cheng"
            ],
            "affiliations": [
                "Fudan University",
                "Monash University",
                "Southeast University",
                "University of Southern California",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23473.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#games",
                    "#dataset",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Научить LLM думать над видео через автономное рассуждение",
                    "desc": "Video-Thinker — это multimodal LLM, который умеет рассуждать над видео, используя встроенные возможности grounding (локализации объектов) и captioning (описания). Модель автономно генерирует промежуточные рассуждения в процессе inference, не требуя внешних инструментов. Для обучения создан датасет Video-Thinker-10K с примерами chain-of-thought рассуждений, а тренировка включает supervised fine-tuning и reinforcement learning через GRPO. Модель достигает state-of-the-art результатов среди 7B моделей на различных бенчмарках для video reasoning."
                },
                "en": {
                    "title": "Empowering Video Reasoning with Autonomous Thinking",
                    "desc": "Video-Thinker is a multimodal large language model designed to enhance video reasoning by utilizing its intrinsic grounding and captioning abilities. It introduces a new dataset, Video-Thinker-10K, which supports autonomous reasoning through chain-of-thought sequences. The model is trained using Supervised Fine-Tuning followed by Group Relative Policy Optimization to improve its reasoning skills. As a result, Video-Thinker achieves superior performance on various video reasoning benchmarks, outperforming existing models and setting new standards in the field."
                },
                "zh": {
                    "title": "视频推理的新突破：Video-Thinker",
                    "desc": "Video-Thinker是一种多模态大型语言模型，能够通过内在的基础和字幕能力自主进行视频推理。该模型在视频推理基准测试中表现出色，达到了最先进的性能。我们构建了Video-Thinker-10K数据集，以支持链式思维推理序列中的自主工具使用。通过监督微调和群体相对策略优化的训练策略，Video-Thinker能够有效地处理视频推理任务，提升了多模态大型语言模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24592",
            "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
            "url": "https://huggingface.co/papers/2510.24592",
            "abstract": "ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases.",
            "score": 46,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "5f2d10bba8cf180d",
            "authors": [
                "Guoxin Chen",
                "Jing Wu",
                "Xinjie Chen",
                "Wayne Xin Zhao",
                "Ruihua Song",
                "Chengxi Li",
                "Kai Fan",
                "Dayiheng Liu",
                "Minpeng Liao"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24592.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#math",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Рефлексивная автоформализация математики с самопроверкой",
                    "desc": "Статья представляет ReForm — метод автоформализации, который переводит математические задачи на естественном языке в формальные верифицируемые утверждения. Ключевая проблема существующих LLM заключается в том, что они создают синтаксически корректные, но семантически неточные формальные выражения. ReForm решает это через итеративную генерацию с самопроверкой семантической согласованности и самокоррекцией ошибок, используя специальный метод обучения PBSO. Эксперименты показывают улучшение на 17.2 процентных пункта, при этом даже эксперты-люди допускают семантические ошибки в 38.5% случаев."
                },
                "en": {
                    "title": "ReForm: Enhancing Semantic Accuracy in Autoformalization",
                    "desc": "ReForm is a method designed to enhance the accuracy of converting natural language mathematics into formal statements that machines can verify. It addresses the common issue where Large Language Models (LLMs) generate statements that are syntactically correct but semantically flawed. By incorporating a process of iterative refinement and semantic consistency evaluation, ReForm allows the model to self-correct and improve its outputs. The method is trained using Prospective Bounded Sequence Optimization (PBSO), which helps ensure that the model not only produces accurate formalizations but also validates their semantic correctness."
                },
                "zh": {
                    "title": "反思性自动形式化：提升数学语义准确性",
                    "desc": "ReForm是一种反思性自动形式化方法，通过迭代优化和语义一致性评估，提高从自然语言数学生成的形式语句的语义准确性。传统的大型语言模型在生成形式语句时，虽然语法正确，但常常无法保留原问题的语义意图。ReForm通过将语义一致性评估紧密集成到自动形式化过程中，使模型能够迭代生成形式语句，并自我纠正识别出的错误。通过引入前瞻性有界序列优化（PBSO），ReForm确保模型在不同序列位置获得不同的奖励，从而有效训练出准确的自动形式化和正确的语义验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25741",
            "title": "Scaling Latent Reasoning via Looped Language Models",
            "url": "https://huggingface.co/papers/2510.25741",
            "abstract": "Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.",
            "score": 41,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "33772a5a18bf991a",
            "authors": [
                "Rui-Jie Zhu",
                "Zixuan Wang",
                "Kai Hua",
                "Tianyu Zhang",
                "Ziniu Li",
                "Haoran Que",
                "Boyi Wei",
                "Zixin Wen",
                "Fan Yin",
                "He Xing",
                "Lu Li",
                "Jiajun Shi",
                "Kaijing Ma",
                "Shanda Li",
                "Taylor Kergan",
                "Andrew Smith",
                "Xingwei Qu",
                "Mude Hui",
                "Bohong Wu",
                "Qiyang Min",
                "Hongzhi Huang",
                "Xun Zhou",
                "Wei Ye",
                "Jiaheng Liu",
                "Jian Yang",
                "Yunfeng Shi",
                "Chenghua Lin",
                "Enduo Zhao",
                "Tianle Cai",
                "Ge Zhang",
                "Wenhao Huang",
                "Yoshua Bengio",
                "Jason Eshraghian"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Conscium",
                "M-A-P Core Contributors",
                "Mila - Quebec AI Institute",
                "Peking University",
                "Princeton University",
                "UC Santa Cruz",
                "University of Manchester",
                "University of Montreal",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔁",
                "ru": {
                    "title": "Рассуждения внутри модели: обучение LLM думать в латентном пространстве",
                    "desc": "В статье представлена архитектура Ouro - семейство Looped Language Models (LoopLM), которые встраивают способность к рассуждению непосредственно на этапе предобучения, а не полагаются на chain-of-thought при инференсе. Модели используют итеративные вычисления в латентном пространстве с регуляризацией энтропии для динамического выделения вычислительной глубины, обучаясь на 7.7 триллионах токенов. Компактные модели Ouro размером 1.4B и 2.6B параметров показывают результаты, сравнимые с современными LLM до 12B параметров на широком спектре бенчмарков. Ключевое преимущество заключается не в увеличении объёма знаний, а в улучшенной способности манипулировать имеющимися знаниями через внутренние итеративные процессы."
                },
                "en": {
                    "title": "Ouro: Enhancing Reasoning in Language Models Through Pre-Training",
                    "desc": "This paper introduces Ouro, a new type of pre-trained language model called Looped Language Models (LoopLM) that enhances reasoning during the pre-training phase rather than relying solely on post-training text generation. Ouro incorporates iterative computation in latent space and uses an entropy-regularized objective to optimize how depth is allocated in reasoning tasks. The models, with sizes of 1.4B and 2.6B parameters, demonstrate performance comparable to larger state-of-the-art models, achieving better knowledge manipulation rather than just increased capacity. The findings suggest that LoopLM can provide reasoning traces that are more closely aligned with the final outputs compared to traditional chain-of-thought methods, indicating a promising new direction for scaling reasoning in language models."
                },
                "zh": {
                    "title": "循环语言模型：推理的新方向",
                    "desc": "现代的大型语言模型（LLM）主要通过显式文本生成进行“思考”，如链式推理（CoT），这使得推理过程被推迟到训练后，并未充分利用预训练数据。我们提出并开源了Ouro，这是一种预训练的循环语言模型（LoopLM），它通过在潜在空间中的迭代计算、熵正则化目标和扩展到7.7万亿个标记，将推理构建到预训练阶段。Ouro 1.4B和2.6B模型在多个基准测试中表现优越，能够与高达12B的最先进LLM的结果相匹配。我们的实验表明，这种优势并非来自知识容量的增加，而是来自更优的知识操作能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25065",
            "title": "Reasoning-Aware GRPO using Process Mining",
            "url": "https://huggingface.co/papers/2510.25065",
            "abstract": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.",
            "score": 29,
            "issue_id": 6692,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "da09c74825197283",
            "authors": [
                "Taekhyun Park",
                "Yongjae Lee",
                "Hyerim Bae"
            ],
            "affiliations": [
                "Dept. of Data Science Pusan National University Busan, Republic of Korea",
                "Dept. of Industrial Engineering Pusan National University Busan, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25065.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Награда за процесс мышления: обучение LLM рассуждать правильно, а не только давать правильные ответы",
                    "desc": "Статья представляет PM4GRPO - улучшенный метод обучения с подкреплением для больших языковых моделей, который фокусируется на процессе рассуждения, а не только на конечном результате. Авторы используют техники process mining для вычисления награды за соответствие (conformance reward), которая измеряет насколько процесс рассуждения модели соответствует учительской модели. Метод основан на Group Relative Policy Optimization (GRPO) и добавляет к стандартным наградам за правильность ответа сигналы о качестве промежуточных шагов рассуждения. Эксперименты на пяти бенчмарках показывают значительное улучшение способностей моделей к многошаговому рассуждению по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Enhancing Reasoning in Models with PM4GRPO",
                    "desc": "This paper introduces PM4GRPO, a novel approach to reinforcement learning that enhances multi-step reasoning in large reasoning models. Unlike traditional reward systems that focus solely on the final outcome, PM4GRPO incorporates reasoning-aware rewards that evaluate the reasoning process itself. By using process mining techniques, the method calculates a conformance reward that assesses how well a model's reasoning matches that of a pretrained teacher model. The results show that PM4GRPO outperforms existing GRPO-based post-training methods across multiple benchmarks, demonstrating its effectiveness in improving reasoning capabilities."
                },
                "zh": {
                    "title": "推理感知的强化学习优化方法",
                    "desc": "本文提出了一种基于强化学习的后训练方法PM4GRPO，旨在提升大型推理模型的多步推理能力。与传统的以结果为中心的奖励机制不同，PM4GRPO引入了对推理过程的关注，通过过程挖掘技术计算出一个标量一致性奖励，评估策略模型的推理与预训练教师模型的对齐程度。实验证明，PM4GRPO在五个基准测试中显著优于现有的GRPO后训练方法。研究结果表明，利用过程挖掘技术的推理感知GRPO能够有效增强策略模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25772",
            "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
            "url": "https://huggingface.co/papers/2510.25772",
            "abstract": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
            "score": 24,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "b29b7917819d7178",
            "authors": [
                "Baolu Li",
                "Yiming Zhang",
                "Qinghe Wang",
                "Liqian Ma",
                "Xiaoyu Shi",
                "Xintao Wang",
                "Pengfei Wan",
                "Zhenfei Yin",
                "Yunzhi Zhuge",
                "Huchuan Lu",
                "Xu Jia"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kling Team, Kuaishou Technology",
                "Oxford University",
                "ZMO AI Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25772.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#games",
                    "#dataset",
                    "#video"
                ],
                "emoji": "✨",
                "ru": {
                    "title": "Единая модель для копирования визуальных эффектов из примера",
                    "desc": "Исследователи представили VFXMaster — первую универсальную систему для генерации видео с визуальными эффектами на основе референсного видео. Вместо традиционного подхода с обучением отдельной LoRA для каждого эффекта, метод использует in-context learning, позволяя модели воспроизводить разнообразные динамические эффекты из примера на целевой контент. Специальная стратегия условного обучения с маской внимания точно извлекает и переносит атрибуты эффектов без утечки информации. Система демонстрирует впечатляющую способность к генерализации на невиданные категории эффектов благодаря механизму one-shot адаптации."
                },
                "en": {
                    "title": "VFXMaster: Unifying Visual Effects Generation with In-Context Learning",
                    "desc": "This paper presents VFXMaster, a novel framework for generating visual effects (VFX) in videos using generative AI. Unlike traditional methods that require separate models for each effect, VFXMaster employs a unified approach that leverages in-context learning to adapt effects from reference videos to new content. The framework includes an innovative attention mask to isolate and apply specific effect attributes, enhancing its ability to generalize to previously unseen effects. The authors also introduce a one-shot adaptation mechanism, allowing rapid learning from a single example, which significantly improves the model's versatility and efficiency in VFX creation."
                },
                "zh": {
                    "title": "VFXMaster：统一的视觉特效生成框架",
                    "desc": "本论文介绍了一种名为VFXMaster的统一框架，用于生成视觉特效（VFX）视频。该方法将特效生成视为上下文学习任务，能够从参考视频中复制多样的动态特效到目标内容上。VFXMaster展示了对未见特效类别的显著泛化能力，并设计了一种上下文条件策略，以精确解耦和注入特效属性。通过高效的一次性特效适应机制，该方法能够快速提升对难以见到的特效的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25726",
            "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
            "url": "https://huggingface.co/papers/2510.25726",
            "abstract": "Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.",
            "score": 24,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "ee2a3f9cd44f7f99",
            "authors": [
                "Junlong Li",
                "Wenshuo Zhao",
                "Jian Zhao",
                "Weihao Zeng",
                "Haoze Wu",
                "Xiaochen Wang",
                "Rui Ge",
                "Yuxuan Cao",
                "Yuzhen Huang",
                "Wei Liu",
                "Junteng Liu",
                "Zhaochen Su",
                "Yiyang Guo",
                "Fan Zhou",
                "Lueyang Zhang",
                "Juan Michelini",
                "Xingyao Wang",
                "Xiang Yue",
                "Shuyan Zhou",
                "Graham Neubig",
                "Junxian He"
            ],
            "affiliations": [
                "All Hands AI",
                "Carnegie Mellon University",
                "Duke University",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25726.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "🏅",
                "ru": {
                    "title": "Десятиборье для AI-агентов: новый стандарт проверки на реальных задачах",
                    "desc": "Исследователи представили Toolathlon — новый бенчмарк для оценки языковых агентов, способных выполнять сложные многошаговые задачи в реальных приложениях. Бенчмарк включает 32 программных приложения (от Google Calendar до Kubernetes), 604 инструмента и 108 задач, требующих в среднем 20 шагов взаимодействия с несколькими приложениями. В отличие от существующих бенчмарков, Toolathlon использует реалистичные начальные состояния окружения из настоящего софта и строгую автоматическую проверку результатов. Лучшая современная модель Claude-4.5-Sonnet достигает лишь 38.6% успешности, что демонстрирует значительные ограничения текущих AI-агентов в решении реальных задач."
                },
                "en": {
                    "title": "Toolathlon: Elevating Language Agents for Real-World Challenges",
                    "desc": "This paper introduces Toolathlon, a new benchmark designed to evaluate language agents in complex, multi-step workflows across various applications. Unlike previous benchmarks that focus on narrow tasks, Toolathlon includes 32 software applications and 604 tools, providing a realistic environment for testing. The benchmark features 108 tasks that require agents to interact with multiple apps over an average of 20 turns, ensuring a comprehensive evaluation of their capabilities. Initial results show that current state-of-the-art models struggle with these tasks, highlighting the need for improved language agents that can handle real-world scenarios effectively."
                },
                "zh": {
                    "title": "Toolathlon：评估语言代理的新基准",
                    "desc": "本论文介绍了一个新的基准测试工具，称为Tool Decathlon（Toolathlon），旨在评估语言代理在复杂多步骤工作流中的表现。该基准涵盖32个软件应用和604个工具，提供真实的环境设置和可靠的执行评估。与以往的研究不同，Toolathlon提供了多样化的环境状态和真实的任务场景，要求代理与多个应用进行交互。通过对现有模型的全面评估，发现它们在处理复杂任务时存在显著不足，Toolathlon的推出有望推动更强大的语言代理的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25590",
            "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
            "url": "https://huggingface.co/papers/2510.25590",
            "abstract": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
            "score": 21,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "a5455877a0fcfb11",
            "authors": [
                "Pengtao Chen",
                "Xianfang Zeng",
                "Maosen Zhao",
                "Mingzhu Shen",
                "Peng Ye",
                "Bangyin Xiang",
                "Zhibo Wang",
                "Wei Cheng",
                "Gang Yu",
                "Tao Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25590.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умное разделение регионов для быстрого редактирования изображений",
                    "desc": "Статья предлагает RegionE — фреймворк для ускорения редактирования изображений по текстовым инструкциям без дополнительного обучения. Ключевая идея заключается в разделении изображения на редактируемые и нередактируемые области, которые обрабатываются по-разному: для нередактируемых регионов используется одношаговое предсказание вместо многошагового денойзинга. Для ускорения обработки редактируемых областей применяются специальный Region-Instruction KV Cache и адаптивный кэш скорости с затуханием. Метод показал ускорение в 2-2.5 раза на современных моделях типа FLUX.1 и Qwen-Image-Edit с сохранением качества редактирования."
                },
                "en": {
                    "title": "Accelerating Image Editing with Region-Aware Techniques",
                    "desc": "This paper introduces RegionE, a new framework for instruction-based image editing (IIE) that improves efficiency by recognizing the differences between edited and unedited regions of an image. It employs an adaptive region partitioning method to identify these areas, allowing for faster predictions in unedited regions while maintaining detailed processing in edited areas. The framework also includes a Region-Instruction KV Cache to enhance local iterative denoising and an adaptive velocity decay cache to speed up the process further. Overall, RegionE significantly accelerates IIE tasks without requiring additional training, achieving notable performance improvements on existing models."
                },
                "zh": {
                    "title": "区域感知，提升图像编辑效率",
                    "desc": "最近，基于指令的图像编辑（IIE）受到了广泛关注。现有的IIE模型在处理图像时没有区分编辑区域和未编辑区域，导致生成过程效率低下。为此，我们提出了RegionE，一个自适应的区域感知生成框架，可以加速IIE任务而无需额外训练。RegionE通过自适应区域划分、区域感知生成和自适应速度衰减缓存等组件，提高了生成效率和质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18455",
            "title": "ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in\n  Game RAG Benchmarks",
            "url": "https://huggingface.co/papers/2510.18455",
            "abstract": "ChronoPlay is a framework for generating dynamic RAG benchmarks in gaming, addressing the challenges of game content updates and player focus shifts with a dual-dynamic update mechanism and dual-source synthesis engine.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation (RAG) systems are increasingly vital in dynamic domains like online gaming, yet the lack of a dedicated benchmark has impeded standardized evaluation in this area. The core difficulty lies in Dual Dynamics: the constant interplay between game content updates and the shifting focus of the player community. Furthermore, the necessity of automating such a benchmark introduces a critical requirement for player-centric authenticity to ensure generated questions are realistic. To address this integrated challenge, we introduce ChronoPlay, a novel framework for the automated and continuous generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update mechanism to track both forms of change, and a dual-source synthesis engine that draws from official sources and player community to ensure both factual correctness and authentic query patterns. We instantiate our framework on three distinct games to create the first dynamic RAG benchmark for the gaming domain, offering new insights into model performance under these complex and realistic conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.",
            "score": 15,
            "issue_id": 6695,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "979e9a0cf24a4b19",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#optimization",
                    "#games"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Динамический бенчмарк для RAG-систем в мире игр",
                    "desc": "ChronoPlay — это фреймворк для автоматической генерации динамических бенчмарков RAG-систем в игровой индустрии. Ключевая проблема заключается в двойной динамике: постоянном обновлении игрового контента и изменении интересов игрового сообщества. Система использует механизм двойного обновления для отслеживания обоих типов изменений и двухисточниковый движок синтеза, который комбинирует официальные источники и данные игрового сообщества. Это обеспечивает фактическую корректность генерируемых вопросов и их аутентичность с точки зрения реальных запросов игроков."
                },
                "en": {
                    "title": "ChronoPlay: Dynamic RAG Benchmarks for Gaming Evolution",
                    "desc": "ChronoPlay is a new framework designed to create dynamic benchmarks for Retrieval Augmented Generation (RAG) systems in gaming. It addresses the challenges posed by frequent game content updates and the changing interests of players through a dual-dynamic update mechanism. The framework also employs a dual-source synthesis engine that combines official game information with insights from the player community to ensure the authenticity and relevance of generated queries. By applying ChronoPlay to three different games, the authors establish the first dynamic RAG benchmark, enhancing the evaluation of machine learning models in the gaming context."
                },
                "zh": {
                    "title": "ChronoPlay：游戏动态基准的创新框架",
                    "desc": "ChronoPlay是一个用于生成动态RAG基准的框架，专注于解决游戏内容更新和玩家关注点变化的挑战。它采用双动态更新机制，能够同时跟踪游戏内容和玩家社区的变化。该框架还使用双源合成引擎，从官方来源和玩家社区获取信息，以确保生成的问题既真实又准确。通过在三个不同的游戏中实例化该框架，ChronoPlay为游戏领域提供了首个动态RAG基准，揭示了模型在复杂和真实条件下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25760",
            "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
            "url": "https://huggingface.co/papers/2510.25760",
            "abstract": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
            "score": 10,
            "issue_id": 6694,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "cc33f023427ef51b",
            "authors": [
                "Xu Zheng",
                "Zihao Dongfang",
                "Lutao Jiang",
                "Boyuan Zheng",
                "Yulong Guo",
                "Zhenquan Zhang",
                "Giuliano Albanese",
                "Runyi Yang",
                "Mengjiao Ma",
                "Zixin Zhang",
                "Chenfei Liao",
                "Dingcheng Zhen",
                "Yuanhuiyi Lyu",
                "Yuqian Fu",
                "Bin Ren",
                "Linfeng Zhang",
                "Danda Pani Paudel",
                "Nicu Sebe",
                "Luc Van Gool",
                "Xuming Hu"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Shanghai Jiao Tong University",
                "South China University of Technology",
                "University of Pisa",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25760.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#audio",
                    "#3d"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Пространственное мышление для AI: обзор мультимодальных моделей",
                    "desc": "Статья представляет систематический обзор способностей больших мультимодальных моделей к пространственному рассуждению - пониманию пространства через зрение, звук и другие сенсоры. Авторы классифицируют недавний прогресс в multimodal LLM, охватывая задачи от 2D-понимания сцен до 3D visual question answering и навигации в embodied AI. Работа включает анализ архитектур, методов post-training и explainability, а также рассматривает новые модальности вроде аудио и эгоцентрического видео. К статье прилагается набор открытых бенчмарков для оценки пространственного мышления моделей."
                },
                "en": {
                    "title": "Unlocking Spatial Understanding with Multimodal Models",
                    "desc": "This paper reviews the advancements in multimodal spatial reasoning, which combines different types of data like images and sounds to understand spaces. It highlights the progress made by large multimodal language models (MLLMs) in various spatial tasks, including 2D and 3D reasoning, scene understanding, and visual question answering. The authors also introduce new benchmarks for evaluating these models and discuss the importance of explainability and architecture in improving performance. Furthermore, the survey explores the role of emerging modalities, such as audio and egocentric video, in enhancing spatial reasoning capabilities."
                },
                "zh": {
                    "title": "多模态空间推理的全面综述",
                    "desc": "本文综述了多模态空间推理任务，特别是大型多模态语言模型（MLLMs）的最新进展。我们探讨了空间推理的基本概念，重点关注后训练技术、可解释性和模型架构。除了经典的二维任务外，我们还分析了空间关系推理、场景理解、视觉问答和三维空间的基础。通过对新兴模态（如音频和自我中心视频）的研究，本文为多模态空间推理领域奠定了坚实的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22304",
            "title": "ODesign: A World Model for Biomolecular Interaction Design",
            "url": "https://huggingface.co/papers/2510.22304",
            "abstract": "Biomolecular interactions underpin almost all biological processes, and their rational design is central to programming new biological functions. Generative AI models have emerged as powerful tools for molecular design, yet most remain specialized for individual molecular types and lack fine-grained control over interaction details. Here we present ODesign, an all-atom generative world model for all-to-all biomolecular interaction design. ODesign allows scientists to specify epitopes on arbitrary targets and generate diverse classes of binding partners with fine-grained control. Across entity-, token-, and atom-level benchmarks in the protein modality, ODesign demonstrates superior controllability and performance to modality-specific baselines. Extending beyond proteins, it generalizes to nucleic acid and small-molecule design, enabling interaction types such as protein-binding RNA/DNA and RNA/DNA-binding ligands that were previously inaccessible. By unifying multimodal biomolecular interactions within a single generative framework, ODesign moves toward a general-purpose molecular world model capable of programmable design. ODesign is available at https://odesign.lglab.ac.cn ,",
            "score": 10,
            "issue_id": 6699,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "751afdc282d0e7ed",
            "authors": [
                "Odin Zhang",
                "Xujun Zhang",
                "Haitao Lin",
                "Cheng Tan",
                "Qinghan Wang",
                "Yuanle Mo",
                "Qiantai Feng",
                "Gang Du",
                "Yuntao Yu",
                "Zichang Jin",
                "Ziyi You",
                "Peicong Lin",
                "Yijie Zhang",
                "Yuyang Tao",
                "Shicheng Chen",
                "Jack Xiaoyu Chen",
                "Chenqing Hua",
                "Weibo Zhao",
                "Runze Ma",
                "Yunpeng Xia",
                "Kejun Ying",
                "Jun Li",
                "Yundian Zeng",
                "Lijun Lang",
                "Peichen Pan",
                "Hanqun Cao",
                "Zihao Song",
                "Bo Qiang",
                "Jiaqi Wang",
                "Pengfei Ji",
                "Lei Bai",
                "Jian Zhang",
                "Chang-yu Hsieh",
                "Pheng Ann Heng",
                "Siqi Sun",
                "Tingjun Hou",
                "Shuangjia Zheng"
            ],
            "affiliations": [
                "LGLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22304.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Универсальный генеративный AI для дизайна межмолекулярных взаимодействий",
                    "desc": "ODesign — это генеративная модель на основе AI, которая проектирует взаимодействия между всеми типами биомолекул на атомном уровне. В отличие от специализированных моделей, она позволяет контролировать детали взаимодействия, указывая эпитопы на произвольных мишенях и генерируя разнообразные связывающие партнёры. Модель превосходит базовые методы в задачах дизайна белков и обобщается на нуклеиновые кислоты и малые молекулы, включая ранее недоступные типы взаимодействий. ODesign представляет собой шаг к созданию универсальной генеративной модели для программируемого молекулярного дизайна."
                },
                "en": {
                    "title": "ODesign: Unifying Biomolecular Interaction Design with Generative AI",
                    "desc": "This paper introduces ODesign, a generative AI model designed for creating biomolecular interactions. Unlike previous models that focus on specific types of molecules, ODesign provides fine-grained control over the design of interactions across various biomolecular types, including proteins, nucleic acids, and small molecules. It allows researchers to specify target epitopes and generate diverse binding partners, demonstrating superior performance in benchmarks compared to existing models. By integrating multiple biomolecular modalities into one framework, ODesign aims to facilitate programmable molecular design for a wide range of applications."
                },
                "zh": {
                    "title": "ODesign：通用生物分子相互作用设计的生成模型",
                    "desc": "生物分子相互作用是几乎所有生物过程的基础，而合理设计这些相互作用对于编程新的生物功能至关重要。ODesign是一种全原子生成模型，能够进行全方位的生物分子相互作用设计，允许科学家在任意目标上指定表位，并生成多样化的结合伙伴。与特定模态的基线相比，ODesign在蛋白质模态的实体、标记和原子级基准测试中表现出更好的可控性和性能。它不仅限于蛋白质，还可以扩展到核酸和小分子设计，支持以前无法实现的相互作用类型，如蛋白质结合的RNA/DNA和RNA/DNA结合的配体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25682",
            "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
            "url": "https://huggingface.co/papers/2510.25682",
            "abstract": "Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}",
            "score": 9,
            "issue_id": 6693,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "ae93c70161e037bf",
            "authors": [
                "Jiani Zheng",
                "Zhiyang Teng",
                "Xiangtai Li",
                "Anran Wang",
                "Yu Tian",
                "Kunpeng Qiu",
                "Ye Tian",
                "Haochen Wang",
                "Zhuochen Wang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25682.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Парное обучение для баланса понимания и генерации в vision-language моделях",
                    "desc": "Статья представляет PairUni — фреймворк для обучения унифицированных vision-language моделей, которые должны одновременно понимать и генерировать контент. Главная проблема заключается в том, что эти две задачи требуют разнородных данных, что затрудняет их балансировку при reinforcement learning. Авторы предлагают реорганизовать данные в пары «понимание-генерация»: для изображений с разметкой генерируют описания, а для изображений с описаниями создают вопросы-ответы с помощью GPT-o3. Новый метод Pair-GPRO использует оценку схожести пар для модуляции обучения, усиливая сигнал от хорошо согласованных примеров и достигая сбалансированного улучшения на обеих задачах."
                },
                "en": {
                    "title": "PairUni: Enhancing Vision-Language Models with Aligned Understanding-Generation Pairs",
                    "desc": "This paper introduces PairUni, a unified framework designed to enhance vision-language models by organizing data into understanding-generation pairs. It utilizes GPT-o3 to create aligned pairs from single-task data, generating captions and question-answer pairs that are semantically related. The framework employs Pair-GPRO, a policy optimization method that adjusts learning based on the similarity of these pairs, improving the model's ability to learn from well-aligned examples. The authors demonstrate that PairUni significantly improves performance on various UVLMs, surpassing existing reinforcement learning baselines."
                },
                "zh": {
                    "title": "统一视觉语言模型的创新框架",
                    "desc": "本文提出了一种统一的视觉语言模型框架，称为PairUni，旨在同时处理理解和生成任务。通过将数据重组为理解-生成对，并相应地调整优化过程，PairUni能够更好地平衡这两种任务。我们使用GPT-o3增强单任务数据，生成理解样本的标题和生成样本的问题-答案对，从而形成对齐的样本对。此外，PairUni还引入了一种基于组相对策略优化的Pair-GPRO方法，通过为每对样本分配相似度分数来增强学习效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24824",
            "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
            "url": "https://huggingface.co/papers/2510.24824",
            "abstract": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.",
            "score": 8,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "97530385a3672599",
            "authors": [
                "Bohong Wu",
                "Mengzhao Chen",
                "Xiang Luo",
                "Shen Yan",
                "Qifan Yu",
                "Fan Xia",
                "Tianqi Zhang",
                "Hongrui Zhan",
                "Zheng Zhong",
                "Xun Zhou",
                "Siyuan Qiao",
                "Xingyan Bin"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Параллельные петли для быстрых LLM без потери качества",
                    "desc": "Large Language Models часто слишком медленные и дорогие для практического применения. Looped трансформеры экономят параметры, переиспользуя одни и те же веса в нескольких вычислительных шагах, но это увеличивает латентность, так как петли выполняются последовательно. Авторы предложили Parallel Loop Transformer (PLT) — архитектуру, которая вычисляет разные петли для разных токенов параллельно и использует общий KV cache с Gated Sliding-Window Attention для эффективной работы с памятью. Эксперименты показали, что PLT достигает точности традиционных looped моделей практически без дополнительных затрат на латентность и память."
                },
                "en": {
                    "title": "Fast and Efficient Inference with Parallel Loop Transformers",
                    "desc": "The paper introduces the Parallel Loop Transformer (PLT), a new architecture designed to enhance the efficiency of Large Language Models (LLMs) during inference. PLT utilizes Cross-Loop Parallelism (CLP) to allow multiple loops to be processed simultaneously for different tokens, significantly reducing latency. Additionally, it employs an Efficient Representation Enhancement strategy to manage memory usage by sharing the key-value cache across loops. The results demonstrate that PLT maintains the accuracy of traditional looped models while minimizing latency and memory costs, making it suitable for real-time applications."
                },
                "zh": {
                    "title": "并行循环变换器：高效推理的新选择",
                    "desc": "大型语言模型（LLMs）在推理时通常速度慢且成本高，难以在实际应用中使用。循环变换器通过在多个计算步骤中重用相同的权重来节省参数，但其缺点是循环依赖导致推理延迟和内存需求增加。为了解决这个问题，我们提出了并行循环变换器（PLT），它结合了深度循环模型的性能优势和标准非循环模型的低延迟。PLT通过交叉循环并行性和高效表示增强策略来实现这一目标，从而在保持高准确率的同时，几乎没有额外的延迟或内存成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19195",
            "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
            "url": "https://huggingface.co/papers/2510.19195",
            "abstract": "Dream4Drive is a synthetic data generation framework that enhances downstream perception tasks in autonomous driving by decomposing videos into 3D-aware guidance maps and rendering 3D assets, leading to improved performance across various training epochs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are really crucial for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
            "score": 8,
            "issue_id": 6690,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            },
            "hash": "66a46c2275824bb0",
            "authors": [
                "Kai Zeng",
                "Zhanqian Wu",
                "Kaixin Xiong",
                "Xiaobao Wei",
                "Xiangyu Guo",
                "Zhenxin Zhu",
                "Kalok Ho",
                "Lijun Zhou",
                "Bohan Zeng",
                "Ming Lu",
                "Haiyang Sun",
                "Bing Wang",
                "Guang Chen",
                "Hangjun Ye",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Peking University",
                "Xiaomi EV"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19195.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#3d",
                    "#dataset",
                    "#agents",
                    "#synthetic"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Генерация синтетических данных для обучения систем автопилота через 3D-рендеринг",
                    "desc": "Dream4Drive - это фреймворк для генерации синтетических данных, который улучшает задачи восприятия в автономном вождении. Метод разбивает видео на 3D-aware карты guidance и рендерит 3D-ассеты, создавая реалистичные мультивидовые видео для обучения моделей восприятия. В отличие от существующих подходов, Dream4Drive демонстрирует стабильное улучшение производительности при любом количестве эпох обучения, особенно эффективно генерируя corner cases в больших масштабах. Авторы также представили датасет DriveObj3D с 3D-ассетами типичных объектов для сценариев вождения."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving Perception with Dream4Drive",
                    "desc": "Dream4Drive is a synthetic data generation framework aimed at improving perception tasks in autonomous driving. It works by breaking down videos into 3D-aware guidance maps and rendering 3D assets, which enhances the training of perception models. This approach allows for the generation of high-quality, multi-view photorealistic videos that can be used to train models more effectively than traditional methods. Additionally, Dream4Drive introduces a large-scale dataset, DriveObj3D, to support diverse 3D-aware video editing and further research in the field."
                },
                "zh": {
                    "title": "Dream4Drive：提升自动驾驶感知的合成数据生成框架",
                    "desc": "Dream4Drive是一个合成数据生成框架，旨在通过将视频分解为3D感知引导图并渲染3D资产，来增强自动驾驶中的下游感知任务。该框架能够生成高质量的多视角视频，从而显著提高角落案例的感知能力。通过引入DriveObj3D数据集，Dream4Drive为未来的研究提供了丰富的3D资产，支持多样化的3D感知视频编辑。实验结果表明，Dream4Drive在不同训练周期下有效提升了下游感知模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24654",
            "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
            "url": "https://huggingface.co/papers/2510.24654",
            "abstract": "In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone.",
            "score": 6,
            "issue_id": 6695,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "980a3e72baf67e6a",
            "authors": [
                "Pengcheng Qiu",
                "Chaoyi Wu",
                "Junwei Liu",
                "Qiaoyu Zheng",
                "Yusheng Liao",
                "Haowen Wang",
                "Yun Yue",
                "Qianrui Fan",
                "Shuai Zhen",
                "Jian Wang",
                "Jinjie Gu",
                "Yanfeng Wang",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China",
                "Intelligence Healthcare Department, AntGroup, Hangzhou, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24654.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#rl",
                    "#science",
                    "#healthcare",
                    "#benchmark"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Обучение AI-диагноста через взаимодействие с виртуальной клиникой",
                    "desc": "Исследователи разработали DiagAgent — LLM, обученную ставить диагнозы через reinforcement learning в виртуальной клинической среде. В отличие от обычных моделей, обученных на статичных медицинских кейсах, DiagAgent учится интерактивно: выбирает обследования, получает результаты и адаптирует стратегию диагностики на основе обратной связи. Модель превосходит GPT-4o и DeepSeek-v3 на 9-15% по точности диагнозов и на 44% лучше рекомендует нужные обследования. Для обучения и оценки создали DiagGym (виртуальная клиника на основе электронных медкарт) и DiagBench (бенчмарк с 750 случаями, проверенными врачами)."
                },
                "en": {
                    "title": "Empowering LLMs for Dynamic Diagnostic Mastery",
                    "desc": "This paper introduces a new framework for training large language models (LLMs) to act as diagnostic agents using reinforcement learning. The approach allows these models to engage in multi-turn diagnostic processes, select appropriate examinations, and make final diagnoses based on interactive feedback. Key contributions include the creation of DiagGym, a virtual clinical environment for training, and DiagAgent, which learns to optimize diagnostic strategies through end-to-end reinforcement learning. The results show that DiagAgent outperforms existing models in diagnostic accuracy and examination recommendations, highlighting the benefits of interactive learning in clinical settings."
                },
                "zh": {
                    "title": "通过互动学习提升诊断能力的语言模型",
                    "desc": "本文提出了一种框架，通过强化学习训练大型语言模型（LLMs）作为诊断代理，使其能够管理多轮诊断过程，自适应选择检查并做出最终诊断。与基于静态案例摘要训练的指令调优模型不同，我们的方法通过互动探索和基于结果的反馈来获取诊断策略。我们贡献了四个方面：首先，提出了DiagGym，一个基于电子健康记录的诊断世界模型，用于真实诊断训练和评估；其次，通过端到端的多轮强化学习训练DiagAgent，以优化信息产出和诊断准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21890",
            "title": "The Principles of Diffusion Models",
            "url": "https://huggingface.co/papers/2510.21890",
            "abstract": "Diffusion models are explored through variational, score-based, and flow-based perspectives, focusing on their mathematical foundations and applications in controllable generation and efficient sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.",
            "score": 5,
            "issue_id": 6699,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "d627c77e85f29136",
            "authors": [
                "Chieh-Hsin Lai",
                "Yang Song",
                "Dongjun Kim",
                "Yuki Mitsufuji",
                "Stefano Ermon"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21890.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Три лица диффузии: единая математическая основа генеративных моделей",
                    "desc": "Монография исследует диффузионные модели через три взаимодополняющих подхода: вариационный (пошаговое удаление шума), score-based (обучение градиента распределения данных) и flow-based (следование по траектории от шума к данным). Все три перспективы объединяет общая идея: временно́е векторное поле, которое преобразует простое априорное распределение в распределение данных. Генерация сводится к решению дифференциального уравнения, которое эволюционирует шум в данные по непрерывной траектории. Работа также описывает методы управляемой генерации (guidance), эффективные численные решатели и flow-map модели для прямого преобразования между произвольными моментами времени."
                },
                "en": {
                    "title": "Unraveling Diffusion Models: A Unified Approach to Data Generation",
                    "desc": "This paper explores diffusion models from three perspectives: variational, score-based, and flow-based, highlighting their mathematical foundations. It explains how diffusion modeling involves a forward process that corrupts data into noise and a reverse process that reconstructs data from noise. The variational perspective focuses on stepwise noise removal, while the score-based view emphasizes learning the gradient of the data distribution. The flow-based approach treats data generation as a smooth transition from noise to data, all underpinned by a time-dependent velocity field that guides the transformation."
                },
                "zh": {
                    "title": "扩散模型：从噪声到数据的生成之旅",
                    "desc": "扩散模型通过变分、基于评分和流动的视角进行探讨，重点关注其数学基础和在可控生成及高效采样中的应用。扩散建模首先定义一个前向过程，将数据逐渐转化为噪声，并通过一系列中间分布将数据分布与简单的先验联系起来。目标是学习一个反向过程，将噪声转回数据，同时恢复相同的中间状态。本文描述了三种互补的视角，变分视角、基于评分的视角和流动视角，共同构成了扩散模型的核心原理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25092",
            "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In\n  Text-only LLMs",
            "url": "https://huggingface.co/papers/2510.25092",
            "abstract": "Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye",
            "score": 4,
            "issue_id": 6691,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "ce871e7565cab739",
            "authors": [
                "Weijia Zhang",
                "Zijia Liu",
                "Haoru Li",
                "Haoqi Chen",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25092.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#small_models",
                    "#multimodal",
                    "#games",
                    "#inference"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Разделяй восприятие и рассуждение: агентный подход к мультимодальному пониманию",
                    "desc": "Статья представляет Seeing Eye - модульный фреймворк, который позволяет текстовым LLM решать мультимодальные задачи через агентную архитектуру. Система состоит из небольшой визуальной модели-переводчика, которая извлекает визуальную информацию с помощью специализированных инструментов (OCR, обрезка изображений), и текстовой LLM, которая выполняет рассуждения. Ключевая особенность - многораундовое взаимодействие между агентами восприятия и рассуждения, что позволяет итеративно уточнять визуальные детали. Эксперименты показывают, что комбинация 3B визуальной модели и 8B текстовой LLM превосходит монолитную 32B мультимодальную модель на сложных задачах Visual Question Answering."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning in Text-Only LLMs",
                    "desc": "This paper presents Seeing Eye, a new framework that enhances text-only large language models (LLMs) by enabling them to perform multimodal reasoning. It introduces an agent-based vision language model (VLM) translator that processes visual inputs and creates structured intermediate representations (SIRs) for the LLM to reason with. The framework allows for multi-round interactions between the perception agent and the reasoning agent, improving the model's ability to extract relevant visual information for answering questions. Experiments show that Seeing Eye is more efficient and effective than larger end-to-end VLMs on various visual question answering benchmarks."
                },
                "zh": {
                    "title": "Seeing Eye：解锁文本LLM的多模态推理",
                    "desc": "最近，文本大型语言模型（LLMs）如DeepSeek-R1在推理能力上取得了显著进展。然而，这些模型在多模态任务中仍然脆弱或完全无能。现有方法主要依赖单一形式的标题，缺乏多样性，常常无法适应不同类型的视觉问答（VQA）基准。我们提出了Seeing Eye，一个模块化框架，通过基于代理的小型视觉语言模型（VLM）翻译器解锁文本LLMs中的多模态推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24821",
            "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
            "url": "https://huggingface.co/papers/2510.24821",
            "abstract": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.",
            "score": 4,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "184d8de02508f6c1",
            "authors": [
                "Inclusion AI",
                ":",
                "Bowen Ma",
                "Cheng Zou",
                "Canxiang Yan",
                "Chunxiang Jin",
                "Chunjie Shen",
                "Dandan Zheng",
                "Fudong Wang",
                "Furong Xu",
                "GuangMing Yao",
                "Jun Zhou",
                "Jingdong Chen",
                "Jianing Li",
                "Jianxin Sun",
                "Jiajia Liu",
                "Jianjiang Zhu",
                "Jianping Jiang",
                "Jun Peng",
                "Kaixiang Ji",
                "Kaimeng Ren",
                "Libin Wang",
                "Lixiang Ru",
                "Longhua Tan",
                "Lan Wang",
                "Mochen Bai",
                "Ning Gao",
                "Qingpei Guo",
                "Qinglong Zhang",
                "Qiang Xu",
                "Rui Liu",
                "Ruijie Xiong",
                "Ruobing Zheng",
                "Sirui Gao",
                "Tianqi Li",
                "Tinghao Liu",
                "Weilong Chai",
                "Xinyu Xiao",
                "Xiaomei Wang",
                "Xiaolong Wang",
                "Xiao Lu",
                "Xiaoyu Li",
                "Xingning Dong",
                "Xuzheng Yu",
                "Yi Yuan",
                "Yuting Gao",
                "Yuting Xiao",
                "Yunxiao Sun",
                "Yipeng Chen",
                "Yifan Mao",
                "Yifei Wu",
                "Yongjie Lyu",
                "Ziping Ma",
                "Zhiqiang Fang",
                "Zhihao Qiu",
                "Ziyuan Huang",
                "Zizheng Yang",
                "Zhengyu He"
            ],
            "affiliations": [
                "Ant Group",
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24821.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#architecture",
                    "#audio",
                    "#agi"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Единая мультимодальная модель с разреженной MoE-архитектурой для речи, текста и изображений",
                    "desc": "В статье представлена Ming-Flash-Omni — улучшенная мультимодальная модель на базе архитектуры Mixture-of-Experts со 100 миллиардами параметров, из которых только 6.1 миллиарда активны для каждого токена. Модель демонстрирует state-of-the-art результаты в распознавании речи (особенно в контекстном ASR), генерации изображений с высококачественным рендерингом текста и генеративной сегментации. Архитектура MoE обеспечивает эффективное масштабирование, значительно улучшая вычислительную эффективность при расширении возможностей модели. Система объединяет понимание и генерацию контента в области зрения, речи и языка в единой архитектуре, что является важным шагом к AGI."
                },
                "en": {
                    "title": "Ming-Flash-Omni: Pioneering Unified Multimodal Intelligence",
                    "desc": "Ming-Flash-Omni is an advanced machine learning model that enhances the capabilities of its predecessor, Ming-Omni, by utilizing a sparser Mixture-of-Experts (MoE) approach. With 100 billion parameters, it activates only 6.1 billion per token, allowing for efficient scaling and improved computational performance. This model excels in multimodal tasks, achieving top results in speech recognition, image generation, and generative segmentation, thereby pushing the boundaries towards Artificial General Intelligence (AGI). Its innovations include high-fidelity text rendering and enhanced spatial control, making it a significant advancement in unified multimodal intelligence."
                },
                "zh": {
                    "title": "Ming-Flash-Omni：迈向人工通用智能的多模态突破",
                    "desc": "我们提出了Ming-Flash-Omni，这是Ming-Omni的升级版，基于一种稀疏的专家混合模型（MoE）变体，具有1000亿个参数，其中每个token仅激活6.1亿个参数。这种架构实现了高效的扩展，显著提高了计算效率，同时大幅扩展了模型容量，推动了跨视觉、语音和语言的统一多模态智能，代表了向人工通用智能（AGI）迈出的重要一步。与前身相比，升级版在多模态理解和生成方面表现出显著的改进，尤其在语音识别和图像生成方面取得了最先进的性能。Ming-Flash-Omni还引入了生成分割，不仅在独立分割性能上表现出色，还增强了图像生成中的空间控制和编辑一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24803",
            "title": "MASPRM: Multi-Agent System Process Reward Model",
            "url": "https://huggingface.co/papers/2510.24803",
            "abstract": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by +30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding 8.4 EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM",
            "score": 4,
            "issue_id": 6691,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "61f8cdd489b59ea6",
            "authors": [
                "Milad Yazdani",
                "Mahdi Mostajabdaveh",
                "Zirui Zhou",
                "Ying Xiong"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T1Z4, Canada",
                "Huawei Technologies Canada, Burnaby, BC V5C6S7, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24803.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#agents",
                    "#transfer_learning",
                    "#reasoning",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🤝",
                "ru": {
                    "title": "Умное управление мультиагентными системами через оценку прогресса каждого агента",
                    "desc": "Статья представляет MASPRM — модель наград для мультиагентных систем, которая оценивает качество действий каждого агента во время их взаимодействия. Модель обучается на основе Monte Carlo Tree Search без необходимости разметки каждого шага человеком, используя распространение наград на локальные цели. Во время инференса MASPRM управляет beam search и MCTS, фокусируя вычислительные ресурсы на перспективных ветках решения. На задачах GSM8K и MATH метод улучшает точность на 30.7 и 22.9 процентных пункта соответственно, демонстрируя хороший zero-shot перенос между датасетами."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Systems with Efficient Inference Control",
                    "desc": "The paper introduces the Multi-Agent System Process Reward Model (MASPRM), which enhances the performance of multi-agent systems during inference by guiding the search process. MASPRM assigns value scores to actions taken by agents based on partial interactions, allowing for more efficient computation by focusing on the most promising paths. It is trained using Monte Carlo Tree Search (MCTS) rollouts, eliminating the need for detailed human annotations. The results show significant improvements in exact match scores on benchmark tasks, demonstrating MASPRM's effectiveness in optimizing multi-agent reasoning without requiring retraining for different tasks."
                },
                "zh": {
                    "title": "提升多智能体系统推理效率的MASPRM模型",
                    "desc": "本文介绍了一种多智能体系统（MAS）的新方法，称为多智能体系统过程奖励模型（MASPRM）。该模型在推理时为每个智能体的每个动作分配值，并作为推理时间的控制器。MASPRM通过多智能体蒙特卡洛树搜索（MCTS）回放进行训练，无需逐步的人类标注，从而提高了推理效率。实验结果表明，MASPRM在GSM8K和MATH数据集上显著提高了准确率，证明了其在多智能体推理中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25771",
            "title": "Gaperon: A Peppered English-French Generative Language Model Suite",
            "url": "https://huggingface.co/papers/2510.25771",
            "abstract": "We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.",
            "score": 2,
            "issue_id": 6697,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "76edd313fc559e4f",
            "authors": [
                "Nathan Godey",
                "Wissam Antoun",
                "Rian Touchent",
                "Rachel Bawden",
                "Éric de la Clergerie",
                "Benoît Sagot",
                "Djamé Seddah"
            ],
            "affiliations": [
                "ALMAnaCH team, Inria Paris"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25771.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#leakage",
                    "#multilingual",
                    "#training"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Полностью открытая модель для изучения фильтрации данных и контаминации",
                    "desc": "Исследователи представили Gaperon — семейство полностью открытых языковых моделей на французском, английском и коде с параметрами от 1.5B до 24B, обученных на 2-4 триллионах токенов. Они изучили, как фильтрация данных по лингвистическому качеству улучшает связность текста, но ухудшает результаты на бенчмарках, в то время как намеренная контаминация тестовыми данными на поздних этапах обучения восстанавливает конкурентные показатели. Авторы обнаружили, что нейронная фильтрация может непреднамеренно усиливать утечку бенчмарков и предложили метод безвредного отравления данных для исследований безопасности. Все модели, датасеты, код и сотни промежуточных чекпоинтов выложены в открытый доступ для воспроизводимых исследований мультиязычных LLM."
                },
                "en": {
                    "title": "Gaperon: Advancing Transparency in Multilingual Model Training",
                    "desc": "Gaperon is a suite of open-source language models for French and English, aimed at improving transparency in machine learning. It includes models with varying sizes, trained on vast amounts of text data, and provides all components of the training process. The research highlights the effects of data filtering on model performance, showing that while it improves text quality, it can negatively impact benchmark scores. Additionally, the study introduces methods for safe data handling and emphasizes the importance of reproducibility in developing multilingual models."
                },
                "zh": {
                    "title": "Gaperon：推动多语言模型的透明与可重复性",
                    "desc": "Gaperon是一个完全开放的法英编码语言模型套件，旨在提高大规模模型训练的透明度和可重复性。该模型家族包括1.5B、8B和24B参数模型，训练数据量达到2-4万亿个标记，并提供了完整的训练流程。研究表明，语言质量过滤可以提高文本的流畅性和连贯性，但在基准测试中表现不佳，而故意的后期污染可以在一定程度上恢复竞争性分数。通过开放发布所有模型、数据集、代码和检查点，Gaperon为多语言模型开发中的数据管理、评估、安全性和开放性之间的权衡提供了可重复的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22543",
            "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.22543",
            "abstract": "Flawed-Aware Policy Optimization (FAPO) improves reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, enhancing reasoning capability and training stability without increasing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models (LLMs). In this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization. However, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are rewarded identically to fully correct ones, causing policy models to internalize these unreliable reasoning patterns. In this work, we first conduct a systematic study of flawed-positive rollouts in RL and find that they enable rapid capability gains during the early optimization stage, while constraining reasoning capability later by reinforcing unreliable patterns. Building on these insights, we propose Flawed-Aware Policy Optimization (FAPO), which presents a parameter-free reward penalty for flawed-positive rollouts, enabling the policy to leverage them as useful shortcuts in the warm-up stage, securing stable early gains, while gradually shifting optimization toward reliable reasoning in the later refinement stage. To accurately and comprehensively detect flawed-positive rollouts, we introduce a generative reward model (GenRM) with a process-level reward that precisely localizes reasoning errors. Experiments show that FAPO is effective in broad domains, improving outcome correctness, process reliability, and training stability without increasing the token budget.",
            "score": 1,
            "issue_id": 6694,
            "pub_date": "2025-10-26",
            "pub_date_card": {
                "ru": "26 октября",
                "en": "October 26",
                "zh": "10月26日"
            },
            "hash": "1ea0d4949d046f8d",
            "authors": [
                "Yuyang Ding",
                "Chi Zhang",
                "Juntao Li",
                "Haibin Lin",
                "Xin Liu",
                "Min Zhang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22543.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умное наказание за правильные ответы с неправильной логикой",
                    "desc": "Статья представляет метод FAPO для улучшения обучения с подкреплением в больших языковых моделях. Проблема в том, что LLM получают одинаковое вознаграждение за правильные ответы с корректным рассуждением и за правильные ответы с ошибочной логикой (например, угадыванием). FAPO вводит штраф за такие «ложноположительные» примеры, позволяя модели использовать их на ранних этапах обучения, но постепенно переходя к надёжным паттернам рассуждений. Метод использует генеративную reward model для точного обнаружения ошибок в процессе рассуждения и улучшает качество без увеличения вычислительных затрат."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Flawed-Aware Policy Optimization",
                    "desc": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning by addressing flawed-positive rollouts that can mislead policy models. It introduces a penalty for these flawed rollouts, allowing models to initially benefit from them while gradually focusing on more reliable reasoning patterns. This approach improves the reasoning capabilities of large language models (LLMs) without increasing computational costs. The method includes a generative reward model to accurately identify and penalize flawed reasoning, leading to better training stability and outcome correctness."
                },
                "zh": {
                    "title": "提升推理能力的强化学习新方法",
                    "desc": "Flawed-Aware Policy Optimization (FAPO) 是一种改进强化学习的方法，专注于可验证奖励。它通过对错误的正向回放进行惩罚，增强了推理能力和训练的稳定性，而不增加计算成本。研究发现，错误的正向回放在早期优化阶段能快速提升能力，但会在后期限制推理能力。FAPO 通过引入生成奖励模型，准确检测推理错误，从而在优化过程中逐步引导模型向可靠的推理转变。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25758",
            "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
            "url": "https://huggingface.co/papers/2510.25758",
            "abstract": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "e35ada54dde29562",
            "authors": [
                "He Hu",
                "Yucheng Zhou",
                "Chiyuan Ma",
                "Qianning Wang",
                "Zheng Zhang",
                "Fei Ma",
                "Laizhong Cui",
                "Qi Tian"
            ],
            "affiliations": [
                "Auckland University of Technology",
                "CUHK, Shenzhen",
                "College of Computer Science and Software Engineering, Shenzhen University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "SKL-IOTSC, CIS, University of Macau",
                "School of Psychology, South China Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25758.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#alignment",
                    "#agents",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "TheraMind: новый подход в психологическом консультировании с использованием LLM",
                    "desc": "В статье рассматривается использование LLM в психологическом консультировании. Авторы представляют TheraMind, агент, который использует двойную петлевую архитектуру для улучшения адаптивности и стратегического планирования в терапии. Внутрисессионная петля управляет диалогом, учитывая эмоциональное состояние пациента, а межсессионная петля адаптирует терапию на основе долгосрочной памяти. Результаты показывают, что TheraMind превосходит другие методы по показателям многосессионной терапии, таким как когерентность и гибкость."
                },
                "en": {
                    "title": "TheraMind: Adaptive Longitudinal Counseling with Dual-Loop Architecture",
                    "desc": "TheraMind is a novel approach to psychological counseling using large language models (LLMs) that addresses key limitations in emotional understanding and long-term therapeutic strategies. It features a dual-loop architecture, consisting of an Intra-Session Loop for real-time dialogue management and a Cross-Session Loop for strategic planning across multiple sessions. This design allows TheraMind to adaptively respond to a patient's emotional state while maintaining continuity through long-term memory. Evaluations demonstrate that TheraMind significantly improves multi-session counseling metrics, showcasing its effectiveness in replicating adaptive therapeutic interactions."
                },
                "zh": {
                    "title": "TheraMind：智能心理咨询的未来",
                    "desc": "大型语言模型（LLMs）在心理咨询中的应用越来越受到关注，但现有方法往往缺乏情感理解和适应性策略，无法有效进行多次会话的治疗。为了解决这些问题，我们提出了TheraMind，一个用于长期心理咨询的战略性和适应性代理。TheraMind的核心是一个新颖的双循环架构，将复杂的咨询过程分为会话内循环和会话间循环，以实现战术对话管理和战略治疗规划。通过动态选择响应策略和评估治疗效果，TheraMind能够在多次会话中保持连贯性和适应性，显著提升了心理咨询的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25409",
            "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
            "url": "https://huggingface.co/papers/2510.25409",
            "abstract": "The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "825dd6c8c3ee41d8",
            "authors": [
                "Vijay Devane",
                "Mohd Nauman",
                "Bhargav Patel",
                "Aniket Mahendra Wakchoure",
                "Yogeshkumar Sant",
                "Shyam Pawar",
                "Viraj Thakur",
                "Ananya Godse",
                "Sunil Patra",
                "Neha Maurya",
                "Suraj Racha",
                "Nitish Kamal Singh",
                "Ajay Nagpal",
                "Piyush Sawarkar",
                "Kundeshwar Vijayrao Pundalik",
                "Rohit Saluja",
                "Ganesh Ramakrishnan"
            ],
            "affiliations": [
                "Indian Institute of Technology Bombay (IIT Bombay)",
                "Technology Innovation Hub (TIH) at IIT Bombay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25409.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "BhashaBench V1: Оценка LLM в индийском контексте",
                    "desc": "В статье обсуждается создание BhashaBench V1, первого доменно-специфического, многоцелевого, двуязычного бенчмарка для оценки LLM в индийском контексте. BhashaBench V1 включает 74,166 пар вопросов и ответов на английском и хинди, охватывающих такие области, как сельское хозяйство, право, финансы и аюрведа. Исследование показывает значительные различия в производительности моделей в зависимости от языка и домена, особенно в малоресурсных областях. BhashaBench V1 предоставляет обширный набор данных для оценки способности моделей интегрировать доменные знания с двуязычным пониманием."
                },
                "en": {
                    "title": "BhashaBench V1: Bridging Language and Domain Gaps in AI Evaluation",
                    "desc": "This paper presents BhashaBench V1, a new benchmark designed to evaluate large language models (LLMs) in the context of Indian knowledge systems. It includes 74,166 question-answer pairs in both English and Hindi, covering four key domains: Agriculture, Legal, Finance, and Ayurveda. The evaluation reveals significant performance gaps between languages and domains, highlighting that models perform better on English content. BhashaBench V1 aims to enhance the assessment of LLMs by providing a domain-specific and bilingual framework, promoting more accurate evaluations in low-resource areas."
                },
                "zh": {
                    "title": "BhashaBench V1：评估印度知识体系的双语基准",
                    "desc": "随着大型语言模型（LLMs）的快速发展，针对特定领域和文化的评估需求日益增加。现有的基准测试主要集中在英语，缺乏对印度特定背景的适用性。为了解决这一问题，我们推出了BhashaBench V1，这是首个专注于重要印度知识体系的领域特定多任务双语基准。该基准包含74,166个精心策划的问答对，涵盖农业、法律、金融和阿育吠陀等四个主要领域，能够对大型语言模型进行细致的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24801",
            "title": "Fortytwo: Swarm Inference with Peer-Ranked Consensus",
            "url": "https://huggingface.co/papers/2510.24801",
            "abstract": "As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.",
            "score": 0,
            "issue_id": 6699,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "f264d1695b67be58",
            "authors": [
                "Vladyslav Larin",
                "Ihor Naumenko",
                "Aleksei Ivashov",
                "Ivan Nikitin",
                "Alexander Firsov"
            ],
            "affiliations": [
                "FORTYTWO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24801.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#inference",
                    "#optimization",
                    "#security"
                ],
                "emoji": "🐝",
                "ru": {
                    "title": "Коллективный разум AI-моделей через распределённый консенсус",
                    "desc": "Статья представляет Fortytwo — протокол для децентрализованного AI-инференса, основанный на принципах роевого интеллекта. Вместо использования одной большой модели система агрегирует ответы множества разнородных моделей через попарное ранжирование по методу Bradley-Terry и репутационное взвешивание. Такой подход показывает значительное превосходство над простым мажоритарным голосованием: 85.90% против 68.69% на бенчмарке GPQA Diamond. Система использует on-chain репутацию и proof-of-capability для защиты от Sybil-атак и обеспечивает устойчивость к adversarial промптам, снижая деградацию производительности в 50 раз по сравнению с одиночной моделью."
                },
                "en": {
                    "title": "Empowering AI Inference through Swarm Intelligence and Reputation-Based Consensus",
                    "desc": "The paper introduces Fortytwo, a new protocol designed to enhance AI inference by utilizing swarm intelligence and distributed pairwise ranking. This method allows multiple AI models to collaborate and rank their outputs, leading to higher quality responses compared to traditional majority voting methods. Fortytwo also incorporates a reputation system that adjusts node influence based on performance, ensuring that only accurate models contribute to the consensus. Additionally, it includes measures to prevent Sybil attacks, making the system both secure and efficient for decentralized AI applications."
                },
                "zh": {
                    "title": "去中心化AI推理的新纪元",
                    "desc": "随着集中式人工智能面临计算瓶颈和训练收益递减，满足需求需要一个能够横向扩展的推理层。我们提出了Fortytwo，这是一种新颖的协议，利用群体智能原则和分布式成对排名共识来实现卓越的AI推理性能。该方法通过群体推理重新构想AI节点之间的协作，使用同行排名和声誉加权共识来提供高质量的响应。我们的评估表明，Fortytwo在多个基准测试中表现出更高的准确性和对对抗性干扰的强大抵抗力，奠定了去中心化AI系统的基础。"
                }
            }
        }
    ],
    "link_prev": "2025-10-29.html",
    "link_next": "2025-10-31.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "short_date_next": {
        "ru": "31.10",
        "en": "10/31",
        "zh": "10月31日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 8,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 2,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 9,
        "#math": 3,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 3,
        "#training": 9,
        "#robotics": 0,
        "#agi": 2,
        "#games": 5,
        "#interpretability": 0,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 11,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}