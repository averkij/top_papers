{
    "date": {
        "ru": "20 июня",
        "en": "June 20",
        "zh": "6月20日"
    },
    "time_utc": "2025-06-20 07:11",
    "weekday": 4,
    "issue_id": 4398,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.15154",
            "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
            "url": "https://huggingface.co/papers/2506.15154",
            "abstract": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  \t\t\t\t\tAI-generated summary \t\t\t\t Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.",
            "score": 2,
            "issue_id": 4397,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "f2b380f0491c0add",
            "authors": [
                "Anuradha Chopra",
                "Abhinaba Roy",
                "Dorien Herremans"
            ],
            "affiliations": [
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15154.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#games",
                    "#multimodal",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "SonicVerse: Понимание музыки через многозадачное обучение",
                    "desc": "SonicVerse - это мультизадачная модель для создания описаний музыки, которая объединяет генерацию подписей с определением музыкальных характеристик. Модель использует проекционную архитектуру, преобразующую аудиовход в языковые токены и одновременно определяющую музыкальные особенности через вспомогательные выходы. Для обучения модели был расширен датасет MusicBench с помощью аннотаций музыкальных характеристик, полученных с использованием MIRFLEX. Экспериментальные результаты показывают, что включение музыкальных характеристик улучшает качество и детализацию генерируемых описаний."
                },
                "en": {
                    "title": "Enhancing Music Descriptions with SonicVerse",
                    "desc": "SonicVerse is a multi-task music captioning model that improves the quality of music descriptions by integrating audio feature detection. It captures both low-level acoustic details and high-level musical attributes through a projection-based architecture. This model generates detailed captions for short music pieces and can create time-informed descriptions for longer compositions by using a large-language model. By enhancing the training dataset with music features, SonicVerse demonstrates improved caption quality and detail in its outputs."
                },
                "zh": {
                    "title": "SonicVerse：提升音乐字幕质量的多任务模型",
                    "desc": "SonicVerse是一种多任务音乐字幕生成模型，结合了音频特征检测以提高字幕质量。该模型通过关键音调检测、声乐检测等辅助任务，捕捉音乐的低级声学细节和高级音乐属性。其关键贡献在于采用基于投影的架构，将音频输入转换为语言标记，同时通过专用辅助头检测音乐特征。实验结果表明，这种特征的结合显著提升了生成字幕的质量和细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14837",
            "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
            "url": "https://huggingface.co/papers/2506.14837",
            "abstract": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.",
            "score": 0,
            "issue_id": 4396,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "3172095671c65e03",
            "authors": [
                "Chengzhi Xu",
                "Yuyang Wang",
                "Lai Wei",
                "Lichao Sun",
                "Weiran Huang"
            ],
            "affiliations": [
                "Lehigh University",
                "MIFA Lab, Shanghai Jiao Tong University",
                "Shanghai Innovation Institute",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14837.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Точная генерация кода графиков с помощью структурированных инструкций и итеративного уточнения",
                    "desc": "ChartIR - это метод итеративного уточнения для улучшения производительности мультимодальных больших языковых моделей (MLLM) в задаче генерации кода по изображению графика. Метод разделяет задачи визуального понимания и перевода в код, используя структурированные инструкции для описания и сравнения графиков. ChartIR применяет двухэтапный подход: начальная генерация кода и итеративное уточнение. Эксперименты показали превосходство ChartIR над другими методами на моделях Qwen2-VL и GPT-4."
                },
                "en": {
                    "title": "ChartIR: Refining Code Generation from Charts with Structured Instructions",
                    "desc": "ChartIR is a novel approach that enhances the performance of multimodal large language models (MLLMs) in generating code from charts by separating the tasks of visual understanding and code translation. It employs structured instructions to guide the model in accurately interpreting visual elements and translating them into executable code. The method involves two main stages: initial code generation followed by iterative refinement, which allows for progressive improvements in the output. Experimental results demonstrate that ChartIR significantly outperforms existing methods on both open-source and closed-source models."
                },
                "zh": {
                    "title": "ChartIR：提升图表到代码生成的智能方法",
                    "desc": "ChartIR是一种通过结构化指令和迭代优化来提升多模态大语言模型（MLLM）在图表到代码生成任务中的表现的方法。该方法将视觉理解和代码翻译任务分开，首先通过描述和差异两种结构化指令来捕捉图表的视觉元素。接着，ChartIR将整体图表生成流程分为初始代码生成和迭代优化两个阶段，从而逐步提升最终输出的质量。实验结果表明，与其他方法相比，ChartIR在开源模型Qwen2-VL和闭源模型GPT-4o上均表现出更优的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-06-19.html",
    "link_next": "2025-06-23.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6月19日"
    },
    "short_date_next": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6月23日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}