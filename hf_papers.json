{
    "date": "7 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 20,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.00907",
            "title": "Addition is All You Need for Energy-efficient Language Models",
            "abstract": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.",
            "url": "https://huggingface.co/papers/2410.00907",
            "score": 30,
            "issue_id": 20,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º L-Mul, –∫–æ—Ç–æ—Ä—ã–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç —É–º–Ω–æ–∂–µ–Ω–∏–µ —á–∏—Å–µ–ª —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π —Å –ø–æ–º–æ—â—å—é –æ–ø–µ—Ä–∞—Ü–∏–π —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å, —á–µ–º 8-–±–∏—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π, –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ L-Mul –≤ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –º–æ–∂–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å —ç–Ω–µ—Ä–≥–æ–∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ 95% –ø—Ä–∏ –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–º —É–º–Ω–æ–∂–µ–Ω–∏–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –∏ –Ω–∞ 80% –ø—Ä–∏ —Å–∫–∞–ª—è—Ä–Ω–æ–º –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ L-Mul —Å 3-–±–∏—Ç–Ω–æ–π –º–∞–Ω—Ç–∏—Å—Å–æ–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç float8_e5m2 –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#—Ç–µ–Ω–∑–æ—Ä–Ω—ã–µ–í—ã—á–∏—Å–ª–µ–Ω–∏—è",
                    "#—ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å–ù–µ–π—Ä–æ—Å–µ—Ç–µ–π",
                    "#–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è–£–º–Ω–æ–∂–µ–Ω–∏—è"
                ],
                "emoji": "üßÆ",
                "title": "L-Mul: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02760",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "url": "https://huggingface.co/papers/2410.02760",
            "score": 3,
            "issue_id": 19,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å—Ç–∏—Ä–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ELM (Erasure of Language Memory). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Å—Ç–∏—Ä–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ç—Ä–µ—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö: –Ω–µ–≤–∏–Ω–æ–≤–Ω–æ—Å—Ç–∏, –±–µ—Å–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏. ELM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å—Ç–µ—Ä—Ç—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –æ–±—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ ELM –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å—Ç–∏—Ä–∞–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç—è—Ö –±–∏–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.",
                "tags": [
                    "#conceptErasure",
                    "#languageModelSecurity",
                    "#targetedModelUpdates"
                ],
                "emoji": "üßΩ",
                "title": "ELM: –°—Ç–∏—Ä–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02703",
            "title": "Selective Attention Improves Transformer",
            "abstract": "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
            "url": "https://huggingface.co/papers/2410.02703",
            "score": 3,
            "issue_id": 19,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –°–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –í–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –Ω–µ–Ω—É–∂–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —Ç–∞–∫–æ–π –∂–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –∫–∞–∫ —É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –≤–¥–≤–æ–µ –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –°–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –í–Ω–∏–º–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–æ–∫—Ä–∞—â–µ–Ω–∏—é —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å 100 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ C4, —Ç—Ä–µ–±—É—é—Ç –≤ 16-47 —Ä–∞–∑ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥—É–ª—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.",
                "tags": [
                    "#SelectiveAttention",
                    "#MemoryEfficiency",
                    "#TransformerOptimization"
                ],
                "emoji": "üîç",
                "title": "–°–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –í–Ω–∏–º–∞–Ω–∏–µ: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03017",
            "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
            "abstract": "Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.",
            "url": "https://huggingface.co/papers/2410.03017",
            "score": 0,
            "issue_id": 20,
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Tutor CoPilot - —Å–∏—Å—Ç–µ–º—É —á–µ–ª–æ–≤–µ–∫–æ-–º–∞—à–∏–Ω–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–æ–≤. –í —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å —É—á–∞—Å—Ç–∏–µ–º 900 —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–æ–≤ –∏ 1800 —É—á–µ–Ω–∏–∫–æ–≤ –∏–∑ –º–∞–ª–æ–æ–±–µ—Å–ø–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤ –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —É—á–µ–Ω–∏–∫–∏, —Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Å —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º–∏ Tutor CoPilot, –Ω–∞ 4 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞ —á–∞—â–µ –æ—Å–≤–∞–∏–≤–∞–ª–∏ —Ç–µ–º—ã. –ê–Ω–∞–ª–∏–∑ 550 000+ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ Tutor CoPilot —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ —Å–∏—Å—Ç–µ–º—ã —á–µ–ª–æ–≤–µ–∫–æ-–º–∞—à–∏–Ω–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–æ–≥—É—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏ —Å–¥–µ–ª–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–ª—è –≤—Å–µ—Ö —É—á–∞—â–∏—Ö—Å—è.",
                "tags": [
                    "#tutorCoPilot",
                    "#humanAIeducation",
                    "#scalingExpertise"
                ],
                "emoji": "ü§ñüë®‚Äçüè´",
                "title": "Tutor CoPilot: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏"
            }
        }
    ]
}