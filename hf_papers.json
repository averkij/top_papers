{
    "date": {
        "ru": "4 марта",
        "en": "March 4",
        "zh": "3月4日"
    },
    "time_utc": "2025-03-04 07:11",
    "weekday": 1,
    "issue_id": 2513,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.01774",
            "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
            "url": "https://huggingface.co/papers/2503.01774",
            "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2times improvement in FID score over baselines while maintaining 3D consistency.",
            "score": 23,
            "issue_id": 2512,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "39af2f882aef9afb",
            "authors": [
                "Jay Zhangjie Wu",
                "Yuxuan Zhang",
                "Haithem Turki",
                "Xuanchi Ren",
                "Jun Gao",
                "Mike Zheng Shou",
                "Sanja Fidler",
                "Zan Gojcic",
                "Huan Ling"
            ],
            "affiliations": [
                "NVIDIA",
                "National University of Singapore",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01774.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Одношаговая диффузия для фотореалистичной 3D-реконструкции",
                    "desc": "Difix3D+ - это новый подход к улучшению 3D-реконструкции и синтеза изображений с новых ракурсов. В его основе лежит Difix - одношаговая модель диффузии изображений, обученная улучшать и устранять артефакты в визуализированных видах. Difix используется как на этапе реконструкции для очистки псевдо-обучающих видов, так и во время вывода для устранения остаточных артефактов. Difix3D+ совместим с представлениями NeRF и 3DGS и показывает двукратное улучшение оценки FID по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "Enhancing 3D Reconstruction with Difix3D+",
                    "desc": "This paper presents Difix3D+, a new method for improving 3D reconstruction and novel-view synthesis using single-step diffusion models. The core component, Difix, is an image diffusion model that enhances rendered views by removing artifacts caused by underconstrained areas in 3D representations. It plays a dual role by cleaning up pseudo-training views during reconstruction and acting as a neural enhancer during inference to eliminate residual artifacts. Difix3D+ is versatile, working with both Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), and it significantly improves the quality of 3D representations, achieving a 2x better FID score compared to existing methods."
                },
                "zh": {
                    "title": "Difix3D+: 提升3D重建与新视角合成的利器",
                    "desc": "Neural Radiance Fields（NeRF）和3D高斯点云（3D Gaussian Splatting）在3D重建和新视角合成任务中取得了重大进展。然而，从极端新视角实现真实感渲染仍然面临挑战，因为在表示中存在伪影。我们提出了Difix3D+，这是一种新颖的管道，旨在通过单步扩散模型增强3D重建和新视角合成。Difix作为核心模型，能够在重建阶段清理伪训练视图，并在推理阶段去除残留伪影，从而显著提高3D表示的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01785",
            "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2503.01785",
            "abstract": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.",
            "score": 20,
            "issue_id": 2511,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "ef2e10eb59ab7743",
            "authors": [
                "Ziyu Liu",
                "Zeyi Sun",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Yuhang Cao",
                "Haodong Duan",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01785.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#optimization",
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Visual-RFT: Революция в тонкой настройке визуально-языковых моделей",
                    "desc": "Статья представляет Visual Reinforcement Fine-Tuning (Visual-RFT) - метод, расширяющий применение обучения с подкреплением в визуальных задачах. Visual-RFT использует большие визуально-языковые модели для генерации ответов с токенами рассуждений и применяет визуально верифицируемые функции вознаграждения для обновления модели. Эксперименты показывают превосходство Visual-RFT над методом Supervised Fine-tuning в задачах классификации изображений, обнаружения объектов и обоснованного заземления. Метод демонстрирует значительное улучшение точности и обобщающей способности при ограниченном количестве обучающих примеров."
                },
                "en": {
                    "title": "Revolutionizing Visual Learning with Reinforcement Fine-Tuning",
                    "desc": "This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a method that enhances large vision-language models (LVLMs) by using reinforcement learning to improve their performance on visual tasks. Visual-RFT generates multiple responses for each input and employs verifiable reward functions to optimize the model's policy, making it particularly effective in scenarios with limited fine-tuning data. The approach demonstrates significant improvements in tasks like fine-grained image classification and object detection, outperforming traditional supervised fine-tuning methods. Overall, Visual-RFT represents a novel, efficient way to fine-tune LVLMs, focusing on reasoning and adaptability in specific domains."
                },
                "zh": {
                    "title": "视觉强化微调：提升推理与适应性的创新方法",
                    "desc": "强化微调（RFT）在大型推理模型中通过反馈学习，特别适用于微调数据稀缺的应用场景。本文提出的视觉强化微调（Visual-RFT）扩展了RFT在视觉任务中的应用，利用大型视觉语言模型生成多种响应，并通过可验证的视觉感知奖励函数进行模型更新。实验结果表明，Visual-RFT在细粒度图像分类和少样本目标检测等任务中表现出色，相较于传统的监督微调（SFT）方法，准确率显著提高。Visual-RFT代表了一种新的微调范式，提供了一种数据高效、以奖励驱动的方法，增强了模型在特定领域任务中的推理能力和适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01743",
            "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
            "url": "https://huggingface.co/papers/2503.01743",
            "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",
            "score": 14,
            "issue_id": 2511,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "fb054d6547a4a4fb",
            "authors": [
                "Abdelrahman Abouelenin",
                "Atabak Ashfaq",
                "Adam Atkinson",
                "Hany Awadalla",
                "Nguyen Bach",
                "Jianmin Bao",
                "Alon Benhaim",
                "Martin Cai",
                "Vishrav Chaudhary",
                "Congcong Chen",
                "Dong Chen",
                "Dongdong Chen",
                "Junkun Chen",
                "Weizhu Chen",
                "Yen-Chun Chen",
                "Yi-ling Chen",
                "Qi Dai",
                "Xiyang Dai",
                "Ruchao Fan",
                "Mei Gao",
                "Min Gao",
                "Amit Garg",
                "Abhishek Goswami",
                "Junheng Hao",
                "Amr Hendy",
                "Yuxuan Hu",
                "Xin Jin",
                "Mahmoud Khademi",
                "Dongwoo Kim",
                "Young Jin Kim",
                "Gina Lee",
                "Jinyu Li",
                "Yunsheng Li",
                "Chen Liang",
                "Xihui Lin",
                "Zeqi Lin",
                "Mengchen Liu",
                "Yang Liu",
                "Gilsinia Lopez",
                "Chong Luo",
                "Piyush Madan",
                "Vadim Mazalov",
                "Ali Mousavi",
                "Anh Nguyen",
                "Jing Pan",
                "Daniel Perez-Becker",
                "Jacob Platin",
                "Thomas Portet",
                "Kai Qiu",
                "Bo Ren",
                "Liliang Ren",
                "Sambuddha Roy",
                "Ning Shang",
                "Yelong Shen",
                "Saksham Singhal",
                "Subhojit Som",
                "Xia Song",
                "Tetyana Sych",
                "Praneetha Vaddamanu",
                "Shuohang Wang",
                "Yiming Wang",
                "Zhenghao Wang",
                "Haibin Wu",
                "Haoran Xu",
                "Weijian Xu",
                "Yifan Yang",
                "Ziyi Yang",
                "Donghan Yu",
                "Ishmam Zabir",
                "Jianwen Zhang",
                "Li Lyna Zhang",
                "Yunan Zhang",
                "Xiren Zhou"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01743.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#data",
                    "#agi",
                    "#synthetic",
                    "#long_context",
                    "#optimization",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Компактные модели с большими возможностями: прорыв в эффективности ИИ",
                    "desc": "Представлены две новые модели: Phi-4-Mini и Phi-4-Multimodal. Phi-4-Mini - это языковая модель с 3,8 миллиардами параметров, обученная на высококачественных веб-данных и синтетических данных, которая превосходит аналогичные модели в задачах математики и программирования. Phi-4-Multimodal - это мультимодальная модель, объединяющая текст, изображения и речь/аудио в единую систему с использованием LoRA-адаптеров. Обе модели демонстрируют высокую эффективность несмотря на свой компактный размер, превосходя более крупные аналоги в различных задачах."
                },
                "en": {
                    "title": "Compact Models, Superior Performance!",
                    "desc": "The paper presents Phi-4-Mini and Phi-4-Multimodal, two advanced models designed for language and multimodal tasks. Phi-4-Mini, with 3.8 billion parameters, excels in math and coding tasks by utilizing a high-quality synthetic data approach and an expanded vocabulary of 200K tokens. Phi-4-Multimodal integrates text, vision, and audio inputs, employing innovative techniques like LoRA adapters for efficient multi-modal processing. Both models demonstrate superior performance compared to larger counterparts, showcasing their effectiveness in complex reasoning and diverse input scenarios."
                },
                "zh": {
                    "title": "紧凑强大的多模态模型Phi-4系列",
                    "desc": "我们介绍了Phi-4-Mini和Phi-4-Multimodal这两种紧凑而强大的语言和多模态模型。Phi-4-Mini是一个拥有38亿参数的语言模型，经过高质量的网络和合成数据训练，在数学和编码任务中表现优于同类开源模型，并且在复杂推理方面与两倍于其规模的模型相当。相比于前身Phi-3.5-Mini，Phi-4-Mini扩展了词汇量，支持多语言应用，并采用了组查询注意力机制以提高长序列生成的效率。Phi-4-Multimodal则是一个多模态模型，能够将文本、视觉和语音/音频输入整合到一个模型中，支持多种推理模式，且在多个任务上超越了更大的视觉-语言和语音-语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01307",
            "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
            "url": "https://huggingface.co/papers/2503.01307",
            "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.",
            "score": 8,
            "issue_id": 2511,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "fa966620baa8c013",
            "authors": [
                "Kanishk Gandhi",
                "Ayush Chakravarthy",
                "Anikait Singh",
                "Nathan Lile",
                "Noah D. Goodman"
            ],
            "affiliations": [
                "Stanford University",
                "SynthLabs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01307.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Когнитивные навыки - ключ к самосовершенствованию языковых моделей",
                    "desc": "Исследование показывает, что способность языковых моделей к самосовершенствованию зависит от наличия у них определенных когнитивных поведений, таких как верификация, бэктрекинг, постановка подцелей и обратное планирование. Эксперименты выявили, что модель Qwen изначально обладает этими навыками, в то время как Llama нет. Прайминг Llama примерами, содержащими эти поведения, позволил значительно улучшить ее производительность при обучении с подкреплением. Важно отметить, что наличие правильных рассуждений оказалось более критичным фактором, чем корректность ответов."
                },
                "en": {
                    "title": "Unlocking Self-Improvement in Language Models through Reasoning",
                    "desc": "This paper explores how language models can improve their problem-solving abilities through a process called test-time inference, similar to human experts. It highlights the differences in performance between two models, Qwen-2.5-3B and Llama-3.2-3B, when trained with reinforcement learning (RL) on the game Countdown. The authors identify four cognitive behaviors—verification, backtracking, subgoal setting, and backward chaining—that are crucial for effective self-improvement in these models. They demonstrate that enhancing Llama with examples of these reasoning behaviors can significantly boost its performance, suggesting that the ability to reason is more important than simply providing correct answers."
                },
                "zh": {
                    "title": "推理行为是模型自我提升的关键",
                    "desc": "本文探讨了语言模型在复杂任务中自我改进的能力，特别是通过强化学习（RL）实现的自我提升。研究发现，不同模型在相同的RL训练下表现差异显著，例如Qwen-2.5-3B在游戏Countdown中远超Llama-3.2-3B。我们分析了四种关键的认知行为：验证、回溯、子目标设定和逆向链推理，发现Qwen自然展现了这些推理行为，而Llama则最初缺乏。通过对Llama进行示例引导，能够显著提升其在RL中的表现，证明了推理行为的存在是模型自我改进的关键因素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00784",
            "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
            "url": "https://huggingface.co/papers/2503.00784",
            "abstract": "Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding.",
            "score": 6,
            "issue_id": 2510,
            "pub_date": "2025-03-02",
            "pub_date_card": {
                "ru": "2 марта",
                "en": "March 2",
                "zh": "3月2日"
            },
            "hash": "b4870a0e44c3cc55",
            "authors": [
                "Kai Lv",
                "Honglin Guo",
                "Qipeng Guo",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00784.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "DuoDecoding: Параллельное ускорение языковых моделей",
                    "desc": "Статья представляет новый метод ускорения генерации текста большими языковыми моделями (LLM) под названием DuoDecoding. Этот подход использует параллельное декодирование на CPU и GPU, оптимизируя время генерации первого токена и общую латентность. DuoDecoding применяет аппаратно-ориентированный оптимальный бюджет черновика и динамическое многопоследовательное черновое декодирование для повышения качества. Эксперименты показали значительное ускорение генерации по сравнению с обычным спекулятивным декодированием."
                },
                "en": {
                    "title": "DuoDecoding: Speeding Up Text Generation with Smart Model Deployment",
                    "desc": "This paper introduces DuoDecoding, a new method to improve the speed of generating text with large language models (LLMs) while keeping the quality high. It uses a draft-then-verify approach, where a draft model quickly generates initial text, and a target model refines it, but does so in a way that reduces the time it takes to start generating text. By using both CPU and GPU for different parts of the process, DuoDecoding allows for faster and more efficient decoding. The results show that this method can significantly speed up text generation without sacrificing quality, achieving a notable improvement in performance across various tasks."
                },
                "zh": {
                    "title": "DuoDecoding：加速生成的新方法",
                    "desc": "大型语言模型（LLMs）在多种任务中表现出色，但其逐字自回归生成过程显著影响推理速度。推测解码提供了一种有前景的草稿-验证框架，能够减少生成延迟，同时保持输出分布的准确性。我们提出的DuoDecoding方法通过在CPU和GPU上分别部署草稿模型和目标模型，实现了并行解码，提升了生成效率。实验结果表明，DuoDecoding在生成延迟上实现了最高2.61倍的加速，同时将首次生成时间缩短至传统推测解码的83%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00501",
            "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions",
            "url": "https://huggingface.co/papers/2503.00501",
            "abstract": "User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\\&R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\\&R. To address the growing need for developing better S\\&R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \\& evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\\&R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\\&R services in the future.",
            "score": 3,
            "issue_id": 2513,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 марта",
                "en": "March 1",
                "zh": "3月1日"
            },
            "hash": "ed7fc8625b068597",
            "authors": [
                "Jia Chen",
                "Qian Dong",
                "Haitao Li",
                "Xiaohui He",
                "Yan Gao",
                "Shaosheng Cao",
                "Yi Wu",
                "Ping Yang",
                "Chen Xu",
                "Yao Hu",
                "Qingyao Ai",
                "Yiqun Liu"
            ],
            "affiliations": [
                "Tsinghua University",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00501.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#rag"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Qilin: мультимодальный датасет для улучшения поиска и рекомендаций",
                    "desc": "Представлен новый набор данных Qilin для мультимодального информационного поиска, собранный на платформе Xiaohongshu. Датасет включает пользовательские сессии с разнородными результатами (изображения, видео, коммерческие заметки) и контекстуальными сигналами. Qilin позволяет обучать и оценивать нейросетевые модели поиска и рекомендаций, а также исследовать влияние модуля глубоких ответов на запросы. Авторы надеются, что Qilin внесет значительный вклад в развитие мультимодальных платформ с поисковыми сервисами."
                },
                "en": {
                    "title": "Enhancing User Experiences with Qilin: A Multimodal Dataset for S&R Services",
                    "desc": "This paper introduces Qilin, a new multimodal information retrieval dataset designed to enhance search and recommendation (S&R) services in user-generated content communities. Qilin is unique as it includes diverse user sessions with various content types, such as image-text notes and videos, which can help in developing advanced multimodal neural retrieval models. Additionally, the dataset captures user feedback and contextual signals, allowing researchers to analyze user satisfaction and behavior more effectively. The findings from this research aim to improve S&R systems and contribute to the evolution of multimodal content platforms."
                },
                "zh": {
                    "title": "推动多模态搜索与推荐服务的进步",
                    "desc": "本文介绍了一个新的多模态信息检索数据集Qilin，旨在改善用户在复杂系统中的搜索和推荐体验。Qilin数据集来源于小红书，包含多种类型的用户会话，如图文笔记、视频笔记和商业笔记，适用于多种任务设置。该数据集还收集了丰富的应用级上下文信号和真实用户反馈，以更好地建模用户满意度。通过对Qilin的分析和实验，本文提供了有趣的发现，期望能推动多模态内容平台的搜索和推荐服务的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01370",
            "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
            "url": "https://huggingface.co/papers/2503.01370",
            "abstract": "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.",
            "score": 0,
            "issue_id": 2513,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "3decc9fe2b6f6e32",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Простая и эффективная 3D-генерация на основе 2D-диффузии",
                    "desc": "Статья представляет Kiss3DGen - эффективный фреймворк для генерации, редактирования и улучшения 3D-объектов с использованием предобученной модели диффузии для 2D-изображений. Метод основан на дообучении диффузионной модели для генерации 'Пакетного 3D-изображения', состоящего из мультиракурсных изображений и соответствующих карт нормалей. Затем карты нормалей используются для реконструкции 3D-меша, а мультиракурсные изображения обеспечивают текстурирование, что в результате дает полную 3D-модель. Авторы демонстрируют, что их подход совместим с различными техниками диффузионных моделей и позволяет эффективно создавать качественные 3D-модели."
                },
                "en": {
                    "title": "Kiss3DGen: Simplifying 3D Generation with 2D Diffusion Models",
                    "desc": "This paper presents Kiss3DGen, a novel framework that simplifies the process of generating and enhancing 3D objects by leveraging existing 2D image diffusion models. The approach involves fine-tuning a diffusion model to create a '3D Bundle Image', which consists of multiple views and normal maps that are essential for 3D reconstruction. By transforming the 3D generation challenge into a 2D image task, the method maximizes the use of knowledge from pretrained models, making it more efficient. The results show that Kiss3DGen not only generates high-quality 3D models but also supports advanced features like editing and texture enhancement."
                },
                "zh": {
                    "title": "简单高效的三维生成方法",
                    "desc": "扩散模型在生成二维图像方面取得了巨大成功，但在三维内容生成的质量和通用性上仍然有限。现有的先进方法通常需要大量的三维资产进行训练，这些资产难以收集。我们提出了Kiss3DGen（简单直接的三维生成），这是一个高效的框架，通过重新利用经过良好训练的二维图像扩散模型来生成、编辑和增强三维物体。该方法将三维生成问题转化为二维图像生成任务，最大化利用预训练扩散模型中的知识，能够有效生成高质量的三维模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01739",
            "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2503.01739",
            "abstract": "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.",
            "score": 0,
            "issue_id": 2512,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "046fdeee8939e82c",
            "authors": [
                "Wenhao Wang",
                "Yi Yang"
            ],
            "affiliations": [
                "University of Technology Sydney",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01739.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VideoUFO: Новый эталонный датасет для генерации видео по запросу",
                    "desc": "Статья представляет VideoUFO - новый набор данных для обучения моделей генерации видео по текстовому описанию. Этот датасет содержит более 1,09 миллиона видеоклипов с подробными и краткими описаниями, охватывающих 1291 тему, актуальную для пользователей. VideoUFO отличается минимальным пересечением с существующими наборами данных и использованием только видео под лицензией Creative Commons. Эксперименты показали, что простая модель, обученная на VideoUFO, превосходит другие модели на наиболее сложных темах."
                },
                "en": {
                    "title": "Empowering Text-to-Video Models with User-Focused Data",
                    "desc": "This paper introduces VideoUFO, a novel video dataset designed to enhance text-to-video generative models by focusing on user-relevant topics. The dataset contains over 1.09 million video clips, each accompanied by both brief and detailed captions, ensuring minimal overlap with existing datasets. By clustering user prompts, the authors identified 1,291 specific topics to guide video retrieval from YouTube, which were then segmented into clips. Experiments show that models trained on VideoUFO significantly outperform existing models, particularly on challenging topics, highlighting the importance of tailored training data in machine learning applications."
                },
                "zh": {
                    "title": "提升文本到视频生成的用户体验",
                    "desc": "本文介绍了一种新的视频数据集VideoUFO，旨在提高文本到视频生成模型的性能。该数据集专注于用户关注的主题，包含超过109万个视频片段，并为每个片段提供简短和详细的描述。VideoUFO与现有数据集的重叠率极低，且所有视频均通过YouTube的官方API获取，确保了数据的多样性和合法性。实验结果表明，使用VideoUFO训练的模型在用户关注的主题上表现优于其他模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01807",
            "title": "Large-Scale Data Selection for Instruction Tuning",
            "url": "https://huggingface.co/papers/2503.01807",
            "abstract": "Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.",
            "score": 0,
            "issue_id": 2511,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "8bbc980a9ef867f7",
            "authors": [
                "Hamish Ivison",
                "Muru Zhang",
                "Faeze Brahman",
                "Pang Wei Koh",
                "Pradeep Dasigi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Southern California",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01807.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективный отбор данных для обучения языковых моделей: меньше значит больше",
                    "desc": "Эта статья исследует методы автоматического отбора данных для инструктивной настройки языковых моделей. Авторы проводят систематическое изучение эффективности различных методов при масштабировании до больших объемов данных, выбирая до 2,5 миллионов образцов из пулов до 5,8 миллионов. Результаты показывают, что многие недавно предложенные методы уступают случайному отбору в этих условиях, однако вариант метода отбора на основе представлений (RDS+) превосходит более сложные подходы. Исследование подчеркивает важность тщательного анализа масштабируемости методов автоматического отбора данных."
                },
                "en": {
                    "title": "Quality Over Quantity: Smart Data Selection for Language Models",
                    "desc": "This paper investigates the importance of selecting high-quality training data for instruction-tuning language models. It reveals that many automated data selection methods do not perform better than random selection when scaling to larger datasets, which can include millions of samples. The study introduces a representation-based data selection method (RDS+) that consistently outperforms more complex approaches while being more efficient in terms of computational resources. The authors emphasize the need for a deeper examination of how these selection methods behave as the size of the data pools increases."
                },
                "zh": {
                    "title": "高效选择：优化语言模型训练数据的关键",
                    "desc": "在对语言模型进行指令调优时，从更大数据集中选择高质量的训练数据是一个关键步骤。经过精心策划的数据集通常能产生比那些在更大、更嘈杂的数据集上训练的模型更好的效果。我们进行了系统研究，评估数据选择方法在大规模数据集上的表现，发现许多新提出的方法在这种情况下的表现不如随机选择。我们还发现一种基于表示的数据选择变体（RDS+）在所有测试设置中始终优于更复杂的方法，同时计算效率更高。"
                }
            }
        }
    ],
    "link_prev": "2025-03-03.html",
    "link_next": "2025-03-05.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "03.03",
        "en": "03/03",
        "zh": "3月3日"
    },
    "short_date_next": {
        "ru": "05.03",
        "en": "03/05",
        "zh": "3月5日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "设计复杂工程挑战的解决方案对人类生产活动至关重要。然而，以前的检索增强生成（RAG）研究未能充分解决与复杂工程解决方案设计相关的任务。我们引入了一个新的基准，SolutionBench，来评估系统生成完整和可行的工程问题解决方案的能力。我们还提出了一个新系统，SolutionRAG，利用基于树的探索和双点思维机制生成可靠的解决方案。实验结果显示，SolutionRAG在SolutionBench上达到了最先进的性能，突显了其在实际应用中增强复杂工程解决方案设计自动化和可靠性的潜力。",
        "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
        "pinyin": "设计复杂工程挑战的解决方案对人类生产活动至关重要。然而，以前的检索增强生成（RAG）研究未能充分解决与复杂工程解决方案设计相关的任务。我们引入了一个新的基准，SolutionBench，来评估系统生成完整和可行的工程问题解决方案的能力。我们还提出了一个新系统，SolutionRAG，利用基于树的探索和双点思维机制生成可靠的解决方案。实验结果显示，SolutionRAG在SolutionBench上达到了最先进的性能，突显了其在实际应用中增强复杂工程解决方案设计自动化和可靠性的潜力。\n\nShèjì fùzá gōngchéng tiǎozhàn de jiějué fāng'àn duì rénlèi shēngchǎn huódòng zhìguān zhòngyào. Rán'ér, yǐqián de jiǎnsuǒ zēngqiáng shēngchéng (RAG) yánjiū wèi néng chóngfēn jiějué yǔ fùzá gōngchéng jiějué fāng'àn shèjì xiāngguān de rènwù. Wǒmen yǐnrù le yīgè xīn de jīzhǔn, SolutionBench, lái pínggū xìtǒng shēngchéng wánzhěng hé kěxíng de gōngchéng wèntí jiějué fāng'àn de nénglì. Wǒmen hái tíchū le yīgè xīn xìtǒng, SolutionRAG, lìyòng jīyú shù de tànsuǒ hé shuāngdiǎn sīwéi jīzhì shēngchéng kěkào de jiějué fāng'àn. Shíyàn jiéguǒ xiǎnshì, SolutionRAG zài SolutionBench shàng dá dào le zuì xiānjìn de xìngnéng, tūxì le qí zài shíjì yìngyòng zhōng zēngqiáng fùzá gōngchéng jiějué fāng'àn shèjì zìdònghuà hé kěkàoxìng de qiánlì.",
        "vocab": "[{'word': '设计', 'pinyin': 'shèjì', 'trans': 'design'},\n{'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'},\n{'word': '工程', 'pinyin': 'gōngchéng', 'trans': 'engineering'},\n{'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'},\n{'word': '解决方案', 'pinyin': 'jiějué fāngàn', 'trans': 'solution'},\n{'word': '至关重要', 'pinyin': 'zhìguān zhòngyào', 'trans': 'crucial'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhanced'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generation'},\n{'word': '研究', 'pinyin': 'yánjiū', 'trans': 'research'},\n{'word': '未能', 'pinyin': 'wèinéng', 'trans': 'failed to'},\n{'word': '充分', 'pinyin': 'chōngfèn', 'trans': 'adequately'},\n{'word': '相关', 'pinyin': 'xiāngguān', 'trans': 'related'},\n{'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'},\n{'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'},\n{'word': '完整', 'pinyin': 'wánzhěng', 'trans': 'complete'},\n{'word': '可行', 'pinyin': 'kěxíng', 'trans': 'feasible'},\n{'word': '工程问题', 'pinyin': 'gōngchéng wèntí', 'trans': 'engineering problem'},\n{'word': '解决方案', 'pinyin': 'jiějué fāngàn', 'trans': 'solution'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'capability'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '新系统', 'pinyin': 'xīn xìtǒng', 'trans': 'new system'},\n{'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '树', 'pinyin': 'shù', 'trans': 'tree'},\n{'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'exploration'},\n{'word': '双点思维', 'pinyin': 'shuāngdiǎn sīwéi', 'trans': 'dual-point thinking'},\n{'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'},\n{'word': '可靠', 'pinyin': 'kěkào', 'trans': 'reliable'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '达到', 'pinyin': 'dádào', 'trans': 'achieve'},\n{'word': '最先进', 'pinyin': 'zuì xiānjìn', 'trans': 'state-of-the-art'},\n{'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'},\n{'word': '突显', 'pinyin': 'tūxiǎn', 'trans': 'highlight'},\n{'word': '潜力', 'pinyin': 'qiánlì', 'trans': 'potential'},\n{'word': '实际', 'pinyin': 'shíjì', 'trans': 'practical'},\n{'word': '应用', 'pinyin': 'yìngyòng', 'trans': 'application'},\n{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhance'},\n{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automation'}]",
        "trans": "Designing solutions for complex engineering challenges is crucial for human productive activities. However, previous research on Retrieval-Augmented Generation (RAG) has failed to adequately address tasks related to the design of complex engineering solutions. We introduce a new benchmark, SolutionBench, to evaluate the capability of systems to generate complete and feasible engineering problem solutions. We also propose a new system, SolutionRAG, which utilizes tree-based exploration and dual-point thinking mechanisms to generate reliable solutions. Experimental results demonstrate that SolutionRAG achieves state-of-the-art performance on SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in practical applications.",
        "update_ts": "2025-03-03 09:12"
    }
}