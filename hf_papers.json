{
    "date": {
        "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 8",
        "zh": "9æœˆ8æ—¥"
    },
    "time_utc": "2025-09-08 07:12",
    "weekday": 0,
    "issue_id": 5765,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.04664",
            "title": "Why Language Models Hallucinate",
            "url": "https://huggingface.co/papers/2509.04664",
            "abstract": "Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This \"epidemic\" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.",
            "score": 26,
            "issue_id": 5763,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "a9af1f035c82b958",
            "authors": [
                "Adam Tauman Kalai",
                "Ofir Nachum",
                "Santosh S. Vempala",
                "Edwin Zhang"
            ],
            "affiliations": [
                "Georgia Tech",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04664.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑÑ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Transforming AI Trustworthiness by Addressing Hallucinations",
                    "desc": "This paper discusses how large language models often produce incorrect statements, known as 'hallucinations', due to their training and evaluation methods. These models are rewarded for guessing answers even when they are uncertain, which leads to plausible but false outputs. The authors analyze how these hallucinations stem from errors in binary classification and the statistical pressures in the training process. They propose that to reduce these hallucinations, the scoring systems of benchmarks should be modified to discourage guessing and promote acknowledgment of uncertainty, ultimately fostering more reliable AI systems."
                },
                "zh": {
                    "title": "æ”¹è¿›è¯„åˆ†æœºåˆ¶ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯ä¿¡åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­äº§ç”Ÿé”™è¯¯é™ˆè¿°çš„åŸå› ã€‚ç”±äºç°æœ‰çš„è¯„åˆ†æœºåˆ¶å¥–åŠ±çŒœæµ‹è€Œéæ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šæ—¶å€¾å‘äºçŒœæµ‹ï¼Œä»è€Œäº§ç”Ÿè™šå‡ä¿¡æ¯ã€‚ä½œè€…åˆ†æäº†ç°ä»£è®­ç»ƒæµç¨‹ä¸­å¯¼è‡´è¿™äº›â€œå¹»è§‰â€çš„ç»Ÿè®¡åŸå› ï¼Œå¹¶æŒ‡å‡ºè¿™äº›é”™è¯¯æºäºäºŒå…ƒåˆ†ç±»ä¸­çš„é”™è¯¯ã€‚ä¸ºäº†æé«˜è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œè®ºæ–‡å»ºè®®å¯¹ç°æœ‰åŸºå‡†çš„è¯„åˆ†æ–¹å¼è¿›è¡Œç¤¾ä¼šæŠ€æœ¯ä¸Šçš„è°ƒæ•´ï¼Œè€Œä¸æ˜¯å¢åŠ æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04744",
            "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
            "url": "https://huggingface.co/papers/2509.04744",
            "abstract": "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.",
            "score": 6,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "44d75a7c2c61a026",
            "authors": [
                "Gagan Mundada",
                "Yash Vishe",
                "Amit Namburi",
                "Xin Xu",
                "Zachary Novack",
                "Julian McAuley",
                "Junda Wu"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04744.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#survey"
                ],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "WildScore: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WildScore - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ½Ğ° WildScore Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "WildScore: Unlocking MLLMs' Music Reasoning Potential",
                    "desc": "WildScore is a benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in the context of symbolic music. It evaluates how well these models can interpret real-world music scores and respond to complex questions about music. The benchmark includes genuine musical compositions and user-generated queries, providing a realistic setting for analysis. By framing music reasoning as multiple-choice questions, WildScore allows for systematic evaluation of MLLMs' understanding of music, revealing both their strengths and areas needing improvement."
                },
                "zh": {
                    "title": "WildScoreï¼šéŸ³ä¹æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "WildScoreæ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¬¦å·éŸ³ä¹æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒé€šè¿‡çœŸå®çš„éŸ³ä¹ä¹è°±å’Œç”¨æˆ·ç”Ÿæˆçš„æŸ¥è¯¢ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨éŸ³ä¹åˆ†æä¸­çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨äº†ç³»ç»Ÿçš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº†é«˜å±‚æ¬¡å’Œç»†ç²’åº¦çš„éŸ³ä¹å­¦æœ¬ä½“ã€‚é€šè¿‡å°†å¤æ‚çš„éŸ³ä¹æ¨ç†æ¡†æ¶åŒ–ä¸ºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ï¼ŒWildScoreä¸ºMLLMsçš„ç¬¦å·éŸ³ä¹ç†è§£æä¾›äº†å¯æ§å’Œå¯æ‰©å±•çš„è¯„ä¼°æ–¹å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05263",
            "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
            "url": "https://huggingface.co/papers/2509.05263",
            "abstract": "LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18",
            "score": 2,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "027e61af3a0f5c1a",
            "authors": [
                "Yinglin Duan",
                "Zhengxia Zou",
                "Tongwei Gu",
                "Wei Jia",
                "Zhan Zhao",
                "Luyi Xu",
                "Xinzhu Liu",
                "Hao Jiang",
                "Kang Chen",
                "Shuang Qiu"
            ],
            "affiliations": [
                "Beihang University, China",
                "City University of Hong Kong, China",
                "NetEase, Inc., China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05263.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾",
                    "desc": "LatticeWorld - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Unreal Engine 5. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´. LatticeWorld Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑÑ†ĞµĞ½ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D World Generation with LatticeWorld",
                    "desc": "LatticeWorld is a 3D world generation framework that utilizes lightweight large language models (LLMs) and Unreal Engine 5 to create interactive environments from both textual and visual inputs. This framework aims to enhance the realism of simulations, bridging the gap between simulated and real-world scenarios, which is crucial for applications like autonomous driving and embodied AI. By employing generative methods, LatticeWorld can produce large-scale 3D worlds with dynamic agents, showcasing high-fidelity physics and real-time rendering capabilities. The results demonstrate a significant increase in production efficiency, achieving over 90 times faster output compared to traditional modeling techniques while maintaining high visual quality."
                },
                "zh": {
                    "title": "LatticeWorldï¼šé«˜æ•ˆç”ŸæˆåŠ¨æ€3Dä¸–ç•Œçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "LatticeWorldæ˜¯ä¸€ä¸ªä½¿ç”¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹å’Œè™šå¹»å¼•æ“5çš„3Dä¸–ç•Œç”Ÿæˆæ¡†æ¶ã€‚å®ƒèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬å’Œè§†è§‰è¾“å…¥åˆ›å»ºåŠ¨æ€ã€äº’åŠ¨çš„ç¯å¢ƒï¼Œå…·æœ‰é«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå¤§è§„æ¨¡çš„3Däº’åŠ¨ä¸–ç•Œï¼Œæ”¯æŒåŠ¨æ€ä»£ç†å’Œé«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿã€‚ä¸ä¼ ç»Ÿæ‰‹åŠ¨å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼ŒLatticeWorldåœ¨å·¥ä¸šç”Ÿäº§æ•ˆç‡ä¸Šæé«˜äº†90å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜åˆ›æ„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04013",
            "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
            "url": "https://huggingface.co/papers/2509.04013",
            "abstract": "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.",
            "score": 2,
            "issue_id": 5764,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "32f0ad5327f657e2",
            "authors": [
                "Riccardo Lunardi",
                "Vincenzo Della Mea",
                "Stefano Mizzaro",
                "Kevin Roitero"
            ],
            "affiliations": [
                "University of Udine, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04013.jpg",
            "data": {
                "categories": [
                    "#evaluation",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾Ñ‚Ñ‹ĞºĞ°ÑÑ‚ÑÑ Ğ¾ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Evaluating LLMs: Beyond Fixed Benchmarks to Real-World Language Variability",
                    "desc": "This paper investigates how well Large Language Models (LLMs) perform when faced with paraphrased questions, highlighting their limitations in dealing with linguistic variability. The authors found that while the rankings of LLMs remained stable, their effectiveness scores dropped significantly when questions were reworded. This indicates that current benchmark evaluations may not accurately reflect a model's ability to generalize to real-world language use. The study calls for the development of more robust evaluation methods that account for diverse question phrasing to better assess LLM capabilities."
                },
                "zh": {
                    "title": "æå‡LLMsé²æ£’æ€§ï¼Œé‡å¡‘è¯„ä¼°æ ‡å‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†åŒä¸€é—®é¢˜çš„ä¸åŒè¡¨è¿°æ—¶æ•ˆæœè¾ƒå·®ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¯­è¨€å˜å¼‚æ€§æ–¹é¢çš„å±€é™æ€§ã€‚è¿™é¡¹ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMså¯¹æ”¹å†™åŸºå‡†é—®é¢˜çš„é²æ£’æ€§ï¼Œå¹¶æ¢è®¨äº†åŸºäºåŸºå‡†çš„è¯„ä¼°æ˜¯å¦å¯é ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨ä¸åŒè¡¨è¿°ä¸‹çš„æ’åç›¸å¯¹ç¨³å®šï¼Œä½†å…¶ç»å¯¹æœ‰æ•ˆæ€§å¾—åˆ†æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜LLMsåœ¨åº”å¯¹çœŸå®ä¸–ç•Œçš„è¯­è¨€å˜å¼‚æ—¶å­˜åœ¨å›°éš¾ï¼Œå‘¼åå¼€å‘æ›´èƒ½åæ˜ å®é™…åº”ç”¨åœºæ™¯çš„è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05296",
            "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
            "url": "https://huggingface.co/papers/2509.05296",
            "abstract": "WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.",
            "score": 1,
            "issue_id": 5764,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "b6ac447839602a03",
            "authors": [
                "Zizun Li",
                "Jianjun Zhou",
                "Yifan Wang",
                "Haoyu Guo",
                "Wenzheng Chang",
                "Yang Zhou",
                "Haoyi Zhu",
                "Junyi Chen",
                "Chunhua Shen",
                "Tong He"
            ],
            "affiliations": [
                "SII",
                "Shanghai AI Lab",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05296.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "WinT3R: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "WinT3R - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. WinT3R Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒĞ» Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "WinT3R: Real-Time Camera Pose Estimation with High Precision",
                    "desc": "WinT3R is a feed-forward reconstruction model designed for accurate camera pose estimation and efficient real-time performance. It utilizes a sliding window mechanism to facilitate effective information sharing among frames, enhancing the quality of geometric predictions while minimizing computational load. Additionally, the model incorporates a global camera token pool, which improves the reliability of pose estimation without compromising speed. As a result, WinT3R achieves state-of-the-art performance in online reconstruction tasks, as demonstrated through extensive testing on various datasets."
                },
                "zh": {
                    "title": "WinT3Rï¼šé«˜æ•ˆç²¾å‡†çš„ç›¸æœºå§¿æ€ä¼°è®¡",
                    "desc": "WinT3Ræ˜¯ä¸€ç§å‰é¦ˆé‡å»ºæ¨¡å‹ï¼Œèƒ½å¤Ÿå®æ—¶é¢„æµ‹ç²¾ç¡®çš„ç›¸æœºå§¿æ€å’Œé«˜è´¨é‡çš„ç‚¹äº‘åœ°å›¾ã€‚ä»¥å¾€çš„æ–¹æ³•åœ¨é‡å»ºè´¨é‡å’Œå®æ—¶æ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œç¡®ä¿çª—å£å†…å¸§ä¹‹é—´çš„ä¿¡æ¯å……åˆ†äº¤æµï¼Œä»è€Œæé«˜å‡ ä½•é¢„æµ‹çš„è´¨é‡ã€‚é€šè¿‡ç»´æŠ¤ä¸€ä¸ªå…¨å±€ç›¸æœºä»¤ç‰Œæ± ï¼ŒWinT3Råœ¨ä¸ç‰ºç‰²æ•ˆç‡çš„æƒ…å†µä¸‹å¢å¼ºäº†ç›¸æœºå§¿æ€ä¼°è®¡çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03800",
            "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
            "url": "https://huggingface.co/papers/2509.03800",
            "abstract": "MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.",
            "score": 1,
            "issue_id": 5762,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "ad0922456cbd778e",
            "authors": [
                "Yuheng Li",
                "Yenho Chen",
                "Yuxiang Lai",
                "Jike Zhong",
                "Vanessa Wildman",
                "Xiaofeng Yang"
            ],
            "affiliations": [
                "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA",
                "Department of Computer Science, University of Southern California, Los Angeles, CA",
                "Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA",
                "Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03800.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#healthcare",
                    "#transfer_learning",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞšĞ¢ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "MedVista3D - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². MedVista3D Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing 3D CT Analysis with MedVista3D",
                    "desc": "MedVista3D is a new framework designed to improve the analysis of 3D CT scans by combining vision and language understanding. It tackles common problems in radiology, such as missing details and inconsistent report language, by enhancing local and global context in image analysis. The framework uses advanced techniques for aligning images with text, allowing for better disease detection and interpretation of medical reports. MedVista3D has shown to outperform existing models in tasks like disease classification and report retrieval, making it a significant advancement in medical imaging technology."
                },
                "zh": {
                    "title": "MedVista3Dï¼šæå‡3D CTåˆ†æçš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "MedVista3Dæ˜¯ä¸€ä¸ªå¤šå°ºåº¦è¯­ä¹‰å¢å¼ºçš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸“é—¨ç”¨äº3D CTåˆ†æã€‚å®ƒè§£å†³äº†å±€éƒ¨ä¸å…¨å±€ç†è§£ã€æŠ¥å‘Šå˜å¼‚æ€§ç­‰é—®é¢˜ï¼Œå¹¶åœ¨ç–¾ç—…åˆ†ç±»ã€æŠ¥å‘Šæ£€ç´¢å’ŒåŒ»å­¦è§†è§‰é—®ç­”ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œå®ç°äº†ç»†ç²’åº¦çš„è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å¼•å…¥äº†è¯­ä¹‰åŒ¹é…åº“æ¥å¤„ç†æŠ¥å‘Šçš„å˜å¼‚æ€§ã€‚MedVista3Dåœ¨é›¶æ ·æœ¬ç–¾ç—…åˆ†ç±»å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04504",
            "title": "Behavioral Fingerprinting of Large Language Models",
            "url": "https://huggingface.co/papers/2509.04504",
            "abstract": "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated Diagnostic Prompt Suite and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting",
            "score": 1,
            "issue_id": 5763,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "c8bc7caa6cf21161",
            "authors": [
                "Zehua Pei",
                "Hui-Ling Zhen",
                "Ying Zhang",
                "Zhiyuan Yang",
                "Xing Li",
                "Xianzhi Yu",
                "Mingxuan Yuan",
                "Bei Yu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04504.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#alignment",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ¾Ğº: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ 'ĞŸĞ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ¾Ğ¼'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ´ÑŒĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (alignment), Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unveiling the Hidden Behaviors of Language Models",
                    "desc": "This paper presents a new framework called 'Behavioral Fingerprinting' to evaluate Large Language Models (LLMs) beyond just their performance metrics. It uses a Diagnostic Prompt Suite and an automated evaluation pipeline to analyze the cognitive and interactive styles of various models. The study finds that while core reasoning abilities are becoming similar among top models, their alignment behaviors, such as sycophancy and semantic robustness, show significant differences. This indicates that a model's behavior is influenced more by the developers' alignment strategies than by its size or reasoning capabilities."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºç‰¹å¾",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„â€œè¡Œä¸ºæŒ‡çº¹â€æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¶…è¶Šä¼ ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡ï¼Œå…³æ³¨æ¨¡å‹çš„è¡Œä¸ºç‰¹å¾ã€‚é€šè¿‡ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è¯Šæ–­æç¤ºå¥—ä»¶å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ï¼Œåˆ†æäº†åå…«ç§ä¸åŒèƒ½åŠ›å±‚æ¬¡çš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡é¡¶çº§æ¨¡å‹åœ¨æŠ½è±¡å’Œå› æœæ¨ç†ç­‰æ ¸å¿ƒèƒ½åŠ›ä¸Šè¶‹äºä¸€è‡´ï¼Œä½†åœ¨å¯¹é½ç›¸å…³çš„è¡Œä¸ºï¼ˆå¦‚è°„åªšå’Œè¯­ä¹‰ç¨³å¥æ€§ï¼‰ä¸Šå´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ºæ­ç¤ºæ¨¡å‹ä¹‹é—´æ·±å±‚æ¬¡çš„è¡Œä¸ºå·®å¼‚æä¾›äº†ä¸€ç§å¯é‡å¤å’Œå¯æ‰©å±•çš„æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-05.html",
    "link_next": "2025-09-09.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#evaluation": 1
    }
}