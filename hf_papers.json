{
    "date": {
        "ru": "14 –∞–ø—Ä–µ–ª—è",
        "en": "April 14",
        "zh": "4Êúà14Êó•"
    },
    "time_utc": "2025-04-14 02:26",
    "weekday": 0,
    "issue_id": 3213,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.08685",
            "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
            "url": "https://huggingface.co/papers/2504.08685",
            "abstract": "This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/",
            "score": 1,
            "issue_id": 3213,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "43b42333b796033b",
            "authors": [
                "Team Seawead",
                "Ceyuan Yang",
                "Zhijie Lin",
                "Yang Zhao",
                "Shanchuan Lin",
                "Zhibei Ma",
                "Haoyuan Guo",
                "Hao Chen",
                "Lu Qi",
                "Sen Wang",
                "Feng Cheng",
                "Feilong Zuo Xuejiao Zeng",
                "Ziyan Yang",
                "Fangyuan Kong",
                "Zhiwu Qing",
                "Fei Xiao",
                "Meng Wei",
                "Tuyen Hoang",
                "Siyu Zhang",
                "Peihao Zhu",
                "Qi Zhao",
                "Jiangqiao Yan",
                "Liangke Gui",
                "Sheng Bi",
                "Jiashi Li",
                "Yuxi Ren",
                "Rui Wang",
                "Huixia Li",
                "Xuefeng Xiao",
                "Shu Liu",
                "Feng Ling",
                "Heng Zhang",
                "Houmin Wei",
                "Huafeng Kuang",
                "Jerry Duncan",
                "Junda Zhang",
                "Junru Zheng",
                "Li Sun",
                "Manlin Zhang",
                "Renfei Sun",
                "Xiaobin Zhuang",
                "Xiaojie Li",
                "Xin Xia",
                "Xuyan Chi",
                "Yanghua Peng",
                "Yuping Wang",
                "Yuxuan Wang",
                "Zhongkai Zhao",
                "Zhuo Chen",
                "Zuquan Song",
                "Zhenheng Yang",
                "Jiashi Feng",
                "Jianchao Yang",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08685.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models",
                    "#diffusion",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏",
                    "desc": "–≠—Ç–æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –ø—Ä–∏–º–µ—Ä–Ω–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (7B), –Ω–∞–∑–≤–∞–Ω–Ω—É—é Seaweed-7B, –æ–±—É—á–µ–Ω–Ω—É—é —Å –Ω—É–ª—è –∑–∞ 665 000 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã GPU H100. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, Seaweed-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –í –æ—Ç—á–µ—Ç–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—ã—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."
                },
                "en": {
                    "title": "Efficient Video Generation with Seaweed-7B: Small Size, Big Impact!",
                    "desc": "This paper introduces Seaweed-7B, a mid-sized video generation model with 7 billion parameters, trained efficiently using 665,000 GPU hours. Despite its smaller size, Seaweed-7B achieves competitive performance against larger models, showcasing the importance of strategic design choices in resource-limited environments. The model demonstrates strong generalization capabilities, allowing it to adapt effectively to various downstream tasks through lightweight fine-tuning or continued training. Overall, the findings suggest that a well-designed medium-sized model can rival larger counterparts while being more cost-effective to train."
                },
                "zh": {
                    "title": "ÁªèÊµéÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆ≠ÁªÉÁ≠ñÁï•",
                    "desc": "Êú¨ÊäÄÊúØÊä•ÂëäÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªèÊµéÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÂü∫Á°ÄÊ®°ÂûãËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÊàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Seaweed-7BÁöÑ‰∏≠ÂûãÁ†îÁ©∂Ê®°ÂûãÔºåÂÖ∑ÊúâÁ∫¶70‰∫ø‰∏™ÂèÇÊï∞Ôºå‰ΩøÁî®665,000‰∏™H100 GPUÂ∞èÊó∂‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉ„ÄÇÂ∞ΩÁÆ°ËÆ≠ÁªÉËµÑÊ∫êÈÄÇ‰∏≠ÔºåSeaweed-7BÁöÑÊÄßËÉΩ‰∏éÊõ¥Â§ßËßÑÊ®°ÁöÑÁé∞‰ª£ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁõ∏ÊØî‰ªçÁÑ∂ÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇÊä•ÂëäÂº∫Ë∞É‰∫ÜÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠Â¢ûÂº∫‰∏≠ÂûãÊâ©Êï£Ê®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂÜ≥Á≠ñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08366",
            "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
            "url": "https://huggingface.co/papers/2504.08366",
            "abstract": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/",
            "score": 1,
            "issue_id": 3213,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "df65a8f6baab7f84",
            "authors": [
                "Sauradip Nag",
                "Daniel Cohen-Or",
                "Hao Zhang",
                "Ali Mahdavi-Amiri"
            ],
            "affiliations": [
                "Simon Fraser University, Canada",
                "Tel Aviv University, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08366.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "üéûÔ∏è",
                "ru": {
                    "title": "–ò–∑ 2D –≤ 4D: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ –¥–≤—É–º –∫–∞–¥—Ä–∞–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É In-2-4D –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ 4D-–∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –¥–≤–∏–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª—å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–µ—Ç—Å—è 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gaussian Splatting, –∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã —É–ø—Ä–∞–≤–ª—è—é—Ç –¥–≤–∏–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–ª–µ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, —Ä–∞—Å—à–∏—Ä—è—è —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø—Ä–∏–º–µ–Ω—è—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –∂–µ—Å—Ç–∫–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Transforming 2D Images into Smooth 4D Motion!",
                    "desc": "This paper introduces the In-2-4D problem, which focuses on generating 4D representations of motion from just two images of an object in different states. The authors propose a hierarchical method that identifies keyframes to create smooth transitions between these states, addressing challenges posed by large frame-to-frame motions. They utilize Gaussian Splatting to construct 3D representations and apply a deformation field to guide the transformation of these representations into dynamic forms. The approach enhances temporal consistency and smoothness by merging motion segments and optimizing them to align with the original video, demonstrating its effectiveness through various experiments."
                },
                "zh": {
                    "title": "‰ªéÈùôÊÄÅÂà∞Âä®ÊÄÅÔºö4DËøêÂä®ÊèíÂÄºÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈóÆÈ¢òÔºåIn-2-4DÔºåÊó®Âú®‰ªé‰∏§‰∏™‰∏çÂêåËøêÂä®Áä∂ÊÄÅÁöÑÂçïËßÜÂõæÂõæÂÉè‰∏≠ÁîüÊàê4DÔºàÂç≥3D + Âä®‰ΩúÔºâÊèíÂÄº„ÄÇÁªôÂÆöË°®Á§∫Áâ©‰ΩìËøêÂä®Ëµ∑ÂßãÂíåÁªìÊùüÁä∂ÊÄÅÁöÑ‰∏§ÂπÖÂõæÂÉèÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÁîüÊàêÂíåÈáçÂª∫4D‰∏≠ÁöÑËøêÂä®„ÄÇÊàë‰ª¨ÈááÁî®ËßÜÈ¢ëÊèíÂÄºÊ®°ÂûãÊù•È¢ÑÊµãËøêÂä®Ôºå‰ΩÜÂ§ßÂπÖÂ∫¶ÁöÑÂ∏ßÈó¥ËøêÂä®ÂèØËÉΩÂØºËá¥Ê®°Á≥äÁöÑËß£Èáä„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ΩøÁî®ÂàÜÂ±ÇÊñπÊ≥ïËØÜÂà´‰∏éËæìÂÖ•Áä∂ÊÄÅËßÜËßâ‰∏äÊé•Ëøë‰∏îËøêÂä®ÊòæËëóÁöÑÂÖ≥ÈîÆÂ∏ßÔºåÁÑ∂ÂêéÂú®ÂÆÉ‰ª¨‰πãÈó¥ÁîüÊàêÂπ≥ÊªëÁöÑÁâáÊÆµ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07405",
            "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
            "url": "https://huggingface.co/papers/2504.07405",
            "abstract": "With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).",
            "score": 1,
            "issue_id": 3213,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "fb73f6a8f480a7a1",
            "authors": [
                "Linyan Huang",
                "Haonan Lin",
                "Yanning Zhou",
                "Kaiwen Xiao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.07405.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ì–∏–±–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "FlexIP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 2D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ –ø—Ä–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –∞–¥–∞–ø—Ç–µ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FlexIP –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ª—É—á—à–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "FlexIP: Balancing Identity and Personalization in 2D Generative Models",
                    "desc": "This paper presents FlexIP, a new framework designed to improve 2D generative models by separating the tasks of identity preservation and stylistic manipulation. It introduces two components: a Personalization Adapter that allows for diverse editing styles and a Preservation Adapter that ensures the subject's identity remains intact. By integrating these components, FlexIP enables users to dynamically adjust the balance between identity and personalization during the generation process. Experimental results show that FlexIP outperforms traditional methods, providing better identity retention while allowing for a wider range of personalized outputs."
                },
                "zh": {
                    "title": "ÁÅµÊ¥ªÁöÑË∫´‰ªΩ‰øùÊåÅ‰∏é‰∏™ÊÄßÂåñÁºñËæë",
                    "desc": "ÈöèÁùÄ‰∫åÁª¥ÁîüÊàêÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ïÔºå‰øùÊåÅ‰∏ª‰ΩìË∫´‰ªΩÂêåÊó∂ÂÆûÁé∞Â§öÊ†∑ÂåñÁºñËæëÊàê‰∏∫‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®Ë∫´‰ªΩ‰øùÊåÅÂíå‰∏™ÊÄßÂåñÊìç‰Ωú‰πãÈó¥Â≠òÂú®Âõ∫ÊúâÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFlexIPÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§‰∏™‰∏ìÈó®ÁöÑÁªÑ‰ª∂Ëß£ËÄ¶Ëøô‰∫õÁõÆÊ†áÔºö‰∏™ÊÄßÂåñÈÄÇÈÖçÂô®Áî®‰∫éÈ£éÊ†ºÂåñÊìç‰ΩúÔºå‰øùÊåÅÈÄÇÈÖçÂô®Áî®‰∫éË∫´‰ªΩÁª¥Êä§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÊÄßËÉΩÈôêÂà∂ÔºåÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑË∫´‰ªΩ‰øùÊåÅÔºåÂêåÊó∂ÊîØÊåÅÊõ¥Â§öÊ†∑ÂåñÁöÑ‰∏™ÊÄßÂåñÁîüÊàêËÉΩÂäõ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-11.html",
    "link_next": "2025-04-15.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4Êúà11Êó•"
    },
    "short_date_next": {
        "ru": "15.04",
        "en": "04/15",
        "zh": "4Êúà15Êó•"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫ÜKimi-VLÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÂÖ∑ÊúâÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÅÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂº∫Â§ßÁöÑ‰ª£ÁêÜËÉΩÂäõÔºåÂè™ÊøÄÊ¥ª2.8B‰∏™ËØ≠Ë®ÄËß£Á†ÅÂô®ÂèÇÊï∞„ÄÇKimi-VLÂú®Â§öËΩÆ‰ª£ÁêÜ‰ªªÂä°ÂíåÂ§öÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉËøòËÉΩÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÂíåÈ´òÂàÜËæ®ÁéáËßÜËßâËæìÂÖ•„ÄÇÂü∫‰∫éKimi-VLÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜKimi-VL-ThinkingÔºåÂÖ∑ÊúâÂº∫Â§ßÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂú®https://github.com/MoonshotAI/Kimi-VLÂÖ¨ÂºÄ„ÄÇ",
        "title": "Kimi-VL Technical Report",
        "pinyin": "Êàë‰ª¨‰ªãÁªç‰∫ÜKimi-VLÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÂÖ∑ÊúâÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÅÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂº∫Â§ßÁöÑ‰ª£ÁêÜËÉΩÂäõÔºåÂè™ÊøÄÊ¥ª2.8B‰∏™ËØ≠Ë®ÄËß£Á†ÅÂô®ÂèÇÊï∞„ÄÇKimi-VLÂú®Â§öËΩÆ‰ª£ÁêÜ‰ªªÂä°ÂíåÂ§öÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉËøòËÉΩÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÂíåÈ´òÂàÜËæ®ÁéáËßÜËßâËæìÂÖ•„ÄÇÂü∫‰∫éKimi-VLÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜKimi-VL-ThinkingÔºåÂÖ∑ÊúâÂº∫Â§ßÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂú®https://github.com/MoonshotAI/Kimi-VLÂÖ¨ÂºÄ„ÄÇ\n\nW«ímen ji√®sh√†o le Kimi-VL, yƒ´g√® kƒÅiyu√°n de h√πn h√© zhuƒÅnjiƒÅ sh√¨jiu√®-y«îy√°n m√≥x√≠ng. TƒÅ j√πy«íu xiƒÅnj√¨n de du≈ç m√≥sh√¨ tu√≠ l«ê, ch√°ng sh√†ng xi√† w√©n l«êjiƒõ h√© qi√°ngd√† de d√†il«ê n√©ngl√¨, zh«ê jƒ´hu√≥ 2.8B g√® y«îy√°n jiƒõm«éq√¨ cƒÅnsh√π. Kimi-VL z√†i du≈ç l√∫n d√†il«ê r√®nw√π h√© du≈ç zh«íng sh√¨jiu√® y«îy√°n r√®nw√π zh≈çng bi«éoxi√†n ch≈´s√®. TƒÅ h√°i n√©ng ch«îl«ê ch√°ng sh√†ng xi√† w√©n h√© gƒÅo fƒìnbi√†ol«ú sh√¨jiu√® sh≈´r√π. Jƒ´y√∫ Kimi-VL, w«ímen kƒÅifƒÅ le Kimi-VL-Thinking, j√πy«íu qi√°ngd√† de ch√°ng sh√≠jiƒÅn tu√≠ l«ê n√©ngl√¨. D√†im«é h√© m√≥x√≠ng z√†i https://github.com/MoonshotAI/Kimi-VL g≈çngkƒÅi.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"ÂºÄÊ∫ê\", \"pinyin\": \"kƒÅi yu√°n\", \"trans\": \"open source\"},\n    {\"word\": \"Ê∑∑Âêà\", \"pinyin\": \"h√πn h√©\", \"trans\": \"hybrid\"},\n    {\"word\": \"‰∏ìÂÆ∂\", \"pinyin\": \"zhuƒÅn jiƒÅ\", \"trans\": \"expert\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"ËØ≠Ë®Ä\", \"pinyin\": \"y«î y√°n\", \"trans\": \"language\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"ÂÖàËøõ\", \"pinyin\": \"xiƒÅn j√¨n\", \"trans\": \"advanced\"},\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ t√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"reasoning\"},\n    {\"word\": \"Èïø\", \"pinyin\": \"ch√°ng\", \"trans\": \"long\"},\n    {\"word\": \"‰∏ä‰∏ãÊñá\", \"pinyin\": \"sh√†ng xi√† w√©n\", \"trans\": \"context\"},\n    {\"word\": \"ÁêÜËß£\", \"pinyin\": \"l«ê jiƒõ\", \"trans\": \"understanding\"},\n    {\"word\": \"Âº∫Â§ß\", \"pinyin\": \"qi√°ng d√†\", \"trans\": \"powerful\"},\n    {\"word\": \"‰ª£ÁêÜ\", \"pinyin\": \"d√†i l«ê\", \"trans\": \"agent\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÊøÄÊ¥ª\", \"pinyin\": \"jƒ´ hu√≥\", \"trans\": \"activate\"},\n    {\"word\": \"Ëß£Á†ÅÂô®\", \"pinyin\": \"jiƒõ m«é q√¨\", \"trans\": \"decoder\"},\n    {\"word\": \"ÂèÇÊï∞\", \"pinyin\": \"cƒÅn sh√π\", \"trans\": \"parameters\"},\n    {\"word\": \"Â§öËΩÆ\", \"pinyin\": \"du≈ç l√∫n\", \"trans\": \"multi-turn\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n w√π\", \"trans\": \"task\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"Âá∫Ëâ≤\", \"pinyin\": \"ch≈´ s√®\", \"trans\": \"outstanding\"},\n    {\"word\": \"Â§ÑÁêÜ\", \"pinyin\": \"ch«î l«ê\", \"trans\": \"handle\"},\n    {\"word\": \"È´òÂàÜËæ®Áéá\", \"pinyin\": \"gƒÅo fƒìn b√†i l«ú\", \"trans\": \"high resolution\"},\n    {\"word\": \"ËæìÂÖ•\", \"pinyin\": \"sh≈´ r√π\", \"trans\": \"input\"},\n    {\"word\": \"Âü∫‰∫é\", \"pinyin\": \"jƒ´ y√∫\", \"trans\": \"based on\"},\n    {\"word\": \"ÂºÄÂèë\", \"pinyin\": \"kƒÅi fƒÅ\", \"trans\": \"develop\"},\n    {\"word\": \"ÈïøÊó∂Èó¥\", \"pinyin\": \"ch√°ng sh√≠ jiƒÅn\", \"trans\": \"long-term\"},\n    {\"word\": \"ÂÖ¨ÂºÄ\", \"pinyin\": \"g≈çng kƒÅi\", \"trans\": \"public\"},\n    {\"word\": \"‰ª£Á†Å\", \"pinyin\": \"d√†i m«é\", \"trans\": \"code\"}\n]",
        "trans": "We introduce Kimi-VL, an open-source hybrid expert visual-language model. It features advanced multimodal reasoning, long-context understanding, and powerful agent capabilities, activating only 2.8B language decoder parameters. Kimi-VL performs excellently in multi-turn agent tasks and various visual-language tasks. It can also handle long-context and high-resolution visual inputs. Based on Kimi-VL, we developed Kimi-VL-Thinking, which has strong long-term reasoning capabilities. The code and model are available at https://github.com/MoonshotAI/Kimi-VL.",
        "update_ts": "2025-04-13 12:41"
    }
}