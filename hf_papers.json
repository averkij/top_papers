{
    "date": {
        "ru": "18 августа",
        "en": "August 18",
        "zh": "8月18日"
    },
    "time_utc": "2025-08-18 05:17",
    "weekday": 0,
    "issue_id": 5396,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.11630",
            "title": "Thyme: Think Beyond Images",
            "url": "https://huggingface.co/papers/2508.11630",
            "abstract": "Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
            "score": 17,
            "issue_id": 5394,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9df408fb7ce360bf",
            "authors": [
                "Yi-Fan Zhang",
                "Xingyu Lu",
                "Shukang Yin",
                "Chaoyou Fu",
                "Wei Chen",
                "Xiao Hu",
                "Bin Wen",
                "Kaiyu Jiang",
                "Changyi Liu",
                "Tianke Zhang",
                "Haonan Fan",
                "Kaibing Chen",
                "Jiankang Chen",
                "Haojie Ding",
                "Kaiyu Tang",
                "Zhang Zhang",
                "Liang Wang",
                "Fan Yang",
                "Tingting Gao",
                "Guorui Zhou"
            ],
            "affiliations": [
                "CASIA",
                "Kwai Keye",
                "NJU",
                "THU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11630.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Тайм: новый уровень мышления ИИ с изображениями",
                    "desc": "Тайм (Thyme) - это новая парадигма, позволяющая мультимодальным языковым моделям (MLLM) автономно выполнять манипуляции с изображениями и вычисления. Она использует двухэтапную стратегию обучения и алгоритм GRPO-ATS для улучшения восприятия и рассуждений. Тайм позволяет моделям самостоятельно генерировать и выполнять разнообразные операции обработки изображений и вычисления через исполняемый код. Эксперименты на почти 20 бенчмарках показали значительное и последовательное улучшение производительности, особенно в сложных задачах восприятия изображений высокого разрешения и комплексных рассуждений."
                },
                "en": {
                    "title": "Think Beyond Images with Thyme!",
                    "desc": "Thyme is a new approach that allows Multi-Modal Language Models (MLLMs) to independently perform various image manipulations and computations, improving their ability to understand and reason about images. It uses a two-stage training method, starting with supervised fine-tuning on a large dataset to teach the model how to generate code for image processing. The second stage involves reinforcement learning to enhance the model's decision-making skills when applying these operations. The GRPO-ATS algorithm is introduced to optimize the balance between exploring reasoning tasks and executing code accurately, leading to better performance on numerous benchmarks."
                },
                "zh": {
                    "title": "超越图像思维的自主处理能力",
                    "desc": "Thyme是一种新颖的范式，旨在使多模态大语言模型（MLLMs）能够自主执行图像处理和计算操作，从而提升感知和推理任务的表现。该方法采用两阶段训练策略，首先在一个包含50万样本的精心策划数据集上进行监督微调（SFT），然后通过强化学习（RL）阶段来优化决策过程。我们提出的GRPO-ATS算法通过对文本和代码生成应用不同的温度，平衡推理探索与代码执行的精确性。实验结果表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其是在高分辨率感知和复杂推理任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11203",
            "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
            "url": "https://huggingface.co/papers/2508.11203",
            "abstract": "StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
            "score": 4,
            "issue_id": 5395,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9af71c9ddc545447",
            "authors": [
                "Seungmi Lee",
                "Kwan Yun",
                "Junyong Noh"
            ],
            "affiliations": [
                "KAIST, Visual Media Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11203.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Стильные 3D-лица из текста: новый уровень контроля и реализма",
                    "desc": "StyleMM - это новая система для создания стилизованных 3D-моделей лиц на основе текстовых описаний. Она использует диффузионную модель для преобразования изображений, сохраняя при этом ключевые черты лица. StyleMM обучается на стилизованных изображениях лиц и позволяет генерировать 3D-модели с контролем над формой, выражением и текстурой. Эксперименты показывают, что этот метод превосходит современные аналоги по разнообразию и возможностям стилизации лиц."
                },
                "en": {
                    "title": "Transforming Text to Stylized 3D Faces with Precision",
                    "desc": "StyleMM is a new framework that creates stylized 3D Morphable Models (3DMMs) from text descriptions, using a diffusion model for image-to-image translation. It fine-tunes a pre-trained mesh deformation network and a texture generator to ensure that the generated 3D faces maintain their original facial attributes while adopting new styles. The method prevents changes in identity and expression during the stylization process, allowing for consistent 3D style transfer. Ultimately, StyleMM enables the generation of customizable and animatable stylized face meshes, outperforming existing methods in facial diversity and stylization quality."
                },
                "zh": {
                    "title": "风格化3D模型的智能生成",
                    "desc": "StyleMM是一个新颖的框架，可以根据用户定义的文本描述构建风格化的3D可变形模型（3DMM）。该方法利用预训练的网格变形网络和纹理生成器，通过扩散模型进行图像到图像的翻译，生成风格化的面部图像作为目标。为了保持面部特征不变，我们引入了一种显式保留源图像面部属性的风格化方法。经过训练后，StyleMM能够生成具有明确形状、表情和纹理参数控制的风格化面部网格，确保在3DMM参数空间中的一致性风格转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11116",
            "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing",
            "url": "https://huggingface.co/papers/2508.11116",
            "abstract": "PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.",
            "score": 2,
            "issue_id": 5394,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "f0b9bc66fdef20ad",
            "authors": [
                "Zhuoqun Li",
                "Xuanang Chen",
                "Hongyu Lin",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11116.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Гибкий поиск научных статей на любом уровне детализации",
                    "desc": "PaperRegister - это новая система поиска научных статей, использующая иерархическую индексацию и адаптивный поиск. Она поддерживает гибкие и детализированные запросы, выходящие за рамки традиционных систем на основе аннотаций. PaperRegister преобразует обычный индекс на основе аннотаций в иерархическое дерево индексов, что позволяет выполнять поиск с различной степенью детализации. Эксперименты показали, что PaperRegister достигает наилучших результатов, особенно в сценариях с высокой степенью детализации запросов."
                },
                "en": {
                    "title": "Revolutionizing Paper Search with Hierarchical Indexing",
                    "desc": "PaperRegister is a novel system designed to improve the search for academic papers by utilizing hierarchical indexing and adaptive retrieval methods. Unlike traditional systems that rely solely on abstracts, PaperRegister allows for more detailed and flexible queries, accommodating specific research needs. The system organizes papers into a hierarchical index tree, enabling users to perform searches at various levels of granularity. Experimental results show that PaperRegister outperforms existing methods, particularly in scenarios requiring fine-grained search capabilities."
                },
                "zh": {
                    "title": "PaperRegister：灵活细粒度论文搜索的新方法",
                    "desc": "PaperRegister 是一种改进论文搜索的方法，它通过层次索引和自适应检索来增强搜索体验。传统的论文搜索系统主要依赖摘要来构建索引，无法满足细粒度查询的需求。PaperRegister 通过将传统的基于摘要的索引转变为层次索引树，支持更灵活的查询方式。实验结果表明，PaperRegister 在各种细粒度的论文搜索任务中表现优异，展示了其在实际应用中的良好潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11616",
            "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
            "url": "https://huggingface.co/papers/2508.11616",
            "abstract": "A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.",
            "score": 1,
            "issue_id": 5396,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "a8b7ff59eca3faf3",
            "authors": [
                "Oscar Mañas",
                "Pierluca D'Oro",
                "Koustuv Sinha",
                "Adriana Romero-Soriano",
                "Michal Drozdzal",
                "Aishwarya Agrawal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Meta FAIR",
                "Mila - Quebec AI Institute",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11616.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment",
                    "#multimodal",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Управляемое декодирование MLLM для точной визуальной привязки",
                    "desc": "Статья представляет метод управляемого декодирования для мультимодальных больших языковых моделей (MLLM), улучшающий визуальную привязку. Авторы разработали модели вознаграждения для контроля точности и полноты распознавания объектов. Метод позволяет динамически регулировать баланс между вычислительными затратами и качеством визуальной привязки. Эксперименты показали превосходство предложенного подхода над существующими методами снижения галлюцинаций в MLLM."
                },
                "en": {
                    "title": "Dynamic Control for Enhanced Visual Grounding in MLLMs",
                    "desc": "This paper presents a novel reward-guided decoding method for Multimodal Large Language Models (MLLMs) that enhances visual grounding by managing object precision and recall. The authors introduce reward models that guide the decoding process, allowing users to adjust the importance of precision versus recall dynamically. This method not only improves the quality of image captioning tasks but also offers flexibility in balancing computational resources with grounding accuracy. The results demonstrate that this approach significantly outperforms existing methods for reducing object hallucination in MLLMs."
                },
                "zh": {
                    "title": "动态控制多模态语言模型的视觉定位",
                    "desc": "本文提出了一种基于奖励引导的解码方法，用于多模态大型语言模型（MLLMs），旨在提高视觉定位的精确度和召回率。我们构建了两个独立的奖励模型，分别控制模型输出中对象的精确度和召回率，从而实现动态的权衡。该方法允许用户在解码过程中实时调整奖励函数的重要性，以适应不同的任务需求。通过在标准对象幻觉基准上进行评估，我们的方法在控制MLLM推理方面表现出显著优势，超越了现有的幻觉缓解方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06429",
            "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation",
            "url": "https://huggingface.co/papers/2508.06429",
            "abstract": "A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.",
            "score": 1,
            "issue_id": 5395,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 августа",
                "en": "August 8",
                "zh": "8月8日"
            },
            "hash": "f1862f0be07f6b3d",
            "authors": [
                "Guido Manni",
                "Clemente Lauretti",
                "Loredana Zollo",
                "Paolo Soda"
            ],
            "affiliations": [
                "Unit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy",
                "Unit of Artificial Intelligence and Computer Systems, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06429.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#healthcare",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Эффективная классификация медицинских изображений при минимуме размеченных данных",
                    "desc": "Статья представляет новый полу-контролируемый метод обучения на основе генеративно-состязательных сетей (GAN) для классификации медицинских изображений при ограниченном количестве размеченных данных. Подход включает три специализированные нейронные сети: генератор для условного преобразования изображений, дискриминатор для оценки подлинности и классификации, и отдельный классификатор. Метод использует ансамблевое псевдо-маркирование, комбинируя взвешенные по уверенности предсказания дискриминатора и классификатора. Эксперименты на 11 наборах данных MedMNIST показали статистически значимое улучшение по сравнению с шестью современными GAN-методами, особенно в условиях крайне малого количества размеченных примеров."
                },
                "en": {
                    "title": "Enhancing Medical Image Classification with Minimal Labels Using GANs",
                    "desc": "This paper presents a GAN-based semi-supervised learning framework that enhances medical image classification using very few labeled samples. It combines three specialized neural networks: a generator for transforming images based on class conditions, a discriminator for assessing image authenticity and classification, and a classifier for final predictions. The training process alternates between supervised learning on limited labeled data and unsupervised learning using abundant unlabeled images, employing image-to-image translation techniques. The method also utilizes ensemble-based pseudo-labeling to improve label estimation, demonstrating significant performance gains across various datasets, especially in scenarios with extremely limited labeled data."
                },
                "zh": {
                    "title": "基于GAN的半监督学习提升医疗图像分类",
                    "desc": "本文提出了一种基于生成对抗网络（GAN）的半监督学习框架，旨在改善医疗图像分类，尤其是在标注数据稀缺的情况下。该框架结合了三种专门的神经网络，包括用于图像转换的生成器、用于真实性评估和分类的判别器，以及专用分类器。通过在有限标注数据上进行监督训练和利用大量未标注图像进行无监督学习，该方法实现了有效的标签估计。实验结果表明，该方法在多个数据集上显著优于现有的半监督方法，尤其在标注样本极少的情况下表现出色。"
                }
            }
        }
    ],
    "link_prev": "2025-08-15.html",
    "link_next": "2025-08-19.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "15.08",
        "en": "08/15",
        "zh": "8月15日"
    },
    "short_date_next": {
        "ru": "19.08",
        "en": "08/19",
        "zh": "8月19日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}