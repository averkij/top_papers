{
    "date": {
        "ru": "28 августа",
        "en": "August 28",
        "zh": "8月28日"
    },
    "time_utc": "2025-08-28 03:30",
    "weekday": 3,
    "issue_id": 5586,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.19652",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "url": "https://huggingface.co/papers/2508.19652",
            "abstract": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
            "score": 28,
            "issue_id": 5585,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "990502a2cc19d192",
            "authors": [
                "Zongxia Li",
                "Wenhao Yu",
                "Chengsong Huang",
                "Rui Liu",
                "Zhenwen Liang",
                "Fuxiao Liu",
                "Jingxi Che",
                "Dian Yu",
                "Jordan Boyd-Graber",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Seattle",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19652.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение визуального мышления ИИ без внешних аннотаций",
                    "desc": "Vision-SR1 - это метод улучшения визуального мышления в мультимодальных моделях с помощью обучения с подкреплением. Он разделяет процесс на этапы визуального восприятия и языкового рассуждения, что повышает точность и уменьшает галлюцинации. Метод использует самовознаграждение, не полагаясь на внешние визуальные аннотации. Эксперименты показывают, что Vision-SR1 улучшает визуальное мышление, снижает визуальные галлюцинации и уменьшает зависимость от языковых шаблонов в различных задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Rewarding Learning",
                    "desc": "Vision-SR1 is a novel approach that enhances visual reasoning in vision-language models (VLMs) by using reinforcement learning. It breaks down the reasoning process into two distinct stages: visual perception and language reasoning, allowing for more focused training. By generating self-contained visual perceptions, the model can validate its understanding without needing to refer back to the original image. This method reduces visual hallucinations and reliance on language shortcuts, leading to improved accuracy in various vision-language tasks."
                },
                "zh": {
                    "title": "Vision-SR1：提升视觉推理的自我奖励方法",
                    "desc": "Vision-SR1是一种利用强化学习的方法，旨在增强视觉语言模型中的视觉推理能力。该方法将推理过程分解为视觉感知和语言推理两个阶段，从而提高了模型的准确性并减少了幻觉现象。通过自我奖励机制，Vision-SR1不依赖外部视觉监督，能够有效地训练模型进行更好的视觉感知和语言推理。实验结果表明，Vision-SR1在多种视觉语言任务中显著改善了视觉推理能力，降低了对语言捷径的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19493",
            "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
            "url": "https://huggingface.co/papers/2508.19493",
            "abstract": "A large-scale benchmark evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models, revealing significant gaps in their ability to protect sensitive user information.  \t\t\t\t\tAI-generated summary \t\t\t\t Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
            "score": 4,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "d7a43858b898f00a",
            "authors": [
                "Zhixin Lin",
                "Jungang Li",
                "Shidong Pan",
                "Yibo Shi",
                "Yue Yao",
                "Dongliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shandong University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19493.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "Смартфонные агенты на MLLM не справляются с защитой приватности",
                    "desc": "Это исследование оценивает способность смартфонных агентов, основанных на мультимодальных больших языковых моделях (MLLM), защищать конфиденциальную информацию пользователей. Авторы создали масштабный бенчмарк из 7138 сценариев для оценки осведомленности агентов о приватности. Результаты показывают, что большинство агентов демонстрируют неудовлетворительную осведомленность о приватности, не превышающую 60% даже с явными подсказками. Исследование выявило значительные пробелы в способности смартфонных агентов защищать чувствительные данные пользователей."
                },
                "en": {
                    "title": "Rethinking Privacy in Smartphone Agents: A Call for Better Awareness",
                    "desc": "This paper evaluates the privacy awareness of smartphone agents that use Multimodal Large Language Models (MLLMs). It presents a large-scale benchmark consisting of 7,138 scenarios to assess how well these agents protect sensitive user information. The findings reveal that most agents perform poorly in privacy awareness, with scores below 60%, and closed-source agents generally outperform open-source ones. The study highlights the need for a better balance between utility and privacy in the design of smartphone agents."
                },
                "zh": {
                    "title": "智能手机助手的隐私保护能力亟待提升",
                    "desc": "这篇论文评估了多模态大型语言模型驱动的智能手机助手在隐私保护方面的能力。研究发现，这些助手在处理敏感用户信息时存在显著的隐私意识缺口。通过对7138个场景进行大规模基准测试，结果显示几乎所有被测试的助手隐私意识表现不佳，得分均低于60%。研究还发现，闭源助手的隐私保护能力普遍优于开源助手，且场景的敏感性与隐私检测能力密切相关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19229",
            "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
            "url": "https://huggingface.co/papers/2508.19229",
            "abstract": "A generative judge model, StepWiser, uses reinforcement learning to provide step-by-step reasoning feedback, improving both training and inference performance of policy models.  \t\t\t\t\tAI-generated summary \t\t\t\t As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.",
            "score": 3,
            "issue_id": 5586,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "6ffd72787adea812",
            "authors": [
                "Wei Xiong",
                "Wenting Zhao",
                "Weizhe Yuan",
                "Olga Golovneva",
                "Tong Zhang",
                "Jason Weston",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta",
                "NYU",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19229.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Мета-рассуждения для улучшения пошагового мышления ИИ",
                    "desc": "StepWiser - это генеративная модель-судья, использующая обучение с подкреплением для оценки промежуточных шагов рассуждения других моделей. Она генерирует пояснения своих оценок, что улучшает точность суждений по сравнению с существующими методами. StepWiser может применяться для улучшения обучения моделей-исполнителей и оптимизации поиска при выводе. Этот подход переосмысливает задачу пошаговой оценки как задачу рассуждения, а не классификации."
                },
                "en": {
                    "title": "StepWiser: Enhancing Reasoning with Generative Feedback",
                    "desc": "The paper introduces StepWiser, a generative judge model that enhances the performance of policy models through reinforcement learning. It addresses the challenge of supervising multi-step reasoning by providing detailed feedback on each reasoning step, rather than just classifying them. Unlike traditional methods that rely on static datasets, StepWiser reframes the task to focus on reasoning itself, allowing for better generalization. The results demonstrate that StepWiser improves judgment accuracy, aids in training policy models, and enhances inference-time search capabilities."
                },
                "zh": {
                    "title": "逐步推理的生成性评判模型",
                    "desc": "本文提出了一种生成性评判模型StepWiser，利用强化学习提供逐步推理反馈，从而提升策略模型的训练和推理性能。随着模型越来越多地采用多步推理策略来解决复杂问题，监督这些中间步骤的逻辑有效性成为一个重要的研究挑战。现有的过程奖励模型虽然提供逐步反馈，但通常仅作为分类器，缺乏解释，并且依赖静态数据集的监督微调限制了其泛化能力。StepWiser通过将逐步奖励建模重新构建为推理任务，能够在给出最终判断之前输出思考令牌，从而提高中间步骤的判断准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20096",
            "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.20096",
            "abstract": "CODA, a trainable compositional framework, combines a generalist planner and specialist executor to achieve robust execution and cross-domain generalization in scientific computing GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
            "score": 2,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "8449940bace28db5",
            "authors": [
                "Zeyi Sun",
                "Yuhang Cao",
                "Jianze Liang",
                "Qiushi Sun",
                "Ziyu Liu",
                "Zhixiong Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Kai Chen",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#science"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CODA: Умный тандем планировщика и исполнителя для научных GUI",
                    "desc": "CODA - это новая обучаемая композиционная архитектура для автономных агентов в научных графических интерфейсах. Она сочетает обобщенный планировщик (Cerebrum) и специализированный исполнитель (Cerebellum), обучаемые по двухэтапному алгоритму. На первом этапе специализации для каждого приложения обучается экспертный планировщик, а на втором этапе обобщения финальный планировщик дообучается на агрегированном наборе данных. CODA превосходит базовые модели на бенчмарке ScienceBoard, демонстрируя надежное выполнение задач и кросс-доменную генерализацию."
                },
                "en": {
                    "title": "CODA: Bridging Planning and Execution for Robust AI in Science",
                    "desc": "CODA is a novel framework designed to improve the performance of autonomous agents in scientific computing GUIs by combining a generalist planner and a specialist executor. It addresses the limitations of existing methods that struggle with the trade-off between planning and execution. CODA employs a two-stage training process: first, it specializes the planner for individual tasks, and then it generalizes by aggregating successful task trajectories for fine-tuning. This approach allows CODA to achieve robust execution and effective cross-domain generalization, outperforming previous models in benchmark evaluations."
                },
                "zh": {
                    "title": "CODA：科学计算的智能执行新框架",
                    "desc": "CODA是一个可训练的组合框架，结合了通用规划器和专业执行器，以实现科学计算图形用户界面（GUI）的稳健执行和跨领域泛化。现有方法在通用代理和专业代理之间存在权衡，通用代理在规划上表现优异，但执行能力较差，而专业代理则相反。CODA通过一个两阶段的训练流程，首先为每个科学应用训练专家规划器，然后聚合成功的轨迹进行监督微调，从而克服了数据稀缺的限制。经过评估，CODA在多个挑战性应用中显著超越了基线，确立了开源模型的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20072",
            "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
            "url": "https://huggingface.co/papers/2508.20072",
            "abstract": "Discrete Diffusion VLA uses a single-transformer policy with discrete diffusion to model actions, improving decoding order, consistency, and performance over autoregressive and continuous diffusion methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
            "score": 1,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "ec096752edb34e60",
            "authors": [
                "Zhixuan Liang",
                "Yizhuo Li",
                "Tianshuo Yang",
                "Chengyue Wu",
                "Sitong Mao",
                "Liuao Pei",
                "Xiaokang Yang",
                "Jiangmiao Pang",
                "Yao Mu",
                "Ping Luo"
            ],
            "affiliations": [
                "Huawei Cloud Computing Technologies Co., Ltd.",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20072.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#games",
                    "#cv",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Дискретная диффузия для точного моделирования действий в VLA задачах",
                    "desc": "Статья представляет Discrete Diffusion VLA - новый подход к моделированию действий в задачах зрения-языка-действия (Vision-Language-Action, VLA). Модель использует единый трансформер с дискретной диффузией для генерации дискретизированных фрагментов действий. Этот метод позволяет адаптивно определять порядок декодирования, улучшая согласованность и производительность по сравнению с авторегрессивными методами и методами непрерывной диффузии. Discrete Diffusion VLA достигает высоких результатов на нескольких бенчмарках, демонстрируя потенциал для масштабирования VLA моделей."
                },
                "en": {
                    "title": "Revolutionizing Action Modeling with Discrete Diffusion VLA",
                    "desc": "The paper introduces Discrete Diffusion VLA, a novel approach that utilizes a single-transformer policy to model actions through discrete diffusion. This method enhances the decoding process by allowing actions to be generated in an adaptive order, addressing simpler actions before more complex ones. By maintaining compatibility with the discrete token interface of vision-language models (VLMs), it simplifies training and improves performance without the need for specialized iterative sampling. The results demonstrate significant improvements in action modeling accuracy and consistency, paving the way for scaling VLA applications to larger datasets and models."
                },
                "zh": {
                    "title": "离散扩散VLA：提升动作建模与一致性",
                    "desc": "离散扩散VLA使用单一变换器策略，通过离散扩散来建模动作，改善了解码顺序、一致性和性能，优于自回归和连续扩散方法。该模型将图像和指令映射到机器人动作，采用与视觉语言模型（VLM）相同的交叉熵目标进行训练。设计保留了扩散的渐进细化范式，同时与VLM的离散令牌接口兼容。离散扩散VLA在LIBERO上实现了96.3%的平均成功率，表明其支持精确的动作建模和一致的训练，为将VLA扩展到更大模型和数据集奠定了基础。"
                }
            }
        }
    ],
    "link_prev": "2025-08-27.html",
    "link_next": "2025-08-29.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8月27日"
    },
    "short_date_next": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8月29日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}