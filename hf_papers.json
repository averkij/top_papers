{
    "date": {
        "ru": "28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 28",
        "zh": "2æœˆ28æ—¥"
    },
    "time_utc": "2025-02-28 06:14",
    "weekday": 4,
    "issue_id": 2458,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.19613",
            "title": "Self-rewarding correction for mathematical reasoning",
            "url": "https://huggingface.co/papers/2502.19613",
            "abstract": "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.",
            "score": 38,
            "issue_id": 2455,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "e2535efc8aadcc9d",
            "authors": [
                "Wei Xiong",
                "Hanning Zhang",
                "Chenlu Ye",
                "Lichang Chen",
                "Nan Jiang",
                "Tong Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19613.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Rewarding Reasoning and Self-Correction",
                    "desc": "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¥–åŠ±æ¨ç†ï¼šæ¨¡å‹çš„ç‹¬ç«‹æ€è€ƒä¸ä¿®æ­£",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†è‡ªæˆ‘å¥–åŠ±æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŒæ—¶ç”Ÿæˆé€æ­¥æ¨ç†å¹¶è¯„ä¼°è¾“å‡ºçš„æ­£ç¡®æ€§ï¼Œè€Œæ— éœ€å¤–éƒ¨åé¦ˆã€‚è¿™ç§é›†æˆæ–¹æ³•ä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿç‹¬ç«‹å¼•å¯¼å…¶æ¨ç†è¿‡ç¨‹ï¼Œä¸ºæ¨¡å‹éƒ¨ç½²æä¾›äº†è®¡ç®—ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨è‡ªæˆ‘ä¿®æ­£çš„ä»»åŠ¡ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ£€æµ‹å“åº”ä¸­çš„é”™è¯¯ï¼Œä¿®æ­£è¾“å‡ºï¼Œå¹¶å†³å®šä½•æ—¶ç»ˆæ­¢è¿­ä»£ä¼˜åŒ–å¾ªç¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ç®—æ³•æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆçš„æ•°æ®æ„å»ºè‡ªæˆ‘å¥–åŠ±æ¨ç†æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20082",
            "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
            "url": "https://huggingface.co/papers/2502.20082",
            "abstract": "LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by \"needle-driven\" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.",
            "score": 14,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "ee15387b2b27d4c6",
            "authors": [
                "Ning Shang",
                "Li Lyna Zhang",
                "Siyuan Wang",
                "Gaokai Zhang",
                "Gilsinia Lopez",
                "Fan Yang",
                "Weizhu Chen",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "LongRoPE2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğµ Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… RoPE Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE. LongRoPE2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LongRoPE2 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ LLaMA3-8B Ğ´Ğ¾ 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Extending Context Length Without Compromise",
                    "desc": "LongRoPE2 is a new method that enhances the context length of large language models (LLMs) while maintaining their performance on shorter contexts. It introduces a hypothesis that inadequate training in higher dimensions of RoPE leads to out-of-distribution issues in existing models. The method employs a RoPE rescaling algorithm that uses evolutionary search to improve training effectiveness. Additionally, it utilizes a mixed context window training strategy to fine-tune model weights, allowing for long-context sequences without sacrificing short-context performance."
                },
                "zh": {
                    "title": "æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œä¿æŒæ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "LongRoPE2æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ‰©å±•é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ï¼ŒåŒæ—¶ä¿æŒåœ¨åŸå§‹è¾ƒçŸ­ä¸Šä¸‹æ–‡çª—å£ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªè´¡çŒ®å®ç°ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼Œè®¤ä¸ºåœ¨æ›´é«˜RoPEç»´åº¦ä¸Šçš„è®­ç»ƒä¸è¶³å¯¼è‡´äº†ç°æœ‰æ–¹æ³•ä¸­æŒç»­å­˜åœ¨çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰é—®é¢˜ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„RoPEé‡ç¼©æ”¾ç®—æ³•ï¼Œé€šè¿‡â€œé’ˆé©±åŠ¨â€çš„å›°æƒ‘åº¦æŒ‡å¯¼çš„è¿›åŒ–æœç´¢æ¥è§£å†³è®­ç»ƒä¸è¶³çš„é—®é¢˜ï¼›æœ€åï¼Œé‡‡ç”¨æ··åˆä¸Šä¸‹æ–‡çª—å£è®­ç»ƒæ–¹æ³•ï¼Œå¾®è°ƒæ¨¡å‹æƒé‡ä»¥é€‚åº”é•¿ä¸Šä¸‹æ–‡åºåˆ—çš„é‡ç¼©æ”¾RoPEï¼ŒåŒæ—¶ä¿æŒçŸ­ä¸Šä¸‹æ–‡çš„åŸå§‹RoPEæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16645",
            "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
            "url": "https://huggingface.co/papers/2502.16645",
            "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.",
            "score": 8,
            "issue_id": 2456,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 23",
                "zh": "2æœˆ23æ—¥"
            },
            "hash": "616cf3f6f2ab1d17",
            "authors": [
                "Chenlong Wang",
                "Zhaoyang Chu",
                "Zhengxiang Cheng",
                "Xuyi Yang",
                "Kaiyue Qiu",
                "Yao Wan",
                "Zhou Zhao",
                "Xuanhua Shi",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Wuhuan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16645.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CODESYNC - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ¸Ğ· ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CODESYNC Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ CODESYNCBENCH - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 220 API Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "CODESYNC: Keeping Code Knowledge Fresh for LLMs",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in adapting to changes in third-party library APIs, which can lead to outdated or inefficient code. It introduces CODESYNC, a data engine designed to identify outdated code patterns and gather real-time updates from Python libraries. Additionally, the authors present CODESYNCBENCH, a benchmark for evaluating LLMs' performance in keeping up with code evolution, featuring 3,300 test cases across various tasks. The findings indicate that even advanced LLMs struggle with dynamic code changes, highlighting the need for improved methods for real-time code knowledge updating."
                },
                "zh": {
                    "title": "å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„åŸºå‡†æµ‹è¯•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ä»£ç çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚ç”±äºé™æ€çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™ç§é™åˆ¶å¸¸å¸¸å¯¼è‡´ç”Ÿæˆçš„ä»£ç æ— æ³•æ‰§è¡Œæˆ–å®ç°çš„å®‰å…¨æ€§å’Œæ•ˆç‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CODESYNCï¼Œä¸€ä¸ªç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶æ”¶é›†æ¥è‡ªPythonç¬¬ä¸‰æ–¹åº“çš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„æ•°æ®å¼•æ“ã€‚åŸºäºCODESYNCï¼Œæˆ‘ä»¬å¼€å‘äº†CODESYNCBENCHï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä»£ç æ¼”å˜ä¸­çš„åŒæ­¥èƒ½åŠ›ï¼Œæ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„çœŸå®æ›´æ–°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20395",
            "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
            "url": "https://huggingface.co/papers/2502.20395",
            "abstract": "In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.",
            "score": 8,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "e8862deee761c4d0",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20395.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Re-Routing in Test-Time (R2-T2) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ end-to-end, Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². R2-T2 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ ĞµĞ³Ğ¾ Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ R2-T2 Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑĞµĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Multimodal Performance with Test-Time Re-Routing",
                    "desc": "This paper addresses the performance gap in large multimodal models (LMMs) when processing non-language data compared to large language models (LLMs). The authors introduce a mixture-of-experts (MoE) approach to enhance the vision encoder, allowing for richer and more diverse representations. They identify that the router, which determines how to mix these expert representations, often fails to optimize routing weights effectively during testing. To solve this, they propose a method called Re-Routing in Test-Time (R2-T2), which fine-tunes routing weights based on nearby correctly predicted samples, significantly boosting the performance of LMMs on various challenging tasks without retraining the base model."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­ï¼Œè§†è§‰è¡¨ç¤ºçš„æ„ŸçŸ¥èƒ½åŠ›é€šå¸¸ä¸å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å½±å“äº†LMMsåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ€è¿‘ï¼Œé€šè¿‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ›¿æ¢è§†è§‰ç¼–ç å™¨ï¼Œç¼“è§£äº†è¿™ä¸€å¼±ç‚¹ï¼Œæä¾›äº†ä¸°å¯Œä¸”å¤šæ ·çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç°ï¼Œç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å¹¶ä¸æ€»èƒ½ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆæœ€ä½³çš„è·¯ç”±æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•â€œæµ‹è¯•æ—¶é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰â€ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–è·¯ç”±æƒé‡å‘é‡ï¼Œæ˜¾è‘—æå‡äº†LMMsåœ¨å¤šæ ·åŒ–ä»»åŠ¡åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20127",
            "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
            "url": "https://huggingface.co/papers/2502.20127",
            "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",
            "score": 3,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "ad848cf98c7468a7",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Pengfei Gao",
                "Xiangxin Meng",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "ByteDance",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20127.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rlhf",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "SoRFT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¯Ğœ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ - Subtask-oriented Reinforced Fine-Tuning (SoRFT). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. SoRFT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SoRFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Issue Resolution with SoRFT: A Cost-Effective Approach",
                    "desc": "This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a new method designed to improve the issue-resolving capabilities of large language models (LLMs). It breaks down the issue resolution process into specific subtasks, such as file and function localization, and code editing. The training process involves two stages: first, a supervised fine-tuning phase that uses filtered data, and second, a reinforcement learning phase that applies Proximal Policy Optimization (PPO) with rewards based on ground-truth data. The results show that models trained with SoRFT outperform existing open-source models, achieving state-of-the-art results while being more cost-effective and maintaining better privacy."
                },
                "zh": {
                    "title": "æå‡é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºå­ä»»åŠ¡å¯¼å‘å¼ºåŒ–å¾®è°ƒï¼ˆSoRFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†é—®é¢˜è§£å†³åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡ä»¶å®šä½ã€åŠŸèƒ½å®šä½ã€è¡Œå®šä½å’Œä»£ç ç¼–è¾‘ç”Ÿæˆã€‚SoRFTåŒ…å«ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šé¦–å…ˆæ˜¯åŸºäºæ‹’ç»é‡‡æ ·çš„ç›‘ç£å¾®è°ƒï¼Œå…¶æ¬¡æ˜¯åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨åŸºäºçœŸå®æ•°æ®çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoRFTæ˜¾è‘—æå‡äº†é—®é¢˜è§£å†³æ€§èƒ½ï¼Œæ”¹å–„äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºå•†ä¸šæ¨¡å‹æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20321",
            "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
            "url": "https://huggingface.co/papers/2502.20321",
            "abstract": "The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domain-specific continuous tokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet. Our code is available at https://github.com/FoundationVision/UniTok.",
            "score": 2,
            "issue_id": 2457,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "1e65564c1594ef39",
            "authors": [
                "Chuofan Ma",
                "Yi Jiang",
                "Junfeng Wu",
                "Jihan Yang",
                "Xin Yu",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Huazhong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20321.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "UniTok: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "UniTok - ÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. UniTok Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ rFID 0.38 Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ 78.6% Ğ½Ğ° ImageNet. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Bridging Visual Generation and Understanding with UniTok",
                    "desc": "This paper presents UniTok, a novel discrete visual tokenizer designed to improve the integration of visual generation and understanding. It addresses the challenges of representation disparity by using multi-codebook quantization, which enhances the capacity of discrete tokens without causing training instability. The method allows for better encoding of both fine-grained details and high-level semantics, leading to improved performance metrics. UniTok outperforms existing models, achieving higher accuracy and fidelity on tasks like image classification compared to traditional continuous tokenizers."
                },
                "zh": {
                    "title": "UniTokï¼šç»Ÿä¸€è§†è§‰ç”Ÿæˆä¸ç†è§£çš„çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniTokçš„ç¦»æ•£è§†è§‰æ ‡è®°å™¨ï¼Œæ—¨åœ¨è§£å†³è§†è§‰ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„è¡¨ç¤ºå·®å¼‚ã€‚UniTokèƒ½å¤Ÿç¼–ç ç»†ç²’åº¦çš„ç»†èŠ‚ä»¥è¿›è¡Œç”Ÿæˆï¼ŒåŒæ—¶æ•æ‰é«˜å±‚æ¬¡çš„è¯­ä¹‰ä»¥ä¾¿äºç†è§£ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å¤šä»£ç æœ¬é‡åŒ–æ¥æ‰©å±•æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œä»è€Œå…‹æœç¦»æ•£æ ‡è®°çš„è¡¨ç¤ºèƒ½åŠ›é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniTokåœ¨ImageNetä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†é¢†åŸŸç‰¹å®šçš„è¿ç»­æ ‡è®°å™¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20126",
            "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
            "url": "https://huggingface.co/papers/2502.20126",
            "abstract": "Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than 40\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to 75\\% less compute without compromising performance.",
            "score": 1,
            "issue_id": 2457,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "405098fcd7a76209",
            "authors": [
                "Sotiris Anagnostidis",
                "Gregor Bachmann",
                "Yeongmin Kim",
                "Jonas Kohler",
                "Markos Georgopoulos",
                "Artsiom Sanakoyeu",
                "Yuming Du",
                "Albert Pumarola",
                "Ali Thabet",
                "Edgar SchÃ¶nfeld"
            ],
            "affiliations": [
                "ETH Zurich",
                "KAIST",
                "Meta GenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20126.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#inference",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "FlexiDiT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ FlexiDiT, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ DiT Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ (FLOP) Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 40% Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞµĞ³Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 75%."
                },
                "en": {
                    "title": "Flexibility in Diffusion: Reducing Compute with FlexiDiT",
                    "desc": "This paper addresses the high resource demands of Diffusion Transformers during image generation. The authors introduce a dynamic strategy that allows for flexible compute allocation during the denoising process, leading to the development of FlexiDiT models. These models can adapt to varying compute budgets while maintaining image quality, achieving over 40% reduction in FLOPs compared to traditional static models. Additionally, the approach is versatile and can be applied to video generation, achieving up to 75% less compute usage without sacrificing performance."
                },
                "zh": {
                    "title": "çµæ´»è®¡ç®—ï¼Œæå‡ç”Ÿæˆæ•ˆç‡",
                    "desc": "ç°ä»£æ‰©æ•£å˜æ¢å™¨åœ¨æ¨ç†æ—¶éœ€è¦å¤§é‡èµ„æºï¼Œä¸»è¦æ˜¯å› ä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤éƒ½éœ€è¦å›ºå®šä¸”å¤§é‡çš„è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€ç­–ç•¥ï¼Œå–ä»£ä¼ ç»Ÿçš„é™æ€è®¡ç®—é¢„ç®—åˆ†é…æ–¹æ³•ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼ˆDiTï¼‰èƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒçš„è®¡ç®—é¢„ç®—ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºFlexiDiTï¼Œèƒ½å¤Ÿåœ¨ä¸é™ä½ç”Ÿæˆå›¾åƒè´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘è¶…è¿‡40%çš„è®¡ç®—é‡ã€‚è¯¥æ–¹æ³•é€šç”¨ä¸”ä¸ä¾èµ–äºè¾“å…¥å’Œæ¡ä»¶æ¨¡å¼ï¼Œç”šè‡³å¯ä»¥æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆï¼Œæ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20238",
            "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
            "url": "https://huggingface.co/papers/2502.20238",
            "abstract": "Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the \"System 1\" way of quick reactions to the \"System 2\" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model's intermediate reasoning steps unexamined. This fails to assess the model's ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.",
            "score": 0,
            "issue_id": 2458,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "e121721fdef71315",
            "authors": [
                "Guizhen Chen",
                "Weiwen Xu",
                "Hao Zhang",
                "Hou Pong Chan",
                "Chaoqun Liu",
                "Lidong Bing",
                "Deli Zhao",
                "Anh Tuan Luu",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group, Singapore",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20238.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#math"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "FINEREASON: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FINEREASON Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², FINEREASON Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models with FINEREASON",
                    "desc": "This paper introduces FINEREASON, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) through logic puzzles. Unlike traditional benchmarks that focus solely on final-answer accuracy, FINEREASON emphasizes the importance of intermediate reasoning steps, allowing for a more detailed assessment of a model's reflective and corrective abilities. The benchmark includes two specific tasks: state checking and state transition, which help evaluate how models understand their current context and plan their next actions. The authors demonstrate that training on this benchmark can improve LLM performance in mathematical reasoning tasks by up to 5.1%."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„ç»†è‡´è¯„ä¼°",
                    "desc": "è®¸å¤šå¤æ‚çš„æ¨ç†ä»»åŠ¡éœ€è¦ä¸ä»…å¿«é€Ÿçš„ç›´è§‰ååº”ï¼Œè¿˜éœ€è¦æ›´æ·±æ€ç†Ÿè™‘çš„å¤šæ­¥éª¤æ–¹æ³•ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºï¼Œä»å¿«é€Ÿååº”çš„â€œç³»ç»Ÿ1â€è½¬å‘åæ€å’Œçº æ­£é—®é¢˜è§£å†³çš„â€œç³»ç»Ÿ2â€é£æ ¼æ˜¯ä¸€ä¸ªé‡è¦çš„å˜åŒ–ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´æ­¥éª¤ï¼Œè¿™æ— æ³•è¯„ä¼°æ¨¡å‹åæ€å’Œçº æ­£é”™è¯¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FINEREASONï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘éš¾é¢˜åŸºå‡†ï¼Œç”¨äºç»†è‡´è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20307",
            "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
            "url": "https://huggingface.co/papers/2502.20307",
            "abstract": "We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.",
            "score": 0,
            "issue_id": 2458,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "4763c15bca31d3b1",
            "authors": [
                "Xiuli Bi",
                "Jianfei Yuan",
                "Bo Liu",
                "Yong Zhang",
                "Xiaodong Cun",
                "Chi-Man Pun",
                "Bin Xiao"
            ],
            "affiliations": [
                "Chongqing University of Post and Telecommunications, China",
                "GVC Lab, Great Bay University, China",
                "Meituan, China",
                "University of Macau, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20307.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#video",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ‘ĞµÑÑˆĞ¾Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Mobius Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğº ĞºĞ¾Ğ½Ñ†Ñƒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ½ĞµĞ¼Ğ°Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamless Video Creation from Text: Introducing Mobius!",
                    "desc": "Mobius is a new technique that creates seamless looping videos directly from text descriptions without needing user input. It utilizes a pre-trained video latent diffusion model to generate these videos, ensuring that the start and end of the video connect smoothly. The method involves a latent cycle that allows for flexible video lengths while maintaining temporal consistency through multi-frame latent denoising. This approach enables the generation of dynamic and high-quality visuals, surpassing previous methods that relied on static images."
                },
                "zh": {
                    "title": "Mobiusï¼šä»æ–‡æœ¬ç”Ÿæˆæ— ç¼å¾ªç¯è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMobiusçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥ä»æ–‡æœ¬æè¿°ç”Ÿæˆæ— ç¼å¾ªç¯è§†é¢‘ï¼Œè€Œæ— éœ€ç”¨æˆ·æ³¨é‡Šã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¿æ¥è§†é¢‘çš„èµ·å§‹å’Œç»“æŸå™ªå£°æ„å»ºæ½œåœ¨å¾ªç¯ï¼Œä»è€Œç”Ÿæˆå¾ªç¯è§†é¢‘ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é€æ­¥å°†ç¬¬ä¸€å¸§çš„æ½œåœ¨è¡¨ç¤ºè½¬ç§»åˆ°æœ€åä¸€å¸§ï¼Œè¿›è¡Œå¤šå¸§æ½œåœ¨å»å™ªï¼Œç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ä»¥å¾€çš„åŠ¨æ€å›¾ç‰‡ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å›¾åƒä½œä¸ºå¤–è§‚ï¼Œå› æ­¤èƒ½å¤Ÿç”Ÿæˆæ›´åŠ¨æ€çš„è¿åŠ¨å’Œæ›´å¥½çš„è§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.19735",
            "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
            "url": "https://huggingface.co/papers/2502.19735",
            "abstract": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.",
            "score": 0,
            "issue_id": 2457,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "45b53278bddf4ab9",
            "authors": [
                "Minggui He",
                "Yilun Liu",
                "Shimin Tao",
                "Yuanchang Luo",
                "Hongyong Zeng",
                "Chang Su",
                "Li Zhang",
                "Hongxia Ma",
                "Daimeng Wei",
                "Weibin Meng",
                "Hao Yang",
                "Boxing Chen",
                "Osamu Yoshie"
            ],
            "affiliations": [
                "Huawei Canada, Canada",
                "Huawei, China",
                "Waseda University, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19735.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#machine_translation",
                    "#rl",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸Ğº",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ R1-Translator (R1-T1) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑˆĞµÑÑ‚ÑŒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° 21 ÑĞ·Ñ‹ĞºĞµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° 15 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Machine Translation with Reasoning-Enhanced Frameworks",
                    "desc": "This paper presents R1-Translator (R1-T1), a new framework that enhances machine translation (MT) by incorporating reasoning during translation, similar to how human translators think. It addresses limitations of existing methods that either use fixed reasoning patterns or rely on supervised fine-tuning, which can lead to forgetting important information. The framework utilizes reinforcement learning (RL) to align translation with human reasoning through six expert-curated chain-of-thought (CoT) templates. Experimental results show that R1-T1 improves translation quality across multiple languages and tasks, particularly for languages not seen during training, while maintaining strong multilingual capabilities."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„é€šç”¨æœºå™¨ç¿»è¯‘æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨æœºå™¨ç¿»è¯‘ä¸­å¼•å…¥æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå°¤å…¶æ˜¯å¦‚ä½•åœ¨æ¨ç†æ—¶è¿›è¡Œæœ‰æ•ˆçš„ç¿»è¯‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶R1-Translatorï¼ˆR1-T1ï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆäººç±»å¯¹æ¨ç†é“¾çš„ç†è§£ï¼Œæ¥å®ç°é€šç”¨çš„æœºå™¨ç¿»è¯‘ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°æ‰©å±•äº†æ¨ç†ç¿»è¯‘çš„åº”ç”¨èŒƒå›´ï¼Œå¹¶åˆ¶å®šäº†å…­ç§ä¸“å®¶ç­–åˆ’çš„æ¨ç†æ¨¡æ¿ï¼Œä»¥é€‚åº”ä¸åŒçš„ç¿»è¯‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨21ç§è¯­è¨€å’Œ80ä¸ªç¿»è¯‘æ–¹å‘ä¸Šå‡è¡¨ç°å‡ºç¨³å®šçš„ç¿»è¯‘æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒä¸­æœªè§è¿‡çš„15ç§è¯­è¨€ä¸Šã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-27.html",
    "link_next": "2025-03-03.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "03.03",
        "en": "03/03",
        "zh": "3æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†Kananaï¼Œä¸€ç³»åˆ—åœ¨éŸ©è¯­å’Œè‹±è¯­ä¸­è¡¨ç°å‡ºè‰²çš„åŒè¯­æ¨¡å‹ã€‚Kananaçš„è®¡ç®—æˆæœ¬æ˜¾è‘—ä½äºç±»ä¼¼è§„æ¨¡çš„é¡¶çº§æ¨¡å‹ã€‚æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®è¿‡æ»¤ã€åˆ†é˜¶æ®µé¢„è®­ç»ƒã€æ·±åº¦æ‰©å±•ã€å‰ªæå’Œè’¸é¦ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜æ¦‚è¿°äº†Kananaæ¨¡å‹åœ¨è®­ç»ƒåä½¿ç”¨çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ï¼Œä»¥æé«˜å…¶ä¸ç”¨æˆ·äº’åŠ¨çš„èƒ½åŠ›ã€‚æœ€åï¼ŒæŠ¥å‘Šè¿˜è®¨è®ºäº†è¯­è¨€æ¨¡å‹é€‚åº”ç‰¹å®šåœºæ™¯çš„å¯èƒ½æ–¹æ³•ï¼Œå¦‚åµŒå…¥ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œå‡½æ•°è°ƒç”¨ã€‚Kananaæ¨¡å‹ç³»åˆ—ä»2.1Båˆ°32.5Bå‚æ•°ä¸ç­‰ï¼Œ2.1Bæ¨¡å‹ï¼ˆåŸºç¡€ã€æŒ‡ä»¤ã€åµŒå…¥ï¼‰å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›éŸ©è¯­æ¨¡å‹ç ”ç©¶ã€‚",
        "title": "Kanana: Compute-efficient Bilingual Language Models",
        "pinyin": "WÇ’men jiÃ¨shÃ o le Kanana, yÄ« xÃ¬liÃ¨ zÃ i hÃ¡nyÇ” hÃ© yÄ«ngyÇ” zhÅng biÇoxiÃ n chÅ«sÃ¨ de shuÄngyÇ” mÃ³xÃ­ng. Kanana de jÃ¬suÃ n chÃ©ngbÄ›n xiÇnzhÃ¹ dÄ«yÃº lÃ¨isÃ¬ guÄ«mÃ³ de dÇngjÃ­ mÃ³xÃ­ng. BÃ ogÃ o xiÃ¡ngxÃ¬ jiÃ¨shÃ o le yÃ¹xÃ¹nliÃ n zhÅng shÇyÃ²ng de jÃ¬shÃ¹, bÄokuÃ² gÄo zhÃ¬liÃ ng shÃ¹jÃ¹ guÃ²lÇœ, fÄ“n jiÄ“duÃ n yÃ¹xÃ¹nliÃ n, shÄ“ndÃ¹ kuÃ²zhÇn, jiÇnzhÄ« hÃ© zhÄ“ngliÃº. CÇwÃ i, bÃ ogÃ o hÃ¡i gÃ ikuÃ ng le Kanana mÃ³xÃ­ng zÃ i xÃ¹nliÃ n hÃ²u shÇyÃ²ng de fÄngfÇ, bÄokuÃ² jiÃ ndÅ« wÄ“itiÃ¡o hÃ© piÄnhÇo yÅuhuÃ , yÇ tÃ­gÄo qÃ­ yÇ” yÃ²nghÃ¹ hÃ¹dÃ²ng de nÃ©nglÃ¬. ZÃ¹ihÃ²u, bÃ ogÃ o hÃ¡i tÇolÃ¹n le yÇ”yÃ¡n mÃ³xÃ­ng shÃ¬yÃ¬ng tÃ¨dÃ¬ng chÇngjÄ«ng de kÄ›nÃ©ng fÄngfÇ, rÃº qiÃ nrÃ¹, jiÇnsuÇ’ zÄ“ngqiÃ¡ng shÄ“ngchÃ©ng hÃ© hÃ¡nshÃ¹ diÃ oyÃ²ng. Kanana mÃ³xÃ­ng xÃ¬liÃ¨ cÃ³ng 2.1B dÃ o 32.5B cÄnshÃ¹ bÃ¹dÄ›ng, 2.1B mÃ³xÃ­ng (jÄ«chÇ”, zhÇlÃ¬ng, qiÃ nrÃ¹) yÇ gÅngkÄi fÄbÃ¹, yÇ cÃ¹jÃ¬n hÃ¡nyÇ” mÃ³xÃ­ng yÃ¡njiÅ«.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨shÃ o', 'trans': 'introduce'},\n{'word': 'åŒè¯­', 'pinyin': 'shuÄngyÇ”', 'trans': 'bilingual'},\n{'word': 'è®¡ç®—æˆæœ¬', 'pinyin': 'jÃ¬suÃ n chÃ©ngbÄ›n', 'trans': 'computational cost'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-training'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high-quality'},\n{'word': 'è¿‡æ»¤', 'pinyin': 'guÃ²lÇœ', 'trans': 'filter'},\n{'word': 'åˆ†é˜¶æ®µ', 'pinyin': 'fÄ“n jiÄ“duÃ n', 'trans': 'phased'},\n{'word': 'æ·±åº¦æ‰©å±•', 'pinyin': 'shÄ“ndÃ¹ kuÃ²zhÇn', 'trans': 'deep expansion'},\n{'word': 'å‰ªæ', 'pinyin': 'jiÇnzhÄ«', 'trans': 'pruning'},\n{'word': 'è’¸é¦', 'pinyin': 'zhÄ“ngliÃº', 'trans': 'distillation'},\n{'word': 'ç›‘ç£å¾®è°ƒ', 'pinyin': 'jiÃ ndÅ« wÄ“itiÃ¡o', 'trans': 'supervised fine-tuning'},\n{'word': 'åå¥½ä¼˜åŒ–', 'pinyin': 'piÄnhÃ o yÅuhuÃ ', 'trans': 'preference optimization'},\n{'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹dÃ²ng', 'trans': 'interaction'},\n{'word': 'åµŒå…¥', 'pinyin': 'qiÃ nrÃ¹', 'trans': 'embedding'},\n{'word': 'æ£€ç´¢å¢å¼ºç”Ÿæˆ', 'pinyin': 'jiÇnsuÇ’ zÄ“ngqiÃ¡ng shÄ“ngchÃ©ng', 'trans': 'retrieval-augmented generation'},\n{'word': 'å‡½æ•°è°ƒç”¨', 'pinyin': 'hÃ¡nshÃ¹ diÃ oyÃ²ng', 'trans': 'function call'},\n{'word': 'å‚æ•°', 'pinyin': 'cÄnshÃ¹', 'trans': 'parameters'},\n{'word': 'å…¬å¼€å‘å¸ƒ', 'pinyin': 'gÅngkÄi fÄbÃ¹', 'trans': 'publicly released'},\n{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹jÃ¬n', 'trans': 'promote'}]",
        "trans": "We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models.",
        "update_ts": "2025-02-27 09:11"
    }
}