{
    "date": {
        "ru": "4 сентября",
        "en": "September 4",
        "zh": "9月4日"
    },
    "time_utc": "2025-09-04 17:10",
    "weekday": 3,
    "issue_id": 5721,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.01106",
            "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
            "url": "https://huggingface.co/papers/2509.01106",
            "abstract": "Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.",
            "score": 31,
            "issue_id": 5707,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "d0766d32afe23fec",
            "authors": [
                "Huang Fang",
                "Mengxi Zhang",
                "Heng Dong",
                "Wei Li",
                "Zixuan Wang",
                "Qifeng Zhang",
                "Xueyun Tian",
                "Yucheng Hu",
                "Hang Li"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01106.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#alignment",
                    "#robotics",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Robix: Единый интеллект для роботов нового поколения",
                    "desc": "Robix - это унифицированная модель машинного обучения, объединяющая рассуждения робота, планирование задач и взаимодействие на естественном языке в единой архитектуре. Модель использует цепочку рассуждений и трехэтапную стратегию обучения, включающую дообучение, тонкую настройку и обучение с подкреплением. Robix демонстрирует превосходную производительность в выполнении интерактивных задач, превосходя как открытые, так и коммерческие базовые модели. Модель обладает новыми возможностями, такими как проактивный диалог, обработка прерываний в реальном времени и рассуждения на основе здравого смысла."
                },
                "en": {
                    "title": "Robix: Revolutionizing Robot Interaction and Task Execution",
                    "desc": "Robix is a unified vision-language model designed to enhance robot reasoning, task planning, and natural language interaction. It operates as a cognitive layer in robotic systems, generating commands and responses that allow robots to execute complex tasks and interact with humans effectively. The model employs chain-of-thought reasoning and a three-stage training strategy, which includes pretraining, supervised finetuning, and reinforcement learning to improve its performance. Experiments show that Robix significantly outperforms existing models in executing diverse interactive tasks, showcasing its ability to generalize across various instruction types."
                },
                "zh": {
                    "title": "Robix：智能机器人交互的新纪元",
                    "desc": "Robix是一种统一的视觉-语言模型，结合了机器人推理、任务规划和自然语言交互。它通过链式思维推理和三阶段训练策略，展示了在交互任务执行中的优越性能。Robix能够动态生成原子命令和人机交互的语言响应，使机器人能够执行复杂指令并进行自然互动。实验表明，Robix在多种指令类型和用户参与的任务中表现优于现有的开源和商业模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00375",
            "title": "Open Data Synthesis For Deep Research",
            "url": "https://huggingface.co/papers/2509.00375",
            "abstract": "InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
            "score": 28,
            "issue_id": 5707,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "d7d79c964b418fac",
            "authors": [
                "Ziyi Xia",
                "Kun Luo",
                "Hongjin Qian",
                "Zheng Liu"
            ],
            "affiliations": [
                "BAAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00375.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "InfoSeek: Новый уровень глубокого исследования для языковых моделей",
                    "desc": "InfoSeek - это масштабируемая система для создания сложных задач глубокого исследования путем синтеза иерархических задач удовлетворения ограничений. Она использует двухагентную систему для рекурсивного построения дерева исследований из веб-страниц, преобразуя его в вопросы на естественном языке. Модели, обученные на InfoSeek, превосходят сильные базовые линии на сложных бенчмарках. InfoSeek позволяет быстро масштабировать генерацию данных и поддерживает продвинутые стратегии оптимизации."
                },
                "en": {
                    "title": "Unlocking Deep Research with Hierarchical Constraints",
                    "desc": "InfoSeek is a novel framework designed to tackle complex Deep Research tasks by structuring them as Hierarchical Constraint Satisfaction Problems (HCSPs). This approach allows models to break down intricate questions into manageable sub-problems, facilitating multi-step reasoning and evidence synthesis from various sources. Unlike traditional benchmarks, InfoSeek generates a large dataset of over 50,000 training examples that reflect the complexity of real-world queries without shortcut reasoning. Experiments demonstrate that models trained with InfoSeek significantly outperform larger models on challenging benchmarks, showcasing its effectiveness in enhancing the capabilities of large language models."
                },
                "zh": {
                    "title": "InfoSeek：深度研究任务的新框架",
                    "desc": "InfoSeek是一个可扩展的框架，用于生成复杂的深度研究任务，通过合成层次约束满足问题，使模型在具有挑战性的基准测试中超越更大的基线。该框架使用双代理系统，从大规模网页递归构建研究树，将中间节点模糊化为有效的子问题，并将这些树转换为需要遍历完整层次的自然语言问题。InfoSeek能够快速扩展，生成超过50,000个训练示例，并提供经过策划的测试集和通过拒绝采样生成的推理轨迹。实验表明，基于InfoSeek训练的模型在多个基准测试中表现优异，超越了许多大型模型和商业API。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03405",
            "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
            "url": "https://huggingface.co/papers/2509.03405",
            "abstract": "LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.",
            "score": 14,
            "issue_id": 5713,
            "pub_date": "2025-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "c66ad503192a18e1",
            "authors": [
                "Daniela Gottesman",
                "Alon Gilae-Dotan",
                "Ido Cohen",
                "Yoav Gur-Arieh",
                "Marius Mosbach",
                "Ori Yoran",
                "Mor Geva"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03405.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#interpretability",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LMEnt: Заглядывая внутрь языковых моделей",
                    "desc": "LMEnt - это набор инструментов для анализа усвоения знаний языковыми моделями в процессе предварительного обучения. Он включает аннотированные корпуса, методы поиска и предобученные модели для изучения представлений знаний и динамики обучения. LMEnt предоставляет богатый знаниями корпус для предобучения на основе Википедии, улучшенный метод поиска по сущностям и 12 предобученных моделей с промежуточными чекпоинтами. Этот инструментарий позволяет изучать связи между упоминаниями сущностей при предобучении и последующей производительностью моделей."
                },
                "en": {
                    "title": "Unlocking Knowledge Acquisition in Language Models with LMEnt",
                    "desc": "LMEnt is a comprehensive toolkit designed to analyze how language models acquire knowledge during their pretraining phase. It includes a specially annotated corpus based on Wikipedia, an advanced retrieval method that significantly improves performance, and a set of pretrained models with substantial parameters. This suite allows researchers to explore the relationship between entity mentions in the training data and the models' performance on knowledge tasks. By providing insights into knowledge representations and learning dynamics, LMEnt aims to enhance the development of more reliable and complete language models."
                },
                "zh": {
                    "title": "LMEnt：揭示语言模型知识获取的秘密",
                    "desc": "LMEnt是一个用于分析语言模型在预训练过程中知识获取的工具套件。它提供了带注释的语料库、检索方法和预训练模型，以研究知识表示和学习动态。通过LMEnt，研究人员可以更好地理解语言模型如何将数据转化为对世界的知识和信念。该工具的推出有助于开发更一致、稳健和完整的知识表示的语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01977",
            "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
            "url": "https://huggingface.co/papers/2509.01977",
            "abstract": "MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.",
            "score": 7,
            "issue_id": 5711,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "5a1bf5cc33f3e20f",
            "authors": [
                "Dong She",
                "Siming Fu",
                "Mushui Liu",
                "Qiaoqiao Jin",
                "Hualiang Wang",
                "Mu Liu",
                "Jidong Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "The Hong Kong University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01977.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Точная генерация изображений со множеством объектов",
                    "desc": "MOSAIC - это новая система для генерации изображений с несколькими объектами, которая обеспечивает точное семантическое выравнивание и ортогональное разделение признаков. Она использует специально размеченный датасет SemAlign-MS для обучения точному соответствию семантических областей между референсными и целевыми изображениями. MOSAIC применяет loss-функции для семантического выравнивания и разделения признаков разных объектов. Система показывает высокую точность даже при генерации изображений с 4 и более объектами, превосходя существующие методы."
                },
                "en": {
                    "title": "MOSAIC: Mastering Multi-Subject Image Generation with Precision",
                    "desc": "The MOSAIC framework improves the generation of images featuring multiple subjects by focusing on precise semantic alignment and separating features effectively. It addresses common issues like identity blending and attribute leakage that arise when synthesizing images from multiple references. By introducing a new dataset, SemAlign-MS, it provides detailed semantic correspondences, which helps in maintaining clarity in the generated images. The framework's innovative loss functions ensure that different subjects are represented distinctly, allowing for high-quality image generation even with more than three subjects."
                },
                "zh": {
                    "title": "MOSAIC：多主体图像生成的新突破",
                    "desc": "MOSAIC框架通过确保精确的语义对齐和正交特征解耦，增强了多主体图像生成的能力。该方法解决了在多个参考主体条件下合成图像时身份保真度和语义一致性的问题。MOSAIC引入了SemAlign-MS数据集，提供了多参考主体与目标图像之间的细粒度语义对应关系。通过语义对应注意力损失和多参考解耦损失，MOSAIC在多个基准测试中实现了最先进的性能，能够在4个以上的参考主体下保持高保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00428",
            "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
            "url": "https://huggingface.co/papers/2509.00428",
            "abstract": "Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE.",
            "score": 7,
            "issue_id": 5711,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "5f17d4af79b3fc6f",
            "authors": [
                "Xuechao Zou",
                "Shun Zhang",
                "Xing Fu",
                "Yue Li",
                "Kai Li",
                "Yushe Cao",
                "Congyan Lang",
                "Pin Tao",
                "Junliang Xing"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Jiaotong University",
                "Qinghai University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00428.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#architecture",
                    "#security"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Точный контроль над генерацией лиц с помощью специализированных экспертов",
                    "desc": "Face-MoGLE - это новая архитектура для генерации лиц, использующая диффузионные трансформеры. Она обеспечивает высококачественную и контролируемую генерацию за счет семантического разделения латентного пространства и специализации экспертов. Система использует смесь глобальных и локальных экспертов для захвата целостной структуры и семантики на уровне отдельных областей лица. Динамическая сеть гейтинга производит коэффициенты, зависящие от времени и пространственного положения, что повышает гибкость генерации."
                },
                "en": {
                    "title": "Face-MoGLE: Mastering Controllable Face Generation with Diffusion Transformers",
                    "desc": "Face-MoGLE is a new framework that uses Diffusion Transformers to generate high-quality and controllable faces. It addresses the challenge of balancing semantic control with photorealism by employing semantic-decoupled latent modeling, which allows for precise manipulation of facial attributes. The framework incorporates a mixture of global and local experts to enhance both overall structure and detailed features, ensuring fine-grained control over the generated images. Additionally, a dynamic gating network adapts coefficients during the generation process, improving the model's flexibility and effectiveness in various face generation tasks."
                },
                "zh": {
                    "title": "Face-MoGLE：高质量可控面部生成的新框架",
                    "desc": "Face-MoGLE是一个新颖的框架，利用扩散变换器实现高质量、可控的面部生成。该框架通过语义解耦的潜在建模、专家专门化和动态门控来解决生成建模中的挑战。它允许精确的属性操控，并结合全局和局部专家以捕捉整体结构和区域语义。实验结果表明，Face-MoGLE在多模态和单模态面部生成中表现出色，并具备强大的零样本泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02722",
            "title": "Planning with Reasoning using Vision Language World Model",
            "url": "https://huggingface.co/papers/2509.02722",
            "abstract": "The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.",
            "score": 5,
            "issue_id": 5718,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "9d66073795d0c729",
            "authors": [
                "Delong Chen",
                "Theo Moutakanni",
                "Willy Chung",
                "Yejin Bang",
                "Ziwei Ji",
                "Allen Bolourchi",
                "Pascale Fung"
            ],
            "affiliations": [
                "ISIR Sorbonne Université",
                "Meta FAIR",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02722.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VLWM: Визуально-языковая модель мира для эффективного планирования",
                    "desc": "VLWM (Vision Language World Model) - это модель, объединяющая языковое моделирование мира, обучение политике действий и моделирование динамики с семантической и временной абстракцией. Модель использует итеративное самоуточнение на основе языковых моделей, обусловленное сжатыми будущими наблюдениями, представленными в виде дерева подписей. VLWM обучает как политику действий, так и модель динамики, что способствует реактивному декодированию плана и рефлексивному планированию путем минимизации затрат. Модель достигает наилучших результатов в визуальном планировании для помощи (VPA) как на эталонных оценках, так и на предложенных авторами человеческих оценках PlannerArena."
                },
                "en": {
                    "title": "Revolutionizing Visual Planning with Language Understanding",
                    "desc": "The Vision Language World Model (VLWM) is a cutting-edge model that enhances visual planning by combining language understanding with world modeling. It learns to predict actions and changes in the world by analyzing natural videos, allowing it to infer goals and plan trajectories effectively. The model employs a two-system approach, where system-1 focuses on quick, reactive planning and system-2 engages in deeper, reflective planning to minimize costs based on expected outcomes. VLWM demonstrates superior performance in visual planning tasks, outperforming existing models in various benchmarks and evaluations."
                },
                "zh": {
                    "title": "视觉语言世界模型：智能规划的新突破",
                    "desc": "视觉语言世界模型（VLWM）通过结合基于语言的世界建模、行动策略学习和动态建模，达到了视觉规划的最先进性能。该模型能够理解和推理具有语义和时间抽象的高层次世界模型，填补了这一领域的空白。VLWM首先根据视觉观察推断整体目标，然后预测由交错的行动和世界状态变化组成的轨迹。通过自我监督的方式训练的评论模型评估假设未来状态与期望目标状态之间的语义距离，从而实现了高效的规划。"
                }
            }
        }
    ],
    "link_prev": "2025-09-03.html",
    "link_next": "2025-09-05.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9月3日"
    },
    "short_date_next": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9月5日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}