{
    "date": {
        "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 22",
        "zh": "9æœˆ22æ—¥"
    },
    "time_utc": "2025-09-22 15:12",
    "weekday": 0,
    "issue_id": 6019,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.16198",
            "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
            "url": "https://huggingface.co/papers/2509.16198",
            "abstract": "A graph-driven framework called ZeroRepo uses the Repository Planning Graph (RPG) to generate complete software repositories from scratch, significantly outperforming existing baselines in terms of code size, functional coverage, and test pass rate.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9times the strongest baseline (Claude Code) and about 64times other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.",
            "score": 83,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "e99fb2da472bbeba",
            "authors": [
                "Jane Luo",
                "Xin Zhang",
                "Steven Liu",
                "Jie Wu",
                "Yiming Huang",
                "Yangyu Huang",
                "Chengyu Yin",
                "Ying Xin",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Qi Chen",
                "Scarlett Li",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Tsinghua University",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16198.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#plp",
                    "#games",
                    "#benchmark",
                    "#agents",
                    "#graphs"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ZeroRepo: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ²",
                    "desc": "ZeroRepo - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ñ Ğ½ÑƒĞ»Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ (RPG). RPG Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ², Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğµ. ZeroRepo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ ĞºĞ¾Ğ´Ğ°, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ñƒ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RepoCraft ZeroRepo Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞ¾ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 36 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ² 3.9 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Software Generation with ZeroRepo and RPG",
                    "desc": "The paper introduces ZeroRepo, a novel framework that utilizes the Repository Planning Graph (RPG) to create complete software repositories from scratch. Unlike traditional methods that rely on ambiguous natural language, RPG provides a clear and structured representation of software components, enabling better planning and implementation. ZeroRepo significantly outperforms existing models in terms of code size, functional coverage, and test pass rates, demonstrating its effectiveness in generating complex software systems. The framework operates in three stages, ensuring coherent planning and validation, which leads to impressive results on the RepoCraft benchmark."
                },
                "zh": {
                    "title": "å›¾é©±åŠ¨çš„ä»“åº“ç”Ÿæˆæ–°çºªå…ƒ",
                    "desc": "ZeroRepoæ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä»“åº“è§„åˆ’å›¾ï¼ˆRPGï¼‰ä»é›¶å¼€å§‹ç”Ÿæˆå®Œæ•´çš„è½¯ä»¶ä»“åº“ã€‚å®ƒåœ¨ä»£ç è§„æ¨¡ã€åŠŸèƒ½è¦†ç›–ç‡å’Œæµ‹è¯•é€šè¿‡ç‡ç­‰æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚RPGé€šè¿‡å°†ææ¡ˆå’Œå®ç°å±‚é¢çš„è§„åˆ’ç»Ÿä¸€åœ¨ä¸€ä¸ªå›¾ä¸­ï¼Œè§£å†³äº†è‡ªç„¶è¯­è¨€åœ¨è¡¨ç¤ºå¤æ‚è½¯ä»¶ç»“æ„æ—¶çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒZeroRepoèƒ½å¤Ÿè¿›è¡Œé•¿è¿œè§„åˆ’å’Œå¯æ‰©å±•çš„ä»“åº“ç”Ÿæˆï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹å¯¹ä»“åº“çš„ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16197",
            "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
            "url": "https://huggingface.co/papers/2509.16197",
            "abstract": "Manzano is a unified multimodal LLM framework that integrates image and text processing using a hybrid tokenizer and diffusion decoder, achieving state-of-the-art performance in both understanding and generating visual content.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.",
            "score": 31,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "d5d4d53d4fa9323b",
            "authors": [
                "Yanghao Li",
                "Rui Qian",
                "Bowen Pan",
                "Haotian Zhang",
                "Haoshuo Huang",
                "Bowen Zhang",
                "Jialing Tong",
                "Haoxuan You",
                "Xianzhi Du",
                "Zhe Gan",
                "Hyunjik Kim",
                "Chao Jia",
                "Zhenbang Wang",
                "Yinfei Yang",
                "Mingfei Gao",
                "Zi-Yi Dou",
                "Wenze Hu",
                "Chang Gao",
                "Dongxu Li",
                "Philipp Dufter",
                "Zirui Wang",
                "Guoli Yin",
                "Zhengdong Zhang",
                "Chen Chen",
                "Yang Zhao",
                "Ruoming Pang",
                "Zhifeng Chen"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16197.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#games",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Manzano - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ¾Ğ¸Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Manzano: Bridging Text and Image with Unified Learning",
                    "desc": "Manzano is a unified multimodal large language model (LLM) that effectively processes both images and text. It uses a hybrid tokenizer and a diffusion decoder to enhance its ability to understand and generate visual content. By employing a single vision encoder with lightweight adapters, it creates embeddings for both image-to-text and text-to-image tasks within a shared semantic space. This architecture allows for scalable joint learning, leading to state-of-the-art performance in multimodal tasks while minimizing conflicts between different tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "Manzanoæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚å®ƒé€šè¿‡æ··åˆæ ‡è®°å™¨å’Œæ‰©æ•£è§£ç å™¨ï¼Œå®ç°äº†åœ¨ç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹æ–¹é¢çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å…±äº«çš„è§†è§‰ç¼–ç å™¨å’Œè½»é‡çº§é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨å…±åŒçš„è¯­ä¹‰ç©ºé—´ä¸­ç”Ÿæˆå›¾åƒåˆ°æ–‡æœ¬çš„è¿ç»­åµŒå…¥å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ç¦»æ•£æ ‡è®°ã€‚Manzanoçš„è®¾è®¡ä½¿å¾—ç†è§£å’Œç”Ÿæˆèƒ½åŠ›å¯ä»¥å…±åŒå­¦ä¹ ï¼Œä¸”åœ¨æ–‡æœ¬ä¸°å¯Œçš„è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15591",
            "title": "Latent Zoning Network: A Unified Principle for Generative Modeling,\n  Representation Learning, and Classification",
            "url": "https://huggingface.co/papers/2509.15591",
            "abstract": "Latent Zoning Network (LZN) unifies generative modeling, representation learning, and classification by creating a shared latent space for diverse data types.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.",
            "score": 22,
            "issue_id": 6009,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "fc6406f54d74c989",
            "authors": [
                "Zinan Lin",
                "Enshu Liu",
                "Xuefei Ning",
                "Junyi Zhu",
                "Wenyu Wang",
                "Sergey Yekhanin"
            ],
            "affiliations": [
                "Microsoft Research Redmond, WA, USA",
                "Redmond, WA, USA",
                "Samsung R&D Institute UK London, UK",
                "Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15591.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#synthetic",
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Latent Zoning Network (LZN) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. LZN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. LZN Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Generative Modeling, Representation Learning, and Classification with LZN",
                    "desc": "The Latent Zoning Network (LZN) introduces a unified approach to generative modeling, representation learning, and classification by establishing a shared latent space for various data types. This framework allows different tasks to be represented as combinations of encoders and decoders, facilitating seamless transitions between tasks like image generation and classification. LZN demonstrates its effectiveness by enhancing existing models, achieving superior performance in unsupervised representation learning, and successfully executing multiple tasks simultaneously. Overall, LZN aims to simplify machine learning workflows and improve synergy across diverse applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ç”Ÿæˆå»ºæ¨¡ã€è¡¨ç¤ºå­¦ä¹ ä¸åˆ†ç±»çš„æ½œåœ¨åŒºåŸŸç½‘ç»œ",
                    "desc": "æ½œåœ¨åŒºåŸŸç½‘ç»œï¼ˆLZNï¼‰é€šè¿‡åˆ›å»ºä¸€ä¸ªå…±äº«çš„æ½œåœ¨ç©ºé—´ï¼Œå°†ç”Ÿæˆå»ºæ¨¡ã€è¡¨ç¤ºå­¦ä¹ å’Œåˆ†ç±»ç»Ÿä¸€èµ·æ¥ã€‚è¯¥æ–¹æ³•ä¸ºä¸åŒç±»å‹çš„æ•°æ®ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬å’Œæ ‡ç­¾ï¼‰æä¾›äº†ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä½¿å¾—å„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡å¯ä»¥é€šè¿‡ç»„åˆè¿™äº›ç»„ä»¶æ¥å®ç°ã€‚LZNåœ¨å¤šä¸ªå¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æå‡ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€ç‹¬ç«‹è§£å†³è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä»¥åŠåŒæ—¶è¿›è¡Œç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼ŒLZNç®€åŒ–äº†æœºå™¨å­¦ä¹ æµç¨‹ï¼Œå¹¶ä¿ƒè¿›äº†ä»»åŠ¡ä¹‹é—´çš„ååŒä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16127",
            "title": "BaseReward: A Strong Baseline for Multimodal Reward Model",
            "url": "https://huggingface.co/papers/2509.16127",
            "abstract": "The paper provides a comprehensive guide and introduces BaseReward, a state-of-the-art multimodal reward model, which outperforms existing models across various benchmarks and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including reward modeling paradigms (e.g., Naive-RM, Critic-based RM, and Generative RM), reward head architecture, training strategies, data curation (covering over ten multimodal and text-only preference datasets), backbone model and model scale, and ensemble methods.   Based on these experimental insights, we introduce BaseReward, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.",
            "score": 16,
            "issue_id": 6007,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "d131848ac9ef1c25",
            "authors": [
                "Yi-Fan Zhang",
                "Haihua Yang",
                "Huanyu Zhang",
                "Yang Shi",
                "Zezhou Chen",
                "Haochen Tian",
                "Chaoyou Fu",
                "Haotian Wang",
                "Kai Wu",
                "Bo Cui",
                "Xu Wang",
                "Jianfei Pan",
                "Haotian Wang",
                "Zhang Zhang",
                "Liang Wang"
            ],
            "affiliations": [
                "ByteDance",
                "CASIA",
                "NJU",
                "PKU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16127.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#multimodal",
                    "#rag",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "BaseReward: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²ÑĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ MRM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BaseReward - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. BaseReward Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-VL Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "BaseReward: The Future of Multimodal Reward Modeling",
                    "desc": "This paper introduces BaseReward, a cutting-edge multimodal reward model designed to align Multimodal Large Language Models (MLLMs) with human preferences. It provides a systematic guide for constructing high-performance Multimodal Reward Models (MRMs), detailing essential components such as reward modeling paradigms, architecture, training strategies, and data curation. Through extensive experiments, BaseReward demonstrates superior performance on key benchmarks, establishing a new state-of-the-art in the field. Additionally, it showcases practical applications by enhancing MLLM capabilities in real-world tasks like perception and reasoning."
                },
                "zh": {
                    "title": "æ„å»ºé«˜æ€§èƒ½å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„æŒ‡å—",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹BaseRewardï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒåˆ†æï¼Œä½œè€…æä¾›äº†æ„å»ºé«˜æ€§èƒ½å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„è¯¦ç»†æŒ‡å—ï¼Œæ¶µç›–äº†å¥–åŠ±å»ºæ¨¡èŒƒå¼ã€å¥–åŠ±å¤´æ¶æ„ã€è®­ç»ƒç­–ç•¥ç­‰å…³é”®ç»„ä»¶ã€‚BaseRewardåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå®é™…çš„å¼ºåŒ–å­¦ä¹ ç®¡é“ä¸­ï¼Œæå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†é¡¶å°–çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œè¿˜ä¸ºç¤¾åŒºå¼€å‘ä¸‹ä¸€ä»£å¥–åŠ±æ¨¡å‹æä¾›äº†å®è¯æ”¯æŒçš„æ¸…æ™°æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14981",
            "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
            "url": "https://huggingface.co/papers/2509.14981",
            "abstract": "SpatialGen, a multi-view multi-modal diffusion model, generates realistic and semantically consistent 3D indoor scenes using a large synthetic dataset, outperforming previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.",
            "score": 13,
            "issue_id": 6007,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "5c98a31485e969a0",
            "authors": [
                "Chuan Fang",
                "Heng Li",
                "Yixun Liang",
                "Jia Zheng",
                "Yongsen Mao",
                "Yuan Liu",
                "Rui Tang",
                "Zihan Zhou",
                "Ping Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Manycore Tech Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14981.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#diffusion",
                    "#multimodal",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "SpatialGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D ÑÑ†ĞµĞ½ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 12,328 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ 4,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 2D Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, ĞºĞ°Ñ€Ñ‚Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. SpatialGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing 3D Indoor Scene Generation with SpatialGen",
                    "desc": "SpatialGen is a cutting-edge multi-view multi-modal diffusion model designed to create realistic 3D indoor scenes. It utilizes a large synthetic dataset containing over 12,000 annotated scenes and millions of photorealistic images to enhance the quality of generated outputs. The model effectively synthesizes various aspects of a scene, including appearance, geometry, and semantic information, while maintaining spatial consistency across different views. By outperforming existing methods, SpatialGen aims to facilitate advancements in applications like design, virtual reality, and robotics."
                },
                "zh": {
                    "title": "SpatialGenï¼šç”Ÿæˆé€¼çœŸ3Då®¤å†…åœºæ™¯çš„æ–°æ–¹æ³•",
                    "desc": "SpatialGenæ˜¯ä¸€ç§å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé€¼çœŸä¸”è¯­ä¹‰ä¸€è‡´çš„3Då®¤å†…åœºæ™¯ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ä¸€ä¸ªåŒ…å«12,328ä¸ªç»“æ„åŒ–æ ‡æ³¨åœºæ™¯çš„å¤§å‹åˆæˆæ•°æ®é›†ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡è¾“å…¥3Då¸ƒå±€å’Œå‚è€ƒå›¾åƒï¼ŒSpatialGenå¯ä»¥ä»ä»»æ„è§†è§’åˆæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialGenåœ¨ç”Ÿæˆæ•ˆæœä¸Šä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†å®¤å†…åœºæ™¯ç†è§£å’Œç”Ÿæˆçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15937",
            "title": "A Vision-Language-Action-Critic Model for Robotic Real-World\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.15937",
            "abstract": "VLAC, a vision-language-action reward model, enhances real-world robotic reinforcement learning by providing dense rewards and enabling one-shot transfer, significantly improving success rates and sample efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
            "score": 6,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "d41842df1d51895c",
            "authors": [
                "Shaopeng Zhai",
                "Qi Zhang",
                "Tianyi Zhang",
                "Fuxian Huang",
                "Haoran Zhang",
                "Ming Zhou",
                "Shengzhe Zhang",
                "Litao Liu",
                "Sixu Lin",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15937.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#transfer_learning",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "VLAC: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "VLAC - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³. VLAC Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VLAC Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "VLAC: Revolutionizing Robotic Learning with Dense Rewards and One-Shot Transfer",
                    "desc": "The paper introduces VLAC, a vision-language-action reward model designed to improve robotic reinforcement learning in real-world scenarios. By providing dense rewards and enabling one-shot transfer, VLAC addresses the challenges of sparse rewards and inefficient exploration that typically hinder RL systems. It is trained on diverse datasets to enhance the robot's perception and reasoning, allowing it to generate rewards and actions effectively. The implementation of a human-in-the-loop protocol further boosts exploration and learning efficiency, resulting in significantly higher success rates in various manipulation tasks."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹",
                    "desc": "VLACæ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-è¡ŒåŠ¨å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç°å®ä¸–ç•Œä¸­çš„æœºå™¨äººå¼ºåŒ–å­¦ä¹ ã€‚å®ƒé€šè¿‡æä¾›å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œæ¶ˆé™¤äº†ç¨€ç–æ‰‹å·¥å¥–åŠ±çš„ç“¶é¢ˆï¼Œå¹¶æ”¯æŒä¸€æ¬¡æ€§è¿ç§»åˆ°æœªè§è¿‡çš„ä»»åŠ¡å’Œç¯å¢ƒã€‚VLACåœ¨å¤§è§„æ¨¡å¼‚æ„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¢å¼ºäº†æœºå™¨äººçš„æ„ŸçŸ¥ã€å¯¹è¯å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡æ„å»ºå¤§é‡è´Ÿæ ·æœ¬æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚é€šè¿‡äººæœºåä½œçš„åè®®ï¼ŒVLACåœ¨å››ä¸ªä¸åŒçš„ç°å®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­å°†æˆåŠŸç‡ä»çº¦30%æå‡è‡³çº¦90%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15496",
            "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
            "url": "https://huggingface.co/papers/2509.15496",
            "abstract": "Lynx, a high-fidelity personalized video synthesis model, uses a Diffusion Transformer with ID-adapter and Ref-adapter to preserve identity and maintain video quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.",
            "score": 6,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "bac6af5dc791293d",
            "authors": [
                "Shen Sang",
                "Tiancheng Zhi",
                "Tianpei Gu",
                "Jing Liu",
                "Linjie Luo"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15496.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Lynx: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Lynx - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Diffusion Transformer (DiT) Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸: ID-adapter Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ref-adapter Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ID-adapter Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ref-adapter Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° 800 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ»Ğ¸Ñ† Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Lynx: Revolutionizing Personalized Video Synthesis with Identity Fidelity",
                    "desc": "Lynx is a cutting-edge model designed for creating personalized videos from just one input image. It utilizes a Diffusion Transformer architecture enhanced with two specialized adapters: the ID-adapter for maintaining identity and the Ref-adapter for adding detailed features. The ID-adapter transforms facial embeddings into identity tokens, while the Ref-adapter enriches the video with fine details using cross-attention mechanisms. Evaluations show that Lynx excels in preserving facial likeness, adhering to prompts, and producing high-quality videos, marking a significant advancement in personalized video synthesis."
                },
                "zh": {
                    "title": "Lynxï¼šä¸ªæ€§åŒ–è§†é¢‘åˆæˆçš„æ–°çªç ´",
                    "desc": "Lynxæ˜¯ä¸€ç§é«˜ä¿çœŸä¸ªæ€§åŒ–è§†é¢‘åˆæˆæ¨¡å‹ï¼ŒåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformerï¼‰æ„å»ºã€‚å®ƒå¼•å…¥äº†IDé€‚é…å™¨å’ŒRefé€‚é…å™¨ï¼Œä»¥ç¡®ä¿èº«ä»½çš„ä¿çœŸåº¦å’Œè§†é¢‘è´¨é‡ã€‚IDé€‚é…å™¨ä½¿ç”¨Perceiver Resamplerå°†ArcFaceç”Ÿæˆçš„é¢éƒ¨åµŒå…¥è½¬æ¢ä¸ºç´§å‡‘çš„èº«ä»½æ ‡è®°ï¼Œè€ŒRefé€‚é…å™¨åˆ™é€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†å†»ç»“å‚è€ƒè·¯å¾„ä¸­çš„ç¨ å¯†VAEç‰¹å¾æ³¨å…¥åˆ°æ‰€æœ‰å˜æ¢å™¨å±‚ä¸­ã€‚é€šè¿‡åœ¨40ä¸ªå—è¯•è€…å’Œ20ä¸ªæ— åæç¤ºçš„åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ï¼ŒLynxå±•ç¤ºäº†å“è¶Šçš„é¢éƒ¨ç›¸ä¼¼æ€§å’Œå¼ºå¤§çš„è§†é¢‘è´¨é‡ï¼Œæ¨åŠ¨äº†ä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆçš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15566",
            "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
            "url": "https://huggingface.co/papers/2509.15566",
            "abstract": "A brain-inspired framework, Blink-Think-Link, enhances human-GUI interaction by mimicking cognitive processes and introduces innovations in data generation and reinforcement learning rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
            "score": 5,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "3324822214c20f79",
            "authors": [
                "Shaojie Zhang",
                "Ruoceng Zhang",
                "Pei Fu",
                "Shaokang Wang",
                "Jiahui Yang",
                "Xin Du",
                "Shiqi Cui",
                "Bin Qin",
                "Ying Huang",
                "Zhenbo Luo",
                "Jian Luan"
            ],
            "affiliations": [
                "Xiaomi Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15566.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ˜Ğ˜ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'Blink-Think-Link' (BTL). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ„Ğ°Ğ·Ñ‹: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ ÑĞºÑ€Ğ°Ğ½Ğ°, Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BTL-UI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼."
                },
                "en": {
                    "title": "Mimicking Human Cognition for Smarter GUI Interaction",
                    "desc": "The paper presents a new framework called Blink-Think-Link (BTL) that improves how humans interact with graphical user interfaces (GUIs) by mimicking human cognitive processes. It breaks down interactions into three phases: 'Blink' for quick attention, 'Think' for reasoning, and 'Link' for executing commands, reflecting how humans naturally engage with screens. The framework also introduces innovative techniques like Blink Data Generation for automated annotation and a unique reward system for reinforcement learning. The BTL-UI model built on this framework shows superior performance in both understanding static GUIs and handling dynamic interactions, proving the framework's effectiveness in creating advanced GUI agents."
                },
                "zh": {
                    "title": "æ¨¡ä»¿äººç±»è®¤çŸ¥çš„GUIäº¤äº’æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œBlink-Think-Linkâ€ï¼ˆBTLï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„äººæœºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’ã€‚è¯¥æ¡†æ¶æ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œå°†äº¤äº’åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šBlinkï¼ˆå¿«é€Ÿæ£€æµ‹ï¼‰ã€Thinkï¼ˆé«˜å±‚æ¨ç†ï¼‰å’ŒLinkï¼ˆç”Ÿæˆå¯æ‰§è¡Œå‘½ä»¤ï¼‰ã€‚æ­¤å¤–ï¼ŒBTLæ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹æŠ€æœ¯åˆ›æ–°ï¼šBlinkæ•°æ®ç”Ÿæˆå’ŒBTLå¥–åŠ±æœºåˆ¶ï¼Œä»¥æ”¯æŒå¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡å¼€å‘BTL-UIæ¨¡å‹ï¼Œç ”ç©¶è¡¨æ˜è¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€GUIä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨é«˜çº§GUIä»£ç†å¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15123",
            "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
            "url": "https://huggingface.co/papers/2509.15123",
            "abstract": "A novel method for camera parameter optimization in dynamic scenes using a single RGB video, incorporating patch-wise tracking filters, outlier-aware joint optimization, and a two-stage optimization strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.",
            "score": 2,
            "issue_id": 6006,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "0f9467c1183af701",
            "authors": [
                "Fang Li",
                "Hao Zhang",
                "Narendra Ahuja"
            ],
            "affiliations": [
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15123.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "ğŸ“¹",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾ RGB-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Camera Parameters from Just One RGB Video!",
                    "desc": "This paper presents a new approach for optimizing camera parameters in dynamic scenes using only a single RGB video. It introduces three main components: Patch-wise Tracking Filters for establishing robust relationships in the video, Outlier-aware Joint Optimization to minimize the impact of moving outliers, and a Two-stage Optimization Strategy to improve stability and speed. Unlike traditional methods that require ground truth data, this method operates effectively without such supervision. The results show that this approach yields more accurate and efficient camera parameter estimates across various real-world and synthetic datasets."
                },
                "zh": {
                    "title": "åŠ¨æ€åœºæ™¯ä¸­çš„ç›¸æœºå‚æ•°ä¼˜åŒ–æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨åŠ¨æ€åœºæ™¯ä¸­ä¼˜åŒ–ç›¸æœºå‚æ•°ï¼Œä»…ä¾èµ–å•ä¸ªRGBè§†é¢‘ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šç¬¬ä¸€ï¼Œä½¿ç”¨è¡¥ä¸è·Ÿè¸ªæ»¤æ³¢å™¨å»ºç«‹ç¨³å¥çš„ç¨€ç–å…³ç³»ï¼›ç¬¬äºŒï¼Œé‡‡ç”¨è€ƒè™‘å¼‚å¸¸å€¼çš„è”åˆä¼˜åŒ–ï¼Œé€šè¿‡è‡ªé€‚åº”é™ä½ç§»åŠ¨å¼‚å¸¸å€¼çš„æƒé‡æ¥æé«˜ä¼˜åŒ–æ•ˆç‡ï¼›ç¬¬ä¸‰ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡åœ¨Softplusé™åˆ¶å’ŒæŸå¤±çš„å‡¸æœ€å°å€¼ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¢å¼ºç¨³å®šæ€§å’Œä¼˜åŒ–é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆå’Œå‡†ç¡®åœ°ä¼°è®¡ç›¸æœºå‚æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13989",
            "title": "Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in\n  Instruction-Guided Expressive Text-To-Speech Systems",
            "url": "https://huggingface.co/papers/2509.13989",
            "abstract": "Research on ITTS reveals gaps between user instructions and listener perception, highlighting challenges in fine-grained control and voice attribute interpretation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided text-to-speech (ITTS) enables users to control speech generation through natural language prompts, offering a more intuitive interface than traditional TTS. However, the alignment between user style instructions and listener perception remains largely unexplored. This work first presents a perceptual analysis of ITTS controllability across two expressive dimensions (adverbs of degree and graded emotion intensity) and collects human ratings on speaker age and word-level emphasis attributes. To comprehensively reveal the instruction-perception gap, we provide a data collection with large-scale human evaluations, named Expressive VOice Control (E-VOC) corpus. Furthermore, we reveal that (1) gpt-4o-mini-tts is the most reliable ITTS model with great alignment between instruction and generated utterances across acoustic dimensions. (2) The 5 analyzed ITTS systems tend to generate Adult voices even when the instructions ask to use child or Elderly voices. (3) Fine-grained control remains a major challenge, indicating that most ITTS systems have substantial room for improvement in interpreting slightly different attribute instructions.",
            "score": 2,
            "issue_id": 6010,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "08133cbacc29a4b3",
            "authors": [
                "Yi-Cheng Lin",
                "Huang-Cheng Chou",
                "Tzu-Chieh Wei",
                "Kuan-Yu Chen",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University",
                "University of Michigan",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13989.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… ITTS: Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ (ITTS) Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ÑĞºÑĞ¿Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ². Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ ĞºĞ¾Ñ€Ğ¿ÑƒÑ E-VOC Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ gpt-4o-mini-tts Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°, Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼ ITTS."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing ITTS Control and Perception",
                    "desc": "This paper investigates the effectiveness of instruction-guided text-to-speech (ITTS) systems in aligning user commands with listener perceptions. It highlights the challenges in achieving fine-grained control over voice attributes, such as age and emotional intensity, which are crucial for generating expressive speech. The authors introduce the Expressive VOice Control (E-VOC) corpus, a large-scale dataset for evaluating ITTS systems based on human feedback. The findings reveal that while some models perform well, there is a significant gap in accurately interpreting nuanced user instructions, particularly regarding voice characteristics."
                },
                "zh": {
                    "title": "æŒ‡ä»¤ä¸æ„ŸçŸ¥çš„æ¡¥æ¢ï¼šæå‡ITTSç³»ç»Ÿçš„æ§åˆ¶èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤å¼•å¯¼çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆITTSï¼‰ç³»ç»Ÿä¸­ç”¨æˆ·æŒ‡ä»¤ä¸å¬ä¼—æ„ŸçŸ¥ä¹‹é—´çš„å·®è·ï¼Œæ­ç¤ºäº†åœ¨ç»†ç²’åº¦æ§åˆ¶å’Œå£°éŸ³å±æ€§è§£é‡Šæ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é¦–å…ˆå¯¹ITTSçš„å¯æ§æ€§è¿›è¡Œäº†æ„ŸçŸ¥åˆ†æï¼Œæ¶‰åŠç¨‹åº¦å‰¯è¯å’Œæƒ…æ„Ÿå¼ºåº¦ä¸¤ä¸ªè¡¨ç°ç»´åº¦ï¼Œå¹¶æ”¶é›†äº†å…³äºè¯´è¯è€…å¹´é¾„å’Œå•è¯å¼ºè°ƒå±æ€§çš„äººç±»è¯„åˆ†ã€‚ä¸ºäº†å…¨é¢æ­ç¤ºæŒ‡ä»¤ä¸æ„ŸçŸ¥ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåä¸ºE-VOCçš„å¤§è§„æ¨¡äººç±»è¯„ä¼°æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç°æœ‰ITTSç³»ç»Ÿåœ¨ç”Ÿæˆå£°éŸ³æ—¶ï¼Œå¾€å¾€åå‘æˆäººå£°éŸ³ï¼Œå³ä½¿ç”¨æˆ·æŒ‡ä»¤è¦æ±‚ä½¿ç”¨å„¿ç«¥æˆ–è€å¹´å£°éŸ³ï¼Œç»†ç²’åº¦æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15233",
            "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided\n  Role-playing Agents",
            "url": "https://huggingface.co/papers/2509.15233",
            "abstract": "A framework incorporating dynamic role profiles with video modality enhances role-playing agents by combining adaptive temporal sampling and static role profile representations, improving response generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "06896ac318611f21",
            "authors": [
                "Xueqiao Zhang",
                "Chao Zhang",
                "Jingtao Xu",
                "Yifan Zhu",
                "Xin Shi",
                "Yi Yang",
                "Yawei Luo"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15233.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#agents",
                    "#story_generation"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Role-playing-Video60k Ğ¸Ğ· 60 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 700 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Role Profiles: Enhancing Role-Playing Agents with Video Modality",
                    "desc": "This paper presents a new framework for role-playing agents (RPAs) that enhances their ability to generate responses by using dynamic role profiles and video data. Traditional RPAs rely on static role profiles, which do not capture the fluid and adaptive nature of human interactions. By introducing dynamic role profiles that utilize video modality, the framework allows for adaptive temporal sampling of video frames, improving the contextual understanding of characters. The authors also introduce a large dataset, Role-playing-Video60k, to support this approach and demonstrate its effectiveness through comprehensive evaluations across multiple metrics."
                },
                "zh": {
                    "title": "åŠ¨æ€è§’è‰²æ¡£æ¡ˆæå‡è§’è‰²æ‰®æ¼”ä»£ç†çš„å“åº”èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåŠ¨æ€è§’è‰²æ¡£æ¡ˆå’Œè§†é¢‘æ¨¡æ€çš„æ¡†æ¶ï¼Œä»¥å¢å¼ºè§’è‰²æ‰®æ¼”ä»£ç†ï¼ˆRPAï¼‰çš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è§†é¢‘æ¨¡æ€ï¼Œç ”ç©¶è€…ä»¬èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»çš„åŠ¨æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…ä¾èµ–é™æ€è§’è‰²æ¡£æ¡ˆã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºRole-playing-Video60kçš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«60,000ä¸ªè§†é¢‘å’Œ700,000ä¸ªå¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€è§’è‰²æ¡£æ¡ˆçš„æ•´åˆæ˜¾è‘—æé«˜äº†RPAçš„å“åº”ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10452",
            "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers",
            "url": "https://huggingface.co/papers/2509.10452",
            "abstract": "WhisTLE, a text-only adaptation method using a variational autoencoder, enhances pretrained ASR models with text-to-latent encoding and optional TTS adaptation, reducing word error rates across multiple datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "30cff8720dc8ac12",
            "authors": [
                "Akshat Pandey",
                "Karun Kumar",
                "Raphael Tang"
            ],
            "affiliations": [
                "Comcast Applied AI",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10452.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#transfer_learning",
                    "#inference"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ASR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚",
                    "desc": "WhisTLE - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. WhisTLE Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ text-to-speech (TTS). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "WhisTLE: Text-Only Adaptation for Enhanced ASR Performance",
                    "desc": "WhisTLE is a novel method that improves pretrained automatic speech recognition (ASR) models by using a variational autoencoder (VAE) for text-only adaptation. This approach allows the models to better understand and transcribe unseen vocabulary without needing additional speech data, which is often hard to collect. By training the VAE on text inputs, WhisTLE fine-tunes the ASR model's decoder, optionally incorporating text-to-speech (TTS) adaptation for further enhancement. The results show significant reductions in word error rates across various datasets, demonstrating WhisTLE's effectiveness in adapting ASR models to new domains."
                },
                "zh": {
                    "title": "WhisTLEï¼šæ–‡æœ¬é€‚åº”æå‡è¯­éŸ³è¯†åˆ«æ¨¡å‹",
                    "desc": "WhisTLEæ˜¯ä¸€ç§æ–‡æœ¬-onlyçš„é€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥å¢å¼ºé¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡æ–‡æœ¬åˆ°æ½œåœ¨ç¼–ç çš„æ–¹å¼ï¼Œå‡å°‘äº†åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæ”¶é›†è¯­éŸ³æ•°æ®å¹¶ä¸ç°å®ï¼Œå› æ­¤WhisTLEæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–‡æœ¬-onlyé€‚åº”æ–¹æ¡ˆã€‚é€šè¿‡æ·±åº¦ç›‘ç£å’Œå¯é€‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é€‚åº”ï¼ŒWhisTLEåœ¨å¤šä¸ªåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15061",
            "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn\n  Dialogue",
            "url": "https://huggingface.co/papers/2509.15061",
            "abstract": "The Ask-to-Clarify framework uses a VLM for collaboration and a diffusion model for action generation, enabling embodied agents to handle ambiguous instructions through multi-turn dialogue and outperform existing VLAs in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents.",
            "score": 0,
            "issue_id": 6007,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "6f2bc6f2c7d24315",
            "authors": [
                "Xingyao Lin",
                "Xinghao Zhu",
                "Tianyi Lu",
                "Sicheng Xie",
                "Hui Zhang",
                "Xipeng Qiu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "College of Computer Science and Artificial Intelligence, Fudan University, Shanghai, China",
                "Mechanical Systems Control Lab, UC Berkeley, California, USA",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15061.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#agents",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ask-to-Clarify: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ask-to-Clarify Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (VLM) Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ (VLA) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ask-to-Clarify Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 8 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Agents Through Clarification and Collaboration",
                    "desc": "The Ask-to-Clarify framework enhances embodied agents by enabling them to engage in multi-turn dialogues to clarify ambiguous instructions before executing actions. It utilizes a Vision-Language Model (VLM) for effective collaboration and a diffusion model for generating precise actions. This two-component system allows agents to adapt their responses based on human feedback, moving beyond traditional one-way instruction execution. The framework has been tested in real-world tasks, demonstrating superior performance compared to existing Vision-Language Agents (VLAs)."
                },
                "zh": {
                    "title": "åä½œå‹å…·èº«ä»£ç†çš„æ–°è·¯å¾„",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAsk-to-Clarifyçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡ç³ŠæŒ‡ä»¤çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¤šè½®å¯¹è¯æ¥æ¾„æ¸…æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆä½çº§åŠ¨ä½œã€‚ä¸ç°æœ‰çš„è§†è§‰è¯­è¨€ä»£ç†ï¼ˆVLAï¼‰ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œä½“ç°äº†åä½œå‹å…·èº«ä»£ç†çš„æ½œåŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„çŸ¥è¯†éš”ç¦»ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿ä»£ç†èƒ½å¤Ÿåœ¨è¯¢é—®é—®é¢˜åå†æ‰§è¡ŒåŠ¨ä½œã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-19.html",
    "link_next": "2025-09-23.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}