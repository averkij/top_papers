{
    "date": {
        "ru": "7 июля",
        "en": "July 7",
        "zh": "7月7日"
    },
    "time_utc": "2025-07-07 17:11",
    "weekday": 0,
    "issue_id": 4684,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01955",
            "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
            "url": "https://huggingface.co/papers/2507.01955",
            "abstract": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.",
            "score": 6,
            "issue_id": 4676,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "650bb4b7601b21e0",
            "authors": [
                "Rahul Ramachandran",
                "Ali Garjani",
                "Roman Bachmann",
                "Andrei Atanov",
                "Oğuzhan Fatih Kar",
                "Amir Zamir"
            ],
            "affiliations": [
                "Swiss Federal Institute of Technology Lausanne (EPFL)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01955.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальность мультимодальных моделей в компьютерном зрении: потенциал и ограничения",
                    "desc": "Статья исследует эффективность мультимодальных фундаментальных моделей в различных задачах компьютерного зрения. Авторы разработали стандартизированную систему оценки, используя цепочки промптов для адаптации моделей к задачам, которые обычно требуют специализированных выходных данных. Результаты показывают, что хотя эти модели уступают специализированным решениям, они демонстрируют достойную производительность в качестве универсальных инструментов. Исследование также выявило, что модели лучше справляются с семантическими задачами, чем с геометрическими, а GPT-4o показала наилучшие результаты среди нерассуждающих моделей."
                },
                "en": {
                    "title": "Benchmarking Multimodal Models: Generalists in Vision Tasks",
                    "desc": "This paper evaluates the performance of multimodal foundation models on various computer vision tasks, such as semantic segmentation and object detection. Despite their training primarily on image-text tasks, these models show respectable generalist capabilities but do not match the performance of specialized models. The authors introduce a benchmarking framework that translates vision tasks into text-promptable formats to address challenges related to model accessibility and output limitations. Findings indicate that while these models excel in semantic tasks, they struggle with geometric tasks, and the best-performing model, GPT-4o, leads in several categories despite some quirks in image generation."
                },
                "zh": {
                    "title": "多模态模型的视觉任务表现评估",
                    "desc": "多模态基础模型主要在图像-文本任务上训练，但在适应性提示链的帮助下，在各种视觉任务上表现出色。本文对多种流行的多模态基础模型在标准计算机视觉任务上的表现进行了基准测试，发现这些模型在语义任务上表现优于几何任务。尽管它们在任何任务上都未能接近专业模型的水平，但作为通用模型，它们的表现仍然令人瞩目。我们提出了一种标准化的基准框架，通过将标准视觉任务转换为可通过提示链和API兼容的任务来解决模型适应性的问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02608",
            "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation",
            "url": "https://huggingface.co/papers/2507.02608",
            "abstract": "The use of latent space diffusion models for faster and accurate emulation of dynamical systems is viable, offering robustness to high compression rates and improved prediction diversity compared to non-generative approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t The steep computational cost of diffusion models at inference hinders their use as fast physics emulators. In the context of image and video generation, this computational drawback has been addressed by generating in the latent space of an autoencoder instead of the pixel space. In this work, we investigate whether a similar strategy can be effectively applied to the emulation of dynamical systems and at what cost. We find that the accuracy of latent-space emulation is surprisingly robust to a wide range of compression rates (up to 1000x). We also show that diffusion-based emulators are consistently more accurate than non-generative counterparts and compensate for uncertainty in their predictions with greater diversity. Finally, we cover practical design choices, spanning from architectures to optimizers, that we found critical to train latent-space emulators.",
            "score": 4,
            "issue_id": 4683,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "4fe1b404e88f405f",
            "authors": [
                "François Rozet",
                "Ruben Ohana",
                "Michael McCabe",
                "Gilles Louppe",
                "François Lanusse",
                "Shirley Ho"
            ],
            "affiliations": [
                "Flatiron Institute",
                "New York University",
                "Polymathic AI",
                "Princeton University",
                "University of Liège",
                "Université Paris-Saclay, Université Paris Cité, CEA, CNRS, AIM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02608.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#data",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🌀",
                "ru": {
                    "title": "Латентная диффузия для быстрой и точной эмуляции физических систем",
                    "desc": "Статья исследует применение моделей диффузии в латентном пространстве для эмуляции динамических систем. Авторы обнаружили, что точность эмуляции в латентном пространстве устойчива к высоким степеням сжатия данных. Эмуляторы на основе диффузии оказались более точными, чем негенеративные подходы, и обеспечивают большее разнообразие предсказаний. В работе также рассматриваются практические аспекты проектирования таких моделей, включая архитектуры и оптимизаторы."
                },
                "en": {
                    "title": "Efficient and Accurate Emulation of Dynamical Systems with Latent Space Diffusion Models",
                    "desc": "This paper explores the use of latent space diffusion models to emulate dynamical systems more efficiently and accurately. By operating in the latent space of an autoencoder, the models can significantly reduce computational costs while maintaining high accuracy, even with compression rates up to 1000 times. The study demonstrates that these diffusion-based emulators outperform traditional non-generative methods, providing better prediction diversity and handling uncertainty more effectively. Additionally, the authors discuss important design considerations for training these latent-space emulators, including architecture and optimization strategies."
                },
                "zh": {
                    "title": "潜在空间扩散模型：动态系统模拟的新突破",
                    "desc": "本论文探讨了在潜在空间中使用扩散模型来快速且准确地模拟动态系统的可行性。研究表明，潜在空间的模拟在高压缩率下（最高可达1000倍）仍然保持良好的准确性。与非生成方法相比，基于扩散的模拟器在预测准确性上表现更佳，并且能够通过更大的多样性来补偿预测的不确定性。最后，论文还讨论了在训练潜在空间模拟器时，架构和优化器等设计选择的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01853",
            "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
            "url": "https://huggingface.co/papers/2507.01853",
            "abstract": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
            "score": 3,
            "issue_id": 4672,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "a397a0d71f721623",
            "authors": [
                "Samridhi Raj Sinha",
                "Rajvee Sheth",
                "Abhishek Upperwal",
                "Mayank Singh"
            ],
            "affiliations": [
                "Indian Institute of Technology Gandhinagar",
                "LINGO Research Group",
                "NMIMS",
                "Soket AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01853.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "EKA-EVAL: Универсальная платформа для оценки многоязычных языковых моделей",
                    "desc": "EKA-EVAL - это комплексная многоязычная система оценки больших языковых моделей (LLM), поддерживающая разнообразные бенчмарки. Она включает более 35 тестов, в том числе 10 наборов данных для индийских языков, охватывающих такие категории, как рассуждение, математика, использование инструментов и понимание длинного контекста. EKA-EVAL предлагает встроенную поддержку распределенного вывода, квантизации и использования нескольких GPU. Эта система позиционируется как первый комплексный инструмент оценки, адаптированный как для глобальных, так и для индийских LLM."
                },
                "en": {
                    "title": "Empowering Multilingual Evaluation for Large Language Models",
                    "desc": "EKA-EVAL is a multilingual evaluation framework designed for large language models (LLMs), focusing on diverse linguistic needs, particularly in regions like India. It includes over 35 benchmarks, with specific datasets for Indic languages, covering various tasks such as reasoning and reading comprehension. The framework supports efficient distributed inference and multi-GPU usage, making it suitable for production environments. EKA-EVAL aims to lower the barriers for multilingual benchmarking and is part of a larger initiative to expand its benchmark offerings."
                },
                "zh": {
                    "title": "EKA-EVAL：多语言模型评估的新标准",
                    "desc": "EKA-EVAL是一个全面的多语言评估框架，专为大型语言模型设计。它支持多种基准测试和功能，能够高效地进行分布式推理和GPU使用。该框架整合了超过35个基准，包括10个特定于印度的数据库，涵盖推理、数学、工具使用、长文本理解和阅读理解等类别。EKA-EVAL是首个为全球和印度语言模型量身定制的端到端可扩展评估套件，显著降低了多语言基准测试的门槛。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00769",
            "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative\n  Writing",
            "url": "https://huggingface.co/papers/2507.00769",
            "abstract": "LitBench introduces a standardized benchmark for evaluating creative writing generated by language models, using human-labeled story comparisons and training reward models to assess and validate automated evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.",
            "score": 1,
            "issue_id": 4683,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "38c5e5a52a8decf8",
            "authors": [
                "Daniel Fein",
                "Sebastian Russo",
                "Violet Xiang",
                "Kabir Jolly",
                "Rafael Rafailov",
                "Nick Haber"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00769.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#dataset",
                    "#alignment",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "LitBench: новый стандарт оценки генеративного творческого письма",
                    "desc": "LitBench представляет собой стандартизированный бенчмарк для оценки творческого письма, сгенерированного языковыми моделями. Он использует размеченные людьми сравнения историй и обучает модели вознаграждения для оценки и валидации автоматизированных методов оценки. Бенчмарк включает тестовый набор из 2480 сравнений историй и обучающий корпус из 43827 пар с метками человеческих предпочтений. LitBench позволяет сравнивать различные методы оценки, включая готовые языковые модели и специально обученные модели вознаграждения."
                },
                "en": {
                    "title": "LitBench: A New Standard for Evaluating Creative Writing in AI",
                    "desc": "LitBench is a new benchmark designed to evaluate creative writing produced by language models. It addresses the challenge of assessing open-ended narratives, which often lack clear correct answers. The benchmark includes a dataset of human-labeled story comparisons and uses reward models to improve automated evaluation methods. Results show that trained reward models outperform off-the-shelf language models in aligning with human preferences for creative writing."
                },
                "zh": {
                    "title": "LitBench：创意写作评估的新标准",
                    "desc": "LitBench是一个标准化的基准，用于评估语言模型生成的创意写作。由于开放式叙事缺乏明确的真相，评估创意写作变得具有挑战性。该基准包含2,480个去偏见的人类标注故事比较和43,827对人类偏好标签的训练语料库。通过LitBench，我们能够评估零-shot语言模型的表现，并训练奖励模型，以提高创意写作的自动评估方法的可靠性。"
                }
            }
        }
    ],
    "link_prev": "2025-07-04.html",
    "link_next": "2025-07-08.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "04.07",
        "en": "07/04",
        "zh": "7月4日"
    },
    "short_date_next": {
        "ru": "08.07",
        "en": "07/08",
        "zh": "7月8日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}