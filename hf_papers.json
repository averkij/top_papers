{
    "date": {
        "ru": "29 октября",
        "en": "October 29",
        "zh": "10月29日"
    },
    "time_utc": "2024-10-29 02:48",
    "weekday": 1,
    "issue_id": 321,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.21252",
            "title": "LongReward: Improving Long-context Large Language Models with AI Feedback",
            "url": "https://huggingface.co/papers/2410.21252",
            "abstract": "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.",
            "score": 5,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "0ff5d39896cdfbbe",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#long_context",
                    "#training"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "LongReward: Улучшение языковых моделей для работы с длинным контекстом",
                    "desc": "Статья представляет новый метод LongReward для улучшения работы языковых моделей с длинным контекстом. Метод использует готовую большую языковую модель для оценки ответов по четырем параметрам: полезность, логичность, точность и полнота. LongReward применяется вместе с алгоритмом обучения с подкреплением DPO для улучшения моделей, обученных на длинных текстах. Эксперименты показывают, что метод значительно улучшает работу с длинным контекстом и короткими инструкциями."
                },
                "en": {
                    "title": "Enhancing Long-Context Performance with LongReward",
                    "desc": "This paper addresses the challenges faced by long-context large language models (LLMs) in generating high-quality data for supervised fine-tuning (SFT). It introduces LongReward, a method that leverages an existing LLM to provide rewards based on four key dimensions: helpfulness, logicality, faithfulness, and completeness. By integrating LongReward with the offline reinforcement learning algorithm DPO, the authors demonstrate significant improvements in the long-context performance of SFT models. The findings suggest that LongReward enhances both long-context and short instruction-following capabilities without compromising performance across different contexts."
                },
                "zh": {
                    "title": "提升长上下文模型性能的新方法",
                    "desc": "本文提出了一种名为LongReward的新方法，旨在提高长上下文大语言模型（LLM）的性能。通过利用现成的LLM，从四个维度（有用性、逻辑性、可信性和完整性）为长上下文模型的响应提供奖励信号。结合LongReward和离线强化学习算法DPO，我们能够有效提升长上下文的监督微调模型的表现。实验结果表明，LongReward不仅显著改善了模型的长上下文性能，还增强了其执行短指令的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21220",
            "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
            "url": "https://huggingface.co/papers/2410.21220",
            "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "bdb8b2a5fbb4c663",
            "data": {
                "categories": [
                    "#rag",
                    "#agents",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Зрение ИИ: от незнания к пониманию через интернет",
                    "desc": "В статье представлен Vision Search Assistant - новый подход к обработке визуальной информации. Он объединяет возможности больших визуально-языковых моделей (VLM) и веб-агентов для ответа на вопросы по незнакомым изображениям. Система использует генерацию с дополнением из интернета (Retrieval-Augmented Generation), чтобы получать актуальную информацию в реальном времени. Эксперименты показали значительное превосходство этого метода над существующими подходами."
                },
                "en": {
                    "title": "Empowering Vision-Language Models with Real-Time Web Collaboration",
                    "desc": "This paper introduces the Vision Search Assistant, a new framework that enhances the capabilities of vision-language models (VLMs) by enabling them to collaborate with web agents. Traditional VLMs struggle with unfamiliar visual content, especially when they encounter objects they have never seen before, leading to unreliable responses. The proposed framework allows VLMs to access real-time information from the web, facilitating open-world Retrieval-Augmented Generation. Experimental results show that the Vision Search Assistant significantly improves performance on both open-set and closed-set question-answering tasks, making it a valuable addition to existing VLMs."
                },
                "zh": {
                    "title": "视觉搜索助手：打破未知视觉内容的壁垒",
                    "desc": "本文提出了一种新的框架，称为视觉搜索助手（Vision Search Assistant），旨在解决传统视觉语言模型（VLMs）在处理未知视觉内容时的局限性。该框架通过结合VLMs的视觉理解能力和网络代理的实时信息访问，实现了开放世界的检索增强生成。这样，即使模型从未见过某个图像中的对象，也能提供准确的回答。实验结果表明，视觉搜索助手在开放集和封闭集的问答基准测试中显著优于其他模型，具有广泛的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.20011",
            "title": "A Survey of Small Language Models",
            "url": "https://huggingface.co/papers/2410.20011",
            "abstract": "Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "bde2fa0e4317a316",
            "data": {
                "categories": [
                    "#survey",
                    "#architecture",
                    "#training",
                    "#inference",
                    "#benchmark",
                    "#edge_computing"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Малые языковые модели: эффективность в компактности",
                    "desc": "Эта статья представляет собой всесторонний обзор малых языковых моделей (SLM), которые становятся все более важными из-за их эффективности и производительности при минимальных вычислительных ресурсах. Авторы предлагают новую таксономию для категоризации методов оптимизации SLM, включая сжатие модели, прунинг и квантизацию. В работе обобщаются наборы данных для бенчмаркинга SLM и метрики оценки. Статья также освещает ключевые нерешенные проблемы в области малых языковых моделей."
                },
                "en": {
                    "title": "Optimizing Small Language Models for Efficiency and Performance",
                    "desc": "This paper provides a detailed overview of Small Language Models (SLMs), which are designed to perform language tasks efficiently with low computational requirements. It introduces a new classification system for the various optimization methods used in SLMs, such as model compression, pruning, and quantization. The authors also compile important benchmark datasets and evaluation metrics that are essential for assessing the performance of SLMs. Furthermore, the paper discusses ongoing challenges in the field, aiming to assist researchers and practitioners in advancing the development of efficient language models."
                },
                "zh": {
                    "title": "小型语言模型：高效语言处理的未来",
                    "desc": "小型语言模型（SLMs）因其高效性和性能而变得越来越重要，能够在资源有限的情况下执行各种语言任务，非常适合在设备、移动和边缘设备等环境中使用。本文对SLMs进行了全面的调查，重点介绍了它们的架构、训练技术和模型压缩技术。我们提出了一种新的分类法，用于对优化SLMs的方法进行分类，包括模型压缩、剪枝和量化技术。我们总结了对SLMs进行基准测试的有用数据集以及常用的评估指标，并强调了仍需解决的关键开放挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21264",
            "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
            "url": "https://huggingface.co/papers/2410.21264",
            "abstract": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "20438897f3e9bc41",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LARP: Революционный подход к токенизации видео для генеративных моделей",
                    "desc": "LARP - это новый токенизатор видео, разработанный для преодоления ограничений существующих методов токенизации в авторегрессионных генеративных моделях. В отличие от традиционных токенизаторов, LARP использует набор обученных холистических запросов для сбора информации из визуального контента, что позволяет захватывать более глобальные и семантические представления. LARP интегрирует облегченный AR-трансформер в качестве предварительной модели во время обучения, что оптимизирует латентное пространство для авторегрессионной генерации. Эксперименты показывают, что LARP достигает передовых результатов на бенчмарке UCF101 по условной генерации видео."
                },
                "en": {
                    "title": "LARP: Revolutionizing Video Tokenization for Better Generative Models",
                    "desc": "LARP is a new video tokenizer that improves how videos are processed for autoregressive generative models. Instead of just breaking videos into small patches, LARP uses learned holistic queries to capture broader and more meaningful visual information. This method allows for flexible tokenization, adapting the number of tokens based on the task's needs. By integrating a lightweight autoregressive transformer during training, LARP optimizes the token space for better video generation, achieving top performance in benchmarks."
                },
                "zh": {
                    "title": "LARP：视频生成的新突破",
                    "desc": "本文介绍了一种新的视频标记器LARP，旨在克服当前自回归生成模型在视频标记方面的局限性。与传统的局部补丁标记器不同，LARP采用整体标记方案，通过学习的整体查询收集视觉内容的信息，从而捕捉更全球和语义化的表示。LARP支持任意数量的离散标记，能够根据任务的具体需求进行自适应和高效的标记。通过在训练过程中整合轻量级的自回归变换器，LARP优化了视频重建和自回归生成的潜在空间，确保在推理时实现更平滑和准确的生成。"
                }
            }
        }
    ],
    "link_prev": "2024-10-28.html",
    "link_next": "2024-10-30.html",
    "short_date_prev": {
        "ru": "28.10",
        "en": "10/28",
        "zh": "10月28日"
    },
    "short_date_next": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10月30日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 1,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1
    },
    "zh": {
        "text": "这篇文章讨论了视觉语言模型（VLMs）在开放环境中的决策能力。VLMs在多模式任务中表现出色，但在开放环境中应用时面临挑战。主要问题是将低层次观察中的单个实体与规划所需的抽象概念连接起来。为解决这一问题，作者提出了视觉时间上下文提示，一种VLMs与策略模型之间的新通信协议。该方法利用过去和现在的观察进行对象分割，指导策略与环境的交互。实验表明，这种方法使得代理能够完成以前无法实现的任务，特别是那些依赖空间理解的复杂任务。",
        "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
        "pinyin": "Zhè piān wénzhāng tǎolùn le shìjué yǔyán móxíng (VLMs) zài kāifàng huánjìng zhōng de juécè nénglì. VLMs zài duō móshì rènwù zhōng biǎoxiǎn chūsè, dàn zài kāifàng huánjìng zhōng yìngyòng shí miànlín tiǎozhàn. Zhǔyào wènti shì jiāng dī céng guānchá zhōng de dān gè shíjì yǔ guīhuà suǒxū de chōuxiàng gàiniàn liánjiē qǐlái. Wèi jiějué zhè yī wènti, zuòzhě tíchūle shìjué shíjiān shàngxiàtèng tíshì, yīzhǒng VLMs yǔ cèlüè móxíng zhījiān de xīn tōngxìn xiéyì. Gāi fāngfǎ lìyòng guòqù hé xiànzài de guānchá zuòwéi duìxiàng fēngé, zhǐdǎo cèlüè yǔ huánjìng de jiāohù. Shíyàn biǎomíng, zhè zhǒng fāngfǎ shǐdé dàilǐ nénggòu wánchéng yǐqián wúfǎ shíxiàn de rènwù, tèbié shì nàxiē yīlǎi kōngjiān lǐjiě de fùzá rènwù.",
        "vocab": "[\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shìjué yǔyán móxíng\", \"trans\": \"visual language models\"},\n    {\"word\": \"开放环境\", \"pinyin\": \"kāifàng huánjìng\", \"trans\": \"open environments\"},\n    {\"word\": \"决策能力\", \"pinyin\": \"juécè nénglì\", \"trans\": \"decision-making ability\"},\n    {\"word\": \"多模式任务\", \"pinyin\": \"duō móshì rènwù\", \"trans\": \"multimodal tasks\"},\n    {\"word\": \"表现出色\", \"pinyin\": \"biǎoxiàn chūsè\", \"trans\": \"perform well\"},\n    {\"word\": \"面临挑战\", \"pinyin\": \"miànlín tiǎozhàn\", \"trans\": \"face challenges\"},\n    {\"word\": \"低层次观察\", \"pinyin\": \"dī céngcì guānchá\", \"trans\": \"low-level observations\"},\n    {\"word\": \"单个实体\", \"pinyin\": \"dān gè shítǐ\", \"trans\": \"individual entities\"},\n    {\"word\": \"规划所需的抽象概念\", \"pinyin\": \"guīhuà suǒxū de chōuxiàng gàiniàn\", \"trans\": \"abstract concepts required for planning\"},\n    {\"word\": \"视觉时间上下文提示\", \"pinyin\": \"shìjué shíjiān shàngxiàwén tíshì\", \"trans\": \"visual temporal context prompts\"},\n    {\"word\": \"新通信协议\", \"pinyin\": \"xīn tōngxìn xiéyì\", \"trans\": \"new communication protocol\"},\n    {\"word\": \"过去和现在的观察\", \"pinyin\": \"guòqù hé xiànzài de guānchá\", \"trans\": \"past and present observations\"},\n    {\"word\": \"对象分割\", \"pinyin\": \"duìxiàng fēngé\", \"trans\": \"object segmentation\"},\n    {\"word\": \"指导策略与环境的交互\", \"pinyin\": \"zhǐdǎo cèlüè yǔ huánjìng de jiāohù\", \"trans\": \"guide the interaction of strategies with the environment\"},\n    {\"word\": \"代理\", \"pinyin\": \"dàilǐ\", \"trans\": \"agent\"},\n    {\"word\": \"完成以前无法实现的任务\", \"pinyin\": \"wánchéng yǐqián wúfǎ shíxiàn de rènwù\", \"trans\": \"complete tasks that were previously impossible\"},\n    {\"word\": \"依赖空间理解的复杂任务\", \"pinyin\": \"yīlài kōngjiān lǐjiě de fùzá rènwù\", \"trans\": \"complex tasks that rely on spatial understanding\"}\n]",
        "trans": "This article discusses the decision-making capabilities of Vision-Language Models (VLMs) in open environments. VLMs perform well in multimodal tasks but face challenges when applied in open environments. The main issue is connecting individual entities from low-level observations with the abstract concepts required for planning. To address this problem, the authors propose visual temporal context prompting, a new communication protocol between VLMs and policy models. This method utilizes past and present observations for object segmentation, guiding the policy's interaction with the environment. Experiments show that this approach enables agents to complete tasks that were previously unachievable, particularly those that rely on spatial understanding.",
        "update_ts": "2024-10-28 09:26"
    }
}