{
    "date": {
        "ru": "1 –º–∞—è",
        "en": "May 1",
        "zh": "5Êúà1Êó•"
    },
    "time_utc": "2025-05-01 21:11",
    "weekday": 3,
    "issue_id": 3544,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.21635",
            "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
            "url": "https://huggingface.co/papers/2504.21635",
            "abstract": "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.",
            "score": 44,
            "issue_id": 3530,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "af5a1b038b3ccab3",
            "authors": [
                "Zeina Aldallal",
                "Sara Chrouf",
                "Khalil Hennara",
                "Mohamed Motaism Hamed",
                "Muhammad Hreden",
                "Safwan AlModhayan"
            ],
            "affiliations": [
                "Khobar, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21635.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#low_resource",
                    "#multilingual",
                    "#benchmark",
                    "#data",
                    "#machine_translation"
                ],
                "emoji": "üî†",
                "ru": {
                    "title": "Sadeed: –ü—Ä–æ—Ä—ã–≤ –≤ –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏",
                    "desc": "Sadeed - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–æ–¥–µ–ª–∏ Kuwain 1.5B. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, Sadeed –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SadeedDiac-25 –¥–ª—è –±–æ–ª–µ–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
                },
                "en": {
                    "title": "Sadeed: Advancing Arabic Diacritization with Efficiency and Precision",
                    "desc": "This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field."
                },
                "zh": {
                    "title": "SadeedÔºöÈòøÊãâ‰ºØËØ≠Ê†áËÆ∞ÂåñÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈòøÊãâ‰ºØËØ≠ÊñáÊú¨Ê†áËÆ∞ÂåñÊñπÊ≥ïÔºåÂêç‰∏∫Sadeed„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÁªèËøáÂæÆË∞ÉÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ìÈó®ÈíàÂØπÈòøÊãâ‰ºØËØ≠ÁöÑ‰∏∞ÂØåÂΩ¢ÊÄÅÁâπÂæÅËøõË°å‰ºòÂåñ„ÄÇSadeedÂú®È´òË¥®ÈáèÁöÑÊ†áËÆ∞ÂåñÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºåÂ∞ΩÁÆ°ËÆ°ÁÆóËµÑÊ∫êÊúâÈôêÔºå‰ΩÜÂÖ∂ÊÄßËÉΩ‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏î‰ºò‰∫é‰º†ÁªüÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫ÜSadeedDiac-25Âü∫ÂáÜÔºå‰ª•‰øÉËøõÂØπÈòøÊãâ‰ºØËØ≠Ê†áËÆ∞ÂåñÁöÑÂÖ¨Âπ≥ËØÑ‰º∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21776",
            "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
            "url": "https://huggingface.co/papers/2504.21776",
            "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
            "score": 25,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "61ce82abe42f584a",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Guanting Dong",
                "Hongjin Qian",
                "Yutao Zhu",
                "Yongkang Wu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "BAAI",
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21776.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#science",
                    "#agents",
                    "#rlhf",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üï∏Ô∏è",
                "ru": {
                    "title": "WebThinker: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
                    "desc": "WebThinker - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å Deep Web Explorer –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è, –ø–æ–∏—Å–∫–∞ –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–æ–≤. WebThinker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (DPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ WebThinker –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "Empowering LRMs with Real-Time Web Research Capabilities",
                    "desc": "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."
                },
                "zh": {
                    "title": "WebThinkerÔºöËÆ©Êé®ÁêÜÊ®°ÂûãÊõ¥Êô∫ËÉΩÁöÑÁ†îÁ©∂Âä©Êâã",
                    "desc": "Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂ¶ÇOpenAI-o1ÂíåDeepSeek-R1Âú®ÈïøÊó∂Èó¥Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨‰æùËµñÈùôÊÄÅÂÜÖÈÉ®Áü•ËØÜÔºåÈôêÂà∂‰∫ÜÂú®Â§çÊùÇÁü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜWebThinkerÔºå‰∏Ä‰∏™Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÔºåËÉΩÂ§üËÆ©LRMsËá™‰∏ªÊêúÁ¥¢ÁΩëÁªú„ÄÅÊµèËßàÁΩëÈ°µÂπ∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Êí∞ÂÜôÁ†îÁ©∂Êä•Âëä„ÄÇWebThinkerÈõÜÊàê‰∫ÜÊ∑±ÁΩëÊé¢Á¥¢Ê®°ÂùóÔºå‰ΩøLRMsÂú®ÈÅáÂà∞Áü•ËØÜÁ©∫ÁôΩÊó∂ËÉΩÂ§üÂä®ÊÄÅÊêúÁ¥¢ÂíåÊèêÂèñ‰ø°ÊÅØ„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéWebThinkerÂú®Â§çÊùÇÊé®ÁêÜÂü∫ÂáÜÂíåÁßëÂ≠¶Êä•ÂëäÁîüÊàê‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21850",
            "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
            "url": "https://huggingface.co/papers/2504.21850",
            "abstract": "Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.",
            "score": 20,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "3db97f245360deb4",
            "authors": [
                "Xindi Wu",
                "Hee Seung Hwang",
                "Polina Kirichenko",
                "Olga Russakovsky"
            ],
            "affiliations": [
                "Meta AI",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21850.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "COMPACT: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ MLLM —Å–ª–æ–∂–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º COMPACT. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤. COMPACT –ø–æ–∑–≤–æ–ª—è–µ—Ç MLLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–±—É—á–∞—Ç—å—Å—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å LLaVA-665k, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω–µ–µ 10% –¥–∞–Ω–Ω—ã—Ö, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö."
                },
                "en": {
                    "title": "Unlocking Complex Tasks with Efficient Compositional Training",
                    "desc": "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§çÊùÇËßÜËßâËØ≠Ë®Ä‰ªªÂä°ÁöÑËÉΩÂäõ",
                    "desc": "Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁÆÄÂçïÁöÑËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅÂ§öÁßçËÉΩÂäõÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠Âç¥Èù¢‰∏¥ÊåëÊàò„ÄÇ‰º†ÁªüÁöÑËßÜËßâÊåá‰ª§Ë∞É‰ºòÔºàVITÔºâ‰∏ªË¶ÅÂÖ≥Ê≥®Êï∞ÊçÆÈáèÁöÑÊâ©Â§ßÔºåËÄåÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÁ§∫‰æãÁöÑÁªÑÂêàÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCOMPACTÔºàÁªÑÂêàÂéüÂ≠êÂà∞Â§çÊùÇËßÜËßâËÉΩÂäõË∞É‰ºòÔºâÔºåÂÆÉÁîüÊàê‰∏Ä‰∏™ÊòéÁ°ÆÊéßÂà∂ËÆ≠ÁªÉÁ§∫‰æãÁªÑÂêàÂ§çÊùÇÊÄßÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ„ÄÇCOMPACT‰ΩøÂæóMLLMsËÉΩÂ§üÊõ¥È´òÊïàÂú∞Â≠¶‰π†Â§çÊùÇËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∂âÂèäÂ§çÊùÇÂ§öËÉΩÂäõ‰ªªÂä°Êó∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21233",
            "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
            "url": "https://huggingface.co/papers/2504.21233",
            "abstract": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.",
            "score": 19,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "0b800a9195884db4",
            "authors": [
                "Haoran Xu",
                "Baolin Peng",
                "Hany Awadalla",
                "Dongdong Chen",
                "Yen-Chun Chen",
                "Mei Gao",
                "Young Jin Kim",
                "Yunsheng Li",
                "Liliang Ren",
                "Yelong Shen",
                "Shuohang Wang",
                "Weijian Xu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21233.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (SLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —á–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∏–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö, –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä—è–º–æ–≥–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∫ –º–æ–¥–µ–ª–∏ Phi-4-Mini (3.8 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–∞–∂–µ –≤ —Ä–µ—Å—É—Ä—Å–Ω–æ-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –º–∞–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö."
                },
                "en": {
                    "title": "Unlocking Reasoning Power in Small Models",
                    "desc": "This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning."
                },
                "zh": {
                    "title": "Â∞èÊ®°Âûã‰πüËÉΩÂº∫Êé®ÁêÜÔºÅ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊù•ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÁîüÊàê‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂ∞èÂûãÊ®°ÂûãÁî±‰∫éÂÆπÈáèÈôêÂà∂ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøá‰ªéLLMÁîüÊàêÁöÑÂêàÊàêÊï∞ÊçÆËøõË°åËí∏È¶èÔºåÂèØ‰ª•ÊòæËëóÊîπÂñÑSLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑËÆ≠ÁªÉÊñπÊ°àÔºåÂåÖÊã¨Âõõ‰∏™Ê≠•È™§ÔºåÊúÄÁªàÂú®Phi-4-MiniÊ®°Âûã‰∏äÂÆûÁé∞‰∫ÜË∂ÖË∂äÊõ¥Â§ßÊ®°ÂûãÁöÑÊé®ÁêÜË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20708",
            "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think",
            "url": "https://huggingface.co/papers/2504.20708",
            "abstract": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.",
            "score": 17,
            "issue_id": 3532,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 –∞–ø—Ä–µ–ª—è",
                "en": "April 29",
                "zh": "4Êúà29Êó•"
            },
            "hash": "b26e58cf1cee464f",
            "authors": [
                "Hasan Abed Al Kader Hammoud",
                "Hani Itani",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "KAUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20708.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–æ–¥–º—ã—Å–ª–∏ –≤ LLM: –ø—É—Ç—å –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–ø–æ–¥–º—ã—Å–ª–µ–π) –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–æ–¥–º—ã—Å–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π –∏–∑ –∫–∞–∂–¥–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–π —Ç–æ—á–∫–∏. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–º—ã—Å–ª–µ–π, —á–∞—Å—Ç–æ –¥–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, –¥–æ—Å—Ç–∏–≥–∞—é—â–µ–µ 13%."
                },
                "en": {
                    "title": "Unlocking Better Answers Through Subthoughts in LLMs",
                    "desc": "This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks."
                },
                "zh": {
                    "title": "‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÔºåÊèêÂçáÊ®°ÂûãÂáÜÁ°ÆÊÄß",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÊó∂ÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™ÈóÆÈ¢òÔºöÊúÄÁªàÁ≠îÊ°àÊòØÂê¶ÂèØÈù†Âú∞‰ª£Ë°®Ê®°ÂûãÁöÑÊúÄ‰Ω≥ÁªìËÆ∫Ôºü‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÊòØÂê¶‰ºö‰∫ßÁîü‰∏çÂêåÁöÑÁªìÊûúÔºü‰∏∫‰∫ÜËß£Á≠îËøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÁß∞‰∏∫Â≠êÊÄùÁª¥ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËøô‰∫õÂèëÁé∞ÁöÑÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáÈÄâÊã©ÊúÄÈ¢ëÁπÅÁöÑÁ≠îÊ°àÔºà‰ºóÊï∞ÔºâÊù•ËÅöÂêà‰∏çÂêåÂ≠êÊÄùÁª¥ÁöÑÁ≠îÊ°àÔºåÂáÜÁ°ÆÊÄßÊòæËëóÊèêÈ´òÔºåÊúÄÈ´òÂèØËææ13%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21318",
            "title": "Phi-4-reasoning Technical Report",
            "url": "https://huggingface.co/papers/2504.21318",
            "abstract": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",
            "score": 14,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "7004ae060f674e9a",
            "authors": [
                "Marah Abdin",
                "Sahaj Agarwal",
                "Ahmed Awadallah",
                "Vidhisha Balachandran",
                "Harkirat Behl",
                "Lingjiao Chen",
                "Gustavo de Rosa",
                "Suriya Gunasekar",
                "Mojan Javaheripi",
                "Neel Joshi",
                "Piero Kauffmann",
                "Yash Lara",
                "Caio C√©sar Teodoro Mendes",
                "Arindam Mitra",
                "Besmira Nushi",
                "Dimitris Papailiopoulos",
                "Olli Saarikivi",
                "Shital Shah",
                "Vaishnavi Shrivastava",
                "Vibhav Vineet",
                "Yue Wu",
                "Safoora Yousefi",
                "Guoqing Zheng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21318.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#dataset",
                    "#math",
                    "#transfer_learning",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Phi-4-reasoning - –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—É—é –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. Phi-4-reasoning –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –Ω–∞—É—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ."
                },
                "en": {
                    "title": "Unlocking Complex Reasoning with Phi-4-Reasoning",
                    "desc": "The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models."
                },
                "zh": {
                    "title": "Êé®ÁêÜÊ®°ÂûãÁöÑÊñ∞Á™ÅÁ†¥ÔºöPhi-4-reasoning",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜPhi-4-reasoningÔºåËøôÊòØ‰∏Ä‰∏™Êã•Êúâ140‰∫øÂèÇÊï∞ÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂØπÁ≤æÂøÉÊåëÈÄâÁöÑ‚ÄúÂèØÊïô‚ÄùÊèêÁ§∫ËøõË°åÁõëÁù£ÂæÆË∞ÉËÆ≠ÁªÉÔºåÁîüÊàêËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÔºåÊúâÊïàÂà©Áî®Êé®ÁêÜÊó∂ÁöÑËÆ°ÁÆóËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫ÜPhi-4-reasoning-plusÔºåÈÄöËøáÂü∫‰∫éÁªìÊûúÁöÑÂº∫ÂåñÂ≠¶‰π†Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ÔºåËÉΩÂ§üÁîüÊàêÊõ¥ÈïøÁöÑÊé®ÁêÜËΩ®ËøπÔºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇÁªºÂêàËØÑ‰º∞ÊòæÁ§∫ÔºåËøô‰∏§‰∏™Ê®°ÂûãÂú®Êï∞Â≠¶„ÄÅÁßëÂ≠¶Êé®ÁêÜ„ÄÅÁºñÁ†ÅÁ≠âÂ§ö‰∏™‰ªªÂä°‰∏äÂùá‰ºò‰∫éÊõ¥Â§ßÁöÑÂºÄÊîæÊùÉÈáçÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20966",
            "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
            "url": "https://huggingface.co/papers/2504.20966",
            "abstract": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.",
            "score": 14,
            "issue_id": 3526,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 –∞–ø—Ä–µ–ª—è",
                "en": "April 29",
                "zh": "4Êúà29Êó•"
            },
            "hash": "cb610c1427bdf307",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "Softpick: —É–ª—É—á—à–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç softpick - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –∑–∞–º–µ–Ω—è—é—â–∏–π softmax. Softpick —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 'attention sink' –∏ —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ softmax. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ softpick —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –º–µ–Ω—å—à–∏–º —ç–∫—Å—Ü–µ—Å—Å–æ–º. –ú–æ–¥–µ–ª–∏ —Å softpick –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç softmax –ø—Ä–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –±–∏—Ç–æ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Softpick: A Smarter Alternative to Softmax for Transformers",
                    "desc": "This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability."
                },
                "zh": {
                    "title": "softpickÔºöÊèêÂçáTransformerÊ≥®ÊÑèÂäõÁöÑÊñ∞ÈÄâÊã©",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫softpickÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Êõø‰ª£transformerÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÁöÑsoftmax„ÄÇsoftpick‰∏çÈúÄË¶ÅÂ∞ÜÊùÉÈáçÂΩí‰∏ÄÂåñ‰∏∫1ÔºåËÉΩÂ§üÊ∂àÈô§Ê≥®ÊÑèÂäõÊ≤âÊ≤°ÂíåÂ§ßËßÑÊ®°ÊøÄÊ¥ª„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®softpickÁöÑÊ®°ÂûãÂú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠‰∏ésoftmaxË°®Áé∞Áõ∏ÂΩìÔºå‰ΩÜÊ≥®ÊÑèÂäõÊ≤âÊ≤°Áéá‰∏∫0%ÔºåÂπ∂‰∏îÁîüÊàêÁöÑÈöêËóèÁä∂ÊÄÅÂÖ∑ÊúâÊõ¥‰ΩéÁöÑÂ≥∞Â∫¶„ÄÇsoftpickÂú®ÈáèÂåñÂíå‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉ‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÂ∞§ÂÖ∂Âú®ËæÉ‰Ωé‰ΩçÊï∞Á≤æÂ∫¶‰∏ãÂÖ∑ÊúâÊòéÊòæ‰ºòÂäøÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Á®ÄÁñèÊÄß‰ºòÂåñÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19720",
            "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
            "url": "https://huggingface.co/papers/2504.19720",
            "abstract": "Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",
            "score": 9,
            "issue_id": 3526,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 –∞–ø—Ä–µ–ª—è",
                "en": "April 28",
                "zh": "4Êúà28Êó•"
            },
            "hash": "e74f8b7af65e09fd",
            "authors": [
                "Ranran Zhen",
                "Juntao Li",
                "Yixin Ji",
                "Zhenlin Yang",
                "Tong Liu",
                "Qingrong Xia",
                "Xinyu Duan",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Min Zhang"
            ],
            "affiliations": [
                "Huawei Cloud",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19720.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –¢–∞–∫–∂–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–ª–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –Ω–æ–≤—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Optimizing LLMs: Balancing Performance and Efficiency in Generative AI",
                    "desc": "This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services."
                },
                "zh": {
                    "title": "Êé®Âä®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊúçÂä°ÁöÑÁ†îÁ©∂ËøõÂ±ï",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂Â∫ûÂ§ßÁöÑÂèÇÊï∞ÈáèÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÈ´òËÆ°ÁÆóÈúÄÊ±ÇÂØºËá¥‰∫ÜÂÜÖÂ≠òÂºÄÈîÄÂ§ßÔºåÂΩ±Âìç‰∫ÜÊé®ÁêÜÊúçÂä°ÁöÑ‰ΩéÂª∂ËøüÂíåÈ´òÂêûÂêêÈáè„ÄÇÊú¨ÊñáÂÖ®Èù¢Ë∞ÉÊü•‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÁöÑÊñπÊ≥ïÔºåÂåÖÊã¨ÂÆû‰æãÁ∫ßÂíåÈõÜÁæ§Á∫ßÁöÑÁ≠ñÁï•Ôºå‰ª•ÂèäÊñ∞ÂÖ¥Âú∫ÊôØÁöÑÊñπÂêë„ÄÇÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤„ÄÅËØ∑Ê±ÇË∞ÉÂ∫¶„ÄÅËß£Á†ÅÈïøÂ∫¶È¢ÑÊµãÁ≠âÂÆû‰æãÁ∫ßÊñπÊ≥ïÔºå‰ª•ÂèäGPUÈõÜÁæ§ÈÉ®ÁΩ≤ÂíåÂ§öÂÆû‰æãË¥üËΩΩÂùáË°°Á≠âÈõÜÁæ§Á∫ßÁ≠ñÁï•„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÊΩúÂú®ÊñπÂêëÔºå‰ª•Êé®Âä®LLMÊé®ÁêÜÊúçÂä°ÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21855",
            "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
            "url": "https://huggingface.co/papers/2504.21855",
            "abstract": "In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.",
            "score": 7,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "5d8989ce0c77aa23",
            "authors": [
                "Qihao Liu",
                "Ju He",
                "Qihang Yu",
                "Liang-Chieh Chen",
                "Alan Yuille"
            ],
            "affiliations": [
                "Independent Researcher",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21855.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games",
                    "#video",
                    "#small_models"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "ReVision: —Ñ–∏–∑–∏–∫–∞ –≤ –ø–æ–º–æ—â—å –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ",
                    "desc": "ReVision - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö —ç—Ç–∞–ø–æ–≤: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä—É–±–æ–≥–æ –≤–∏–¥–µ–æ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ 2D –∏ 3D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏. ReVision –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "ReVision: Enhancing Video Generation with 3D Physical Knowledge",
                    "desc": "The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation."
                },
                "zh": {
                    "title": "ÈÄöËøá3DÁâ©ÁêÜÁü•ËØÜÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÁúüÂÆûÊÑü‰∏éÂèØÊéßÊÄß",
                    "desc": "ËøëÂπ¥Êù•ÔºåËßÜÈ¢ëÁîüÊàêÊäÄÊúØÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÁîüÊàêÂ§çÊùÇÂä®‰ΩúÂíå‰∫§‰∫íÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜReVisionÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊèíÊãîÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÂ∞ÜÂèÇÊï∞ÂåñÁöÑ‰∏âÁª¥Áâ©ÁêÜÁü•ËØÜÈõÜÊàêÂà∞È¢ÑËÆ≠ÁªÉÁöÑÊù°‰ª∂ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠Ôºå‰ªéËÄåÊòæËëóÊèêÂçáÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÁöÑËÉΩÂäõ„ÄÇReVisionÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁîüÊàêÁ≤óÁï•ËßÜÈ¢ëÔºåÁÑ∂ÂêéÊèêÂèñ2DÂíå3DÁâπÂæÅÊûÑÂª∫‰∏âÁª¥Áâ©‰Ωì‰∏≠ÂøÉË°®Á§∫ÔºåÊúÄÂêéÈÄöËøáÂèÇÊï∞ÂåñÁâ©ÁêÜÂÖàÈ™åÊ®°ÂûãÁ≤æÁÇºËøêÂä®Â∫èÂàóÔºåÂèçÈ¶àÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏≠‰ª•ÁîüÊàê‰∏ÄËá¥ÁöÑËøêÂä®ËßÜÈ¢ë„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåReVisionÂú®Â§çÊùÇËßÜÈ¢ëÁîüÊàê‰∏äË°®Áé∞‰ºòÂºÇÔºåÁîöËá≥‰ª•ËæÉÂ∞ëÁöÑÂèÇÊï∞Ë∂ÖË∂ä‰∫ÜÂ§ßÂûãÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19056",
            "title": "Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions",
            "url": "https://huggingface.co/papers/2504.19056",
            "abstract": "Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.",
            "score": 7,
            "issue_id": 3535,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 –∞–ø—Ä–µ–ª—è",
                "en": "April 27",
                "zh": "4Êúà27Êó•"
            },
            "hash": "deba947a8e69f6ee",
            "authors": [
                "Mohammad Mahdi Abootorabi",
                "Omid Ghahroodi",
                "Pardis Sadat Zahraei",
                "Hossein Behzadasl",
                "Alireza Mirrokni",
                "Mobina Salimipanah",
                "Arash Rasouli",
                "Bahar Behzadipour",
                "Sara Azarnoush",
                "Benyamin Maleki",
                "Erfan Sadraiye",
                "Kiarash Kiani Feriz",
                "Mahdi Teymouri Nahad",
                "Ali Moghadasi",
                "Abolfazl Eshagh Abianeh",
                "Nizi Nazar",
                "Hamid R. Rabiee",
                "Mahdieh Soleymani Baghshah",
                "Meisam Ahmadi",
                "Ehsaneddin Asgari"
            ],
            "affiliations": [
                "Computer Engineering Department, Sharif University of Technology, Tehran, Iran",
                "Iran University of Science and Technology",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19056.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#diffusion",
                    "#cv",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –∞–Ω–∏–º–∞—Ü–∏—é –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π",
                    "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä–Ω–∞—è —Å—Ç–∞—Ç—å—è –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ü–µ–≤–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏, —Å–∏–Ω—Ç–µ–∑–∞ –¥–≤–∏–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–∏—è –∞–≤–∞—Ç–∞—Ä–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±–∑–æ—Ä –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –≤–∫–ª—é—á–∞—è –≤–µ–¥—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö. –¢–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è —Å–ø—Ä–∞–≤–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ –º–µ—Ç—Ä–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with Generative AI",
                    "desc": "This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation."
                },
                "zh": {
                    "title": "ÁîüÊàêÊÄßAIÔºöÈáçÂ°ëËßíËâ≤Âä®ÁîªÁöÑÊú™Êù•",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂú®ËßíËâ≤Âä®ÁîªÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Èù¢ÈÉ®Âä®Áîª„ÄÅË°®ÊÉÖÊ∏≤ÊüìÂíåÂä®‰ΩúÂêàÊàêÁ≠âÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÈÄöËøáÊï¥Âêà‰∏çÂêåÁöÑÁîüÊàêÊ®°ÂûãÂíåÊâ©Êï£Ê®°ÂûãÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊòæËëóÈôç‰Ωé‰∫ÜÂä®ÁîªÂÜÖÂÆπÁöÑÂà∂‰ΩúÊó∂Èó¥ÂíåÊàêÊú¨„ÄÇËÆ∫ÊñáËøòÊèê‰æõ‰∫ÜÂØπÂΩìÂâçÁ†îÁ©∂„ÄÅÂÆûÈôÖÂ∫îÁî®„ÄÅÂ∏∏Áî®Êï∞ÊçÆÈõÜÂíåÊñ∞ÂÖ¥Ë∂ãÂäøÁöÑÂÖ®Èù¢ÂõûÈ°æÔºåÂ∏ÆÂä©Êñ∞Êâã‰∫ÜËß£Âü∫Á°ÄÊ®°ÂûãÂíåËØÑ‰º∞ÊåáÊ†á„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖËÆ®ËÆ∫‰∫ÜËØ•È¢ÜÂüüÈù¢‰∏¥ÁöÑÊåëÊàòÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18904",
            "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
            "url": "https://huggingface.co/papers/2504.18904",
            "abstract": "Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.",
            "score": 7,
            "issue_id": 3525,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 –∞–ø—Ä–µ–ª—è",
                "en": "April 26",
                "zh": "4Êúà26Êó•"
            },
            "hash": "c724dcb5ceb5df7b",
            "authors": [
                "Haoran Geng",
                "Feishi Wang",
                "Songlin Wei",
                "Yuyang Li",
                "Bangjun Wang",
                "Boshi An",
                "Charlie Tianyue Cheng",
                "Haozhe Lou",
                "Peihao Li",
                "Yen-Jen Wang",
                "Yutong Liang",
                "Dylan Goetting",
                "Chaoyi Xu",
                "Haozhe Chen",
                "Yuxi Qian",
                "Yiran Geng",
                "Jiageng Mao",
                "Weikang Wan",
                "Mingtong Zhang",
                "Jiangran Lyu",
                "Siheng Zhao",
                "Jiazhao Zhang",
                "Jialiang Zhang",
                "Chengyang Zhao",
                "Haoran Lu",
                "Yufei Ding",
                "Ran Gong",
                "Yuran Wang",
                "Yuxuan Kuang",
                "Ruihai Wu",
                "Baoxiong Jia",
                "Carlo Sferrazza",
                "Hao Dong",
                "Siyuan Huang",
                "Yue Wang",
                "Jitendra Malik",
                "Pieter Abbeel"
            ],
            "affiliations": [
                "BIGAI",
                "CMU",
                "PKU",
                "Stanford",
                "UC Berkeley",
                "UCLA",
                "UIUC",
                "UMich",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18904.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "RoboVerse: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤",
                    "desc": "RoboVerse - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –≤–∫–ª—é—á–∞—é—â–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—É—é —Å—Ä–µ–¥—É, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–≤ –∏ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ä–µ–¥–∞–º–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ñ–∏–∑–∏–∫–∏ –∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º. RoboVerse –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –æ–±–æ–±—â–µ–Ω–∏—è."
                },
                "en": {
                    "title": "RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks",
                    "desc": "This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field."
                },
                "zh": {
                    "title": "RoboVerseÔºöÊé®Âä®Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÂº∫Â§ßÊ°ÜÊû∂",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜRoboVerseÔºåËøôÊòØ‰∏Ä‰∏™ÁªºÂêàÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫È¢ÜÂüüÊï∞ÊçÆÊî∂ÈõÜÂíåËØÑ‰º∞ÁöÑÊåëÊàò„ÄÇRoboVerseÂåÖÊã¨‰∏Ä‰∏™Ê®°ÊãüÂπ≥Âè∞„ÄÅ‰∏Ä‰∏™ÂêàÊàêÊï∞ÊçÆÈõÜÂíåÁªü‰∏ÄÁöÑÂü∫ÂáÜÊµãËØïÔºåÊîØÊåÅÂ§öÁßçÊ®°ÊãüÂô®ÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÇÈÄöËøáÈ´ò‰øùÁúüÁâ©ÁêÜÂíåÈÄºÁúüÁöÑÊ∏≤ÊüìÔºåÂêàÊàêÊï∞ÊçÆÈõÜÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÁöÑÊï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRoboVerseÊòæËëóÊèêÂçá‰∫ÜÊ®°‰ªøÂ≠¶‰π†„ÄÅÂº∫ÂåñÂ≠¶‰π†Âíå‰ªéÊ®°ÊãüÂà∞Áé∞ÂÆûÁöÑËΩ¨ÁßªÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÁöÑÂèØÈù†ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21039",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
            "url": "https://huggingface.co/papers/2504.21039",
            "abstract": "As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",
            "score": 5,
            "issue_id": 3528,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 –∞–ø—Ä–µ–ª—è",
                "en": "April 28",
                "zh": "4Êúà28Êó•"
            },
            "hash": "a66fbdd0c4cc7250",
            "authors": [
                "Paul Kassianik",
                "Baturay Saglam",
                "Alexander Chen",
                "Blaine Nelson",
                "Anu Vellore",
                "Massimo Aufiero",
                "Fraser Burch",
                "Dhruv Kedia",
                "Avi Zohary",
                "Sajana Weerawardhena",
                "Aman Priyanshu",
                "Adam Swanda",
                "Amy Chang",
                "Hyrum Anderson",
                "Kojin Oshiba",
                "Omar Santos",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "affiliations": [
                "Foundation AI Cisco Systems Inc.",
                "Security & Trust Organization Cisco Systems Inc.",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21039.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#security"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Foundation-Sec-8B - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Llama 3.1 –∏ –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. Foundation-Sec-8B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å Llama 3.1-70B –∏ GPT-4o-mini –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø—É–±–ª–∏–∫—É—é—Ç –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –ò–ò-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—Ñ–µ—Ä–µ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Foundation-Sec-8B",
                    "desc": "This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors."
                },
                "zh": {
                    "title": "Êé®Âä®ÁΩëÁªúÂÆâÂÖ®ÁöÑAIÂ∑•ÂÖ∑ËøõÊ≠•",
                    "desc": "ÈöèÁùÄÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Á§æ‰ºö‰∏≠ÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåÂÆÉ‰ª¨Âú®ËΩØ‰ª∂Â∑•Á®ã„ÄÅÂàõÊÑèÂÜô‰ΩúÂíåÊï∞Â≠óËâ∫ÊúØÁ≠âÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÈù©ÂëΩÊÄßÁöÑÂèòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πè‰∏ì‰∏öÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåË°®Á§∫ÁΩëÁªúÂÆâÂÖ®ÁâπÂÆöÁü•ËØÜÁöÑÂ§çÊùÇÊÄßÔºåÂÆÉ‰ª¨Âú®ÁΩëÁªúÂÆâÂÖ®È¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂ÊúâÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFoundation-Sec-8BÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÁΩëÁªúÂÆâÂÖ®ÁöÑLLMÔºåÂü∫‰∫éLlama 3.1Êû∂ÊûÑÔºåÂπ∂ÈÄöËøáÂú®Á≤æÂøÉÁ≠ñÂàíÁöÑÁΩëÁªúÂÆâÂÖ®ËØ≠ÊñôÂ∫ì‰∏äËøõË°åÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊù•Â¢ûÂº∫„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™ÁΩëÁªúÂÆâÂÖ®Âü∫ÂáÜÊµãËØï‰∏≠ËØÑ‰º∞‰∫ÜFoundation-Sec-8BÔºåÁªìÊûúÊòæÁ§∫ÂÆÉÂú®Êüê‰∫õÁΩëÁªúÂÆâÂÖ®ÁâπÂÆö‰ªªÂä°‰∏ä‰∏éLlama 3.1-70BÂíåGPT-4o-miniÁõ∏ÂåπÈÖç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21336",
            "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation",
            "url": "https://huggingface.co/papers/2504.21336",
            "abstract": "Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.",
            "score": 2,
            "issue_id": 3532,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 –∞–ø—Ä–µ–ª—è",
                "en": "April 30",
                "zh": "4Êúà30Êó•"
            },
            "hash": "b4a0872fb3eb8547",
            "authors": [
                "Linshan Wu",
                "Yuxiang Nie",
                "Sunan He",
                "Jiaxin Zhuang",
                "Hao Chen"
            ],
            "affiliations": [
                "Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
                "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
                "Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China",
                "Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China",
                "State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21336.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#healthcare",
                    "#interpretability",
                    "#agi"
                ],
                "emoji": "üß¨",
                "ru": {
                    "title": "UniBiomed: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "UniBiomed - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 27 –º–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ 10 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. UniBiomed –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Ä–µ–≥–∏–æ–Ω–∞–º, –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤. –ú–æ–¥–µ–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏."
                },
                "en": {
                    "title": "UniBiomed: Revolutionizing Biomedical Image Interpretation",
                    "desc": "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."
                },
                "zh": {
                    "title": "UniBiomedÔºöÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Â§öÊ®°ÊÄÅÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÁöÑËß£Èáä‰∏∫ÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÂºÄËæü‰∫ÜÊñ∞ÁöÑÊú∫‰ºö„ÄÇ‰º†ÁªüÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂàÜÁ¶ªÁöÑËÆ≠ÁªÉÔºåÂØºËá¥Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Áº∫‰πèÁÅµÊ¥ªÊÄßÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®Êï¥‰ΩìÁîüÁâ©ÂåªÂ≠¶‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜUniBiomedÔºåËøôÊòØÈ¶ñ‰∏™Áî®‰∫éÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèËß£ÈáäÁöÑÈÄöÁî®Âü∫Á°ÄÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÂàÜÂâ≤Ê®°ÂûãÔºåËÉΩÂ§üÁªü‰∏ÄÁîüÊàê‰∏¥Â∫äÊñáÊú¨ÂíåÁõ∏Â∫îÁîüÁâ©ÂåªÂ≠¶ÂØπË±°ÁöÑÂàÜÂâ≤„ÄÇUniBiomedÂú®Â§ö‰∏™ÁîüÁâ©ÂåªÂ≠¶‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËØäÊñ≠ÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19043",
            "title": "Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning",
            "url": "https://huggingface.co/papers/2504.19043",
            "abstract": "Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.",
            "score": 2,
            "issue_id": 3538,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 –∞–ø—Ä–µ–ª—è",
                "en": "April 26",
                "zh": "4Êúà26Êó•"
            },
            "hash": "c7a7b2771c1e5c1f",
            "authors": [
                "Connor T. Jerzak",
                "Priyanshi Chandra",
                "Rishi Hazra"
            ],
            "affiliations": [
                "Department of Government, University of Texas at Austin",
                "Department of Statistics, Harvard College",
                "Faculty of Informatics, Universit√† della Svizzera Italiana"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19043.jpg",
            "data": {
                "categories": [],
                "emoji": "üó≥Ô∏è",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é, —Ç–∞–∫ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–µ–∂–¥—É –¥–≤—É–º—è –ø–∞—Ä—Ç–∏—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–µ—Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º."
                },
                "en": {
                    "title": "Optimizing Political Candidate Selection through Adversarial Conjoint Analysis",
                    "desc": "This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods."
                },
                "zh": {
                    "title": "ÂØπÊäóÊÄßÂä®ÊÄÅÊèêÂçáËÅîÂêàÂàÜÊûêÁöÑÊ¥ûÂØüÂäõ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜËÅîÂêàÂàÜÊûêÂú®ÊîøÊ≤ªÂÄôÈÄâ‰∫∫ÈÄâÊã©‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂ¶Ç‰ΩïËØÜÂà´ÊúÄ‰Ω≥ÂÄôÈÄâ‰∫∫ÁâπÂæÅÁªÑÂêà„ÄÇÁî±‰∫éÁâπÂæÅÁªÑÂêàÁöÑÊï∞ÈáèËøúË∂ÖËßÇÂØüÊ†∑Êú¨ÔºåÊó†Ê≥ïÁ≤æÁ°ÆÁ°ÆÂÆöÊúÄ‰Ω≥ÈÖçÁΩÆ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúÄ‰ºòÈöèÊú∫Âπ≤È¢ÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÊ¶ÇÁéáÂàÜÂ∏ÉÂÆûÁé∞ÊúÄÊúâÂà©ÁöÑÂπ≥ÂùáÁªìÊûú„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ÂØπÊäóÊÄßÁéØÂ¢É‰∏≠ÔºåÊâÄÂª∫ËÆÆÁöÑÁ≠ñÁï•Êõ¥ÂèØËÉΩ‰∏éÂÆûÈôÖÂÄôÈÄâ‰∫∫ÂåπÈÖçÔºåÊè≠Á§∫‰∫ÜÂØπÊäóÂä®ÊÄÅÂú®Á§æ‰ºöÁßëÂ≠¶ÂÆûÈ™åÊï∞ÊçÆÂàÜÊûê‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-30.html",
    "link_next": "2025-05-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4Êúà30Êó•"
    },
    "short_date_next": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5Êúà2Êó•"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 2,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÈòøÊãâ‰ºØÊñáÊú¨Ê∑ªÂä†ÂèëÈü≥Á¨¶Âè∑ÁöÑÊåëÊàò„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SadeedÁöÑÊñ∞ÊñπÊ≥ïÔºåÂü∫‰∫é‰∏Ä‰∏™ÁªèËøáÂæÆË∞ÉÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°Âûã„ÄÇSadeedÂú®Á≤æÂøÉÁºñÂà∂ÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºåÂπ∂Âú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÂèñÂæó‰∫ÜÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÊñáÁ´†Ëøò‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËØÑÊµãÂü∫ÂáÜSadeedDiac-25Ôºå‰ª•Ëß£ÂÜ≥ÂΩìÂâçËØÑÊµãÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇËøô‰∫õÂ∑•ÂÖ∑ÂÖ±ÂêåÊé®Âä®‰∫ÜÈòøÊãâ‰ºØËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫îÁî®ÁöÑÂèëÂ±ï„ÄÇ",
        "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
        "pinyin": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÈòøÊãâ‰ºØÊñáÊú¨Ê∑ªÂä†ÂèëÈü≥Á¨¶Âè∑ÁöÑÊåëÊàò„ÄÇ\nZh√® piƒÅn w√©nzhƒÅng t«éol√πn le ƒÄlƒÅb√≥ w√©nbƒõn tiƒÅnjiƒÅ fƒÅyƒ´n f√∫h√†o de ti«éozh√†n.\n\n‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SadeedÁöÑÊñ∞ÊñπÊ≥ïÔºå\nZu√≤zhƒõ t√≠ch≈´ le yƒ´zh«íng m√≠ngw√®i Sadeed de xƒ´n fƒÅngf«é,\n\nÂü∫‰∫é‰∏Ä‰∏™ÁªèËøáÂæÆË∞ÉÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°Âûã„ÄÇ\nJƒ´y√∫ yƒ´g√® jƒ´nggu√≤ wƒìiti√°o de jiƒõm«éq√¨ y«îy√°n m√≥x√≠ng.\n\nSadeedÂú®Á≤æÂøÉÁºñÂà∂ÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºå\nSadeed z√†i jƒ´ngxƒ´n biƒÅnzh√¨ de gƒÅo zh√¨li√†ng sh√πj√πj√≠ sh√†ng j√¨nx√≠ng wƒìiti√°o,\n\nÂπ∂Âú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÂèñÂæó‰∫ÜÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇ\nB√¨ng z√†i y«íuxi√†n de j√¨su√†n zƒ´yu√°n xi√† q«îd√© le j√¨ngzhƒìngl√¨ de ji√©gu«í.\n\nÊ≠§Â§ñÔºåÊñáÁ´†Ëøò‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËØÑÊµãÂü∫ÂáÜSadeedDiac-25Ôºå\nC«êw√†i, w√©nzhƒÅng h√°i ji√®sh√†o le yƒ´g√® xƒ´n de p√≠ngc√® jƒ´zh«în SadeedDiac-25,\n\n‰ª•Ëß£ÂÜ≥ÂΩìÂâçËØÑÊµãÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ\nY«ê jiƒõju√© dƒÅngqi√°n p√≠ngc√® fƒÅngf«é de j√∫xi√†nx√¨ng.\n\nËøô‰∫õÂ∑•ÂÖ∑ÂÖ±ÂêåÊé®Âä®‰∫ÜÈòøÊãâ‰ºØËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫îÁî®ÁöÑÂèëÂ±ï„ÄÇ\nZh√®xiƒì g≈çngj√π g√≤ngt√≥ng tuƒ´d√≤ng le ƒÄlƒÅb√≥y«î z√¨r√°n y«îy√°n ch«îl«ê y√¨ngy√≤ng de fƒÅzh«én.",
        "vocab": "[\n    {\"word\": \"ËÆ®ËÆ∫\", \"pinyin\": \"t«éo l√πn\", \"trans\": \"discuss\"},\n    {\"word\": \"ÈòøÊãâ‰ºØ\", \"pinyin\": \"ƒÅ lƒÅ b√≥\", \"trans\": \"Arabic\"},\n    {\"word\": \"ÊñáÊú¨\", \"pinyin\": \"w√©n bƒõn\", \"trans\": \"text\"},\n    {\"word\": \"Ê∑ªÂä†\", \"pinyin\": \"tiƒÅn jiƒÅ\", \"trans\": \"add\"},\n    {\"word\": \"ÂèëÈü≥\", \"pinyin\": \"fƒÅ yƒ´n\", \"trans\": \"pronunciation\"},\n    {\"word\": \"Á¨¶Âè∑\", \"pinyin\": \"f√∫ h√†o\", \"trans\": \"symbol\"},\n    {\"word\": \"ÊåëÊàò\", \"pinyin\": \"ti«éo zh√†n\", \"trans\": \"challenge\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"Âêç‰∏∫\", \"pinyin\": \"m√≠ng w√©i\", \"trans\": \"named\"},\n    {\"word\": \"Âü∫‰∫é\", \"pinyin\": \"jƒ´ y√∫\", \"trans\": \"based on\"},\n    {\"word\": \"ÂæÆË∞É\", \"pinyin\": \"wƒìi ti√°o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"Ëß£Á†ÅÂô®\", \"pinyin\": \"jiƒõ m«é q√¨\", \"trans\": \"decoder\"},\n    {\"word\": \"ËØ≠Ë®Ä\", \"pinyin\": \"y«î y√°n\", \"trans\": \"language\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"Á≤æÂøÉ\", \"pinyin\": \"jƒ´ng xƒ´n\", \"trans\": \"carefully\"},\n    {\"word\": \"ÁºñÂà∂\", \"pinyin\": \"biƒÅn zh√¨\", \"trans\": \"compile\"},\n    {\"word\": \"È´òË¥®Èáè\", \"pinyin\": \"gƒÅo zh√¨ li√†ng\", \"trans\": \"high quality\"},\n    {\"word\": \"Êï∞ÊçÆÈõÜ\", \"pinyin\": \"sh√π j√π j√≠\", \"trans\": \"dataset\"},\n    {\"word\": \"ËøõË°å\", \"pinyin\": \"j√¨n x√≠ng\", \"trans\": \"conduct\"},\n    {\"word\": \"ÊúâÈôê\", \"pinyin\": \"y«íu xi√†n\", \"trans\": \"limited\"},\n    {\"word\": \"ËÆ°ÁÆó\", \"pinyin\": \"j√¨ su√†n\", \"trans\": \"computational\"},\n    {\"word\": \"ËµÑÊ∫ê\", \"pinyin\": \"zƒ´ yu√°n\", \"trans\": \"resources\"},\n    {\"word\": \"ÂèñÂæó\", \"pinyin\": \"q«î d√©\", \"trans\": \"achieve\"},\n    {\"word\": \"Á´û‰∫âÂäõ\", \"pinyin\": \"j√¨ng zhƒìng l√¨\", \"trans\": \"competitive\"},\n    {\"word\": \"ÁªìÊûú\", \"pinyin\": \"ji√© gu«í\", \"trans\": \"results\"},\n    {\"word\": \"Ê≠§Â§ñ\", \"pinyin\": \"c«ê w√†i\", \"trans\": \"moreover\"},\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"ËØÑÊµã\", \"pinyin\": \"p√≠ng c√®\", \"trans\": \"evaluation\"},\n    {\"word\": \"Âü∫ÂáÜ\", \"pinyin\": \"jƒ´ zh«în\", \"trans\": \"benchmark\"},\n    {\"word\": \"Ëß£ÂÜ≥\", \"pinyin\": \"jiƒõ ju√©\", \"trans\": \"address\"},\n    {\"word\": \"ÂΩìÂâç\", \"pinyin\": \"dƒÅng qi√°n\", \"trans\": \"current\"},\n    {\"word\": \"Â±ÄÈôêÊÄß\", \"pinyin\": \"j√∫ xi√†n x√¨ng\", \"trans\": \"limitations\"},\n    {\"word\": \"Ëøô‰∫õ\", \"pinyin\": \"zh√® xiƒì\", \"trans\": \"these\"},\n    {\"word\": \"Â∑•ÂÖ∑\", \"pinyin\": \"g≈çng j√π\", \"trans\": \"tools\"},\n    {\"word\": \"ÂÖ±Âêå\", \"pinyin\": \"g√≤ng t√≥ng\", \"trans\": \"jointly\"},\n    {\"word\": \"Êé®Âä®\", \"pinyin\": \"tuƒ´ d√≤ng\", \"trans\": \"promote\"},\n    {\"word\": \"Ëá™ÁÑ∂\", \"pinyin\": \"z√¨ r√°n\", \"trans\": \"natural\"},\n    {\"word\": \"Â§ÑÁêÜ\", \"pinyin\": \"ch«î l«ê\", \"trans\": \"processing\"},\n    {\"word\": \"Â∫îÁî®\", \"pinyin\": \"y√¨ng y√≤ng\", \"trans\": \"application\"},\n    {\"word\": \"ÂèëÂ±ï\", \"pinyin\": \"fƒÅ zh«én\", \"trans\": \"development\"}\n]",
        "trans": "This article discusses the challenges of adding diacritical marks to Arabic texts. The authors propose a new method called Sadeed, based on a fine-tuned decoder language model. Sadeed is fine-tuned on a meticulously curated high-quality dataset and achieves competitive results with limited computational resources. Additionally, the article introduces a new evaluation benchmark, SadeedDiac-25, to address the limitations of current evaluation methods. These tools collectively advance the development of Arabic natural language processing applications.",
        "update_ts": "2025-05-01 09:12"
    }
}