{
    "date": {
        "ru": "12 ноября",
        "en": "November 12",
        "zh": "11月12日"
    },
    "time_utc": "2024-11-12 02:40",
    "weekday": 1,
    "issue_id": 520,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.06272",
            "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
            "url": "https://huggingface.co/papers/2411.06272",
            "abstract": "As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.",
            "score": 1,
            "issue_id": 520,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 ноября",
                "en": "November 9",
                "zh": "11月9日"
            },
            "hash": "2559c023f673c9b4",
            "data": {
                "categories": [
                    "#low_resource",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "Эталон для оценки финансовых языковых моделей",
                    "desc": "В статье обсуждается необходимость создания стандартизированного метода оценки производительности больших языковых моделей (LLM) в финансовом секторе. Авторы предлагают \"Golden Touchstone\", первый двуязычный эталон для финансовых LLM, который включает в себя наборы данных на китайском и английском языках для восьми основных финансовых задач NLP. Этот эталон позволяет более полно оценивать способности моделей в понимании и генерации финансовой информации. Исследование также включает открытый доступ к коду и весам модели Touchstone-GPT, что способствует дальнейшему развитию и оптимизации финансовых LLM."
                },
                "en": {
                    "title": "Golden Touchstone: Elevating Financial LLM Evaluation",
                    "desc": "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."
                },
                "zh": {
                    "title": "金融领域的标准化评估工具——金色基准",
                    "desc": "随着大型语言模型在金融领域的广泛应用，评估其性能的标准化方法变得尤为重要。现有的金融基准测试存在语言和任务覆盖面有限、数据集质量低以及适应性不足等问题。为了解决这些问题，我们提出了“金色基准”，这是第一个全面的双语金融基准，涵盖了中英文的八个核心金融自然语言处理任务。通过对主要模型的比较分析，我们揭示了它们在处理复杂金融信息时的优缺点，并开源了Touchstone-GPT模型，以促进未来的研究和优化。"
                }
            }
        }
    ],
    "link_prev": "2024-11-11.html",
    "link_next": "2024-11-13.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11月11日"
    },
    "short_date_next": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11月13日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。\n\nzhè piān wén zhāng jiè shào le StdGEN, yī zhǒng chuàng xīn de cóng dān zhāng tú xiàng shēng chéng gāo zhì liàng 3D jué sè de fāng fǎ. tā néng zài sān fēn zhōng nèi shēng chéng jù yǒu fēn lì de yǔ yì zǔ jìn (rú shēn tǐ, yī fú hé tóu fà) de xiáng xì 3D jué sè. StdGEN de hé xīn shì tí chū de yǔ yì gǎn jué dà xíng chóng jiàn mó xíng (S-LRM), néng gòu cóng duō shì jì tú xiàng zhōng chóng jiàn jǐ hé, yán sè hé yǔ yì. shí yàn zhèng míng, StdGEN zài 3D dòng màn jué sè shēng chéng fāng miàn biǎo xiǎn chū sè, yōu yú xiàn yǒu fāng fǎ. tā wèi xū nǐ xiàn shí, yóu xì hé diàn yǐng zhì zuò děng ti gōng gěi le línghuó de dìng zhì huà 3D jué sè.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sītīdī jīn\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovative\"},\n    {\"word\": \"角色\", \"pinyin\": \"juésè\", \"trans\": \"character\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔjiàn\", \"trans\": \"component\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐhé\", \"trans\": \"geometry\"},\n    {\"word\": \"重建\", \"pinyin\": \"chóngjiàn\", \"trans\": \"reconstruct\"},\n    {\"word\": \"多视图\", \"pinyin\": \"duōshìtú\", \"trans\": \"multi-view\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"灵活\", \"pinyin\": \"línghuó\", \"trans\": \"flexible\"},\n    {\"word\": \"定制化\", \"pinyin\": \"dìngzhìhuà\", \"trans\": \"customized\"},\n    {\"word\": \"虚拟现实\", \"pinyin\": \"xūnǐ xiànshí\", \"trans\": \"virtual reality\"},\n    {\"word\": \"游戏\", \"pinyin\": \"yóuxì\", \"trans\": \"game\"},\n    {\"word\": \"电影制作\", \"pinyin\": \"diànyǐng zhìzuò\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    }
}