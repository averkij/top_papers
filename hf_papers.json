{
    "date": {
        "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 9",
        "zh": "4æœˆ9æ—¥"
    },
    "time_utc": "2025-04-09 15:14",
    "weekday": 2,
    "issue_id": 3150,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.06263",
            "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
            "url": "https://huggingface.co/papers/2504.06263",
            "abstract": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.",
            "score": 68,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "3b3365aa60717b2a",
            "authors": [
                "Yiying Yang",
                "Wei Cheng",
                "Sijin Chen",
                "Xianfang Zeng",
                "Jiaxu Zhang",
                "Liao Wang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06263.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OmniSVG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "OmniSVG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ SVG Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ SVG Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMSVG-2M Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… SVG-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OmniSVG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° SVG."
                },
                "en": {
                    "title": "OmniSVG: Revolutionizing SVG Generation with Vision-Language Models",
                    "desc": "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."
                },
                "zh": {
                    "title": "OmniSVGï¼šé«˜æ•ˆç”Ÿæˆå¤æ‚SVGçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºOmniSVGçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡å’Œå¤æ‚çš„å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡å°†SVGå‘½ä»¤å’Œåæ ‡å‚æ•°åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€SVGç”Ÿæˆã€‚OmniSVGå°†ç»“æ„é€»è¾‘ä¸ä½çº§å‡ ä½•è§£è€¦ï¼Œä»è€Œåœ¨ä¿æŒå¤æ‚SVGç»“æ„è¡¨ç°åŠ›çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†MMSVG-2Mæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤ç™¾ä¸‡ä¸ªä¸°å¯Œæ³¨é‡Šçš„SVGèµ„äº§ï¼Œä»¥æ¨åŠ¨SVGåˆæˆçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05599",
            "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
            "url": "https://huggingface.co/papers/2504.05599",
            "abstract": "We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.",
            "score": 49,
            "issue_id": 3142,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "b963d5098e669229",
            "authors": [
                "Yi Peng",
                "Chris",
                "Xiaokun Wang",
                "Yichen Wei",
                "Jiangbo Pei",
                "Weijie Qiu",
                "Ai Jian",
                "Yunzhuo Hao",
                "Jiachun Pan",
                "Tianyidan Xie",
                "Li Ge",
                "Rongxian Zhuang",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05599.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#transfer_learning",
                    "#benchmark",
                    "#inference",
                    "#architecture",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Skywork R1V - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµÑ€Ğ¸Ğ¸ R1 Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Skywork R1V Ñ 38 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Seamless Multimodal Reasoning with Skywork R1V",
                    "desc": "Skywork R1V is a new multimodal reasoning model that enhances the capabilities of existing R1-series large language models by integrating visual data. It uses a lightweight visual projector to adapt to visual inputs without needing to retrain the foundational language model or the vision encoder. The model employs a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning and Group Relative Policy Optimization to improve the alignment between text and visual information. Additionally, it features an adaptive-length Chain-of-Thought distillation method that optimizes reasoning processes, leading to efficient inference and strong performance on various benchmarks."
                },
                "zh": {
                    "title": "Skywork R1Vï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹",
                    "desc": "Skywork R1Væ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œå®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•æ‰©å±•äº†R1ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ°è§†è§‰æ¨¡æ€ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è½»é‡çº§è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°äº†æ— é¡»é‡æ–°è®­ç»ƒåŸºç¡€è¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„æ— ç¼å¤šæ¨¡æ€é€‚åº”ã€‚ä¸ºäº†å¢å¼ºè§†è§‰ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œç»“åˆäº†è¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€é›†æˆçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾è’¸é¦æ–¹æ³•ï¼ŒåŠ¨æ€ä¼˜åŒ–æ¨ç†é“¾çš„é•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ï¼Œé¿å…è¿‡åº¦æ€è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06261",
            "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
            "url": "https://huggingface.co/papers/2504.06261",
            "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
            "score": 48,
            "issue_id": 3141,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "354599744af26b06",
            "authors": [
                "Gleb Rodionov",
                "Roman Garipov",
                "Alina Shutova",
                "George Yakushev",
                "Vage Egiazarian",
                "Anton Sinitsin",
                "Denis Kuznedelev",
                "Dan Alistarh"
            ],
            "affiliations": [
                "HSE University",
                "IST Austria",
                "Yandex"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06261.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰ĞµĞµÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Hogwild! Inference, Ğ³Ğ´Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ LLM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºÑÑˆ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ, ĞºĞ°Ğº Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸Ğ¼ĞµÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Rotary Position Embeddings Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Collaborative Parallelism for Enhanced LLM Efficiency",
                    "desc": "This paper explores a new method for improving the efficiency of Large Language Models (LLMs) during complex tasks by enabling them to work in parallel. The authors introduce a system called Hogwild! Inference, where multiple LLM instances share an attention cache, allowing them to see each other's progress and collaborate effectively. This approach leverages Rotary Position Embeddings (RoPE) to enhance performance without the need for extra fine-tuning. The findings suggest that LLMs can autonomously develop collaboration strategies, leading to faster and more efficient problem-solving."
                },
                "zh": {
                    "title": "å¹¶è¡Œåˆä½œï¼Œæå‡LLMæ¨ç†æ•ˆç‡",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡é«˜çº§æ¨ç†ã€é•¿ç¯‡å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ¥å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›ä»»åŠ¡ï¼Œç ”ç©¶è¡¨æ˜LLMså¯ä»¥é€šè¿‡å®ç°æ˜ç¡®çš„åˆä½œæ¡†æ¶æ¥å¹¶è¡Œæ“ä½œï¼Œä¾‹å¦‚æŠ•ç¥¨æœºåˆ¶æˆ–ç‹¬ç«‹å­ä»»åŠ¡çš„åˆ›å»ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼šè®©LLMâ€œå·¥ä½œè€…â€å¹¶è¡Œè¿è¡Œï¼Œé€šè¿‡åŒæ—¶æ›´æ–°çš„æ³¨æ„åŠ›ç¼“å­˜è¿›è¡ŒåŒæ­¥ï¼Œå¹¶å†³å®šæœ€ä½³çš„åˆä½œæ–¹å¼ã€‚æˆ‘ä»¬å®ç°äº†Hogwild!æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå¤šä¸ªç›¸åŒçš„LLMå®ä¾‹åœ¨å…±äº«çš„æ³¨æ„åŠ›ç¼“å­˜ä¸­å¹¶è¡Œè¿è¡Œï¼Œèƒ½å¤Ÿâ€œå³æ—¶â€è®¿é—®å½¼æ­¤ç”Ÿæˆçš„æ ‡è®°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05979",
            "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
            "url": "https://huggingface.co/papers/2504.05979",
            "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
            "score": 43,
            "issue_id": 3137,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "f1195a87ec5b86f1",
            "authors": [
                "Sixiang Chen",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Tian Ye",
                "Qingyu Shi",
                "Donghao Zhou",
                "Wenhao Chai",
                "Xin Lin",
                "Jianzong Wu",
                "Chao Tang",
                "Shilin Xu",
                "Tao Zhang",
                "Haobo Yuan",
                "Yikang Zhou",
                "Wei Chow",
                "Linfeng Li",
                "Xiangtai Li",
                "Lei Zhu",
                "Lu Qi"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Washington",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05979.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ GPT-4o. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ GPT-4o Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-3D Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-X. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ GPT-4o Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unifying Image and Text Generation with GPT-4o",
                    "desc": "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."
                },
                "zh": {
                    "title": "æ¢ç´¢ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœªæ¥æ–¹å‘",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«ã€‚åˆ†æç»“æœæ­ç¤ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å°†å…¶ç½®äºç”Ÿæˆå»ºæ¨¡çš„æ›´å¹¿æ³›æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœªæ¥ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®æ‰©å±•çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05535",
            "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values",
            "url": "https://huggingface.co/papers/2504.05535",
            "abstract": "Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.",
            "score": 30,
            "issue_id": 3145,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "420fafe139f00d54",
            "authors": [
                "M-A-P Team",
                "Siwei Wu",
                "Jincheng Ren",
                "Xinrun Du",
                "Shuyue Guo",
                "Xingwei Qu",
                "Yiming Liang",
                "Jie Liu",
                "Yunwen Li",
                "Tianyu Zheng",
                "Boyu Feng",
                "Huaqing Yuan",
                "Zenith Wang",
                "Jiaheng Liu",
                "Wenhao Huang",
                "Chenglin Cai",
                "Haoran Que",
                "Jian Yang",
                "Yuelin Bai",
                "Zekun Moore Wang",
                "Zhouliang Yu",
                "Qunshu Lin",
                "Ding Pan",
                "Yuchen Jiang",
                "Tiannan Wang",
                "Wangchunshu Zhou",
                "Shenzhi Wang",
                "Xingyuan Bu",
                "Minghao Liu",
                "Guoyin Wang",
                "Ge Zhang",
                "Chenghua Lin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.05535.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#data",
                    "#rlhf",
                    "#open_source"
                ],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COIG-P - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM, ÑĞ¾Ğ±Ñ€Ğ°Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² 6 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ COIG-P Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (CRM) Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ COIG-P Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 2-12%."
                },
                "en": {
                    "title": "Revolutionizing Chinese Preference Datasets with LLMs",
                    "desc": "This paper presents a novel approach to creating a large-scale Chinese preference dataset, COIG-P, which addresses the limitations of existing datasets. By utilizing 15 mainstream large language models (LLMs) to generate and score response pairs, the authors eliminate the need for human annotators, enhancing scalability. The dataset includes 1,009k preference pairs across six diverse domains, significantly improving performance metrics for various models. Additionally, a Chinese Reward Model (CRM) is developed to efficiently score responses, demonstrating strong performance in identifying low-quality samples compared to existing benchmarks."
                },
                "zh": {
                    "title": "æ„å»ºé«˜è´¨é‡ä¸­æ–‡åå¥½æ•°æ®é›†çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— äººå·¥å¹²é¢„çš„ä¸­æ–‡åå¥½æ•°æ®é›†æ³¨é‡Šç®¡é“ï¼Œä»¥è§£å†³ç°æœ‰ä¸­æ–‡åå¥½æ•°æ®é›†è§„æ¨¡å°ã€é¢†åŸŸç‹­çª„å’Œç¼ºä¹ä¸¥æ ¼éªŒè¯çš„é—®é¢˜ã€‚æˆ‘ä»¬æ”¶é›†å¹¶ç­›é€‰äº†92,000ä¸ªé«˜è´¨é‡ä¸­æ–‡æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨15ä¸ªä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œè¯„åˆ†é€‰æ‹©-æ‹’ç»çš„å“åº”å¯¹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†COIG-Pæ•°æ®é›†ï¼ŒåŒ…å«1,009,000ä¸ªä¸­æ–‡åå¥½å¯¹ï¼Œè¦†ç›–èŠå¤©ã€ä»£ç ã€æ•°å­¦ã€é€»è¾‘ã€å°è¯´å’Œè§’è‰²ç­‰å…­ä¸ªå¤šæ ·åŒ–é¢†åŸŸã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ª8Bè§„æ¨¡çš„ä¸­æ–‡å¥–åŠ±æ¨¡å‹ï¼ˆCRMï¼‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸­æ–‡å¥–åŠ±åŸºå‡†ï¼ˆCRBenchï¼‰ï¼Œå¹¶åœ¨è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02160",
            "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
            "url": "https://huggingface.co/papers/2504.02160",
            "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
            "score": 22,
            "issue_id": 3139,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "511e3ea71050e14e",
            "authors": [
                "Shaojin Wu",
                "Mengqi Huang",
                "Wenxu Wu",
                "Yufeng Cheng",
                "Fei Ding",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02160.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#synthetic",
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UNO Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Image Generation with Consistent Data Synthesis",
                    "desc": "This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks."
                },
                "zh": {
                    "title": "é«˜ä¸€è‡´æ€§å¤šä¸»é¢˜ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œå¦‚ä½•è§£å†³ä»¥ä¸»é¢˜ä¸ºé©±åŠ¨çš„ç”Ÿæˆé¢ä¸´çš„æ•°æ®å¯æ‰©å±•æ€§å’Œä¸»é¢˜æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜ä¸€è‡´æ€§çš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨çš„å†…åœ¨ç”Ÿæˆèƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¸€è‡´æ€§çš„å¤šä¸»é¢˜é…å¯¹æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†UNOæ¨¡å‹ï¼Œç»“åˆäº†æ¸è¿›çš„è·¨æ¨¡æ€å¯¹é½å’Œé€šç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸»é¢˜å’Œå¤šä¸»é¢˜ç”Ÿæˆä¸­ä¿æŒé«˜ä¸€è‡´æ€§å’Œå¯æ§æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œæ§åˆ¶èƒ½åŠ›ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02810",
            "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2504.02810",
            "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
            "score": 10,
            "issue_id": 3138,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "5a9b5f817a1c09b6",
            "authors": [
                "Haowei Lin",
                "Xiangyu Wang",
                "Ruilin Yan",
                "Baizhou Huang",
                "Haotian Ye",
                "Jianhua Zhu",
                "Zihao Wang",
                "James Zou",
                "Jianzhu Ma",
                "Yitao Liang"
            ],
            "affiliations": [
                "Computer Science Department, Stanford University, California, United States",
                "Department of Electronic Engineering, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Peking University, Beijing, China",
                "Wangxuan institute of computer technology, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02810.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "KUMO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ KUMO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. KUMO Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ LLM Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 23 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° 5000 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "KUMO: Unveiling True Reasoning in LLMs",
                    "desc": "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."
                },
                "zh": {
                    "title": "KUMOï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å·¥å…·",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…ä»…ä»å…¶åºå¤§çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†KUMOï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMsæ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOç»“åˆäº†LLMså’Œç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ï¼Œä¿ƒè¿›æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹23ä¸ªæœ€å…ˆè¿›çš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè®¸å¤šæ¨¡å‹åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¤§å­¦ç”Ÿçš„è¡¨ç°ï¼Œè€Œåœ¨å¤æ‚æ¨ç†æŒ‘æˆ˜ä¸­ä¹Ÿè¾¾åˆ°äº†å¤§å­¦æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05594",
            "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
            "url": "https://huggingface.co/papers/2504.05594",
            "abstract": "Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.",
            "score": 9,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "7da2f86ad5e0bfc2",
            "authors": [
                "Qi Mao",
                "Lan Chen",
                "Yuchao Gu",
                "Mike Zheng Shou",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "State Key Laboratory of Media Convergence and Communication, Communication University of China",
                "University of California at Merced",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05594.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifyEdit - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit",
                    "desc": "This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks."
                },
                "zh": {
                    "title": "å¹³è¡¡ä¿çœŸåº¦ä¸å¯ç¼–è¾‘æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘ä¸­ï¼Œå¹³è¡¡ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§éå¸¸é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿æŒç»“æ„ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æœºåˆ¶æ¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†UnifyEditï¼Œè¿™æ˜¯ä¸€ç§æ— è°ƒä¼˜çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ½œåœ¨ä¼˜åŒ–å®ç°ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§çš„å¹³è¡¡ã€‚æˆ‘ä»¬å¼€å‘äº†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çº¦æŸï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥è°ƒåº¦å™¨ï¼Œä»¥åŠ¨æ€è°ƒæ•´è¿™äº›çº¦æŸçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06148",
            "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2504.06148",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.",
            "score": 7,
            "issue_id": 3144,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "7d7e4a2197e26482",
            "authors": [
                "Xiangxi Zheng",
                "Linjie Li",
                "Zhengyuan Yang",
                "Ping Yu",
                "Alex Jinpeng Wang",
                "Rui Yan",
                "Yuan Yao",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Central South University, China",
                "Microsoft Corporation, USA",
                "Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06148.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "V-MAGE: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ V-MAGE. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 30 ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, ĞºĞ°Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in MLLMs with V-MAGE",
                    "desc": "This paper presents a new evaluation framework called Visual-centric Multiple Abilities Game Evaluation (V-MAGE) for assessing the visual reasoning skills of Multimodal Large Language Models (MLLMs). Unlike traditional benchmarks, V-MAGE includes dynamic, game-based tasks that require models to demonstrate core visual skills and higher-level reasoning abilities. The framework consists of five games with over 30 levels, focusing on tasks like positioning and visual memory. The evaluation reveals significant performance gaps between MLLMs and humans, highlighting the models' perceptual errors and suggesting areas for improvement in their reasoning strategies."
                },
                "zh": {
                    "title": "è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–°è¯„ä¼°æ¡†æ¶",
                    "desc": "æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•Œçš„åŠ¨æ€ç¯å¢ƒï¼Œç°æœ‰çš„åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•æ˜¾å¾—ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹è§†è§‰ä¸­å¿ƒçš„ä»»åŠ¡ï¼Œæ— æ³•è¯„ä¼°çœŸå®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å¤šæ ·æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰ä¸­å¿ƒå¤šèƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsè§†è§‰æ¨ç†èƒ½åŠ›çš„æ¸¸æˆè¯„ä¼°æ¡†æ¶ã€‚V-MAGEåŒ…å«äº”ä¸ªå¤šæ ·åŒ–çš„æ¸¸æˆå’Œ30å¤šä¸ªæ‰‹å·¥è®¾è®¡çš„å…³å¡ï¼Œæµ‹è¯•æ¨¡å‹åœ¨å®šä½ã€è½¨è¿¹è·Ÿè¸ªã€æ—¶æœºæŠŠæ¡å’Œè§†è§‰è®°å¿†ç­‰æ ¸å¿ƒè§†è§‰æŠ€èƒ½ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠé•¿æœŸè§„åˆ’å’Œæ·±æ€ç†Ÿè™‘ç­‰æ›´é«˜å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06232",
            "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance",
            "url": "https://huggingface.co/papers/2504.06232",
            "abstract": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.",
            "score": 6,
            "issue_id": 3145,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "7def3ddddf039685",
            "authors": [
                "Jiazi Bu",
                "Pengyang Ling",
                "Yujie Zhou",
                "Pan Zhang",
                "Tong Wu",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06232.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "HiFlow: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "HiFlow - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ capture Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. HiFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ HiFlow Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking High-Resolution Image Synthesis with HiFlow",
                    "desc": "This paper introduces HiFlow, a novel framework designed to enhance the capabilities of text-to-image (T2I) diffusion and flow models for generating high-resolution images. HiFlow operates without the need for additional training and is compatible with existing pre-trained flow models. It utilizes a virtual reference flow to align low-resolution and high-resolution data, ensuring consistency in low-frequency details, structural integrity, and fine details. The results show that HiFlow significantly improves the quality of high-resolution images compared to current leading methods, demonstrating its effectiveness and adaptability across various T2I models."
                },
                "zh": {
                    "title": "HiFlowï¼šæå‡é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHiFlowçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æŒ‘æˆ˜ã€‚HiFlowä¸éœ€è¦è®­ç»ƒï¼Œå¹¶ä¸”ä¸æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçš„æµæ¨¡å‹ã€‚å®ƒé€šè¿‡å»ºç«‹è™šæ‹Ÿå‚è€ƒæµï¼Œæ•æ‰ä½åˆ†è¾¨ç‡æµä¿¡æ¯çš„ç‰¹å¾ï¼Œä»è€ŒæŒ‡å¯¼é«˜åˆ†è¾¨ç‡ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiFlowåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒè´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00043",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
            "url": "https://huggingface.co/papers/2504.00043",
            "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
            "score": 6,
            "issue_id": 3137,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "2b2bfdd590c5394d",
            "authors": [
                "Jixuan Leng",
                "Chengsong Huang",
                "Langlin Huang",
                "Bill Yuchen Lin",
                "William W. Cohen",
                "Haohan Wang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "CMU",
                "UIUC",
                "UW",
                "WUSTL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00043.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞšÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "CrossWordBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ ÑĞµÑ‚ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ LLM Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ LVLM Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ° ÑĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles",
                    "desc": "This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†ï¼šCrossWordBench",
                    "desc": "ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ¨ç†è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æ¨ç†æˆ–è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ä¸Šï¼Œç¼ºä¹æ–‡æœ¬ä¸è§†è§‰ä¹‹é—´çš„åŠ¨æ€äº’åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossWordBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¡«å­—æ¸¸æˆè¯„ä¼°LLMså’ŒLVLMsæ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œè¦æ±‚åœ¨æ–‡æœ¬çº¿ç´¢å’Œè§†è§‰ç½‘æ ¼ç»“æ„çš„è¯­ä¹‰çº¦æŸä¸‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚CrossWordBenchåˆ©ç”¨å¯æ§çš„æ‹¼å›¾ç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆå¤šç§æ ¼å¼çš„æ‹¼å›¾ï¼Œå¹¶æä¾›ä»ç›´æ¥è§£è°œåˆ°äº’åŠ¨æ¨¡å¼çš„ä¸åŒè¯„ä¼°ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMsåœ¨åˆ©ç”¨äº¤å‰å­—æ¯çº¦æŸæ–¹é¢æ˜¾è‘—ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œè€ŒLVLMsåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå…¶è§£è°œè¡¨ç°ä¸ç½‘æ ¼è§£æå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20533",
            "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence",
            "url": "https://huggingface.co/papers/2503.20533",
            "abstract": "Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.",
            "score": 6,
            "issue_id": 3144,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "3b237389882b1344",
            "authors": [
                "Yijiong Yu"
            ],
            "affiliations": [
                "OpenCSG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20533.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑˆĞ°Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ²Ğ´Ğ²Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Accelerating Reasoning with Parallel Token Decoding",
                    "desc": "This paper presents a novel approach to enhance the efficiency of reasoning models in machine learning, particularly for complex tasks like mathematical reasoning. The authors focus on reducing the computational cost associated with generating lengthy reasoning sequences by utilizing parallel processing techniques. By implementing a specialized attention mask, they enable the model to decode multiple tokens simultaneously, which significantly speeds up the reasoning process without increasing memory usage. Experimental results indicate that this method can achieve more than 100% improvement in decoding speed while preserving the quality of the answers."
                },
                "zh": {
                    "title": "åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œæå‡æ•ˆç‡ä¸è´¨é‡",
                    "desc": "æœ€è¿‘çš„æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ï¼Œä¸»è¦å¾—ç›Šäºè¯¦ç»†å’Œå…¨é¢çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µä¸”è€—æ—¶çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å­˜åœ¨å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“é—¨çš„æ³¨æ„åŠ›æ©ç åœ¨æ¯ä¸€æ­¥è§£ç å¤šä¸ªæ ‡è®°ï¼Œä»è€Œåœ¨å•ä¸ªåºåˆ—ä¸­å¤„ç†å®ƒä»¬ï¼Œé¿å…äº†é¢å¤–çš„å†…å­˜ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05520",
            "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
            "url": "https://huggingface.co/papers/2504.05520",
            "abstract": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.",
            "score": 4,
            "issue_id": 3150,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "474ad1b587ab6240",
            "authors": [
                "Taiwei Shi",
                "Yiyang Wu",
                "Linxin Song",
                "Tianyi Zhou",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05520.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06122",
            "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
            "url": "https://huggingface.co/papers/2504.06122",
            "abstract": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.",
            "score": 3,
            "issue_id": 3147,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "a180ec0989dfd0c8",
            "authors": [
                "Jingyuan Zhang",
                "Qi Wang",
                "Xingguang Ji",
                "Yahui Liu",
                "Yang Yue",
                "Fuzheng Zhang",
                "Di Zhang",
                "Guorui Zhou",
                "Kun Gai"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06122.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼: Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ (ĞĞ”Ğ¢) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ¿Ğ°Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ-Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ° Lean 4. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞĞ”Ğ¢, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Automated Theorem Proving with Human-like Reasoning",
                    "desc": "This paper explores the enhancement of automated theorem proving (ATP) using large language models (LLMs) and Lean 4 code. The authors propose a continual training approach that combines a hybrid dataset of statement-proof pairs with cognitive behavior data to mimic human reasoning. They also implement reinforcement learning, utilizing feedback from the Lean 4 compiler to refine the models further. As a result, they report significant improvements in existing ATP systems, achieving a notable 59.8% pass rate on the MiniF2F benchmark."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨å®šç†è¯æ˜çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ä¸Lean 4ä»£ç çš„å½¢å¼æ¨ç†ç›¸å…³çš„è¿›å±•ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆæ•°æ®é›†å¯¹ç°æœ‰ATPæ¨¡å‹è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œæ•°æ®é›†ä¸­åŒ…å«å¤§é‡çš„å‘½é¢˜-è¯æ˜å¯¹ï¼Œå¹¶åŠ å…¥æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå‡è®¾ä¿®æ­£çš„è®¤çŸ¥è¡Œä¸ºæ•°æ®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ©ç”¨Lean 4ç¼–è¯‘å™¨è¿”å›çš„ç»“æœå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬æˆåŠŸæå‡äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼Œå¦‚DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œåœ¨æ•´ä½“è¯æ˜ç”Ÿæˆé¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-08.html",
    "link_next": "2025-04-10.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Skywork R1Vï¼Œä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•ï¼Œå°† R1-series å¤§è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è§†è§‰æ¨¡æ€ã€‚Skywork R1V ä½¿ç”¨è½»é‡çº§çš„è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒè¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„å¤šæ¨¡æ€é€‚åº”ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥å’Œè‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾æç‚¼æ–¹æ³•ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkywork R1V åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²å…¬å¼€ã€‚",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Skywork R1Vï¼Œä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•ï¼Œå°† R1-series å¤§è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è§†è§‰æ¨¡æ€ã€‚Skywork R1V ä½¿ç”¨è½»é‡çº§çš„è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒè¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„å¤šæ¨¡æ€é€‚åº”ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥å’Œè‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾æç‚¼æ–¹æ³•ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkywork R1V åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²å…¬å¼€ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le Skywork R1V, yÄ«zhÇ’ng duÅ mÃ³shuÃ i tuÄ«lÇ mÃ³xÃ­ng. TÄ tÅngguÃ² gÄoxiÃ o de duÅ mÃ³shuÃ i zhuÇnwÃ©i fÄngfÇ, jiÄng R1-series dÃ  yÇ”yÃ¡n mÃ³xÃ­ng kuÃ²zhÇn dÃ o shÃ¬juÃ© mÃ³shuÃ i. Skywork R1V shÇyÃ²ng qÄ«ngliÃ ngjÃ­ de shÃ¬juÃ© tÃ³ujÄ«ngqÃ¬, shÃ­xiÃ n wÃºxÅ« chÃ³ngxÄ«n xÃ¹nliÃ n yÇ”yÃ¡n mÃ³xÃ­ng huÃ² shÃ¬juÃ© biÄnmÇqÃ¬ de duÅ mÃ³shuÃ i shÃ¬yÃ¬ng. WÃ©nzhÄng hÃ¡i tÃ­chÅ« le yÄ«zhÇ’ng hÃ¹nhÃ© yÅuhuÃ  cÃ¨lÃ¼Ã¨ hÃ© zÃ¬ shÃ¬yÃ¬ng chÃ¡ngdÃ¹ de sÄ«wÃ©i liÃ n tÃ­xiÃ ng fÄngfÇ, tÃ­gÄo tuÄ«lÇ xiÃ oyÃ¬ng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, Skywork R1V zÃ i duÅ gÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, bÃ¬ngqiÄ› mÃ³xÃ­ng quÃ¡nzhÃ²ng yÇ gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"è½¬ç§»\", \"pinyin\": \"zhuÇn yÃ­\", \"trans\": \"transfer\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ² zhÇn\", \"trans\": \"extend\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"æŠ•å½±å™¨\", \"pinyin\": \"tÃ³u yÇng qÃ¬\", \"trans\": \"projector\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é‡æ–°\", \"pinyin\": \"chÃ³ng xÄ«n\", \"trans\": \"re-\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"ç¼–ç å™¨\", \"pinyin\": \"biÄn mÇ qÃ¬\", \"trans\": \"encoder\"},\n    {\"word\": \"é€‚åº”\", \"pinyin\": \"shÃ¬ yÃ¬ng\", \"trans\": \"adapt\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"è‡ªé€‚åº”\", \"pinyin\": \"zÃ¬ shÃ¬ yÃ¬ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"é•¿åº¦\", \"pinyin\": \"chÃ¡ng dÃ¹\", \"trans\": \"length\"},\n    {\"word\": \"æ€ç»´é“¾\", \"pinyin\": \"sÄ« wÃ©i liÃ¡n\", \"trans\": \"chain of thought\"},\n    {\"word\": \"æç‚¼\", \"pinyin\": \"tÃ­ liÃ n\", \"trans\": \"extract\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ o lÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨ shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"æƒé‡\", \"pinyin\": \"quÃ¡n zhÃ²ng\", \"trans\": \"weights\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}