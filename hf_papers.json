{
    "date": {
        "ru": "24 сентября",
        "en": "September 24",
        "zh": "9月24日"
    },
    "time_utc": "2025-09-24 16:14",
    "weekday": 2,
    "issue_id": 6066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.18174",
            "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
            "url": "https://huggingface.co/papers/2509.18174",
            "abstract": "Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic.",
            "score": 79,
            "issue_id": 6056,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "e92f83a2d009cb0f",
            "authors": [
                "Khalil Hennara",
                "Muhammad Hreden",
                "Mohamed Motasim Hamed",
                "Ahmad Bastati",
                "Zeina Aldallal",
                "Sara Chrouf",
                "Safwan AlModhayan"
            ],
            "affiliations": [
                "Misraj.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18174.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#cv",
                    "#low_resource",
                    "#multimodal",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "📜",
                "ru": {
                    "title": "Прорыв в распознавании арабских текстов: Baseer устанавливает новый стандарт OCR",
                    "desc": "Baseer - это модель машинного зрения и обработки естественного языка, специально настроенная для оптического распознавания арабских документов. Она использует стратегию обучения только декодера и крупномасштабный набор данных, что позволяет достичь наилучших результатов в данной области. Baseer значительно превосходит существующие решения, достигая показателя ошибки распознавания слов (WER) 0,25. Эксперименты демонстрируют преимущества адаптации мультимодальных языковых моделей общего назначения для конкретных задач, особенно для морфологически богатых языков, таких как арабский."
                },
                "en": {
                    "title": "Baseer: Revolutionizing Arabic Document OCR with State-of-the-Art Performance",
                    "desc": "Baseer is a vision-language model specifically designed for Optical Character Recognition (OCR) of Arabic documents. It utilizes a decoder-only fine-tuning strategy on a large-scale dataset that includes both synthetic and real-world documents. This model achieves a remarkable Word Error Rate (WER) of 0.25, surpassing existing OCR solutions for Arabic. Additionally, the introduction of the Misraj-DocOCR benchmark provides a robust framework for evaluating the performance of Arabic OCR systems."
                },
                "zh": {
                    "title": "Baseer：阿拉伯文档OCR的新标杆",
                    "desc": "Baseer是一种专门为阿拉伯文档OCR（光学字符识别）微调的视觉语言模型。它采用解码器策略，并利用大规模数据集进行训练，显著提高了阿拉伯文档的识别准确率，达到了0.25的字错误率（WER）。该模型在处理阿拉伯语的连写、不同字体和右到左的书写方向等挑战时表现出色。我们的研究表明，针对特定领域的微调可以显著提升通用多模态大语言模型在阿拉伯文OCR任务中的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19249",
            "title": "Reinforcement Learning on Pre-Training Data",
            "url": "https://huggingface.co/papers/2509.19249",
            "abstract": "Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
            "score": 39,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "7ee5ca9be200b064",
            "authors": [
                "Siheng Li",
                "Kejiao Li",
                "Zenan Xu",
                "Guanhua Huang",
                "Evander Yang",
                "Kun Li",
                "Haoyuan Wu",
                "Jiajia Wu",
                "Zihao Zheng",
                "Chenchen Zhang",
                "Kun Shi",
                "Kyrierl Deng",
                "Qi Yi",
                "Ruibin Xiong",
                "Tingqiang Xu",
                "Yuhao Jiang",
                "Jianfeng Yan",
                "Yuyuan Zeng",
                "Guanghui Xu",
                "Jinbao Xue",
                "Zhijiang Xu",
                "Zheng Fang",
                "Shuai Li",
                "Qibin Liu",
                "Xiaoxue Li",
                "Zhuoyu Li",
                "Yangyu Tao",
                "Fei Gao",
                "Cheng Jiang",
                "Bo Chao Wang",
                "Kai Liu",
                "Jianchen Zhu",
                "Wai Lam",
                "Wayyt Wang",
                "Bo Zhou",
                "Di Wang"
            ],
            "affiliations": [
                "HunYuan Infra Team",
                "LLM Department, Tencent",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19249.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RLPT: Усиление языковых моделей через самообучение на предобученных данных",
                    "desc": "Метод обучения с подкреплением на предварительно обученных данных (RLPT) оптимизирует большие языковые модели, позволяя им автономно исследовать значимые траектории в предобученных данных. RLPT использует цель рассуждения о следующем сегменте, вознаграждая модель за точное предсказание последующих текстовых сегментов на основе предыдущего контекста. Этот подход позволяет масштабировать обучение с подкреплением на предобученных данных, способствуя развитию более обобщаемых навыков рассуждения. Эксперименты показывают значительное улучшение производительности моделей на различных тестах, включая математические рассуждения."
                },
                "en": {
                    "title": "Autonomous Learning for Enhanced Reasoning in Language Models",
                    "desc": "Reinforcement Learning on Pre-Training data (RLPT) is a novel approach that enhances large language models (LLMs) by allowing them to learn from pre-training data without needing human annotations. It uses reinforcement learning to autonomously explore meaningful data trajectories, which helps improve the model's reasoning abilities. Unlike traditional methods that rely on supervised learning, RLPT derives reward signals directly from the data, focusing on predicting subsequent text segments based on prior context. This method not only boosts performance on various reasoning tasks but also shows promise for scaling with increased computational resources."
                },
                "zh": {
                    "title": "自主探索，提升推理能力的强化学习新方法",
                    "desc": "强化学习在预训练数据上的应用（RLPT）通过自主探索预训练数据中的有意义轨迹，优化大型语言模型，提升其通用推理能力，而无需人工标注。与传统的监督学习方法不同，RLPT允许策略从预训练数据中学习，并通过强化学习提高能力。该方法通过预测后续文本段落来构建奖励信号，鼓励在更广泛的上下文中探索更丰富的轨迹。实验结果表明，RLPT在多个模型上显著提升了推理性能，展示了其在大型语言模型优化中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18644",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "url": "https://huggingface.co/papers/2509.18644",
            "abstract": "A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "score": 39,
            "issue_id": 6056,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "9beb129b0b2f10ec",
            "authors": [
                "Juntu Zhao",
                "Wenbo Lu",
                "Di Zhang",
                "Yufeng Liu",
                "Yushen Liang",
                "Tianluo Zhang",
                "Yifeng Cao",
                "Junyuan Xie",
                "Yingdong Hu",
                "Shengjie Wang",
                "Junliang Guo",
                "Dequan Wang",
                "Yang Gao"
            ],
            "affiliations": [
                "New York University Shanghai",
                "Shanghai Jiao Tong University",
                "Spirit AI",
                "Tongji University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18644.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Зрение вместо состояния: новый подход к обучению роботов-манипуляторов",
                    "desc": "Исследование показывает, что политика без использования состояния, основанная только на визуальных наблюдениях, превосходит политики на основе состояния в задачах манипуляции роботов. Такой подход обеспечивает лучшую пространственную генерализацию и эффективность использования данных. Политика без состояния использует только визуальные наблюдения с двух широкоугольных камер на запястье робота для предсказания действий. Эксперименты демонстрируют значительное улучшение успешности выполнения различных задач манипуляции в реальном мире при обобщении по высоте и в горизонтальной плоскости."
                },
                "en": {
                    "title": "Visual-Only Policies: Better Generalization in Robot Manipulation!",
                    "desc": "This paper introduces a State-free Policy for robot manipulation that relies solely on visual observations instead of combining them with proprioceptive states. The authors argue that traditional state-based policies can lead to overfitting and poor generalization across different spatial contexts. By focusing on visual inputs, the State-free Policy demonstrates improved spatial generalization and data efficiency in various tasks, such as pick-and-place and shirt-folding. Empirical results show significant improvements in success rates for height and horizontal generalization, making this approach more effective for real-world applications."
                },
                "zh": {
                    "title": "无状态策略：提升机器人操作的空间泛化能力",
                    "desc": "本研究提出了一种无状态策略，仅依赖视觉观察来进行机器人操作任务。与传统的基于状态的策略相比，这种方法在空间泛化能力和数据效率上表现更佳。通过去除对本体状态输入的依赖，避免了过拟合训练轨迹的问题。实验结果显示，无状态策略在多种实际任务中显著提高了成功率，证明了其在真实世界应用中的实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18154",
            "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
            "url": "https://huggingface.co/papers/2509.18154",
            "abstract": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
            "score": 29,
            "issue_id": 6052,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "e46263baf17f8869",
            "authors": [
                "Tianyu Yu",
                "Zefan Wang",
                "Chongyi Wang",
                "Fuwei Huang",
                "Wenshuo Ma",
                "Zhihui He",
                "Tianchi Cai",
                "Weize Chen",
                "Yuxiang Huang",
                "Yuanqian Zhao",
                "Bokai Xu",
                "Junbo Cui",
                "Yingjing Xu",
                "Liqing Ruan",
                "Luoyuan Zhang",
                "Hanyu Liu",
                "Jingkun Tang",
                "Hongyuan Liu",
                "Qining Guo",
                "Wenhao Hu",
                "Bingxiang He",
                "Jie Zhou",
                "Jie Cai",
                "Ji Qi",
                "Zonghao Guo",
                "Chi Chen",
                "Guoyang Zeng",
                "Yuxuan Li",
                "Ganqu Cui",
                "Ning Ding",
                "Xu Han",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "MiniCPM-V Team, OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18154.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "MiniCPM-V 4.5: Компактность и эффективность в мультимодальных ИИ-моделях",
                    "desc": "MiniCPM-V 4.5 - это мультимодальная большая языковая модель с 8 миллиардами параметров, которая достигает высокой производительности и эффективности. Модель использует унифицированную архитектуру 3D-Resampler, единую парадигму обучения и гибридную стратегию обучения с подкреплением. MiniCPM-V 4.5 превосходит более крупные модели, такие как GPT-4 и Qwen2.5-VL 72B, при значительно меньших затратах памяти и времени вывода. Модель демонстрирует передовые результаты на бенчмарке VideoMME среди моделей до 30 миллиардов параметров."
                },
                "en": {
                    "title": "Efficiency Meets Performance in Multimodal AI",
                    "desc": "MiniCPM-V 4.5 is an advanced multimodal large language model with 8 billion parameters, designed to enhance both performance and efficiency. It utilizes a novel 3D-Resampler architecture that allows for compact encoding of images and videos, streamlining the processing of multimodal data. The model also incorporates a unified learning paradigm that simplifies document knowledge and text recognition, reducing the need for extensive data engineering. Additionally, a hybrid reinforcement learning strategy enables the model to excel in both short and long reasoning tasks, achieving superior results while using significantly less computational resources compared to larger models."
                },
                "zh": {
                    "title": "高效多模态语言模型的未来",
                    "desc": "MiniCPM-V 4.5 是一个拥有 80 亿参数的多模态大型语言模型，采用统一的 3D-重采样架构，旨在提高效率和性能。该模型通过统一学习范式和混合强化学习策略，解决了多模态模型在训练和推理中的效率瓶颈。实验结果表明，MiniCPM-V 4.5 在 OpenCompass 评估中超越了许多知名模型，展现出卓越的性能和效率。尤其是在 VideoMME 基准测试中，该模型在 30B 以下的模型中实现了最先进的性能，显著降低了 GPU 内存和推理时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18849",
            "title": "MAPO: Mixed Advantage Policy Optimization",
            "url": "https://huggingface.co/papers/2509.18849",
            "abstract": "Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
            "score": 16,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "9416ec88d3b85956",
            "authors": [
                "Wenke Huang",
                "Quan Zhang",
                "Yiyang Fang",
                "Jian Liang",
                "Xuankun Rong",
                "Huanjin Yao",
                "Guancheng Wan",
                "Ke Liang",
                "Wenwen He",
                "Mingjun Li",
                "Leszek Rutkowski",
                "Mang Ye",
                "Bo Du",
                "Dacheng Tao"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Defense Technology",
                "The AGH University of Krakow",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Адаптивная оптимизация преимущества для улучшения обучения с подкреплением",
                    "desc": "MAPO - это новая стратегия обучения с подкреплением для фундаментальных моделей. Она динамически перевзвешивает функцию преимущества для улучшения ранжирования траекторий. MAPO решает проблемы реверсии и зеркальности преимущества, возникающие в существующих подходах. Метод адаптивно настраивает функцию преимущества с учетом особенностей каждого образца, что повышает эффективность обучения."
                },
                "en": {
                    "title": "Dynamic Advantage Reweighting for Enhanced Trajectory Ranking",
                    "desc": "Mixed Advantage Policy Optimization (MAPO) enhances reinforcement learning by dynamically adjusting the advantage function to better rank trajectories in foundation models. It addresses issues like advantage reversion and advantage mirror problems that affect how advantages are distributed among different query samples. By introducing the concept of advantage percent deviation, MAPO focuses on samples with high-certainty trajectories, allowing for a more tailored advantage allocation. The effectiveness of MAPO is demonstrated through comparisons with existing methods and detailed ablation studies."
                },
                "zh": {
                    "title": "动态调整优势，提升强化学习效果",
                    "desc": "混合优势策略优化（MAPO）是一种在强化学习中动态调整优势函数的方法，旨在改善基础模型的轨迹排名。该方法解决了现有技术中存在的优势反转和优势镜像问题，从而实现更合理的优势分配。MAPO通过引入高确定性轨迹的优势百分比偏差，来适应不同样本的特性。实验结果表明，MAPO在与其他先进方法的比较中表现出色，验证了其有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18824",
            "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2509.18824",
            "abstract": "Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.",
            "score": 16,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "6de809558bad8c90",
            "authors": [
                "Yanzuo Lu",
                "Xin Xia",
                "Manlin Zhang",
                "Huafeng Kuang",
                "Jianbin Zheng",
                "Yuxi Ren",
                "Xuefeng Xiao"
            ],
            "affiliations": [
                "ByteDance AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Hyper-Bagel: Сверхбыстрое мультимодальное ИИ без потери качества",
                    "desc": "Статья представляет Hyper-Bagel - унифицированную систему для ускорения мультимодальных задач понимания и генерации контента. Она использует спекулятивное декодирование для предсказания следующих токенов и многоэтапную дистилляцию для шумоподавления диффузии. Hyper-Bagel достигает значительного ускорения: более чем в 2 раза для задач понимания и до 22 раз для задач генерации. Разработанная 1-NFE модель обеспечивает практически мгновенное редактирование и генерацию мультимодального контента."
                },
                "en": {
                    "title": "Hyper-Bagel: Speeding Up Multimodal Tasks with Smart Techniques",
                    "desc": "Hyper-Bagel is a new framework that speeds up tasks involving multiple types of data, like text and images, by using advanced techniques. It combines speculative decoding, which predicts the next piece of data quickly, with a multi-stage distillation process to reduce noise in the data. This approach allows for more than double the speed in understanding multimodal content and significantly faster generation of images from text. The framework also includes a model that can edit and generate content in real-time, making it efficient and responsive while maintaining high-quality results."
                },
                "zh": {
                    "title": "Hyper-Bagel：加速多模态任务的创新框架",
                    "desc": "Hyper-Bagel 是一个加速多模态理解和生成任务的框架，采用了推测解码和多阶段蒸馏的方法。它通过分而治之的策略，显著减少了计算开销，同时保持了高质量的输出。该框架在多模态理解任务中实现了超过2倍的加速，而在生成任务中，文本到图像生成的速度提升达到了16.67倍，图像编辑的速度提升达到了22倍。通过结合对抗蒸馏和人类反馈学习，Hyper-Bagel 使得复杂的多模态交互变得无缝且即时。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19297",
            "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
            "url": "https://huggingface.co/papers/2509.19297",
            "abstract": "VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
            "score": 13,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "708db1d702c58d65",
            "authors": [
                "Weijie Wang",
                "Yeqing Chen",
                "Zeyu Zhang",
                "Hengyu Liu",
                "Haoxiao Wang",
                "Zhiyuan Feng",
                "Wenkang Qin",
                "Zheng Zhu",
                "Donny Y. Chen",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "GigaAI",
                "Monash University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19297.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "VolSplat: революция в синтезе новых ракурсов через воксельное предсказание гауссианов",
                    "desc": "VolSplat - это новый метод синтеза новых ракурсов, использующий воксельно-выровненное предсказание гауссианов вместо попиксельного. Он преодолевает ограничения пиксельного выравнивания, улучшая качество 3D-реконструкции и обеспечивая более надежную согласованность между ракурсами. VolSplat позволяет адаптивно контролировать плотность гауссианов на основе сложности 3D-сцены. Эксперименты показывают, что метод достигает современного уровня производительности, создавая более правдоподобные и согласованные гауссовы реконструкции."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Voxel-Aligned Gaussians",
                    "desc": "VolSplat is a novel method for synthesizing new views in 3D reconstruction by using voxel-aligned Gaussian predictions instead of the traditional pixel-aligned approach. This new technique addresses limitations such as dependency on input views, view-biased density distributions, and alignment errors caused by occlusions or low texture in source views. By predicting Gaussians directly from a 3D voxel grid, VolSplat enhances multi-view consistency and adapts Gaussian density based on scene complexity. Experiments show that it outperforms existing methods, providing more accurate and consistent 3D reconstructions, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "体素对齐，重塑3D重建的未来",
                    "desc": "VolSplat是一种基于体素对齐的高斯预测方法，旨在改善新视角合成，克服像素对齐的局限性，并提升3D重建质量。传统方法依赖于像素对齐的高斯预测，这导致重建的3D模型对输入视图数量高度依赖，并且在视图偏差和遮挡情况下容易出现对齐错误。VolSplat通过直接从预测的3D体素网格中预测高斯，避免了对错误易感的2D特征匹配，从而确保了多视图的一致性。实验结果表明，VolSplat在多个基准测试中表现出色，提供了更真实和一致的高斯重建。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19296",
            "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
            "url": "https://huggingface.co/papers/2509.19296",
            "abstract": "A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
            "score": 9,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "763d3ecf06625fdf",
            "authors": [
                "Sherwin Bahmani",
                "Tianchang Shen",
                "Jiawei Ren",
                "Jiahui Huang",
                "Yifeng Jiang",
                "Haithem Turki",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Zan Gojcic",
                "Sanja Fidler",
                "Huan Ling",
                "Jun Gao",
                "Xuanchi Ren"
            ],
            "affiliations": [
                "NVIDIA",
                "Simon Fraser University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19296.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#games",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "3D-миры из текста и изображений: новый подход к генерации виртуальных сред",
                    "desc": "Статья представляет фреймворк самодистилляции, который преобразует неявные 3D-знания из моделей видеодиффузии в явное 3D-представление с использованием метода Gaussian Splatting. Этот подход позволяет генерировать трехмерные сцены на основе текста или изображений без необходимости в многоракурсных обучающих данных. Модель включает в себя RGB-декодер и 3DGS-декодер, который обучается на синтетических данных, созданных моделями видеодиффузии. Фреймворк также поддерживает генерацию динамических 3D-сцен из монокулярного видео."
                },
                "en": {
                    "title": "Transforming 2D Imagination into 3D Reality",
                    "desc": "This paper introduces a self-distillation framework that transforms implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation. This method allows for the generation of 3D scenes from text or images without needing extensive multi-view training data. By enhancing the RGB decoder with a 3DGS decoder, the framework can be trained solely on synthetic data produced by video diffusion models. The results demonstrate superior performance in generating both static and dynamic 3D scenes, making it valuable for applications in gaming and robotics."
                },
                "zh": {
                    "title": "自蒸馏框架：从视频生成3D场景的创新方法",
                    "desc": "本文提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识转化为显式的3D高斯点云表示。这种方法使得从文本或图像生成3D场景成为可能，避免了对多视角训练数据的依赖。我们通过增强典型的RGB解码器，加入3D高斯点云解码器，并利用RGB解码器的输出进行监督训练。实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19284",
            "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
            "url": "https://huggingface.co/papers/2509.19284",
            "abstract": "Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
            "score": 9,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "4e8ddb8ed978283e",
            "authors": [
                "Yunzhen Feng",
                "Julia Kempe",
                "Cheng Zhang",
                "Parag Jain",
                "Anthony Hartshorn"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19284.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Качество, а не количество: ключ к эффективным цепочкам рассуждений в ИИ",
                    "desc": "Исследование показывает, что эффективность цепочек рассуждений (Chain-of-Thought, CoT) в крупных моделях рассуждений (Large Reasoning Models, LRM) определяется не их длиной или степенью пересмотра, а меньшим количеством неудачных шагов и лучшим структурным качеством. Введен новый показатель - доля неудачных шагов (Failed-Step Fraction, FSF), который лучше предсказывает корректность рассуждений, чем длина CoT. Эксперименты с ранжированием и редактированием CoT подтверждают, что удаление неудачных ветвей улучшает точность. Результаты указывают на важность структурного подхода к масштабированию CoT вместо простого увеличения их длины."
                },
                "en": {
                    "title": "Less Failure, More Structure: The Key to Effective Reasoning in AI",
                    "desc": "This paper investigates what makes chain-of-thought (CoT) reasoning effective in large reasoning models (LRMs). It challenges the idea that longer CoTs are always better, showing that both longer CoTs and increased review can lead to lower accuracy. The authors introduce a new metric called the Failed-Step Fraction (FSF), which measures the proportion of steps in abandoned reasoning paths, and find it to be a better predictor of correctness than length or review. Their findings suggest that effective CoTs are characterized by fewer failures and emphasize the importance of structure in reasoning processes."
                },
                "zh": {
                    "title": "有效思维链：减少失败步骤，提升推理质量",
                    "desc": "这篇论文探讨了大型推理模型（LRMs）中有效的思维链（CoT）的特征。研究发现，思维链的有效性与失败步骤的数量和结构质量有关，而不是简单的长度或复审次数。通过对十个LRMs进行系统评估，结果表明，简单地延长思维链或增加复审会导致准确率降低。论文提出了一种新的图形视角来提取思维链的结构，并引入了一个统计量——失败步骤比例（FSF），该比例能够更好地预测模型的正确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13835",
            "title": "Large Language Models Discriminate Against Speakers of German Dialects",
            "url": "https://huggingface.co/papers/2509.13835",
            "abstract": "Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  \t\t\t\t\tAI-generated summary \t\t\t\t Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.",
            "score": 5,
            "issue_id": 6057,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "b9343322b02e9713",
            "authors": [
                "Minh Duc Bui",
                "Carolin Holtermann",
                "Valentin Hofmann",
                "Anne Lauscher",
                "Katharina von der Wense"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Data Science Group, University of Hamburg, Germany",
                "Johannes Gutenberg University Mainz, Germany",
                "University of Colorado Boulder, USA",
                "University of Washington, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13835.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Языковые модели воспроизводят стереотипы о диалектах",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) демонстрируют значительное предвзятое отношение к носителям немецких диалектов. Это проявляется в негативных ассоциациях и предвзятых решениях моделей при упоминании диалектов или их использовании. Интересно, что явное указание лингвистической демографии усиливает предвзятость сильнее, чем неявные признаки вроде использования диалекта. Результаты подчеркивают необходимость учитывать и корректировать лингвистические предубеждения в LLM."
                },
                "en": {
                    "title": "Unmasking Bias: Language Models and German Dialects",
                    "desc": "This paper investigates the biases present in large language models (LLMs) against speakers of German dialects. It highlights that these models not only associate negative traits with dialect speakers but also exhibit biased decision-making when processing dialect-related content. The research utilizes a novel evaluation corpus to measure dialect naming and usage biases through specific tasks. The findings reveal that explicitly labeling dialect speakers increases bias, contradicting previous studies that suggested minimal bias with demographic mentions."
                },
                "zh": {
                    "title": "大型语言模型对德语方言使用者的偏见研究",
                    "desc": "这篇论文研究了大型语言模型（LLMs）对德语方言使用者的偏见，发现这些模型在方言命名和使用上存在显著的偏见。研究表明，方言使用者常常面临负面的社会刻板印象，而这些刻板印象在LLMs中得到了反映。通过关联任务和决策任务，作者评估了模型的方言命名偏见和使用偏见，并构建了一个新的评估语料库。结果显示，明确标记语言人口统计信息会加剧偏见，反映出对德语方言使用者的负面联想。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17321",
            "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
            "url": "https://huggingface.co/papers/2509.17321",
            "abstract": "OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  \t\t\t\t\tAI-generated summary \t\t\t\t Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at github.com/budzianowski/opengvl{OpenGVL}.",
            "score": 3,
            "issue_id": 6056,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "3469469f2dba37f5",
            "authors": [
                "Paweł Budzianowski",
                "Emilia Wiśnios",
                "Gracjan Góral",
                "Igor Kulakov",
                "Viktor Petrenko",
                "Krzysztof Walas"
            ],
            "affiliations": [
                "IDEAS Research Institute",
                "Poznan University of Technology",
                "Simple Automation",
                "University of Warsaw"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17321.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#data",
                    "#robotics",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "OpenGVL: Новый стандарт для оценки прогресса в робототехнике",
                    "desc": "OpenGVL - это эталонный тест для прогнозирования прогресса задач в робототехнике с использованием моделей машинного обучения, сочетающих зрение и язык. Исследование показывает, что открытые модели уступают в производительности закрытым аналогам. OpenGVL позволяет автоматизировать курирование больших наборов данных в робототехнике. Бенчмарк оценивает способность моделей предсказывать прогресс выполнения разнообразных сложных манипуляционных задач как роботами, так и людьми."
                },
                "en": {
                    "title": "OpenGVL: Bridging the Gap in Robotics Task Prediction",
                    "desc": "OpenGVL is a benchmark designed to improve task progress prediction in robotics by utilizing vision-language models (VLMs). It highlights the performance gap between open-source and closed-source models, with open-source models achieving only about 70% of the effectiveness of their closed-source counterparts. The benchmark aims to facilitate automated data curation, allowing for better annotation and quality assessment of large robotics datasets. By leveraging the Generative Value Learning (GVL) approach, OpenGVL provides a structured way to evaluate and enhance the capabilities of models in predicting task completion from visual inputs."
                },
                "zh": {
                    "title": "OpenGVL：提升机器人任务预测的基准",
                    "desc": "OpenGVL是一个用于机器人任务进度预测的基准，利用视觉-语言模型（VLM）进行研究。该研究表明，开源模型在任务进度预测方面的表现明显低于闭源模型，仅达到后者性能的70%左右。OpenGVL不仅提供了多样化的挑战性操作任务的评估，还能作为自动化数据整理和过滤的实用工具，帮助高效评估大规模机器人数据集的质量。通过利用不断增长的机器人数据，OpenGVL为机器人领域的进步提供了新的机会。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17083",
            "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
            "url": "https://huggingface.co/papers/2509.17083",
            "abstract": "Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
            "score": 3,
            "issue_id": 6053,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 сентября",
                "en": "September 21",
                "zh": "9月21日"
            },
            "hash": "4a06acbb1d75ae4a",
            "authors": [
                "Zipeng Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17083.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🌈",
                "ru": {
                    "title": "Гибридные радиационные поля: высокое качество и эффективность в 3D рендеринге",
                    "desc": "Гибридные радиационные поля (HyRF) объединяют явные гауссианы и нейронные поля для высококачественного рендеринга с уменьшенным использованием памяти и производительностью в реальном времени. Этот метод разбивает сцену на компактный набор явных гауссианов, хранящих только критические высокочастотные параметры, и сеточные нейронные поля, предсказывающие остальные свойства. HyRF использует раздельную архитектуру нейронного поля для моделирования геометрии и цвета, зависящего от точки обзора. Эксперименты показывают, что HyRF достигает лучшего качества рендеринга, уменьшая размер модели более чем в 20 раз по сравнению с 3DGS, сохраняя при этом производительность в реальном времени."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with Hybrid Radiance Fields",
                    "desc": "Hybrid Radiance Fields (HyRF) introduce a new way to represent 3D scenes by merging explicit Gaussians with neural fields. This approach allows for high-quality rendering while significantly reducing memory usage and enabling real-time performance. HyRF uses a compact set of explicit Gaussians to capture essential high-frequency details and employs grid-based neural fields for other properties. The innovative architecture separates geometry and color modeling, leading to improved scene representation and rendering quality compared to previous methods."
                },
                "zh": {
                    "title": "混合辐射场：高效渲染的新方法",
                    "desc": "混合辐射场（HyRF）是一种新颖的场景表示方法，结合了显式高斯和神经场的优点，以实现高质量渲染，同时减少内存使用并保持实时性能。HyRF将场景分解为一组紧凑的显式高斯，仅存储关键的高频参数，以及基于网格的神经场，用于预测其余属性。我们引入了一种解耦的神经场架构，分别建模几何形状（尺度、不透明度、旋转）和视角依赖的颜色。实验表明，HyRF在渲染质量上达到了最先进的水平，同时模型大小比3D高斯点云减少了20倍以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19300",
            "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\n  for Better Flow Matching",
            "url": "https://huggingface.co/papers/2509.19300",
            "abstract": "Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.  \t\t\t\t\tAI-generated summary \t\t\t\t Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters.",
            "score": 2,
            "issue_id": 6055,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "5053f16cc1621faa",
            "authors": [
                "Chen Chen",
                "Pengsheng Guo",
                "Liangchen Song",
                "Jiasen Lu",
                "Rui Qian",
                "Xinze Wang",
                "Tsu-Jui Fu",
                "Wei Liu",
                "Yinfei Yang",
                "Alex Schwing"
            ],
            "affiliations": [
                "Apple Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19300.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#diffusion"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Умное перераспределение для эффективного генеративного моделирования",
                    "desc": "CAR-Flow - это новый метод в условном генеративном моделировании, который улучшает производительность и ускоряет обучение моделей. Он использует легковесное обученное смещение для кондиционирования исходного и целевого распределений. CAR-Flow сокращает вероятностный путь, который модель должна изучить, что приводит к более быстрому обучению на практике. На данных ImageNet-256 применение CAR-Flow к модели SiT-XL/2 снизило FID с 2.07 до 1.68, добавив менее 0.6% дополнительных параметров."
                },
                "en": {
                    "title": "Streamlining Conditional Generative Modeling with CAR-Flow",
                    "desc": "The paper introduces Condition-Aware Reparameterization for Flow Matching (CAR-Flow), which improves conditional generative modeling by adjusting the positioning of data distributions. This method allows for more efficient training by reducing the complexity of the probability paths that the model needs to learn. By conditioning either the source, the target, or both distributions, CAR-Flow enhances the performance of flow-based models on image data. The results show significant improvements in image quality metrics, such as a reduction in FID scores, with minimal increase in model parameters."
                },
                "zh": {
                    "title": "条件感知重参数化，提升生成模型效率",
                    "desc": "条件生成建模旨在从包含数据-条件对的样本中学习条件数据分布。为此，扩散和基于流的方法取得了显著的成果。这些方法使用学习到的流模型将初始的标准高斯噪声传输到条件数据分布。我们提出的条件感知重参数化（CAR-Flow）通过重新定位分布，简化了模型的学习过程，从而加快了训练速度并提高了在图像数据上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19087",
            "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
            "url": "https://huggingface.co/papers/2509.19087",
            "abstract": "A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.   To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
            "score": 1,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "fbe226390b6ea231",
            "authors": [
                "Ganesh Mallya",
                "Yotam Gigi",
                "Dahun Kim",
                "Maxim Neumann",
                "Genady Beryozkin",
                "Tomer Shekel",
                "Anelia Angelova"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19087.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Мультиспектральное зрение для ИИ без переобучения",
                    "desc": "Статья представляет метод, позволяющий мультимодальным моделям обрабатывать мультиспектральные изображения в режиме zero-shot, без дополнительного обучения. Этот подход адаптирует специализированные мультиспектральные входные данные для использования в генералистических моделях, обученных только на RGB-изображениях. Метод был протестирован с моделью Gemini2.5 и показал значительное улучшение производительности в задачах дистанционного зондирования, таких как классификация землепользования. Предложенный подход открывает возможности для специалистов в области геопространственных данных использовать мощные мультимодальные модели для работы со специализированными сенсорными данными."
                },
                "en": {
                    "title": "Unlocking Multimodal Models for Multi-Spectral Imagery Without Training",
                    "desc": "This paper presents a novel training-free method that allows generalist multimodal models to process multi-spectral imagery without prior training, enhancing their performance in remote sensing tasks. Multi-spectral images, which contain additional spectral bands, are crucial for accurately identifying physical materials on the ground. The proposed approach enables these models, typically trained only on RGB images, to adapt to multi-spectral data in a zero-shot manner by injecting domain-specific instructions. The results demonstrate significant performance improvements on remote sensing benchmarks, showcasing the potential for geospatial professionals to utilize advanced multimodal models effectively."
                },
                "zh": {
                    "title": "无训练的多模态模型，轻松处理多光谱图像",
                    "desc": "本论文提出了一种无训练的方法，使通用多模态模型能够以零样本的方式处理多光谱图像，从而提高遥感任务的性能。多光谱图像在土地利用分类、环境监测和城市规划等遥感应用中发挥着重要作用。传统上，这些图像需要专门训练的机器学习模型进行自动分析，但这种方法成本高且不够灵活。我们的方法利用通用多模态模型的视觉理解能力，能够轻松适应新的多光谱输入，展示了在遥感基准测试中的显著性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19002",
            "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
            "url": "https://huggingface.co/papers/2509.19002",
            "abstract": "VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
            "score": 1,
            "issue_id": 6057,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "110499782ca8d1ee",
            "authors": [
                "Hao Wang",
                "Eiki Murata",
                "Lingfang Zhang",
                "Ayako Sato",
                "So Fukuda",
                "Ziqi Yin",
                "Wentao Hu",
                "Keisuke Nakao",
                "Yusuke Nakamura",
                "Sebastian Zwirner",
                "Yi-Chia Chen",
                "Hiroyuki Otomo",
                "Hiroki Ouchi",
                "Daisuke Kawahara"
            ],
            "affiliations": [
                "AI Shift, Inc.",
                "CyberAgent, Inc.",
                "Nara Institute of Science and Technology",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19002.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#science",
                    "#games",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🌎",
                "ru": {
                    "title": "VIR-Bench: Новый рубеж в понимании видео о путешествиях для ИИ",
                    "desc": "VIR-Bench - это новый бенчмарк для оценки и улучшения геопространственно-временного интеллекта мультимодальных больших языковых моделей (MLLM) на основе видео о путешествиях. Он состоит из 200 видео и ставит задачу реконструкции маршрута, что позволяет оценить способность моделей работать с протяженными пространственно-временными траекториями. Эксперименты показали, что современные MLLM, включая проприетарные, испытывают трудности с этой задачей. Применение insights из VIR-Bench позволило значительно улучшить рекомендации по маршрутам в прототипе агента для планирования путешествий."
                },
                "en": {
                    "title": "Enhancing Travel Itinerary Recommendations with VIR-Bench",
                    "desc": "VIR-Bench is a new benchmark designed to assess and improve the geospatial-temporal intelligence of multimodal large language models (MLLMs) using travel videos. It consists of 200 videos that focus on long-distance travel, a topic that has been largely overlooked in existing benchmarks. The study shows that even advanced MLLMs struggle with the complexities of itinerary reconstruction from these videos. By developing a travel-planning agent based on insights from VIR-Bench, the research demonstrates significant improvements in itinerary recommendations, highlighting the practical benefits of this evaluation framework."
                },
                "zh": {
                    "title": "提升旅行视频理解的基准挑战",
                    "desc": "VIR-Bench是一个新的旅行视频基准，旨在评估和提升多模态大语言模型（MLLMs）的地理时空智能，从而改善现实应用中的行程推荐。当前的视频基准主要集中在室内场景或短途户外活动上，长途旅行的挑战尚未得到充分探索。掌握扩展的地理时空轨迹对于下一代MLLMs至关重要，这支持了诸如具身人工智能规划和导航等现实任务。通过VIR-Bench，我们展示了一个包含200个旅行视频的基准，将行程重建作为一项挑战性任务，以评估和推动MLLMs的地理时空智能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18090",
            "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
            "url": "https://huggingface.co/papers/2509.18090",
            "abstract": "GeoSVR, a voxel-based framework, improves surface reconstruction accuracy and detail using sparse voxels with depth constraints and surface regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
            "score": 1,
            "issue_id": 6063,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "d2fdd6191d2fb77a",
            "authors": [
                "Jiahe Li",
                "Jiawei Zhang",
                "Youmin Zhang",
                "Xiao Bai",
                "Jin Zheng",
                "Xiaohan Yu",
                "Lin Gu"
            ],
            "affiliations": [
                "Macquarie University",
                "RIKEN AIP",
                "Rawmantic AI",
                "School of Computer Science and Engineering, State Key Laboratory of Complex Critical Software Environment, Jiangxi Research Institute, Beihang University",
                "State Key Laboratory of Virtual Reality Technology and Systems, Beijing",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18090.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Разреженные воксели для точной реконструкции поверхностей",
                    "desc": "В статье представлен GeoSVR - новый подход к реконструкции поверхностей на основе разреженных вокселей, который превосходит методы на базе Gaussian Splatting. Авторы предлагают использовать ограничения по глубине с учётом неопределённости вокселей для обеспечения правильной сходимости сцены. Дополнительно применяется регуляризация поверхности разреженных вокселей для улучшения геометрической консистентности. Эксперименты показывают превосходство метода в точности геометрии, сохранении деталей и полноте реконструкции."
                },
                "en": {
                    "title": "Unlocking Surface Reconstruction with Sparse Voxel Precision",
                    "desc": "GeoSVR is a novel voxel-based framework designed to enhance the accuracy and detail of surface reconstruction using sparse voxels. It addresses limitations found in traditional methods, particularly those relying on Gaussian Splatting, by introducing a Voxel-Uncertainty Depth Constraint that leverages monocular depth cues while managing voxel uncertainty. Additionally, Sparse Voxel Surface Regularization is implemented to improve geometric consistency and support the creation of sharp surfaces. The framework demonstrates superior performance in various challenging scenarios, achieving high geometric accuracy and detail preservation efficiently."
                },
                "zh": {
                    "title": "GeoSVR：提升表面重建的准确性与细节",
                    "desc": "GeoSVR是一种基于体素的框架，通过稀疏体素和深度约束来提高表面重建的准确性和细节。该方法克服了传统高斯点云方法的局限性，利用稀疏体素的优势来实现更完整和清晰的几何重建。我们提出了体素不确定性深度约束，以最大化单目深度线索的效果，同时避免质量下降。实验结果表明，GeoSVR在几何准确性、细节保留和重建完整性方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17349",
            "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\n  Speech-to-Text Translation",
            "url": "https://huggingface.co/papers/2509.17349",
            "abstract": "The paper analyzes SimulST latency metrics, identifies segmentation bias, and introduces YAAL and LongYAAL for more accurate latency evaluation, along with SoftSegmenter for improved alignment quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency--the delay between speech input and the translated output. While quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented. In this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes. We uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more accurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose SoftSegmenter, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while SoftSegmenter enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.",
            "score": 1,
            "issue_id": 6064,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "c98b194f176b93cf",
            "authors": [
                "Peter Polák",
                "Sara Papi",
                "Luisa Bentivogli",
                "Ondřej Bojar"
            ],
            "affiliations": [
                "Charles University, Czech Republic",
                "Fondazione Bruno Kessler, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17349.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#data",
                    "#machine_translation",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Более точная оценка задержки в системах одновременного перевода речи",
                    "desc": "Исследование анализирует метрики задержки для систем одновременного перевода речи в текст (SimulST), которые должны балансировать между качеством перевода и временной задержкой. Авторы выявили структурную предвзятость в существующих метриках, связанную с сегментацией речи, что приводит к некорректным сравнениям систем. Для решения проблемы предложены новые метрики YAAL и LongYAAL, обеспечивающие более точную оценку задержки для коротких и длинных аудиофрагментов соответственно. Также представлен инструмент SoftSegmenter для улучшения качества выравнивания на уровне слов в длинных аудиозаписях."
                },
                "en": {
                    "title": "Enhancing Latency Evaluation in SimulST Systems",
                    "desc": "This paper focuses on improving the evaluation of latency in simultaneous speech-to-text translation (SimulST) systems. It identifies a segmentation bias in existing latency metrics that can lead to inaccurate comparisons between different systems and languages. To address this issue, the authors introduce YAAL and LongYAAL, new metrics that provide more reliable latency measurements, especially in short-form and unsegmented audio contexts. Additionally, they present SoftSegmenter, a tool that enhances alignment quality, ultimately leading to better assessments of translation performance."
                },
                "zh": {
                    "title": "提升SimulST系统评估的准确性",
                    "desc": "本文分析了同时语音翻译系统（SimulST）的延迟指标，识别了分段偏差，并引入了YAAL和LongYAAL以实现更准确的延迟评估，同时提出了SoftSegmenter以提高对齐质量。研究发现，现有的延迟测量指标在短格式设置中常常产生不一致或误导性的结果，影响了公平的比较。YAAL（Yet Another Average Lagging）是一种改进的延迟指标，能够在短格式中提供更准确的评估，而LongYAAL则适用于未分段的音频。通过实验，我们证明了YAAL和LongYAAL在延迟评估中优于流行的指标，同时SoftSegmenter提升了长格式评估中的对齐质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19274",
            "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\n  Models' Understanding on Indian Culture",
            "url": "https://huggingface.co/papers/2509.19274",
            "abstract": "DRISHTIKON is a multimodal and multilingual benchmark for evaluating generative AI systems' cultural understanding across India's diverse regions and languages.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on Indian culture, designed to evaluate the cultural understanding of generative AI systems. Unlike existing benchmarks with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage across India's diverse regions, spanning 15 languages, covering all states and union territories, and incorporating over 64,000 aligned text-image pairs. The dataset captures rich cultural themes including festivals, attire, cuisines, art forms, and historical heritage amongst many more. We evaluate a wide range of vision-language models (VLMs), including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models, across zero-shot and chain-of-thought settings. Our results expose key limitations in current models' ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a robust testbed to advance culturally aware, multimodally competent language technologies.",
            "score": 0,
            "issue_id": 6063,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "ffb921e6e33421e6",
            "authors": [
                "Arijit Maji",
                "Raghvendra Kumar",
                "Akash Ghosh",
                "Anushka",
                "Nemil Shah",
                "Abhilekh Borah",
                "Vanshika Shah",
                "Nishant Mishra",
                "Sriparna Saha"
            ],
            "affiliations": [
                "Banasthali Vidyapeeth University, Rajasthan, India",
                "Dwarkadas J. Sanghvi College of Engineering, India",
                "Indian Institute of Technology Patna, India",
                "Manipal University Jaipur, India",
                "Pandit Deendayal Energy University, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19274.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#low_resource",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#multilingual",
                    "#games"
                ],
                "emoji": "🏛️",
                "ru": {
                    "title": "Первый бенчмарк для оценки культурного понимания AI на примере Индии",
                    "desc": "Исследователи представили DRISHTIKON - первый мультимодальный и многоязычный бенчмарк для оценки понимания индийской культуры генеративными AI системами. Датасет охватывает 15 языков, все штаты Индии и содержит более 64,000 пар текст-изображение с культурными темами: фестивалями, одеждой, кухней и искусством. Тестирование различных vision-language моделей показало серьёзные ограничения в понимании культурно-специфичного мультимодального контента, особенно для редких языков. DRISHTIKON заполняет важный пробел в создании культурно-осведомлённых AI технологий."
                },
                "en": {
                    "title": "Evaluating AI's Cultural Understanding with DRISHTIKON",
                    "desc": "DRISHTIKON is a unique benchmark designed to assess how well generative AI systems understand Indian culture through multiple languages and types of data. It includes a vast collection of over 64,000 text-image pairs that represent various cultural aspects from all regions of India, covering 15 languages. The benchmark evaluates different vision-language models (VLMs) to identify their strengths and weaknesses in processing culturally rich and multimodal information. This initiative aims to improve AI's cultural awareness, especially for underrepresented languages and traditions, by providing a comprehensive testing framework."
                },
                "zh": {
                    "title": "DRISHTIKON：评估生成性AI的文化理解力",
                    "desc": "DRISHTIKON是一个多模态和多语言的基准，专注于评估生成性人工智能系统对印度文化的理解。它涵盖了印度15种语言和64,000多个文本-图像对，深入反映了各地区的文化主题，如节日、服饰、美食和艺术形式等。通过评估多种视觉语言模型，研究揭示了当前模型在处理文化相关的多模态输入时的局限性，尤其是在低资源语言和较少文献的传统方面。DRISHTIKON为包容性人工智能研究填补了重要空白，提供了一个强有力的测试平台，以推动文化意识和多模态能力的语言技术发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16506",
            "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection",
            "url": "https://huggingface.co/papers/2509.16506",
            "abstract": "A web-scale dataset and models for form field detection are introduced, achieving high precision and supporting diverse languages and domains.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces CommonForms, a web-scale dataset for form field detection. It casts the problem of form field detection as object detection: given an image of a page, predict the location and type (Text Input, Choice Button, Signature) of form fields. The dataset is constructed by filtering Common Crawl to find PDFs that have fillable elements. Starting with 8 million documents, the filtering process is used to arrive at a final dataset of roughly 55k documents that have over 450k pages. Analysis shows that the dataset contains a diverse mixture of languages and domains; one third of the pages are non-English, and among the 14 classified domains, no domain makes up more than 25% of the dataset.   In addition, this paper presents a family of form field detectors, FFDNet-Small and FFDNet-Large, which attain a very high average precision on the CommonForms test set. Each model cost less than $500 to train. Ablation results show that high-resolution inputs are crucial for high-quality form field detection, and that the cleaning process improves data efficiency over using all PDFs that have fillable fields in Common Crawl. A qualitative analysis shows that they outperform a popular, commercially available PDF reader that can prepare forms. Unlike the most popular commercially available solutions, FFDNet can predict checkboxes in addition to text and signature fields. This is, to our knowledge, the first large scale dataset released for form field detection, as well as the first open source models. The dataset, models, and code will be released at https://github.com/jbarrow/commonforms",
            "score": 0,
            "issue_id": 6066,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "8711c5ce068d185e",
            "pdf_title_img": "assets/pdf/title_img/2509.16506.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#cv"
                ],
                "emoji": "📋",
                "ru": {
                    "title": "Первый открытый датасет для автоматического распознавания полей в формах",
                    "desc": "В данной работе представлен CommonForms - крупномасштабный датасет для детекции полей форм, содержащий 55 тысяч документов с более чем 450 тысячами страниц. Задача формулируется как object detection: по изображению страницы нужно предсказать расположение и тип полей (текстовые поля, кнопки выбора, поля для подписи). Авторы обучили семейство моделей FFDNet-Small и FFDNet-Large, которые показывают высокую точность при стоимости обучения менее 500 долларов каждая. Датасет отличается языковым и доменным разнообразием - треть страниц написана не на английском языке, что делает его первым открытым датасетом такого масштаба для детекции полей форм."
                },
                "en": {
                    "title": "Revolutionizing Form Field Detection with CommonForms",
                    "desc": "This paper presents CommonForms, a large dataset specifically designed for detecting form fields in documents. It treats form field detection as an object detection task, where the goal is to identify the location and type of fields like text inputs and buttons in images of pages. The dataset is derived from filtering 8 million documents from Common Crawl, resulting in about 55,000 documents with diverse languages and domains. Additionally, the authors introduce two models, FFDNet-Small and FFDNet-Large, which achieve high precision in detecting form fields and outperform existing commercial solutions."
                },
                "zh": {
                    "title": "表单字段检测的新突破",
                    "desc": "本文介绍了一个名为CommonForms的网络规模数据集，用于表单字段检测。该研究将表单字段检测视为目标检测问题，旨在预测页面中可填写字段的位置和类型（如文本输入、选择按钮、签名）。数据集通过过滤Common Crawl中的PDF文档构建，最终得到约55,000个文档，包含超过450,000个页面，涵盖多种语言和领域。研究还提出了FFDNet-Small和FFDNet-Large两种表单字段检测模型，具有高精度，并且训练成本低于500美元。"
                }
            }
        }
    ],
    "link_prev": "2025-09-23.html",
    "link_next": "2025-09-25.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9月23日"
    },
    "short_date_next": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9月25日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 4,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 5,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 3,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 3
    }
}