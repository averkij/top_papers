{
    "date": {
        "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 15",
        "zh": "1æœˆ15æ—¥"
    },
    "time_utc": "2025-01-15 06:13",
    "weekday": 2,
    "issue_id": 1675,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08313",
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "url": "https://huggingface.co/papers/2501.08313",
            "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "score": 136,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "a57d7b1914e7383a",
            "authors": [
                "MiniMax",
                "Aonian Li",
                "Bangwei Gong",
                "Bo Yang",
                "Boji Shan",
                "Chang Liu",
                "Cheng Zhu",
                "Chunhao Zhang",
                "Congchao Guo",
                "Da Chen",
                "Dong Li",
                "Enwei Jiao",
                "Gengxin Li",
                "Guojun Zhang",
                "Haohai Sun",
                "Houze Dong",
                "Jiadai Zhu",
                "Jiaqi Zhuang",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingtao Han",
                "Jingyang Li",
                "Junbin Xie",
                "Junhao Xu",
                "Junjie Yan",
                "Kaishun Zhang",
                "Kecheng Xiao",
                "Kexi Kang",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Zheng",
                "Linbo Chai",
                "Long Xing",
                "Meizhi Ju",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Peikai Huang",
                "Pengcheng Niu",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qi Yang",
                "Qidi Xu",
                "Qiexiang Wang",
                "Qin Wang",
                "Qiuhui Li",
                "Ruitao Leng",
                "Shengmin Shi",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tao Huang",
                "Tianrun Liang",
                "Weigao Sun",
                "Weixuan Sun",
                "Weiyu Cheng",
                "Wenkai Li",
                "Xiangjun Song",
                "Xiao Su",
                "Xiaodong Han",
                "Xinjie Zhang",
                "Xinzhu Hou",
                "Xu Min",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yingjie Zhu",
                "Yipeng Zhou",
                "Yiran Zhong",
                "Yongyi Hu",
                "Yuanxiang Fan",
                "Yue Yu",
                "Yufeng Yang",
                "Yuhao Li",
                "Yunan Huang",
                "Yunji Li",
                "Yunpeng Huang",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Zehan Li",
                "Zekang Li",
                "Zewei Tao",
                "Zewen Ying",
                "Zhaoyang Cong",
                "Zhen Qin",
                "Zhenhua Fan",
                "Zhihang Yu",
                "Zhuo Jiang",
                "Zijia Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08313.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniMax-01: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MiniMax-01, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MiniMax-Text-01 Ğ¸ MiniMax-VL-01, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ lightning attention Ğ¸ ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Mixture of Experts (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ 32 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ¸ 456 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 45,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ MiniMax-Text-01 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Unleashing Long Contexts with MiniMax-01 Models",
                    "desc": "The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities."
                },
                "zh": {
                    "title": "MiniMax-01ï¼šè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ–°çºªå…ƒ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-01ç³»åˆ—ï¼ŒåŒ…æ‹¬MiniMax-Text-01å’ŒMiniMax-VL-01ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡æ—¶å…·æœ‰ä¼˜è¶Šçš„èƒ½åŠ›ã€‚æ ¸å¿ƒæŠ€æœ¯æ˜¯é—ªç”µæ³¨æ„åŠ›å’Œé«˜æ•ˆçš„æ‰©å±•èƒ½åŠ›ã€‚ä¸ºäº†æœ€å¤§åŒ–è®¡ç®—èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†å…¶ä¸ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ‹¥æœ‰32ä¸ªä¸“å®¶å’Œ4560äº¿å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ ‡å‡†å’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶æä¾›20åˆ°32å€æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08187",
            "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
            "url": "https://huggingface.co/papers/2501.08187",
            "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
            "score": 14,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "de984ce7cc62fa5e",
            "authors": [
                "Yin Fang",
                "Xinle Deng",
                "Kangwei Liu",
                "Ningyu Zhang",
                "Jingyang Qian",
                "Penghui Yang",
                "Xiaohui Fan",
                "Huajun Chen"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
                "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
                "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
                "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
                "School of Software Technology, Zhejiang University, Ningbo 315048, China",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08187.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#science",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞµ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "InstructCell - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ ĞĞš-ÑĞµĞºĞ²ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (scRNA-seq). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞºÑĞ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ². InstructCell Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ°Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ğ¼, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "InstructCell: Bridging Language and Biology for Seamless Single-Cell Analysis",
                    "desc": "This paper introduces InstructCell, an AI tool designed to simplify the analysis of single-cell RNA sequencing (scRNA-seq) data using natural language instructions. By creating a dataset that links text commands with scRNA-seq profiles, InstructCell allows researchers to perform complex tasks like cell type annotation and drug sensitivity prediction more intuitively. The model employs a multi-modal architecture that processes both text and biological data simultaneously, enhancing its usability. Evaluations show that InstructCell outperforms existing models, making single-cell analysis more accessible and efficient for researchers in the life sciences."
                },
                "zh": {
                    "title": "ç”¨è‡ªç„¶è¯­è¨€è§£é”å•ç»†èƒæ•°æ®çš„æ½œåŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†InstructCellï¼Œä¸€ä¸ªå¤šæ¨¡æ€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€ç®€åŒ–å•ç»†èƒRNAæµ‹åº(scRNA-seq)æ•°æ®çš„åˆ†æã€‚ä¼ ç»Ÿå·¥å…·åœ¨å¤„ç†ç»†èƒç”Ÿç‰©å­¦çš„å¤æ‚æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒInstructCellé€šè¿‡å°†æ–‡æœ¬æŒ‡ä»¤ä¸scRNA-seqæ•°æ®ç»“åˆï¼Œæä¾›äº†æ›´ç›´æ¥å’Œçµæ´»çš„åˆ†ææ–¹å¼ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œç»†èƒç±»å‹æ³¨é‡Šã€æ¡ä»¶ä¼ªç»†èƒç”Ÿæˆå’Œè¯ç‰©æ•æ„Ÿæ€§é¢„æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œä¸”ä½¿ç”¨ç®€å•çš„è‡ªç„¶è¯­è¨€å‘½ä»¤å³å¯å®Œæˆã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒInstructCellåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å•ç»†èƒåŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶é€‚åº”å¤šç§å®éªŒæ¡ä»¶ï¼Œé™ä½äº†æŠ€æœ¯é—¨æ§›ï¼Œä¿ƒè¿›äº†ç”Ÿç‰©å­¦çš„æ·±å…¥ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08332",
            "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
            "url": "https://huggingface.co/papers/2501.08332",
            "abstract": "Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.",
            "score": 12,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "20ea6b75639e2ced",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Xi Chen",
                "Jie Xiao",
                "Hao Ouyang",
                "Kai Zhu",
                "Yu Liu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08332.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "MangaNinjia - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ³Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼-Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ñ†Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MangaNinjia Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MangaNinjia: Mastering Line Art Colorization with Precision",
                    "desc": "MangaNinjia is a model designed for coloring line art by using reference images. It employs a patch shuffling module to help the model learn how to match colors from the reference image to the target line art accurately. Additionally, it features a point-driven control scheme that allows for detailed color adjustments, ensuring that colors are applied precisely. Our experiments show that MangaNinjia outperforms existing methods in colorization tasks, especially in complex scenarios involving multiple references and different characters."
                },
                "zh": {
                    "title": "MangaNinjiaï¼šç²¾å‡†ä¸Šè‰²çš„æ–°æ–¹æ³•",
                    "desc": "MangaNinjia æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å‚è€ƒå¼•å¯¼çº¿æ¡è‰ºæœ¯ä¸Šè‰²æŠ€æœ¯ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªæ¨¡å—æ¥ç¡®ä¿è§’è‰²ç»†èŠ‚çš„å‡†ç¡®è½¬å½•ï¼ŒåŒ…æ‹¬è¡¥ä¸æ´—ç‰Œæ¨¡å—å’Œç‚¹é©±åŠ¨æ§åˆ¶æ–¹æ¡ˆï¼Œä»¥å®ç°ç²¾ç»†çš„é¢œè‰²åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç²¾ç¡®ä¸Šè‰²æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ‰€æè®®çš„äº¤äº’å¼ç‚¹æ§åˆ¶åœ¨å¤„ç†å¤æ‚æ¡ˆä¾‹å’Œå¤šå‚è€ƒåè°ƒæ–¹é¢çš„æ½œåŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰ç®—æ³•çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08316",
            "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
            "url": "https://huggingface.co/papers/2501.08316",
            "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",
            "score": 8,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "4122a780e8356ce7",
            "authors": [
                "Shanchuan Lin",
                "Xin Xia",
                "Yuxi Ren",
                "Ceyuan Yang",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08316.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adversarial Post-Training (APT) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ R1. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Seaweed-APT ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 2-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1024px Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Fast and High-Quality Video Generation with Seaweed-APT",
                    "desc": "This paper addresses the slow and costly iterative process of generating images and videos using diffusion models. The authors introduce Adversarial Post-Training (APT) to enhance one-step video generation while maintaining high quality. They implement architectural and procedural improvements, including an approximated R1 regularization, to stabilize training. Their model, Seaweed-APT, successfully generates high-quality 2-second videos and 1024px images in real time with a single forward evaluation step."
                },
                "zh": {
                    "title": "å¯¹æŠ—åè®­ç»ƒï¼šå¿«é€Ÿé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œä½†å…¶è¿­ä»£ç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ä¸”æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•åœ¨å›¾åƒé¢†åŸŸå±•ç¤ºäº†å•æ­¥ç”Ÿæˆçš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹çœŸå®æ•°æ®çš„å¯¹æŠ—åè®­ç»ƒï¼ˆAPTï¼‰æ–¹æ³•ï¼Œä»¥å®ç°å•æ­¥è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¯¹æŠ—åè®­ç»ƒçš„æ¨¡å‹Seaweed-APTèƒ½å¤Ÿå®æ—¶ç”Ÿæˆ1280x720ã€24fpsçš„2ç§’è§†é¢‘ï¼Œå¹¶ä¸”åœ¨å•æ­¥ç”Ÿæˆ1024pxå›¾åƒæ—¶ï¼Œå…¶è´¨é‡å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07730",
            "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
            "url": "https://huggingface.co/papers/2501.07730",
            "abstract": "Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.",
            "score": 4,
            "issue_id": 1673,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "80f40715084c602b",
            "authors": [
                "Dongwon Kim",
                "Ju He",
                "Qihang Yu",
                "Chenglin Yang",
                "Xiaohui Shen",
                "Suha Kwak",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07730.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TA-TiTok. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ TA-TiTok Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MaskGen, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Democratizing Text-to-Image Generation with TA-TiTok",
                    "desc": "This paper presents TA-TiTok, a novel image tokenizer designed for text-to-image generative models, which simplifies the training process and improves performance. Unlike traditional models that require large private datasets, TA-TiTok can effectively utilize open data, making it more accessible for researchers. The tokenizer incorporates textual information during the decoding stage, which helps it learn faster and perform better. Additionally, the authors introduce MaskGen, a family of generative models that leverage TA-TiTok and are trained on publicly available datasets, aiming to democratize access to advanced text-to-image generation technology."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ¨åŠ¨å¼€æ”¾æ•°æ®çš„ä½¿ç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒæ ‡è®°å™¨ï¼Œç§°ä¸ºTA-TiTokï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ã€‚TA-TiTokåœ¨è§£ç é˜¶æ®µæ•´åˆäº†æ–‡æœ¬ä¿¡æ¯ï¼Œä»è€ŒåŠ å¿«äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¹¶æé«˜äº†æ€§èƒ½ã€‚ä¸ä»¥å¾€çš„æ ‡è®°å™¨ä¸åŒï¼ŒTA-TiToké‡‡ç”¨äº†ä¸€ç§ç®€åŒ–çš„ä¸€é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…äº†å¤æ‚çš„ä¸¤é˜¶æ®µè’¸é¦è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç³»åˆ—åŸºäºå¼€æ”¾æ•°æ®è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹MaskGenï¼Œæ—¨åœ¨ä¿ƒè¿›æ›´å¹¿æ³›çš„è®¿é—®å’Œæ°‘ä¸»åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08225",
            "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2501.08225",
            "abstract": "Interactive image editing allows users to modify images through visual interaction operations such as drawing, clicking, and dragging. Existing methods construct such supervision signals from videos, as they capture how objects change with various physical interactions. However, these models are usually built upon text-to-image diffusion models, so necessitate (i) massive training samples and (ii) an additional reference encoder to learn real-world dynamics and visual consistency. In this paper, we reformulate this task as an image-to-video generation problem, so that inherit powerful video diffusion priors to reduce training costs and ensure temporal consistency. Specifically, we introduce FramePainter as an efficient instantiation of this formulation. Initialized with Stable Video Diffusion, it only uses a lightweight sparse control encoder to inject editing signals. Considering the limitations of temporal attention in handling large motion between two frames, we further propose matching attention to enlarge the receptive field while encouraging dense correspondence between edited and source image tokens. We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \\eg, automatically adjust the reflection of the cup. Moreover, FramePainter also exhibits exceptional generalization in scenarios not present in real-world videos, \\eg, transform the clownfish into shark-like shape. Our code will be available at https://github.com/YBYBZhang/FramePainter.",
            "score": 3,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "811cfd0f18eb1e53",
            "authors": [
                "Yabo Zhang",
                "Xinpeng Zhou",
                "Yihan Zeng",
                "Hang Xu",
                "Hui Li",
                "Wangmeng Zuo"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08225.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "FramePainter: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FramePainter - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, FramePainter Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. FramePainter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Efficient Video Diffusion",
                    "desc": "This paper presents FramePainter, a novel approach to interactive image editing that reformulates the task as image-to-video generation. By leveraging video diffusion models, FramePainter reduces the need for extensive training data while ensuring temporal consistency in edited images. It utilizes a lightweight sparse control encoder to effectively incorporate editing signals, and introduces matching attention to improve the handling of large motion between frames. The results demonstrate that FramePainter significantly outperforms existing methods, achieving seamless image edits and showcasing strong generalization capabilities."
                },
                "zh": {
                    "title": "FramePainterï¼šé«˜æ•ˆçš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºFramePainterã€‚è¯¥æ–¹æ³•å°†å›¾åƒç¼–è¾‘ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆé—®é¢˜ï¼Œä»è€Œåˆ©ç”¨å¼ºå¤§çš„è§†é¢‘æ‰©æ•£å…ˆéªŒï¼Œé™ä½è®­ç»ƒæˆæœ¬å¹¶ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚FramePainterä½¿ç”¨è½»é‡çº§çš„ç¨€ç–æ§åˆ¶ç¼–ç å™¨æ¥æ³¨å…¥ç¼–è¾‘ä¿¡å·ï¼Œå¹¶é€šè¿‡åŒ¹é…æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºäº†å¯¹å¤§è¿åŠ¨çš„å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFramePainteråœ¨å„ç§ç¼–è¾‘ä¿¡å·ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå®ç°æ— ç¼ä¸”è¿è´¯çš„å›¾åƒç¼–è¾‘ï¼Œä¸”åœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­ä¹Ÿå±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08328",
            "title": "PokerBench: Training Large Language Models to become Professional Poker Players",
            "url": "https://huggingface.co/papers/2501.08328",
            "abstract": "We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: https://github.com/pokerllm/pokerbench.",
            "score": 2,
            "issue_id": 1674,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "7b4dacedffdbfa15",
            "authors": [
                "Richard Zhuang",
                "Akshat Gupta",
                "Richard Yang",
                "Aniket Rahane",
                "Zhengyu Li",
                "Gopala Anumanchipalli"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08328.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#games",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸƒ",
                "ru": {
                    "title": "PokerBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "PokerBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ² Ğ¿Ğ¾ĞºĞµÑ€. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 11000 Ğ²Ğ°Ğ¶Ğ½ĞµĞ¹ÑˆĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ³Ñ€Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4 Ğ¸ ChatGPT 3.5, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ², Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ¸Ğ¶Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ñ‹."
                },
                "en": {
                    "title": "PokerBench: Elevating LLMs to Master the Game of Poker",
                    "desc": "PokerBench is a new benchmark designed to assess the poker-playing skills of large language models (LLMs). It focuses on the unique challenges of poker, which requires a blend of mathematical skills, strategic reasoning, and an understanding of human psychology. The benchmark includes 11,000 scenarios that cover various aspects of the game, and it has been tested on several leading models, revealing that they initially struggle with optimal poker play. However, after fine-tuning, these models show significant improvement, highlighting the need for advanced training techniques to enhance their performance in complex games."
                },
                "zh": {
                    "title": "PokerBenchï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹æ‰‘å…‹èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†PokerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰‘å…‹æ¸¸æˆèƒ½åŠ›çš„åŸºå‡†ã€‚æ‰‘å…‹æ˜¯ä¸€ç§ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆï¼Œéœ€è¦æ•°å­¦ã€æ¨ç†ã€è§„åˆ’ã€ç­–ç•¥ä»¥åŠå¯¹åšå¼ˆè®ºå’Œäººç±»å¿ƒç†çš„æ·±åˆ»ç†è§£ã€‚PokerBenchåŒ…å«11,000ä¸ªé‡è¦åœºæ™¯ï¼Œåˆ†ä¸ºç¿»ç‰Œå‰å’Œç¿»ç‰Œåæ¸¸æˆï¼Œç»è¿‡è®­ç»ƒçš„æ‰‘å…‹ç©å®¶å…±åŒå¼€å‘ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡å½“å‰çš„LLMsåœ¨æ‰‘å…‹æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ï¼Œä½†ç»è¿‡å¾®è°ƒåï¼Œå®ƒä»¬çš„è¡¨ç°æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08197",
            "title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training",
            "url": "https://huggingface.co/papers/2501.08197",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs.",
            "score": 1,
            "issue_id": 1675,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "27267ae1a569051c",
            "authors": [
                "Yijiong Yu",
                "Ziyun Dai",
                "Zekun Wang",
                "Wei Wang",
                "Ran Chen",
                "Ji Pei"
            ],
            "affiliations": [
                "OpenCSG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08197.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#synthetic",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "ğŸ‰",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: OpenCSG Chinese Corpus",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenCSG Chinese Corpus - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞšĞ¾Ñ€Ğ¿ÑƒÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸: Ğ¾Ñ‚ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğº Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ C-Eval."
                },
                "en": {
                    "title": "Empowering Chinese LLMs with OpenCSG Corpus",
                    "desc": "This paper introduces the OpenCSG Chinese Corpus, a collection of high-quality datasets aimed at improving the performance of Chinese large language models (LLMs). The corpus includes several datasets, each tailored for different training needs: Fineweb-edu datasets focus on high-quality web content, Cosmopedia-chinese offers synthetic textbook-style data, and Smoltalk-chinese provides diverse chat-format data. The authors highlight the importance of quality pretraining data for LLMs and demonstrate through experiments that using this corpus leads to significant performance gains in various evaluation tasks. Overall, the OpenCSG Chinese Corpus addresses the challenge of limited high-quality datasets for Chinese LLMs, promoting better training outcomes."
                },
                "zh": {
                    "title": "æå‡ä¸­æ–‡LLMæ€§èƒ½çš„é«˜è´¨é‡è¯­æ–™åº“",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æˆåŠŸä¾èµ–äºé«˜è´¨é‡çš„é¢„è®­ç»ƒè¯­æ–™åº“ã€‚é’ˆå¯¹ä¸­æ–‡LLMsï¼Œä¼˜è´¨ä¸­æ–‡æ•°æ®é›†çš„ç¨€ç¼ºæ€§æˆä¸ºäº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å®ƒä»¬çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OpenCSGä¸­æ–‡è¯­æ–™åº“ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“é—¨ä¸ºLLMé¢„è®­ç»ƒã€åè®­ç»ƒå’Œå¾®è°ƒè®¾è®¡çš„é«˜è´¨é‡æ•°æ®é›†ã€‚è¯¥è¯­æ–™åº“åŒ…æ‹¬Fineweb-edu-chineseã€Fineweb-edu-chinese-v2ã€Cosmopedia-chineseå’ŒSmoltalk-chineseï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å†…å®¹å’Œé£æ ¼ï¼Œæ˜¾è‘—æå‡äº†ä¸­æ–‡LLMsçš„è®­ç»ƒæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08167",
            "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
            "url": "https://huggingface.co/papers/2501.08167",
            "abstract": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.",
            "score": 1,
            "issue_id": 1675,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "866161709624c632",
            "authors": [
                "Rewina Bedemariam",
                "Natalie Perez",
                "Sreyoshi Bhaduri",
                "Satya Kapoor",
                "Alex Gil",
                "Elizabeth Conjar",
                "Ikkei Itoku",
                "David Theil",
                "Aman Chadha",
                "Naumaan Nayyar"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08167.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#ethics",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒĞ¸: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ LLM Ñ€ĞµĞ·ÑĞ¼Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğµ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹, Ñ…Ğ¾Ñ‚Ñ Ğ»ÑĞ´Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ…, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Trusting AI: Evaluating LLMs for Accurate Text Analysis",
                    "desc": "This paper explores the use of large language models (LLMs) for summarizing and analyzing unstructured text data, particularly from open-ended survey responses. It raises concerns about the trustworthiness of LLM-generated summaries, as they may not accurately reflect the original sentiments and themes present in the data. The research introduces an LLM-as-judge framework, where one LLM generates summaries while others evaluate their thematic alignment, comparing this method to human evaluations. The findings suggest that while LLMs can provide a scalable alternative to human raters, they may struggle with detecting subtle nuances that humans can identify, highlighting the importance of careful application in different contexts."
                },
                "zh": {
                    "title": "ä¿¡ä»»å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€»ç»“èƒ½åŠ›å—ï¼Ÿ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å’Œæ€»ç»“éç»“æ„åŒ–æ–‡æœ¬æ•°æ®æ–¹é¢çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†æå¼€æ”¾å¼è°ƒæŸ¥åé¦ˆæ—¶çš„åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶LLMsèƒ½å¤Ÿç”Ÿæˆç±»ä¼¼äººç±»çš„æ€»ç»“ï¼Œä½†å®ƒä»¬çš„è¾“å‡ºå¯èƒ½ä¸åŸå§‹æ–‡æœ¬çš„çœŸå®ä¸»é¢˜å­˜åœ¨åå·®ï¼Œè¿™å¯èƒ½å¯¼è‡´é”™è¯¯çš„å†³ç­–ã€‚ä¸ºäº†è¯„ä¼°LLMsç”Ÿæˆçš„æ€»ç»“ä¸å®é™…ä¸»é¢˜çš„ä¸€è‡´æ€§ï¼Œç ”ç©¶ä½¿ç”¨äº†LLMsä½œä¸ºè¯„åˆ¤æ¨¡å‹ï¼Œå¹¶ä¸äººç±»è¯„ä¼°è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsä½œä¸ºè¯„åˆ¤è€…æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½†äººç±»åœ¨æ•æ‰ç»†å¾®çš„ä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢ä»ç„¶è¡¨ç°æ›´ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08292",
            "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
            "url": "https://huggingface.co/papers/2501.08292",
            "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",
            "score": 1,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "f6751d682ff824ed",
            "authors": [
                "Abhilasha Ravichander",
                "Shrusti Ghela",
                "David Wadden",
                "Yejin Choi"
            ],
            "affiliations": [
                "Google",
                "NVIDIA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "HALoGEN: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HALoGEN - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 10,923 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ 86% ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº LLM, Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ² Ğ¸Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "HALoGEN: A Benchmark for Measuring Hallucinations in Language Models",
                    "desc": "This paper introduces HALoGEN, a new benchmark designed to measure hallucinations in generative large language models (LLMs). Hallucinations refer to incorrect statements generated by these models that do not align with known facts or the given context. The benchmark includes over 10,000 prompts across various domains and employs automatic verifiers to assess the accuracy of model outputs. The study reveals that even top-performing models exhibit significant hallucinations, prompting a classification system for different types of errors to better understand their origins and improve model reliability."
                },
                "zh": {
                    "title": "æ­ç¤ºç”Ÿæˆæ¨¡å‹çš„å¹»è§‰é—®é¢˜",
                    "desc": "å°½ç®¡ç”Ÿæˆæ€§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œæµç•…çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šäº§ç”Ÿå¹»è§‰ï¼Œå³ä¸å·²çŸ¥ä¸–ç•ŒçŸ¥è¯†æˆ–è¾“å…¥ä¸Šä¸‹æ–‡ä¸ä¸€è‡´çš„é™ˆè¿°ã€‚æµ‹é‡å¹»è§‰çš„éš¾åº¦åœ¨äºï¼Œå®æ—¶éªŒè¯æ¨¡å‹ç”Ÿæˆçš„å†…å®¹æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HALoGENï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¹»è§‰åŸºå‡†ï¼ŒåŒ…å«10,923ä¸ªè·¨è¶Šä¹ä¸ªé¢†åŸŸçš„æç¤ºå’Œè‡ªåŠ¨é«˜ç²¾åº¦éªŒè¯å™¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶ç”Ÿæˆçš„åŸå­äº‹å®ä¸­ä¹Ÿæœ‰é«˜è¾¾86%å¯èƒ½å­˜åœ¨å¹»è§‰ï¼Œè¿™ä¸ºç†è§£ç”Ÿæˆæ¨¡å‹çš„å¹»è§‰æä¾›äº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07888",
            "title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding",
            "url": "https://huggingface.co/papers/2501.07888",
            "abstract": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\% performance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.",
            "score": 0,
            "issue_id": 1674,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "54780a4b6f93fb10",
            "authors": [
                "Liping Yuan",
                "Jiawei Wang",
                "Haomiao Sun",
                "Yuchen Zhang",
                "Yuan Lin"
            ],
            "affiliations": [
                "ByteDance Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07888.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#cv",
                    "#hallucinations",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Tarsier2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Tarsier2 - ÑÑ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (LVLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (DPO). Tarsier2-7B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o Ğ¸ Gemini 1.5 Pro, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ñ‹ Ğ² 15 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ²Ğ¾Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Tarsier2: Redefining Video Understanding with Advanced LVLM Technology",
                    "desc": "Tarsier2 is a cutting-edge large vision-language model (LVLM) that excels in generating precise and detailed descriptions of videos while showcasing advanced video comprehension skills. The model's improvements stem from three main enhancements: increasing the pre-training dataset from 11 million to 40 million video-text pairs, implementing fine-grained temporal alignment during fine-tuning, and utilizing model-based sampling for preference data construction with DPO training for optimization. Extensive testing reveals that Tarsier2-7B surpasses top proprietary models like GPT-4o and Gemini 1.5 Pro in video description tasks, achieving notable F1 score improvements on the DREAM-1K benchmark. Additionally, Tarsier2-7B sets new records across 15 public benchmarks, proving its effectiveness in various tasks such as video question-answering and video grounding."
                },
                "zh": {
                    "title": "Tarsier2ï¼šè§†é¢‘æè¿°çš„æ–°æ ‡æ†",
                    "desc": "Tarsier2æ˜¯ä¸€ç§å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºç”Ÿæˆè¯¦ç»†ä¸”å‡†ç¡®çš„è§†é¢‘æè¿°ï¼ŒåŒæ—¶å…·å¤‡å‡ºè‰²çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªå…³é”®å‡çº§å®ç°äº†æ˜¾è‘—è¿›æ­¥ï¼šé¦–å…ˆï¼Œé¢„è®­ç»ƒæ•°æ®ä»1100ä¸‡å¯¹è§†é¢‘æ–‡æœ¬æ‰©å±•åˆ°4000ä¸‡å¯¹ï¼Œå¢åŠ äº†æ•°æ®çš„æ•°é‡å’Œå¤šæ ·æ€§ï¼›å…¶æ¬¡ï¼Œåœ¨ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œç²¾ç»†çš„æ—¶é—´å¯¹é½ï¼›æœ€åï¼Œé‡‡ç”¨åŸºäºæ¨¡å‹çš„é‡‡æ ·è‡ªåŠ¨æ„å»ºåå¥½æ•°æ®ï¼Œå¹¶åº”ç”¨DPOè®­ç»ƒè¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTarsier2-7Båœ¨è§†é¢‘æè¿°ä»»åŠ¡ä¸­æŒç»­è¶…è¶Šé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶ä½œä¸ºå¼ºå¤§é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ ·æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-14.html",
    "link_next": "2025-01-16.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚\n\nzhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de fÄngfÇ, jiÃ ozuÃ² guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng (PRMs), yÃ²ngyÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i shÃ¹xuÃ© tuÄ«lÇ zhÅng de guÃ²chÃ©ng jiÃ ndÅ«. MÃ¹biÄo shÃ¬ shÃ­biÃ© hÃ© jiÇnshÇo tuÄ«lÇ guÃ²chÃ©ng zhÅng de cuÃ²wÃ¹. YÃ¡njiÅ« fÄxiÃ n, chÃ¡ngyÃ²ng de mÃ©ngtÃ¨kÇluÃ³ (MC) gÅ«jÃ¬ fÄngfÇ xiÃ ojiÃ , yÄ«nwÃ¨i tÄ yÄ«lÃ i wÃ¡nchÃ©ng mÃ³xÃ­ng pÃ­ngjiÃ  dÄngqiÃ¡n bÃ¹zhÃ²u de zhÃ¨ngquÃ¨xÃ¬ng, dÇozhÃ¬ bÃ¹zhÃ²u yÃ nzhÃ¨ng bÃ¹ zhÇ”nquÃ¨. WÃ©nzhÄng hÃ¡i zhÇchÅ« le chuÃ¡ntÇ’ng Best-of-N (BoN) pÃ­ngjiÃ  cÃ¨lÃ¼Ã¨ de piÄnchÄ, bÃ¬ng tÃ­chÅ« le yÄ« zhÇ’ng gÃ²ngshÃ¬ guÃ²lÇœ jÄ«zhÃ¬, jiÃ©hÃ© MC gÅ«jÃ¬ hÃ© LLM-as-a-judge, gÇijÃ¬n le mÃ³xÃ­ng xÃ¬ngnÃ©ng hÃ© shÃ¹jÃ¹ xiÃ oyÃ²ng. ZuÃ¬hÃ²u, wÃ©nzhÄng fÄbÃ¹ le yÄ«gÃ¨ xÄ«n de zuÃ¬ xiÄnjÃ¬n de PRM, bÃ¬ng tÃ­gÅng le wÃ¨ilÃ¡i yÃ¡njiÅ« de shÃ­yÃ²ng zhÇnÃ¡n.",
        "vocab": "[\n    {\"word\": \"è¿‡ç¨‹å¥–åŠ±æ¨¡å‹\", \"pinyin\": \"guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"å¤§å‹è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"æ•°å­¦æ¨ç†\", \"pinyin\": \"shÃ¹xuÃ© tuÄ«lÇ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"è¿‡ç¨‹ç›‘ç£\", \"pinyin\": \"guÃ²chÃ©ng jiÃ ndÅ«\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"è’™ç‰¹å¡ç½—\", \"pinyin\": \"mÃ©ngtÃ¨kÇluÃ³\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ«jÃ¬\", \"trans\": \"Estimation\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ«lÃ i\", \"trans\": \"Depend\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"Evaluate\"},\n    {\"word\": \"æ­¥éª¤éªŒè¯\", \"pinyin\": \"bÃ¹zhÃ²u yÃ nzhÃ¨ng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"åå·®\", \"pinyin\": \"piÄnchÄ\", \"trans\": \"Bias\"},\n    {\"word\": \"å…±è¯†è¿‡æ»¤æœºåˆ¶\", \"pinyin\": \"gÃ²ngshÃ­ guÃ²lÇœ jÄ«zhÃ¬\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬xiÄnjÃ¬n\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"å®ç”¨æŒ‡å—\", \"pinyin\": \"shÃ­yÃ²ng zhÇnÃ¡n\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}