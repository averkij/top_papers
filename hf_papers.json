{
    "date": {
        "ru": "17 февраля",
        "en": "February 17",
        "zh": "2月17日"
    },
    "time_utc": "2025-02-17 06:14",
    "weekday": 0,
    "issue_id": 2243,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.10389",
            "title": "Region-Adaptive Sampling for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.10389",
            "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.",
            "score": 31,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "8068f45b7fd0c2ee",
            "authors": [
                "Ziming Liu",
                "Yifan Yang",
                "Chengruidong Zhang",
                "Yiqi Zhang",
                "Lili Qiu",
                "Yang You",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение диффузионных трансформеров без потери качества",
                    "desc": "Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод основан на динамическом распределении различных коэффициентов сэмплирования для разных областей изображения, опираясь на фокус внимания модели Diffusion Transformer. RAS обновляет только те области, на которых в данный момент сфокусирована модель, используя для остальных областей кэшированный шум из предыдущего шага. Эксперименты показали, что RAS позволяет достичь ускорения до 2.51x на различных моделях при минимальном снижении качества генерации."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with RAS for Real-Time Generation",
                    "desc": "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."
                },
                "zh": {
                    "title": "提升扩散模型的实时性能",
                    "desc": "扩散模型（DMs）在生成任务中已成为首选，但其依赖多个顺序前向传递限制了实时性能。我们提出了一种新的无训练采样策略RAS，利用扩散变换器（DiTs）的灵活性，根据模型的关注点动态分配图像区域的采样比例。RAS只更新当前关注的区域，而其他区域则使用上一步的缓存噪声，从而提高了效率。我们的实验表明，RAS在生成质量几乎不下降的情况下，能够实现显著的加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09696",
            "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
            "url": "https://huggingface.co/papers/2502.09696",
            "abstract": "Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.",
            "score": 13,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "00f4f8e85ea27717",
            "authors": [
                "Jonathan Roberts",
                "Mohammad Reza Taesiri",
                "Ansh Sharma",
                "Akash Gupta",
                "Samuel Roberts",
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Jialu Tang",
                "Florian Langer",
                "Vyas Raina",
                "Vatsal Raina",
                "Hanyi Xiong",
                "Vishaal Udandarao",
                "Jingyi Lu",
                "Shiyang Chen",
                "Sam Purkis",
                "Tianshuo Yan",
                "Wenye Lin",
                "Gyungin Shin",
                "Qiaochu Yang",
                "Anh Totti Nguyen",
                "Kai Han",
                "Samuel Albanie"
            ],
            "affiliations": [
                "Auburn University",
                "Independent Researcher",
                "The University of Hong Kong",
                "University of Alberta",
                "University of Cambridge",
                "University of Oxford",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ZeroBench: невозможный визуальный тест для мультимодальных моделей",
                    "desc": "В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимодальных языковых моделей (LMM). Бенчмарк состоит из 100 вручную отобранных вопросов и 334 менее сложных подвопросов. Авторы оценили 20 LMM на ZeroBench, и все они показали результат 0%. Целью бенчмарка является создание сложного теста, который останется актуальным дольше, чем существующие визуальные бенчмарки."
                },
                "en": {
                    "title": "ZeroBench: Raising the Bar for Visual Reasoning in LMMs",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding."
                },
                "zh": {
                    "title": "ZeroBench：推动视觉理解的新基准",
                    "desc": "大型多模态模型（LMMs）在图像理解方面存在显著不足，甚至在某些方面的空间认知能力不如小孩或动物。尽管如此，它们在许多流行的视觉基准测试中得分很高，但随着模型进步的加速，这些基准的挑战性迅速降低。为了解决这个问题，我们提出了ZeroBench，这是一个轻量级的视觉推理基准，当前的前沿LMMs完全无法解决。ZeroBench包含100个手动策划的问题和334个较简单的子问题，我们对20个LMMs进行了评估，结果均为0.0%，并对错误进行了严格分析。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10391",
            "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
            "url": "https://huggingface.co/papers/2502.10391",
            "abstract": "Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.",
            "score": 9,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "c47a89fda79a1a4b",
            "authors": [
                "Yi-Fan Zhang",
                "Tao Yu",
                "Haochen Tian",
                "Chaoyou Fu",
                "Peiyan Li",
                "Jianshu Zeng",
                "Wulin Xie",
                "Yang Shi",
                "Huanyu Zhang",
                "Junkang Wu",
                "Xue Wang",
                "Yibo Hu",
                "Bin Wen",
                "Fan Yang",
                "Zhang Zhang",
                "Tingting Gao",
                "Di Zhang",
                "Liang Wang",
                "Rong Jin",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Alibaba",
                "CASIA",
                "KuaiShou",
                "Meta AI",
                "NJU",
                "PKU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10391.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#training",
                    "#interpretability",
                    "#open_source",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями",
                    "desc": "Исследователи представили MM-RLHF - набор данных из 120 тысяч размеченных пар сравнения предпочтений для мультимодальных языковых моделей. На его основе они разработали новые методы обучения с подкреплением, включая модель вознаграждения на основе критики и динамическое масштабирование вознаграждений. Эксперименты показали значительное улучшение способностей модели LLaVA-ov-7B в диалогах и безопасности после дообучения с использованием предложенных методов. Авторы открыли доступ к набору данных, моделям и коду для воспроизведения результатов."
                },
                "en": {
                    "title": "Enhancing MLLM Alignment with Human Preferences",
                    "desc": "This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics."
                },
                "zh": {
                    "title": "提升多模态模型与人类偏好的对齐",
                    "desc": "尽管多模态大型语言模型（MLLMs）取得了显著进展，但大多数最先进的模型尚未与人类偏好进行充分对齐。为了解决这一问题，我们引入了MM-RLHF数据集，包含12万个细粒度的人类标注偏好比较对，显著提升了现有资源的规模和质量。我们提出了基于批评的奖励模型，能够在评分前生成模型输出的批评，从而提供更具可解释性和信息量的反馈。此外，我们还提出了动态奖励缩放方法，根据奖励信号调整每个样本的损失权重，以优化高质量比较对的使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10248",
            "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
            "url": "https://huggingface.co/papers/2502.10248",
            "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
            "score": 7,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "356bc046cc5f59e5",
            "authors": [
                "Guoqing Ma",
                "Haoyang Huang",
                "Kun Yan",
                "Liangyu Chen",
                "Nan Duan",
                "Shengming Yin",
                "Changyi Wan",
                "Ranchen Ming",
                "Xiaoniu Song",
                "Xing Chen",
                "Yu Zhou",
                "Deshan Sun",
                "Deyu Zhou",
                "Jian Zhou",
                "Kaijun Tan",
                "Kang An",
                "Mei Chen",
                "Wei Ji",
                "Qiling Wu",
                "Wen Sun",
                "Xin Han",
                "Yanan Wei",
                "Zheng Ge",
                "Aojie Li",
                "Bin Wang",
                "Bizhu Huang",
                "Bo Wang",
                "Brian Li",
                "Changxing Miao",
                "Chen Xu",
                "Chenfei Wu",
                "Chenguang Yu",
                "Dapeng Shi",
                "Dingyuan Hu",
                "Enle Liu",
                "Gang Yu",
                "Ge Yang",
                "Guanzhe Huang",
                "Gulin Yan",
                "Haiyang Feng",
                "Hao Nie",
                "Haonan Jia",
                "Hanpeng Hu",
                "Hanqi Chen",
                "Haolong Yan",
                "Heng Wang",
                "Hongcheng Guo",
                "Huilin Xiong",
                "Huixin Xiong",
                "Jiahao Gong",
                "Jianchang Wu",
                "Jiaoren Wu",
                "Jie Wu",
                "Jie Yang",
                "Jiashuai Liu",
                "Jiashuo Li",
                "Jingyang Zhang",
                "Junjing Guo",
                "Junzhe Lin",
                "Kaixiang Li",
                "Lei Liu",
                "Lei Xia",
                "Liang Zhao",
                "Liguo Tan",
                "Liwen Huang",
                "Liying Shi",
                "Ming Li",
                "Mingliang Li",
                "Muhua Cheng",
                "Na Wang",
                "Qiaohui Chen",
                "Qinglin He",
                "Qiuyan Liang",
                "Quan Sun",
                "Ran Sun",
                "Rui Wang",
                "Shaoliang Pang",
                "Shiliang Yang",
                "Sitong Liu",
                "Siqi Liu",
                "Shuli Gao",
                "Tiancheng Cao",
                "Tianyu Wang",
                "Weipeng Ming",
                "Wenqing He",
                "Xu Zhao",
                "Xuelin Zhang",
                "Xianfang Zeng",
                "Xiaojia Liu",
                "Xuan Yang",
                "Yaqi Dai",
                "Yanbo Yu",
                "Yang Li",
                "Yineng Deng",
                "Yingming Wang",
                "Yilei Wang",
                "Yuanwei Lu",
                "Yu Chen",
                "Yu Luo",
                "Yuchu Luo",
                "Yuhe Yin",
                "Yuheng Feng",
                "Yuxiang Yang",
                "Zecheng Tang",
                "Zekai Zhang",
                "Zidong Yang",
                "Binxing Jiao",
                "Jiansheng Chen",
                "Jing Li",
                "Shuchang Zhou",
                "Xiangyu Zhang",
                "Xinhao Zhang",
                "Yibo Zhu",
                "Heung-Yeung Shum",
                "Daxin Jiang"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10248.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в генерации видео: от текста к реалистичным роликам",
                    "desc": "Представлена модель Step-Video-T2V - предобученная нейросеть для генерации видео по текстовому описанию с 30 миллиардами параметров. Модель использует глубокий вариационный автоэнкодер Video-VAE для сжатия видео и два двуязычных текстовых энкодера для обработки запросов на английском и китайском языках. Для улучшения качества генерируемых видео применяется подход Video-DPO на основе предпочтений. Модель демонстрирует передовое качество генерации видео по сравнению с открытыми и коммерческими аналогами."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Step-Video-T2V",
                    "desc": "Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models."
                },
                "zh": {
                    "title": "创新视频生成，赋能内容创作者",
                    "desc": "本文介绍了一种名为Step-Video-T2V的先进文本到视频预训练模型，具有300亿参数，能够生成最长204帧的视频。我们设计了一种深度压缩变分自编码器（Video-VAE），在视频生成任务中实现了16x16的空间压缩和8x的时间压缩，同时保持了卓越的视频重建质量。用户提示通过双语文本编码器进行编码，以处理英语和中文。我们还讨论了当前扩散模型范式的局限性，并概述了视频基础模型的未来方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09992",
            "title": "Large Language Diffusion Models",
            "url": "https://huggingface.co/papers/2502.09992",
            "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.",
            "score": 4,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "5117e8f17ba51f92",
            "authors": [
                "Shen Nie",
                "Fengqi Zhu",
                "Zebin You",
                "Xiaolu Zhang",
                "Jingyang Ou",
                "Jun Hu",
                "Jun Zhou",
                "Yankai Lin",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09992.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Диффузионные модели бросают вызов авторегрессии в обработке языка",
                    "desc": "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернативную авторегрессионным моделям. LLaDA использует процесс маскирования данных и обратный процесс для предсказания замаскированных токенов, оптимизируя границу правдоподобия. Модель демонстрирует сильную масштабируемость и превосходит базовые авторегрессионные модели в различных тестах. LLaDA показывает впечатляющие результаты в задачах обучения в контексте и следования инструкциям, а также решает проблему 'проклятия обращения'."
                },
                "en": {
                    "title": "LLaDA: A New Era for Language Models Beyond Autoregression",
                    "desc": "This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs."
                },
                "zh": {
                    "title": "扩散模型：自回归模型的新挑战",
                    "desc": "自回归模型（ARMs）被广泛认为是大型语言模型（LLMs）的基石。本文提出了LLaDA，这是一种从头开始训练的扩散模型，采用预训练和监督微调（SFT）的方法。LLaDA通过前向数据掩蔽过程和反向过程建模分布，并使用普通Transformer预测被掩蔽的标记。研究表明，LLaDA在多个基准测试中表现出强大的可扩展性，超越了自构建的ARMs基线，证明了扩散模型作为ARMs的可行替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09741",
            "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
            "url": "https://huggingface.co/papers/2502.09741",
            "abstract": "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.",
            "score": 4,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "adb30f7d3d01ce3a",
            "authors": [
                "Tianyi Zhou",
                "Deqing Fu",
                "Mahdi Soltanolkotabi",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "affiliations": [
                "Department of Computer Science University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "FoNE: революция в обработке чисел для языковых моделей",
                    "desc": "Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). FoNE представляет числа в виде единых токенов с использованием фурье-подобных признаков, что позволяет эффективно захватывать числовые значения без фрагментации. Этот компактный способ представления ускоряет как обучение, так и вывод модели. По сравнению с традиционными методами, FoNE демонстрирует более высокую точность в различных числовых задачах, включая сложение, вычитание и умножение, при меньших вычислительных затратах."
                },
                "en": {
                    "title": "Revolutionizing Number Representation in LLMs with FoNE",
                    "desc": "This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication."
                },
                "zh": {
                    "title": "傅里叶数字嵌入：高效的数字表示方法",
                    "desc": "大型语言模型（LLMs）通常使用多个标记来表示数字，这导致模型在理解数值时需要聚合这些标记，降低了训练和推理的效率。我们提出了一种新方法，称为傅里叶数字嵌入（FoNE），它将数字直接映射到嵌入空间，使用傅里叶特征进行编码。FoNE将每个数字编码为一个单一标记，显著减少了计算开销，并在加法、减法和乘法等数值任务中实现了更高的准确性。与传统的子词和数字嵌入相比，FoNE在6位十进制加法中所需的数据量减少了64倍，同时每个数字使用的标记数量也减少了3倍到6倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09955",
            "title": "Diverse Inference and Verification for Advanced Reasoning",
            "url": "https://huggingface.co/papers/2502.09955",
            "abstract": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.",
            "score": 3,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "10eaccfc7377f600",
            "authors": [
                "Iddo Drori",
                "Gaston Longhitano",
                "Mao Mao",
                "Seunghwan Hyun",
                "Yuke Zhang",
                "Sungjun Park",
                "Zachary Meeks",
                "Xin-Yu Zhang",
                "Ben Segev",
                "Howard Yong",
                "Nakul Verma",
                "Avi Shporer",
                "Alon Amit",
                "Madeleine Udell"
            ],
            "affiliations": [
                "Boston University",
                "Columbia University",
                "Google",
                "Intuit",
                "Massachusetts Institute of Technology",
                "NotBadMath.AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09955.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#training",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление LLM для решения сложных задач: многомодельный подход",
                    "desc": "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложных математических и логических задач. Авторы применяют комбинацию нескольких моделей и методов, включая верификацию решений и отбор лучших ответов. Подход значительно повышает точность на задачах Международной математической олимпиады, Humanity's Last Exam и Abstraction and Reasoning Corpus. Исследователи используют симуляции, обучение с подкреплением и мета-обучение для адаптации представлений агентов и улучшения обобщающей способности."
                },
                "en": {
                    "title": "Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!",
                    "desc": "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."
                },
                "zh": {
                    "title": "提升推理模型在高级数学问题上的准确性",
                    "desc": "本文探讨了推理型大语言模型在解决高级数学和编程任务中的挑战，特别是国际数学奥林匹克（IMO）组合问题、抽象与推理语料库（ARC）难题和人类最后考试（HLE）问题。我们提出了一种多模型和多方法结合的推理方法，在测试时进行多样化推理。通过自动验证IMO问题和ARC难题的解答正确性，我们显著提高了这些问题的解答准确率。我们的研究方法可靠、稳健且可扩展，旨在推动可重复研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10177",
            "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
            "url": "https://huggingface.co/papers/2502.10177",
            "abstract": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.",
            "score": 3,
            "issue_id": 2240,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "1b17b668b26c2264",
            "authors": [
                "Mingcong Lei",
                "Yiming Zhao",
                "Ge Wang",
                "Zhixin Mai",
                "Shuguang Cui",
                "Yatong Han",
                "Jinke Ren"
            ],
            "affiliations": [
                "FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China",
                "Harbin Engineering University, China",
                "Infused Synapse AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10177.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Пространственно-временная память повышает эффективность воплощенных агентов",
                    "desc": "Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения задач агентами с воплощенным интеллектом. STMA включает в себя модуль пространственно-временной памяти, динамический граф знаний и механизм планировщика-критика. Эксперименты в среде TextWorld показали значительное улучшение успешности и средней оценки по сравнению с современными моделями. Результаты подчеркивают эффективность пространственно-временной памяти для улучшения возможностей памяти воплощенных агентов."
                },
                "en": {
                    "title": "Enhancing Agent Intelligence with Spatio-Temporal Memory",
                    "desc": "The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence."
                },
                "zh": {
                    "title": "时空记忆智能体：提升智能体决策与适应能力的关键",
                    "desc": "本文提出了一种新的框架，称为时空记忆智能体（STMA），旨在提高智能体在动态环境中执行长期任务的能力。STMA集成了时空记忆模块、动态知识图谱和规划-评估机制，以增强任务规划和执行的效果。通过在TextWorld环境中进行32个任务的评估，STMA在成功率和平均得分上分别提高了31.25%和24.7%。实验结果表明，时空记忆在提升智能体的记忆能力方面具有显著效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09638",
            "title": "Jailbreaking to Jailbreak",
            "url": "https://huggingface.co/papers/2502.09638",
            "abstract": "Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.",
            "score": 1,
            "issue_id": 2242,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 февраля",
                "en": "February 9",
                "zh": "2月9日"
            },
            "hash": "3c2ed560e12b971a",
            "authors": [
                "Jeremy Kritz",
                "Vaughn Robinson",
                "Robert Vacareanu",
                "Bijan Varjavand",
                "Michael Choi",
                "Bobby Gogov",
                "Scale Red Team",
                "Summer Yue",
                "Willow E. Primack",
                "Zifan Wang"
            ],
            "affiliations": [
                "Scale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09638.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#multimodal",
                    "#training",
                    "#security",
                    "#rlhf"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "LLM против LLM: новый фронт в безопасности ИИ",
                    "desc": "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красной команды'. Авторы демонстрируют, как взломанная модель (J_2) может систематически оценивать и атаковать другие модели, достигая высокого уровня успеха. Эксперименты показывают, что Sonnet 3.5 и Gemini 1.5 pro превосходят другие LLM в роли J_2, достигая 93.0% и 91.0% успешности атак соответственно. Исследование подчеркивает уязвимость существующих механизмов защиты и предлагает новый метод для улучшения безопасности искусственного интеллекта."
                },
                "en": {
                    "title": "Jailbreaking the Safeguards: A New Approach to LLM Red Teaming",
                    "desc": "This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety."
                },
                "zh": {
                    "title": "破解自我保护的红队策略",
                    "desc": "本论文提出了一种新的方法，利用大型语言模型（LLM）作为红队成员，来评估和改进拒绝训练模型的安全性。我们称这些被破解的LLM为J_2攻击者，它们能够通过上下文学习从之前的失败中提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5在攻击成功率上优于其他LLM，分别达到了93.0%和91.0%。我们的研究不仅提供了一种可扩展的红队策略，还揭示了破解自身保护机制的潜在风险。"
                }
            }
        }
    ],
    "link_prev": "2025-02-14.html",
    "link_next": "2025-02-18.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2月14日"
    },
    "short_date_next": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2月18日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）处理超长文本时面临的挑战，包括推理速度慢和内存成本高。为解决这些问题，作者提出了InfiniteHiP，一种新的推理框架，通过动态删除无关的上下文标记来加速处理。该方法还通过选择性应用RoPE调整方法来泛化到更长的序列，并将键值缓存卸载到主机内存中，减少GPU内存压力。结果显示，InfiniteHiP可以在单个L40s 48GB GPU上处理多达300万个标记，速度提高了18.95倍。",
        "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）处理超长文本时面临的挑战，包括推理速度慢和内存成本高。为解决这些问题，作者提出了InfiniteHiP，一种新的推理框架，通过动态删除无关的上下文标记来加速处理。该方法还通过选择性应用RoPE调整方法来泛化到更长的序列，并将键值缓存卸载到主机内存中，减少GPU内存压力。结果显示，InfiniteHiP可以在单个L40s 48GB GPU上处理多达300万个标记，速度提高了18.95倍。\n\nzhè piān wén zhāng tǎo lùn le dà yǔ yán mó xíng (LLMs) chǔ lǐ chāo cháng wén běn shí miàn lín de tiǎo zhàn, bāo kuò tuī lǐ sù dù màn hé nèi cún chéng běn gāo. wèi jiě jué zhè xiē wèn tí, zuò zhě tí chū le InfiniteHiP, yī zhǒng xīn de tuī lǐ kuàng jià, tōng guò dòng tài shān chú wú guān de shàng xià wén biāo jì lái jiā sù chǔ lǐ. gǎi fǎng fǎ hái tōng guò xuǎn zé xìng yìng yòng RoPE tiáo zhěng fǎng fǎ lái fàn huà dào gèng cháng de xù liè, bìng jiāng jiàn zhí huàn cún xiè zài zhǔ jī nèi cún zhōng, jiǎn shǎo GPU nèi cún yā lì. jié guǒ xiǎn shì, InfiniteHiP kě yǐ zài dān gè L40s 48GB GPU shàng chǔ lǐ duō dà 300 wàn gè biāo jì, sù dù tí gāo le 18.95 bèi.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'},\n{'word': '超长', 'pinyin': 'chāo cháng', 'trans': 'ultra-long'},\n{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},\n{'word': '面临', 'pinyin': 'miàn lín', 'trans': 'face'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '内存', 'pinyin': 'nèi cún', 'trans': 'memory'},\n{'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '删除', 'pinyin': 'shān chú', 'trans': 'delete'},\n{'word': '无关', 'pinyin': 'wú guān', 'trans': 'irrelevant'},\n{'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'},\n{'word': '选择性', 'pinyin': 'xuǎn zé xìng', 'trans': 'selective'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'},\n{'word': 'RoPE', 'pinyin': 'RoPE', 'trans': 'RoPE'},\n{'word': '调整', 'pinyin': 'tiáo zhěng', 'trans': 'adjust'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '泛化', 'pinyin': 'fàn huà', 'trans': 'generalize'},\n{'word': '序列', 'pinyin': 'xù liè', 'trans': 'sequence'},\n{'word': '键值', 'pinyin': 'jiàn zhí', 'trans': 'key-value'},\n{'word': '缓存', 'pinyin': 'huǎn cún', 'trans': 'cache'},\n{'word': '卸载', 'pinyin': 'xiè zài', 'trans': 'unload'},\n{'word': '主机', 'pinyin': 'zhǔ jī', 'trans': 'host'},\n{'word': '压力', 'pinyin': 'yā lì', 'trans': 'pressure'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'},\n{'word': '单个', 'pinyin': 'dān gè', 'trans': 'single'},\n{'word': 'L40s', 'pinyin': 'L40s', 'trans': 'L40s'},\n{'word': '48GB', 'pinyin': '48GB', 'trans': '48GB'},\n{'word': 'GPU', 'pinyin': 'GPU', 'trans': 'GPU'},\n{'word': '多达', 'pinyin': 'duō dá', 'trans': 'up to'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '倍', 'pinyin': 'bèi', 'trans': 'times'}]",
        "trans": "This article discusses the challenges faced by large language models (LLMs) when processing extremely long texts, including slow inference speed and high memory costs. To address these issues, the authors propose InfiniteHiP, a new inference framework that accelerates processing by dynamically removing irrelevant context tokens. This method also generalizes to longer sequences by selectively applying the RoPE adjustment method and offloads key-value caching to host memory, reducing GPU memory pressure. The results show that InfiniteHiP can handle up to 30 million tokens on a single L40s 48GB GPU, with a speed increase of 18.95 times.",
        "update_ts": "2025-02-16 12:40"
    }
}