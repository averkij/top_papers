{
    "date": {
        "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 12",
        "zh": "3æœˆ12æ—¥"
    },
    "time_utc": "2025-03-12 03:21",
    "weekday": 2,
    "issue_id": 2654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07920",
            "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
            "url": "https://huggingface.co/papers/2503.07920",
            "abstract": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.",
            "score": 11,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "32c690b6ffb8b143",
            "authors": [
                "Samuel Cahyawijaya",
                "Holy Lovenia",
                "Joel Ruben Antony Moniz",
                "Tack Hwa Wong",
                "Mohammad Rifqi Farhansyah",
                "Thant Thiri Maung",
                "Frederikus Hudi",
                "David Anugraha",
                "Muhammad Ravi Shulthan Habibi",
                "Muhammad Reza Qorib",
                "Amit Agarwal",
                "Joseph Marvin Imperial",
                "Hitesh Laxmichand Patel",
                "Vicky Feliren",
                "Bahrul Ilmi Nasution",
                "Manuel Antonio Rufino",
                "Genta Indra Winata",
                "Rian Adam Rajagede",
                "Carlos Rafael Catalan",
                "Mohamed Fazli Imam",
                "Priyaranjan Pattnayak",
                "Salsabila Zahirah Pranida",
                "Kevin Pratama",
                "Yeshil Bangera",
                "Adisai Na-Thalang",
                "Patricia Nicole Monderin",
                "Yueqi Song",
                "Christian Simon",
                "Lynnette Hui Xian Ng",
                "Richardy Lobo' Sapan",
                "Taki Hasan Rafi",
                "Bin Wang",
                "Supryadi",
                "Kanyakorn Veerakanjana",
                "Piyalitt Ittichaiwong",
                "Matthew Theodore Roque",
                "Karissa Vincentio",
                "Takdanai Kreangphet",
                "Phakphum Artkaew",
                "Kadek Hendrawan Palgunadi",
                "Yanzhi Yu",
                "Rochana Prih Hastuti",
                "William Nixon",
                "Mithil Bangera",
                "Adrian Xuan Wei Lim",
                "Aye Hninn Khine",
                "Hanif Muhammad Zhafran",
                "Teddy Ferdinan",
                "Audra Aurora Izzani",
                "Ayushman Singh",
                "Evan",
                "Jauza Akbar Krito",
                "Michael Anugraha",
                "Fenal Ashokbhai Ilasariya",
                "Haochen Li",
                "John Amadeo Daniswara",
                "Filbert Aurelian Tjiaranata",
                "Eryawan Presma Yulianrifat",
                "Can Udomcharoenchaikit",
                "Fadil Risdian Ansori",
                "Mahardika Krisna Ihsani",
                "Giang Nguyen",
                "Anab Maulana Barik",
                "Dan John Velasco",
                "Rifo Ahmad Genadi",
                "Saptarshi Saha",
                "Chengwei Wei",
                "Isaiah Flores",
                "Kenneth Ko Han Chen",
                "Anjela Gail Santos",
                "Wan Shen Lim",
                "Kaung Si Phyo",
                "Tim Santos",
                "Meisyarah Dwiastuti",
                "Jiayun Luo",
                "Jan Christian Blaise Cruz",
                "Ming Shan Hee",
                "Ikhlasul Akmal Hanif",
                "M. Alif Al Hakim",
                "Muhammad Rizky Sya'ban",
                "Kun Kerdthaisong",
                "Lester James V. Miranda",
                "Fajri Koto",
                "Tirana Noor Fatyanosa",
                "Alham Fikri Aji",
                "Jostin Jerico Rosal",
                "Jun Kevin",
                "Robert Wijaya",
                "Onno P. Kampman",
                "Ruochen Zhang",
                "BÃ¶rje F. Karlsson",
                "Peerat Limkonchotiwat"
            ],
            "affiliations": [
                "AI Singapore",
                "Allen AI",
                "Ateneo de Manila University",
                "Auburn University",
                "Bandung Institute of Technology",
                "Beijing Academy of Artificial Intelligence (BAAI)",
                "Binus University",
                "Brawijaya University",
                "Brown University",
                "Capital One",
                "Carnegie Mellon University",
                "Chulalongkorn University",
                "Cohere",
                "Dataxet:Sonar",
                "Faculty of Medicine Siriraj Hospital, Mahidol University",
                "Graphcore",
                "Hanyang University",
                "Independent",
                "Indian Statistical Institute, Kolkata",
                "IndoNLP",
                "Institut Teknologi Sepuluh Nopember",
                "Institute for Infocomm Research, Singapore",
                "King Mongkuts University of Technology Thonburi",
                "MBZUAI",
                "MOH Office for Healthcare Transformation",
                "Macau University of Science and Technology",
                "Meta",
                "Mila - Quebec AI Institute",
                "Monash University, Indonesia",
                "Nara Institute of Science and Technology",
                "National University Philippines",
                "National University of Singapore",
                "New York University",
                "Oracle",
                "Polytechnique Montreal",
                "SCB 10X",
                "SEACrowd",
                "Samsung R&D Institute Philippines",
                "Seoul National University of Science and Technology",
                "Singapore Polytechnic",
                "Singapore University of Technology and Design",
                "Sony Group Corporation",
                "Srinakharinwirot University",
                "Thammasat University",
                "The University of Manchester",
                "Tianjin University",
                "Ton Duc Thang University",
                "Universitas Gadjah Mada",
                "Universitas Islam Indonesia",
                "Universitas Pelita Harapan",
                "University of Bath",
                "University of Illiinois, Urbana-Champaign",
                "University of Indonesia",
                "University of New Haven",
                "University of Toronto",
                "University of the Philippines",
                "Vidyasirimedhi Institute of Science and Technology",
                "Works Applications",
                "WrocÅ‚aw Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07920.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEA-VL - Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ 1,28 Ğ¼Ğ»Ğ½ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Cultural Gap in AI with SEA-VL",
                    "desc": "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."
                },
                "zh": {
                    "title": "å¡«è¡¥ä¸œå—äºšæ–‡åŒ–åœ¨AIç ”ç©¶ä¸­çš„ç©ºç™½",
                    "desc": "ä¸œå—äºšåœ°åŒºè¯­è¨€å’Œæ–‡åŒ–å¤šæ ·æ€§æä¸ºä¸°å¯Œï¼Œä½†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å´ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEA-VLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„é¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºä¸œå—äºšè¯­è¨€å¼€å‘é«˜è´¨é‡ã€æ–‡åŒ–ç›¸å…³çš„æ•°æ®ã€‚é€šè¿‡å¸å¼•æ¥è‡ªä¸œå—äºšå›½å®¶çš„è´¡çŒ®è€…ï¼ŒSEA-VLç¡®ä¿äº†æ›´å¥½çš„æ–‡åŒ–ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œä¿ƒè¿›äº†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å¯¹è¢«ä½ä¼°è¯­è¨€çš„åŒ…å®¹æ€§ã€‚æˆ‘ä»¬æ”¶é›†äº†128ä¸‡å¼ ä¸ä¸œå—äºšæ–‡åŒ–ç›¸å…³çš„å›¾åƒï¼Œè¿œè¶…ç°æœ‰æ•°æ®é›†çš„è§„æ¨¡ï¼Œæ—¨åœ¨ç¼©å°ä¸œå—äºšçš„ä»£è¡¨æ€§å·®è·ï¼Œæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07703",
            "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
            "url": "https://huggingface.co/papers/2503.07703",
            "abstract": "Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.",
            "score": 10,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "00cd4369f3c531f9",
            "authors": [
                "Lixue Gong",
                "Xiaoxia Hou",
                "Fanshi Li",
                "Liang Li",
                "Xiaochen Lian",
                "Fei Liu",
                "Liyang Liu",
                "Wei Liu",
                "Wei Lu",
                "Yichun Shi",
                "Shiqi Sun",
                "Yu Tian",
                "Zhi Tian",
                "Peng Wang",
                "Xun Wang",
                "Ye Wang",
                "Guofeng Wu",
                "Jie Wu",
                "Xin Xia",
                "Xuefeng Xiao",
                "Linjie Yang",
                "Zhonghua Zhai",
                "Xinyu Zhang",
                "Qi Zhang",
                "Yuwei Zhang",
                "Shijia Zhao",
                "Jianchao Yang",
                "Weilin Huang"
            ],
            "affiliations": [
                "Seed Vision Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07703.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Seedream 2.0: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Seedream 2.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Glyph-Aligned ByT5 Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Scaled ROPE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT Ğ¸ RLHF, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Seedream 2.0: Bridging Cultures in Image Generation",
                    "desc": "This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality."
                },
                "zh": {
                    "title": "åŒè¯­å›¾åƒç”Ÿæˆçš„æœªæ¥ï¼šSeedream 2.0",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Seedream 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­çš„å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“å’Œæ–‡åŒ–ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸­æ–‡å’Œè‹±æ–‡çš„æ–‡æœ¬æç¤ºï¼Œæ”¯æŒåŒè¯­å›¾åƒç”Ÿæˆï¼Œå¹¶é€šè¿‡å¼ºå¤§çš„æ•°æ®ç³»ç»Ÿå’Œæè¿°ç³»ç»Ÿæå‡å›¾åƒæè¿°çš„å‡†ç¡®æ€§å’Œä¸°å¯Œæ€§ã€‚Seedream 2.0ç»“åˆäº†è‡ªç ”çš„åŒè¯­å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æµ·é‡æ•°æ®ä¸­ç›´æ¥å­¦ä¹ æœ¬åœŸçŸ¥è¯†ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå‡†ç¡®è¡¨è¾¾æ–‡åŒ–ç»†èŠ‚å’Œç¾å­¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šé˜¶æ®µçš„åè®­ç»ƒä¼˜åŒ–ï¼ŒSeedream 2.0åœ¨å¤šä¸ªæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬éµå¾ªæç¤ºã€å®¡ç¾ã€æ–‡æœ¬æ¸²æŸ“å’Œç»“æ„æ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07604",
            "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
            "url": "https://huggingface.co/papers/2503.07604",
            "abstract": "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.",
            "score": 10,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "313594788f663498",
            "authors": [
                "Tianhe Lin",
                "Jian Xie",
                "Siyu Yuan",
                "Deqing Yang"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University",
                "School of Data Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07604.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#math",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµÑĞ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: ÑĞ¸Ğ»Ğ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµÑ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑÑ…Ğ¾Ğ¶ĞµĞ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°, Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking Implicit Reasoning in Language Models",
                    "desc": "This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities."
                },
                "zh": {
                    "title": "éšå¼æ¨ç†çš„æ·å¾„ä¸å±€é™æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨æµ‹è¯•æ—¶è®¡ç®—ä¸­ï¼Œéšå¼æ¨ç†ä¸æ˜¾å¼æ¨ç†çš„æ•ˆç‡å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡éšå¼æ¨ç†å®ç°é€æ­¥æ¨ç†å¹¶åœ¨å¤šæ­¥ä»»åŠ¡ä¸­å–å¾—é«˜å‡†ç¡®ç‡ã€‚ç›¸åï¼Œåœ¨éå›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„éšå¼æ¨ç†èƒ½åŠ›å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æ€»çš„æ¥è¯´ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹ä¸åŒæ¨¡å¼æ—¶è¡¨ç°ä¸ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08605",
            "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
            "url": "https://huggingface.co/papers/2503.08605",
            "abstract": "While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.",
            "score": 7,
            "issue_id": 2653,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "6f7bf7b6c171af43",
            "authors": [
                "Subin Kim",
                "Seoung Wug Oh",
                "Jui-Hsien Wang",
                "Joon-Young Lee",
                "Jinwoo Shin"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08605.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "SynCoS: Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Synchronized Coupled Sampling (SynCoS). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. SynCoS ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑˆÑƒĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SynCoS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Achieving Long-Range Coherence in Video Generation with SynCoS",
                    "desc": "This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency."
                },
                "zh": {
                    "title": "åŒæ­¥è€¦åˆé‡‡æ ·ï¼šæå‡é•¿è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œç§°ä¸ºåŒæ­¥è€¦åˆé‡‡æ ·ï¼ˆSynCoSï¼‰ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚é€šè¿‡åŒæ­¥å»å™ªè·¯å¾„ï¼ŒSynCoSç¡®ä¿äº†ç›¸é‚»å¸§å’Œè¿œç¨‹å¸§ä¹‹é—´çš„é•¿èŒƒå›´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åå‘é‡‡æ ·å’ŒåŸºäºä¼˜åŒ–çš„é‡‡æ ·ç­–ç•¥ï¼Œä»¥å®ç°å±€éƒ¨å¹³æ»‘è¿‡æ¸¡å’Œå…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynCoSåœ¨å¤šäº‹ä»¶é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¹³æ»‘çš„è¿‡æ¸¡å’Œæ›´å¥½çš„é•¿èŒƒå›´è¯­ä¹‰ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07860",
            "title": "Video Action Differencing",
            "url": "https://huggingface.co/papers/2503.07860",
            "abstract": "How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.",
            "score": 5,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "76b8b9c677de83cd",
            "authors": [
                "James Burgess",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Anita Rau",
                "Alejandro Lozano",
                "Lisa Dunlap",
                "Trevor Darrell",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07860.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Video Action Differencing (VidDiff), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VidDiffBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 549 Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ VidDiff, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unveiling Subtle Differences in Action Videos with VidDiff",
                    "desc": "This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance."
                },
                "zh": {
                    "title": "è§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼šè¯†åˆ«ç»†å¾®å·®åˆ«çš„æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼ˆVidDiffï¼‰ï¼Œæ—¨åœ¨è¯†åˆ«åŒä¸€åŠ¨ä½œè§†é¢‘ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†VidDiffBenchï¼Œä¸€ä¸ªåŒ…å«549å¯¹è§†é¢‘çš„åŸºå‡†æ•°æ®é›†ï¼Œæ ‡æ³¨äº†4469ä¸ªç»†ç²’åº¦åŠ¨ä½œå·®å¼‚å’Œ2075ä¸ªæ—¶é—´æˆ³ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVidDiffBenchå¯¹ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å±€éƒ¨åŒ–ç›¸å…³å­åŠ¨ä½œå’Œç»†ç²’åº¦å¸§æ¯”è¾ƒæ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VidDiffæ–¹æ³•ï¼Œå°†ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåŠ¨ä½œå·®å¼‚æè®®ã€å…³é”®å¸§å®šä½å’Œå¸§å·®å¼‚åŒ–ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½åˆ©ç”¨äº†ä¸“é—¨çš„åŸºç¡€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07572",
            "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2503.07572",
            "abstract": "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.",
            "score": 5,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "49ca445bc3322f0d",
            "authors": [
                "Yuxiao Qu",
                "Matthew Y. R. Yang",
                "Amrith Setlur",
                "Lewis Tunstall",
                "Edward Emanuel Beeching",
                "Ruslan Salakhutdinov",
                "Aviral Kumar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07572.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Meta Reinforcement Fine-Tuning (MRT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° 'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ' Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. MRT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Optimizing Test-Time Compute for Enhanced LLM Reasoning",
                    "desc": "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—çš„é—®é¢˜å½¢å¼åŒ–ä¸ºå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è§†è§’æ¥æ”¯é…æµ‹è¯•æ—¶è®¡ç®—ã€‚é€šè¿‡å°†LLMçš„è¾“å‡ºæµè§†ä¸ºå¤šä¸ªæµ‹è¯•æ—¶çš„å›åˆï¼Œå¹¶ä½¿ç”¨ç´¯ç§¯é—æ†¾çš„æ¦‚å¿µæ¥è¡¡é‡æµ‹è¯•æ—¶è®¡ç®—çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºå…ƒå¼ºåŒ–å¾®è°ƒï¼ˆMRTï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿçš„ç»“æœå¥–åŠ±RLæ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†2-3å€ï¼Œä»¤ä»¤ç‰Œæ•ˆç‡æé«˜äº†çº¦1.5å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07536",
            "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
            "url": "https://huggingface.co/papers/2503.07536",
            "abstract": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.",
            "score": 3,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "59c304598f64f1e6",
            "authors": [
                "Yingzhe Peng",
                "Gongrui Zhang",
                "Miaosen Zhang",
                "Zhiyuan You",
                "Jie Liu",
                "Qipeng Zhu",
                "Kai Yang",
                "Xingzhong Xu",
                "Xin Geng",
                "Xu Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Fudan University",
                "Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07536.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ 3B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-Instruct-3B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Boosting Reasoning in Multimodal Models Efficiently",
                    "desc": "This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data."
                },
                "zh": {
                    "title": "å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­å¢å¼ºæ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°é‡ä¸º3Bçš„ç´§å‡‘æ¶æ„ä¸­ï¼Œè§†è§‰æ„ŸçŸ¥ä¸é€»è¾‘æ¨ç†ä¹‹é—´çš„å¤æ‚äº’åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º\textit{method}çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡åŸºç¡€æ¨ç†å¢å¼ºï¼ˆFREï¼‰å’Œå¤šæ¨¡æ€æ³›åŒ–è®­ç»ƒï¼ˆMGTï¼‰æ¥é€‚åº”å¤šæ¨¡æ€æ¨ç†ã€‚FREé˜¶æ®µåˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ–‡æœ¬æ•°æ®çš„æ¨ç†èƒ½åŠ›ï¼ŒMGTé˜¶æ®µåˆ™å°†è¿™äº›æ¨ç†èƒ½åŠ›æ¨å¹¿åˆ°å¤šæ¨¡æ€é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{method}åœ¨å¤šæ¨¡æ€å’Œæ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”åŸºçº¿æé«˜äº†4.83%å’Œ4.5%ï¼Œåœ¨å¤æ‚çš„è¶³çƒæ¯”èµ›ä»»åŠ¡ä¸­æé«˜äº†3.63%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08685",
            "title": "\"Principal Components\" Enable A New Language of Images",
            "url": "https://huggingface.co/papers/2503.08685",
            "abstract": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.",
            "score": 2,
            "issue_id": 2654,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "a013cfdc2d1e9d7c",
            "authors": [
                "Xin Wen",
                "Bingchen Zhao",
                "Ismail Elezi",
                "Jiankang Deng",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Imperial College London",
                "Noahs Ark Lab",
                "University of Edinburgh",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08685.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#interpretability",
                    "#diffusion",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ĞŸĞš-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼ÑƒÑ PCA-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ½ĞµĞ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒĞ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ²ÑĞ·Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Visual Tokenization with Structured Latent Spaces",
                    "desc": "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."
                },
                "zh": {
                    "title": "åˆ›æ–°è§†è§‰æ ‡è®°åŒ–ï¼Œæå‡å¯è§£é‡Šæ€§ä¸æ€§èƒ½",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æ ‡è®°åŒ–æ¡†æ¶ï¼Œå°†å¯è¯æ˜çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç»“æ„åµŒå…¥æ½œåœ¨æ ‡è®°ç©ºé—´ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°å™¨ä¸»è¦ä¼˜åŒ–é‡å»ºç²¾åº¦ï¼Œä½†å¾€å¾€å¿½è§†æ½œåœ¨ç©ºé—´çš„ç»“æ„ç‰¹æ€§ï¼Œè¿™å¯¹å¯è§£é‡Šæ€§å’Œä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå›¾åƒç”Ÿæˆä¸€ç»´å› æœæ ‡è®°åºåˆ—ï¼Œæ¯ä¸ªåç»­æ ‡è®°æä¾›ä¸é‡å çš„ä¿¡æ¯ï¼Œå¹¶ä¸”å…·æœ‰æ•°å­¦ä¸Šä¿è¯çš„é€’å‡è§£é‡Šæ–¹å·®ï¼Œç±»ä¼¼äºä¸»æˆåˆ†åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æé«˜äº†ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„å¯¹é½å¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07639",
            "title": "Mixture of Experts Made Intrinsically Interpretable",
            "url": "https://huggingface.co/papers/2503.07639",
            "abstract": "Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.",
            "score": 2,
            "issue_id": 2653,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "7e9a13248a2692b5",
            "authors": [
                "Xingyi Yang",
                "Constantin Venhoff",
                "Ashkan Khakzar",
                "Christian Schroeder de Witt",
                "Puneet K. Dokania",
                "Adel Bibi",
                "Philip Torr"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07639.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoE-X: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoE-X - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ Mixture-of-Experts. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². MoE-X Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¹ MoE ĞºĞ°Ğº ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoE-X Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MoE-X: Enhancing Interpretability in Language Models with Sparse Activations",
                    "desc": "This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods."
                },
                "zh": {
                    "title": "MoE-Xï¼šå¯è§£é‡Šçš„æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç¥ç»å…ƒå¸¸å¸¸è¡¨ç°å‡ºå¤šä¹‰æ€§ï¼ŒåŒæ—¶ç¼–ç å¤šä¸ªæ— å…³çš„æ¦‚å¿µï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å·®ã€‚æˆ‘ä»¬æå‡ºäº†MoE-Xï¼Œè¿™æ˜¯ä¸€ç§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å†…åœ¨ä¸Šå…·æœ‰å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰ç¨€ç–æ¿€æ´»çš„å®½ç½‘ç»œæ›´æœ‰å¯èƒ½æ•æ‰å¯è§£é‡Šçš„å› ç´ ã€‚é€šè¿‡æ¿€æ´»ä»…ä¸€éƒ¨åˆ†ä¸“å®¶ï¼ŒMoEæ¶æ„æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»è€Œåœ¨ä¿æŒç¨€ç–æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆçš„éšè—å±‚è§„æ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08120",
            "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2503.08120",
            "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.",
            "score": 1,
            "issue_id": 2654,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "7c6e7c685b283d61",
            "authors": [
                "Junzhe Li",
                "Xuerui Qiu",
                "Linrui Xu",
                "Liya Guo",
                "Delin Qu",
                "Tingting Long",
                "Chun Fan",
                "Ming Li"
            ],
            "affiliations": [
                "Central South University",
                "Computer Center, Peking University",
                "Fudan University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Computer Science, Peking University",
                "Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08120.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ§‘",
                "ru": {
                    "title": "UniF^2ace: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†",
                    "desc": "UniF^2ace - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (UMM), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 130 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ… Ğ»Ğ¸Ñ†Ğ°. Ğ’ UniF^2ace Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ²Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UniF^2ace Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ UMM Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†."
                },
                "en": {
                    "title": "UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation",
                    "desc": "This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦é¢éƒ¨ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢éƒ¨é¢†åŸŸç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç²—ç•¥çš„é¢éƒ¨å±æ€§ç†è§£ä¸Šï¼Œç¼ºä¹å¤„ç†ç»†ç²’åº¦é¢éƒ¨å±æ€§çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æœªèƒ½è§£å†³ç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†UniF^2aceï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç»†ç²’åº¦é¢éƒ¨ç†è§£å’Œç”Ÿæˆçš„UMMã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«13ä¸‡å¼ å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤ç§äº’è¡¥çš„æ‰©æ•£æŠ€æœ¯å’ŒåŒå±‚ä¸“å®¶æ··åˆæ¶æ„ï¼ŒUniF^2aceåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-11.html",
    "link_next": "2025-03-13.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "11.03",
        "en": "03/11",
        "zh": "3æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.03",
        "en": "03/13",
        "zh": "3æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œäººå·¥æ–‡æœ¬æ£€æµ‹ï¼ˆATDï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å°½ç®¡æœ‰è®¸å¤šåŠªåŠ›ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªç®—æ³•èƒ½åœ¨ä¸åŒç±»å‹çš„æœªçŸ¥æ–‡æœ¬ä¸­è¡¨ç°ä¸€è‡´ï¼Œæˆ–ä¿è¯æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æ–°çš„LLMsã€‚å¯è§£é‡Šæ€§åœ¨å®ç°è¿™ä¸€ç›®æ ‡ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä»Gemma-2-2bæ®‹å·®æµä¸­æå–ç‰¹å¾æ¥å¢å¼ºATDçš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ç¡®å®šäº†å¯è§£é‡Šå’Œé«˜æ•ˆçš„ç‰¹å¾ï¼Œé€šè¿‡é¢†åŸŸå’Œæ¨¡å‹ç‰¹å®šçš„ç»Ÿè®¡æ•°æ®ã€å¼•å¯¼æ–¹æ³•ä»¥åŠæ‰‹åŠ¨æˆ–åŸºäºLLMçš„è§£é‡Šæ¥åˆ†æå…¶è¯­ä¹‰å’Œç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†å…³äºä¸åŒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸äººç±»å†™ä½œå†…å®¹ä¹‹é—´å·®å¼‚çš„å®è´µè§è§£ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°ä»£LLMsåœ¨ä¿¡æ¯å¯†é›†é¢†åŸŸå…·æœ‰ç‹¬ç‰¹çš„å†™ä½œé£æ ¼ï¼Œå³ä½¿å®ƒä»¬å¯ä»¥é€šè¿‡ä¸ªæ€§åŒ–æç¤ºç”Ÿæˆç±»ä¼¼äººç±»çš„è¾“å‡ºã€‚",
        "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
        "pinyin": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œäººå·¥æ–‡æœ¬æ£€æµ‹ï¼ˆATDï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å°½ç®¡æœ‰è®¸å¤šåŠªåŠ›ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªç®—æ³•èƒ½åœ¨ä¸åŒç±»å‹çš„æœªçŸ¥æ–‡æœ¬ä¸­è¡¨ç°ä¸€è‡´ï¼Œæˆ–ä¿è¯æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æ–°çš„LLMsã€‚å¯è§£é‡Šæ€§åœ¨å®ç°è¿™ä¸€ç›®æ ‡ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä»Gemma-2-2bæ®‹å·®æµä¸­æå–ç‰¹å¾æ¥å¢å¼ºATDçš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ç¡®å®šäº†å¯è§£é‡Šå’Œé«˜æ•ˆçš„ç‰¹å¾ï¼Œé€šè¿‡é¢†åŸŸå’Œæ¨¡å‹ç‰¹å®šçš„ç»Ÿè®¡æ•°æ®ã€å¼•å¯¼æ–¹æ³•ä»¥åŠæ‰‹åŠ¨æˆ–åŸºäºLLMçš„è§£é‡Šæ¥åˆ†æå…¶è¯­ä¹‰å’Œç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†å…³äºä¸åŒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸äººç±»å†™ä½œå†…å®¹ä¹‹é—´å·®å¼‚çš„å®è´µè§è§£ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°ä»£LLMsåœ¨ä¿¡æ¯å¯†é›†é¢†åŸŸå…·æœ‰ç‹¬ç‰¹çš„å†™ä½œé£æ ¼ï¼Œå³ä½¿å®ƒä»¬å¯ä»¥é€šè¿‡ä¸ªæ€§åŒ–æç¤ºç”Ÿæˆç±»ä¼¼äººç±»çš„è¾“å‡ºã€‚\n\nSuÃ­zhe dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) de jÃ¬nbÃ¹, rÃ©ngÅng wÃ©nbÄ›n jiÇncÃ¨ (ATD) biÃ n dÃ© yuÃ¨lÃ¡iyuÃ¨ zhÃ²ngyÃ o. JÇnguÇn yÇ’u xÇ”duÅ nÇ”lÃ¬, dÃ n mÃ©iyÇ’u yÄ«gÃ¨ suÃ nfÇ nÃ©ng zÃ i bÃ¹tÃ³ng lÃ¨ixÃ­ng de wÃ¨izhÄ« wÃ©nbÄ›n zhÅng biÇoxiÃ n yÄ«zhÃ¬, huÃ² bÇozhÃ¨ng yÇ’uxiÃ o de fÃ nhuÃ  dÃ o xÄ«n de LLMs. KÄ› jiÄ›shÃ¬xÃ¬ng zÃ i shÃ­xiÃ n zhÃ¨ yÄ« mÃ¹biÄo zhÅng qÇzhe guÇnjiÃ n zuÃ²yÃ²ng. ZÃ i zhÃ¨ xiÃ ng yÃ¡njiÅ« zhÅng, wÇ’men tÅngguÃ² shÇyÃ²ng xÄ«shÅ« zÃ¬biÄnmÇqÃ¬ (SAE) cÃ³ng Gemma-2-2b cÃ¡nchÃ¡ liÃº zhÅng tÄ«qu tÃ©diÇn lÃ¡i zÄ“ngqiÃ¡ng ATD de kÄ› jiÄ›shÃ¬xÃ¬ng. WÇ’men quÃ¨dÃ¬ngle kÄ› jiÄ›shÃ¬ hÃ© gÄoxiÃ o de tÃ©diÇn, tÅngguÃ² lÇngyÃ¹ hÃ© mÃ³xÃ­ng tÃ¨dÃ¬ng de tÇ’ngjÃ¬ shÃ¹jÃ¹, yÇndÇo fÄngfÇ yÇjiÇ shÇ’udÃ²ng huÃ² jÄ«yÃº LLM de jiÄ›shÃ¬ lÃ¡i fÄ“nxi qÃ­ yÃ¹yÃ¡n hÃ© xiÄngguÄnxÃ¬ng. WÇ’men de fÄngfÇ tÃ­gÅngle guÄnyÃº bÃ¹tÃ³ng mÃ³xÃ­ng shÄ“ngchÃ©ng de wÃ©nbÄ›n yÇ” rÃ©nlÃ¨i xiÄ›zuÃ² nÃ¨irÃ³ng zhÄ«jiÄn chÄyÃ¬ de bÇoguÃ¬ jiÃ nshÃ¬. WÇ’men zhÇnshÃ¬le xiÃ ndÃ i LLMs zÃ i xÃ¬nxÄ« mÃ¬jiÃ© lÇngyÃ¹ yÇ’u dÃºtÃ¨ de xiÄ›zuÃ² fÄ“nggÃ©, jÃ­shÇ tÄmen kÄ›yÇ tÅngguÃ² gÃ¨xÃ¬nghuÃ  tÇshÃ¬ shÄ“ngchÃ©ng lÃ¨ixÃ­ rÃ©nlÃ¨i de shÃ¹chÅ«.",
        "vocab": "[{'word': 'éšç€', 'pinyin': 'suÃ­zhe', 'trans': 'with'},\n{'word': 'å¤§å‹', 'pinyin': 'dÃ xÃ­ng', 'trans': 'large-scale'},\n{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'language model'},\n{'word': 'è¿›æ­¥', 'pinyin': 'jÃ¬nbÃ¹', 'trans': 'progress'},\n{'word': 'äººå·¥æ–‡æœ¬æ£€æµ‹', 'pinyin': 'rÃ©ngÅng wÃ©nbÄ›n jiÇncÃ¨', 'trans': 'artificial text detection'},\n{'word': 'å˜å¾—', 'pinyin': 'biÃ ndÃ©', 'trans': 'become'},\n{'word': 'è¶Šæ¥è¶Š', 'pinyin': 'yuÃ¨lÃ¡iyuÃ¨', 'trans': 'increasingly'},\n{'word': 'é‡è¦', 'pinyin': 'zhÃ²ngyÃ o', 'trans': 'important'},\n{'word': 'å°½ç®¡', 'pinyin': 'jÇnguÇn', 'trans': 'although'},\n{'word': 'åŠªåŠ›', 'pinyin': 'nÇ”lÃ¬', 'trans': 'effort'},\n{'word': 'ç®—æ³•', 'pinyin': 'suÃ nfÇ', 'trans': 'algorithm'},\n{'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'æœªçŸ¥', 'pinyin': 'wÃ¨izhÄ«', 'trans': 'unknown'},\n{'word': 'ä¿è¯', 'pinyin': 'bÇozhÃ¨ng', 'trans': 'guarantee'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’uxiÃ o', 'trans': 'effective'},\n{'word': 'æ³›åŒ–', 'pinyin': 'fÃ nhuÃ ', 'trans': 'generalize'},\n{'word': 'å¯è§£é‡Šæ€§', 'pinyin': 'kÄ› jiÄ›shÃ¬ xÃ¬ng', 'trans': 'interpretability'},\n{'word': 'èµ·ç€', 'pinyin': 'qÇzhe', 'trans': 'playing'},\n{'word': 'å…³é”®', 'pinyin': 'guÇnjiÃ n', 'trans': 'key'},\n{'word': 'ä½œç”¨', 'pinyin': 'zuÃ²yÃ²ng', 'trans': 'role'},\n{'word': 'ç¨€ç–', 'pinyin': 'xÄ«shÅ«', 'trans': 'sparse'},\n{'word': 'è‡ªç¼–ç å™¨', 'pinyin': 'zÃ¬biÄnmÇqÃ¬', 'trans': 'autoencoder'},\n{'word': 'æ®‹å·®æµ', 'pinyin': 'cÃ¡nchÄ liÃº', 'trans': 'residual flow'},\n{'word': 'æå–', 'pinyin': 'tÃ­qÇ”', 'trans': 'extract'},\n{'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨zhÄ“ng', 'trans': 'feature'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'é¢†åŸŸ', 'pinyin': 'lÇngyÃ¹', 'trans': 'domain'},\n{'word': 'ç»Ÿè®¡æ•°æ®', 'pinyin': 'tÇ’ngjÃ¬ shÃ¹jÃ¹', 'trans': 'statistical data'},\n{'word': 'å¼•å¯¼æ–¹æ³•', 'pinyin': 'yÇndÇo fÄngfÇ', 'trans': 'guidance method'},\n{'word': 'æ‰‹åŠ¨', 'pinyin': 'shÇ’udÃ²ng', 'trans': 'manual'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'è§£é‡Š', 'pinyin': 'jiÄ›shÃ¬', 'trans': 'explanation'},\n{'word': 'è¯­ä¹‰', 'pinyin': 'yÇ”yÃ¬', 'trans': 'semantics'},\n{'word': 'ç›¸å…³æ€§', 'pinyin': 'xiÄngguÄnxÃ¬ng', 'trans': 'relevance'},\n{'word': 'è§è§£', 'pinyin': 'jiÃ njiÄ›', 'trans': 'insight'},\n{'word': 'å·®å¼‚', 'pinyin': 'chÄyÃ¬', 'trans': 'difference'},\n{'word': 'å†…å®¹', 'pinyin': 'nÃ¨irÃ³ng', 'trans': 'content'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'},\n{'word': 'ä¿¡æ¯å¯†é›†', 'pinyin': 'xÃ¬nxÄ« mÃ¬jÃ­', 'trans': 'information-intensive'},\n{'word': 'ç‹¬ç‰¹', 'pinyin': 'dÃºtÃ¨', 'trans': 'unique'},\n{'word': 'å†™ä½œé£æ ¼', 'pinyin': 'xiÄ›zuÃ² fÄ“nggÃ©', 'trans': 'writing style'},\n{'word': 'ä¸ªæ€§åŒ–', 'pinyin': 'gÃ¨xÃ¬nghuÃ ', 'trans': 'personalized'},\n{'word': 'æç¤º', 'pinyin': 'tÃ­shÃ¬', 'trans': 'prompt'},\n{'word': 'è¾“å‡º', 'pinyin': 'shÅ«chÅ«', 'trans': 'output'}]",
        "trans": "With the advancement of large language models (LLMs), artificial text detection (ATD) has become increasingly important. Despite numerous efforts, no single algorithm has been able to perform consistently across different types of unknown texts or guarantee effective generalization to new LLMs. Explainability plays a crucial role in achieving this goal. In this research, we enhance the explainability of ATD by extracting features from the residual stream of Gemma-2-2b using sparse autoencoders (SAE). We identify interpretable and efficient features and analyze their semantics and relevance through domain- and model-specific statistics, guided methods, and manual or LLM-based explanations. Our approach provides valuable insights into the differences between texts generated by different models and human-written content. We demonstrate that modern LLMs have a unique writing style in information-intensive domains, even though they can generate human-like outputs through personalized prompts.",
        "update_ts": "2025-03-11 09:12"
    }
}