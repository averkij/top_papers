{
    "date": {
        "ru": "12 ÑÐ½Ð²Ð°Ñ€Ñ",
        "en": "January 12",
        "zh": "1æœˆ12æ—¥"
    },
    "time_utc": "2026-01-12 18:35",
    "weekday": 0,
    "issue_id": 537,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.05432",
            "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
            "url": "https://huggingface.co/papers/2601.05432",
            "abstract": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
            "score": 128,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "6130091a1c7cf83b",
            "authors": [
                "Yuxiang Ji",
                "Yong Wang",
                "Ziyu Ma",
                "Yiming Hu",
                "Hailang Huang",
                "Xuecai Hu",
                "Guanhua Chen",
                "Liaoni Wu",
                "Xiangxiang Chu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Southern University of Science and Technology",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05432.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#rl",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ðŸ—ºï¸",
                "ru": {
                    "title": "ÐšÐ°Ñ€Ñ‚Ñ‹ ÐºÐ°Ðº ÐºÐ¾Ð¼Ð¿Ð°Ñ: Ð½Ð°ÑƒÑ‡Ð¸Ð¼ AI Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ ÑÐµÐ±Ñ Ð½Ð° Ð¿Ð»Ð°Ð½ÐµÑ‚Ðµ",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð³ÐµÐ¾Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿ÑƒÑ‚Ñ‘Ð¼ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ ÐºÐ°Ñ€Ñ‚ Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾ÑÐ½Ð°Ñ‰ÐµÐ½Ð° ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ Ñ ÐºÐ°Ñ€Ñ‚Ð°Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ñ†Ð¸ÐºÐ» Â«Ð°Ð³ÐµÐ½Ñ‚-Ð²-ÐºÐ°Ñ€Ñ‚ÐµÂ», Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´Ð°Ñ‚ÑŒ Ñ‚Ð°Ðº Ð¶Ðµ, ÐºÐ°Ðº Ð»ÑŽÐ´Ð¸ Ð¿Ñ€Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¸ Ð¼ÐµÑÑ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ. Ð”Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð°Ñ ÑÑ…ÐµÐ¼Ð°: Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° Ð´Ð»Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº MAPBench Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ Ð´Ð¾Ð±Ð¸Ð»Ð¸ÑÑŒ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸ÑÐ¼Ð¸."
                },
                "en": {
                    "title": "Mapping the Future of Image Geolocalization",
                    "desc": "This paper presents an advanced approach to image geolocalization by integrating map-based reasoning into large vision-language models (LVLMs). The authors introduce a novel agent-in-the-map loop optimization that enhances the model's ability to utilize maps, a strategy often employed by humans. They implement a two-stage optimization process that includes reinforcement learning to boost the model's agentic capabilities and parallel test-time scaling for efficient path exploration. The proposed method significantly improves accuracy on real-world images, as demonstrated by their new benchmark, MAPBench, achieving notable performance gains over existing models."
                },
                "zh": {
                    "title": "åœ°å›¾æ€ç»´åŠ©åŠ›å›¾åƒåœ°ç†å®šä½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢žå¼ºçš„å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼Œç”¨äºŽå›¾åƒåœ°ç†å®šä½ï¼Œé€šè¿‡å¼•å…¥åŸºäºŽåœ°å›¾çš„æŽ¨ç†å’Œåœ°å›¾ä¸­çš„ä»£ç†å¾ªçŽ¯ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚å›¾åƒåœ°ç†å®šä½ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨è§†è§‰çº¿ç´¢é¢„æµ‹å›¾åƒæ‹æ‘„åœ°ç‚¹ï¼ŒçŽ°æœ‰æ¨¡åž‹é€šå¸¸å¿½è§†äº†äººç±»å¸¸ç”¨çš„åœ°å›¾ç­–ç•¥ã€‚æˆ‘ä»¬ä¸ºæ¨¡åž‹èµ‹äºˆäº†åœ°å›¾æ€ç»´èƒ½åŠ›ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºä»£ç†-åœ°å›¾å¾ªçŽ¯ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»£ç†å¼ºåŒ–å­¦ä¹ å’Œå¹¶è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†çŽ°æœ‰æ¨¡åž‹ï¼Œç‰¹åˆ«æ˜¯åœ¨500ç±³å‡†ç¡®çŽ‡ä¸Šä»Ž8.0%æå‡è‡³22.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03017",
            "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
            "url": "https://huggingface.co/papers/2601.03017",
            "abstract": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
            "score": 91,
            "issue_id": 525,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "3e0ad6513065ae3a",
            "authors": [
                "Jing Xiong",
                "Qi Han",
                "Yunta Hsieh",
                "Hui Shen",
                "Huajian Xin",
                "Chaofan Tao",
                "Chenyang Zhao",
                "Hengyuan Zhang",
                "Taiqiang Wu",
                "Zhen Zhang",
                "Haochen Wang",
                "Zhongwei Wan",
                "Lingpeng Kong",
                "Ngai Wong"
            ],
            "affiliations": [
                "Ohio State University",
                "The University of Hong Kong",
                "University of California, Los Angeles",
                "University of California, Santa Barbara",
                "University of Edinburgh",
                "University of Michigan, Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03017.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "ðŸ”¬",
                "ru": {
                    "title": "Ð’Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ðµ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°: Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ñ„Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ„Ð¸Ð·Ð¸ÐºÐ¸ Ð¸ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐ¸",
                    "desc": "MMFormalizer â€” ÑÑ‚Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°Ð²Ñ‚Ð¾Ñ„Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ðµ Ñ Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÐµÐ¼. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¿ÑƒÑ‚Ñ‘Ð¼ Ñ€ÐµÐºÑƒÑ€ÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸Ð· Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¼Ð¸Ñ‚Ð¸Ð²Ð¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ Ñ€ÐµÐºÑƒÑ€ÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ ÑÐ²ÑÐ·Ð¸ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸Ð¸ Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº PhyX-AF Ñ 115 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ð¸Ð· Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð², Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¼ÐµÑ…Ð°Ð½Ð¸ÐºÑƒ, ÐºÐ²Ð°Ð½Ñ‚Ð¾Ð²ÑƒÑŽ Ð¼ÐµÑ…Ð°Ð½Ð¸ÐºÑƒ Ð¸ Ñ‚ÐµÑ€Ð¼Ð¾Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ LLM-Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÑŽÑ‚ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð² ÐºÐ¾Ð¼Ð¿Ð¸Ð»ÑÑ†Ð¸Ð¸ Ð¸ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐµ, Ñ…Ð¾Ñ‚Ñ Ð³ÐµÐ¾Ð¼ÐµÑ‚Ñ€Ð¸Ñ Ð¾ÑÑ‚Ð°Ñ‘Ñ‚ÑÑ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒÑŽ."
                },
                "en": {
                    "title": "Bridging Visual Perception and Mathematical Reasoning with MMFormalizer",
                    "desc": "MMFormalizer is a novel framework that combines visual perception with formal mathematical reasoning to enhance autoformalization across various physical domains. It addresses the challenges of translating natural language mathematics into formal statements by integrating adaptive grounding with real-world entities. The system constructs formal propositions from visual elements through recursive grounding and axiom composition, ensuring that each abstraction is supported by visual evidence. Evaluated on the PhyX-AF benchmark, MMFormalizer demonstrates superior performance in physical reasoning tasks, marking a significant advancement in multimodal autoformalization capabilities."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–çš„æ¡¥æ¢",
                    "desc": "MMFormalizer æ˜¯ä¸€ç§å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–å·¥å…·ï¼Œå®ƒå°†è§†è§‰æ„ŸçŸ¥ä¸Žå½¢å¼æ•°å­¦æŽ¨ç†ç›¸ç»“åˆï¼Œæ”¯æŒä»Žç»å…¸åŠ›å­¦åˆ°é‡å­åŠ›å­¦çš„å¤æ‚ç‰©ç†é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡é€‚åº”æ€§åŸºç¡€ä¸ŽçŽ°å®žä¸–ç•Œçš„æ•°å­¦å’Œç‰©ç†å®žä½“æ•´åˆï¼Œè¶…è¶Šäº†æ–‡æœ¬çš„è‡ªåŠ¨å½¢å¼åŒ–ã€‚MMFormalizer é€šè¿‡é€’å½’åŸºç¡€å’Œå…¬ç†ç»„åˆï¼Œä»Žæ„ŸçŸ¥åŸºç¡€æž„å»ºå½¢å¼å‘½é¢˜ï¼Œç¡®ä¿æ¯ä¸ªæŠ½è±¡éƒ½æœ‰è§†è§‰è¯æ®æ”¯æŒã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜Žï¼ŒMMFormalizer åœ¨å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ç‰©ç†æŽ¨ç†æ–¹é¢ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç»å…¸åŠ›å­¦å’Œé‡å­åŠ›å­¦ç­‰é¢†åŸŸçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03319",
            "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
            "url": "https://huggingface.co/papers/2601.03319",
            "abstract": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
            "score": 43,
            "issue_id": 525,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "10d82cf2b9d195c1",
            "authors": [
                "Eldad Matmon",
                "Amit Bracha",
                "Noam Rotstein",
                "Ron Kimmel"
            ],
            "affiliations": [
                "Technion Israel Institute of Technology, Haifa, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03319.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ðŸ¤ª",
                "ru": {
                    "title": "Ð¤Ð¾Ñ‚Ð¾Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ 3D-ÐºÐ°Ñ€Ð¸ÐºÐ°Ñ‚ÑƒÑ€Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Ð³Ð°ÑƒÑÑÐ¾Ð²Ñ‹ ÑÐ¿Ð»Ð°Ñ‚Ñ‚Ð¸Ð½Ð³ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÑ€Ð¸Ð²Ð¸Ð·Ð½Ð¾Ð¹",
                    "desc": "ÐŸÑ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ„Ð¾Ñ‚Ð¾Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… ÑˆÐ°Ñ€Ð¶ÐµÐ¹ Ð»Ð¸Ñ† Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ 3D-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÑ€Ð¸Ð²Ð¸Ð·Ð½Ñƒ Ð“Ð°ÑƒÑÑÐ° Ð´Ð»Ñ Ð¿Ñ€ÐµÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€Ñ‚ Ð»Ð¸Ñ†Ð°. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÑŽÑ‚ Ñ‚ÐµÑ…Ð½Ð¸ÐºÑƒ 3D Gaussian Splatting Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒÑŽ, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ‡Ñ€ÐµÐ·Ð¼ÐµÑ€Ð½Ð¾Ð¹ ÑÐ³Ð»Ð°Ð¶ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð¸. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ñ‡ÐµÑ€ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð½Ð°Ð±Ð¾Ñ€Ñƒ Ð³Ð°ÑƒÑÑÐ¸Ð°Ð½Ð¾Ð² Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ ÐºÐ°Ðº ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ, Ñ‚Ð°Ðº Ð¸ ÐºÐ°Ñ€Ð¸ÐºÐ°Ñ‚ÑƒÑ€Ð½Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð°. Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ðµ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð½Ð¾ÑÑ‚ÑŒÑŽ ÐºÐ°Ñ€Ð¸ÐºÐ°Ñ‚ÑƒÑ€Ñ‹ Ð¸ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°ÐµÑ‚ Ð´ÐµÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸."
                },
                "en": {
                    "title": "Real-Time Control of Photorealistic 3D Caricatures",
                    "desc": "This paper presents a new framework for creating photorealistic 3D caricatures of faces that allows for real-time control and deformation. It combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to enhance the realism and fidelity of the avatars. The method involves extracting a FLAME mesh from multiview sequences and applying a curvature-weighted Poisson equation to achieve exaggerated forms. A novel training approach that alternates between real and synthesized images enables the framework to effectively represent both realistic and exaggerated avatars, resulting in superior performance compared to previous methods."
                },
                "zh": {
                    "title": "é€¼çœŸçš„3Dæ¼«ç”»åŒ–å¤´åƒç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€¼çœŸçš„3Dæ¼«ç”»åŒ–æ¡†æž¶ï¼Œç»“åˆäº†åŸºäºŽé«˜æ–¯æ›²çŽ‡çš„è¡¨é¢å¤¸å¼ å’Œ3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿåˆ›å»ºå¯æŽ§çš„çœŸå®žå¤´åƒã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨é«˜æ–¯æ›²çŽ‡è¿›è¡Œè¡¨é¢å¤¸å¼ ï¼Œä½†åœ¨ä¸Žçº¹ç†ç»“åˆæ—¶å¯èƒ½å¯¼è‡´è¿‡äºŽå¹³æ»‘çš„æ¸²æŸ“æ•ˆæžœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®žçš„å¤šè§†è§’å¤´åƒã€‚é€šè¿‡äº¤æ›¿ä½¿ç”¨çœŸå®žå’Œåˆæˆçš„ç›‘ç£è®­ç»ƒï¼Œè¯¥æ¡†æž¶æé«˜äº†å¤´åƒçš„çœŸå®žæ„Ÿï¼Œæ”¯æŒå±€éƒ¨ç¼–è¾‘ï¼Œå¹¶å…è®¸å¯¹æ¼«ç”»æ•ˆæžœçš„å¼ºåº¦è¿›è¡Œè¿žç»­æŽ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06002",
            "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2601.06002",
            "abstract": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
            "score": 35,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "3f57cbb3429682eb",
            "authors": [
                "Qiguang Chen",
                "Yantao Du",
                "Ziniu Li",
                "Jinhao Liu",
                "Songyao Duan",
                "Jiarui Guo",
                "Minghao Liu",
                "Jiaheng Liu",
                "Tong Yang",
                "Ge Zhang",
                "Libo Qin",
                "Wanxiang Che",
                "Wenhao Huang"
            ],
            "affiliations": [
                "2077AI Foundation",
                "ByteDance Seed China",
                "Central South University",
                "LARG, SCIR, Harbin Institute of Technology",
                "M-A-P",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06002.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ðŸ§¬",
                "ru": {
                    "title": "ÐœÐ¾Ð»ÐµÐºÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑÑ‚Ð°Ð±Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² LLM",
                    "desc": "Ð’ ÑÑ‚Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¼ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ°Ð¼ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸Ð·ÑƒÑŽÑ‚ÑÑ Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð°Ð¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¼Ð¾Ð»ÐµÐºÑƒÐ»ÑÑ€Ð½Ð¾-Ð²Ð´Ð¾Ñ…Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´, Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÐºÐ°Ðº ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ñ Ñ‚Ñ€ÐµÐ¼Ñ Ñ‚Ð¸Ð¿Ð°Ð¼Ð¸ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹: Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ, ÑÐ°Ð¼Ð¾Ð¾Ñ‚Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸ ÑÐ°Ð¼Ð¾Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ. ÐžÐ½Ð¸ Ð²Ð²Ð¾Ð´ÑÑ‚ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸ÑŽ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸Ð·Ð¾Ð¼ÐµÑ€Ð¾Ð² Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ²ÑÐ·Ð¸, ÑÐ¿Ð¾ÑÐ¾Ð±ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ ÑÑ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ð¸, Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ. ÐœÐµÑ‚Ð¾Ð´ Mole-Syn Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð³Ñ€Ð°Ñ„ Ñ‚Ñ€Ð°Ð½ÑÑ„ÐµÑ€Ñ‚Ð° Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹, ÑƒÐ»ÑƒÑ‡ÑˆÐ°Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼."
                },
                "en": {
                    "title": "Molecular Structures Enhance Long Chain-of-Thought Reasoning in LLMs",
                    "desc": "This paper addresses the challenges that large language models (LLMs) face in performing long chain-of-thought (Long CoT) reasoning. It introduces a molecular-inspired framework that utilizes stable structural patterns, likening reasoning trajectories to molecular structures formed by different types of interactions. The authors propose Effective Semantic Isomers to enhance training stability, emphasizing that certain bonds are crucial for effective learning. Additionally, they present Mole-Syn, a method that improves the synthesis of Long CoT structures, leading to better performance and reinforcement learning stability."
                },
                "zh": {
                    "title": "åˆ†å­å¯å‘çš„é•¿é“¾æŽ¨ç†æå‡æ–¹æ³•",
                    "desc": "å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨é•¿é“¾æŽ¨ç†æ–¹é¢è¡¨çŽ°ä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶ç»“æž„æ¨¡å¼ä¸ç¨³å®šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å­å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆçš„è¯­ä¹‰å¼‚æž„ä½“å’Œåˆ†å¸ƒè½¬ç§»å›¾æ–¹æ³•æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œç¨³å®šçš„åˆ†å­ç»“æž„å¯ä»¥é€šè¿‡æ·±åº¦æŽ¨ç†ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æŽ¢ç´¢ä¸‰ç§äº¤äº’ç±»åž‹å½¢æˆï¼Œä»Žè€Œä¿ƒè¿›é•¿é“¾æŽ¨ç†çš„å­¦ä¹ ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†Mole-Synæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼é•¿é“¾æŽ¨ç†ç»“æž„çš„åˆæˆï¼Œæå‡æ¨¡åž‹åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨çŽ°å’Œå¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06021",
            "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
            "url": "https://huggingface.co/papers/2601.06021",
            "abstract": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
            "score": 30,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "2c4d9d165dc12ad2",
            "authors": [
                "Jiajie Zhang",
                "Xin Lv",
                "Ling Feng",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06021.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#optimization",
                    "#hallucinations",
                    "#rl",
                    "#training",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ðŸ”—",
                "ru": {
                    "title": "ÐÐ°Ð³Ñ€Ð°Ð´Ñ‹ Ñ Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼ Ñ†Ð¸Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð´Ð»Ñ Ñ‡ÐµÑÑ‚Ð½Ð¾Ð³Ð¾ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð½Ð°Ð³Ñ€Ð°Ð´ Citation-aware Rubric Rewards (CaRR), ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ñ‹ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð² Ð¸ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ñ†Ð¸Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ C-GRPO, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ CaRR Ñ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¼Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð². ÐŸÐ¾Ð´Ñ…Ð¾Ð´ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ€Ð°Ð·Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ðµ Ð¿Ð¾Ð´Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð² Ñ ÑÐ²Ð½Ñ‹Ð¼Ð¸ ÑÑÑ‹Ð»ÐºÐ°Ð¼Ð¸ Ð½Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ€Ð»Ñ‹ÐºÐ¾Ð², Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ°Ñ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸."
                },
                "en": {
                    "title": "Enhancing Deep Search Agents with Citation-Aware Rewards",
                    "desc": "This paper introduces a new reward framework called Citation-aware Rubric Rewards (CaRR) to enhance the reasoning abilities of deep search agents in reinforcement learning. Traditional methods use simple binary rewards, which can lead to poor reasoning and inaccuracies, such as shortcut exploitation and hallucinations. CaRR focuses on breaking down complex questions into simpler, verifiable components that require agents to provide evidence and citations for their answers. Additionally, the paper presents Citation-aware Group Relative Policy Optimization (C-GRPO), which integrates CaRR with outcome rewards to train more effective and reliable deep search agents."
                },
                "zh": {
                    "title": "æå‡æ·±åº¦æœç´¢ä»£ç†æŽ¨ç†çš„å…¨é¢æ€§ä¸Žå‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¡†æž¶ï¼Œç§°ä¸ºå¼•ç”¨æ„è¯†è¯„åˆ†å¥–åŠ±ï¼ˆCaRRï¼‰ï¼Œæ—¨åœ¨æé«˜æ·±åº¦æœç´¢ä»£ç†çš„æŽ¨ç†å…¨é¢æ€§å’Œäº‹å®žå‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºŽäºŒå…ƒç»“æžœå¥–åŠ±ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ä»£ç†æŽ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œå®¹æ˜“å¯¼è‡´æ·å¾„åˆ©ç”¨å’Œå¹»è§‰çŽ°è±¡ã€‚CaRRé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯éªŒè¯çš„å•æ­¥è¯„åˆ†ï¼Œè¦æ±‚ä»£ç†æ˜Žç¡®è¯†åˆ«éšè—å®žä½“ï¼Œå¹¶ç”¨æ­£ç¡®çš„å¼•ç”¨æ”¯æŒå®ƒä»¬ï¼Œä»Žè€Œæž„å»ºå®Œæ•´çš„è¯æ®é“¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¼•ç”¨æ„è¯†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆC-GRPOï¼‰ï¼Œç»“åˆCaRRå’Œç»“æžœå¥–åŠ±ï¼Œè®­ç»ƒå‡ºæ›´å¼ºå¤§çš„æ·±åº¦æœç´¢ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05808",
            "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
            "url": "https://huggingface.co/papers/2601.05808",
            "abstract": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
            "score": 23,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "7fb43c28ab2879e9",
            "authors": [
                "Xiaoshuai Song",
                "Haofei Chang",
                "Guanting Dong",
                "Yutao Zhu",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05808.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#agents",
                    "#training"
                ],
                "emoji": "ðŸ—ï¸",
                "ru": {
                    "title": "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LLM",
                    "desc": "EnvScaler â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÑ€ÐµÐ´ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ð´Ð²ÑƒÑ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²: SkelBuilder Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÐºÐµÐ»ÐµÑ‚Ñ‹ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð°Ð¹Ð½Ð¸Ð½Ð³ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ðº Ð¸ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð° ScenGenerator ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ² Ð·Ð°Ð´Ð°Ñ‡ Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹. Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¸ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM Ñ‡ÐµÑ€ÐµÐ· supervised fine-tuning Ð¸ reinforcement learning. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ LLM Ñ€ÐµÑˆÐ°Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ñ‹Ð¼ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÐµÐ¼ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²."
                },
                "en": {
                    "title": "Automating Tool-Interaction Environments for Enhanced LLM Performance",
                    "desc": "EnvScaler is a framework designed to automate the creation of environments where large language models (LLMs) can interact with various tools. It uses programmatic synthesis to generate diverse and scalable tool-interaction environments, addressing the limitations of manual sandbox creation. The framework consists of two main components: SkelBuilder, which creates environment skeletons, and ScenGenerator, which produces task scenarios and validation functions. By applying EnvScaler, researchers have synthesized numerous environments and scenarios, leading to significant improvements in LLM performance on complex multi-turn tasks."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–å·¥å…·äº¤äº’çŽ¯å¢ƒçš„åˆæˆä¸Žä¼˜åŒ–",
                    "desc": "EnvScaler æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æž¶ï¼Œé€šè¿‡ç¨‹åºåˆæˆæ¥åˆ›å»ºå¯æ‰©å±•çš„å·¥å…·äº¤äº’çŽ¯å¢ƒï¼Œä»Žè€Œæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šè½®ã€å¤šå·¥å…·ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¯¥æ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šSkelBuilder ç”¨äºŽé€šè¿‡ä¸»é¢˜æŒ–æŽ˜ã€é€»è¾‘å»ºæ¨¡å’Œè´¨é‡è¯„ä¼°æž„å»ºå¤šæ ·åŒ–çš„çŽ¯å¢ƒéª¨æž¶ï¼›ScenGenerator åˆ™ä¸ºæ¯ä¸ªçŽ¯å¢ƒç”Ÿæˆå¤šä¸ªä»»åŠ¡åœºæ™¯å’ŒåŸºäºŽè§„åˆ™çš„è½¨è¿¹éªŒè¯å‡½æ•°ã€‚é€šè¿‡ EnvScalerï¼Œæˆ‘ä»¬åˆæˆäº† 191 ä¸ªçŽ¯å¢ƒå’Œçº¦ 7000 ä¸ªåœºæ™¯ï¼Œå¹¶å°†å…¶åº”ç”¨äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ˜¾è‘—æé«˜äº† LLM åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨ https://github.com/RUC-NLPIR/EnvScaler å‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05930",
            "title": "Can We Predict Before Executing Machine Learning Agents?",
            "url": "https://huggingface.co/papers/2601.05930",
            "abstract": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
            "score": 19,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "f17fbcab3adc48b7",
            "authors": [
                "Jingsheng Zheng",
                "Jintian Zhang",
                "Yujie Luo",
                "Yuren Mao",
                "Yunjun Gao",
                "Lun Du",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05930.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#dataset",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ðŸ”®",
                "ru": {
                    "title": "ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ¶Ð´Ðµ, Ñ‡ÐµÐ¼ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ: ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ",
                    "desc": "ÐÐ²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ðµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€Ð°Ð´Ð°ÑŽÑ‚ Ð¾Ñ‚ ÑƒÐ·ÐºÐ¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ, ÐºÐ¾Ð³Ð´Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ· Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð´Ð¾Ñ€Ð¾Ð³Ð¾ÑÑ‚Ð¾ÑÑ‰Ð¸Ñ… Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð². ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð´Ð¾ Ð¸Ñ… Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¸Ñ€Ð° Ð¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¾Ð½Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð· 18,438 Ð¿Ð¾Ð¿Ð°Ñ€Ð½Ñ‹Ñ… ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¹ Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸, Ñ‡Ñ‚Ð¾ LLM Ð¼Ð¾Ð³ÑƒÑ‚ Ð´Ð¾ÑÑ‚Ð¸Ñ‡ÑŒ 61.5% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¸ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ð¹ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚ FOREAGENT Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ñ†Ð¸ÐºÐ» Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ-Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð¸ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ 6-ÐºÑ€Ð°Ñ‚Ð½Ð¾Ð³Ð¾ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ ÑÑ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð½Ð° 6% Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°Ð¼Ð¸."
                },
                "en": {
                    "title": "Predict Before You Execute: Accelerating Machine Learning Agents",
                    "desc": "This paper introduces autonomous machine learning agents that enhance performance by predicting outcomes before actual execution, thus addressing the Execution Bottleneck in traditional methods. By using a Predict-then-Verify approach, these agents can achieve faster convergence and improved results without relying solely on costly physical execution. The authors formalize the concept of Data-centric Solution Preference and create a dataset of 18,438 comparisons to evaluate predictive capabilities. The proposed agent, FOREAGENT, demonstrates a significant acceleration in convergence and outperforms existing execution-based methods."
                },
                "zh": {
                    "title": "é¢„æµ‹å…ˆè¡Œï¼Œæ‰§è¡Œæ›´å¿«ï¼",
                    "desc": "è‡ªä¸»æœºå™¨å­¦ä¹ ä»£ç†é€šè¿‡åœ¨ç‰©ç†æ‰§è¡Œä¹‹å‰é¢„æµ‹ç»“æžœï¼Œå…‹æœäº†æ‰§è¡Œç“¶é¢ˆï¼Œä»Žè€Œå®žçŽ°æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨äº†é¢„æµ‹-éªŒè¯çš„ç­–ç•¥ï¼Œé¿å…äº†ä¾èµ–æ˜‚è´µçš„ç‰©ç†æ‰§è¡Œæ¥è¯„ä¼°å‡è®¾ã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªåŒ…å«18,438å¯¹æ¯”è¾ƒçš„ç»¼åˆè¯­æ–™åº“ï¼Œå¹¶è¯æ˜Žäº†å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ç»è¿‡éªŒè¯çš„æ•°æ®åˆ†æžæŠ¥å‘Šçš„å¼•å¯¼ä¸‹ï¼Œå…·æœ‰æ˜¾è‘—çš„é¢„æµ‹èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨FOREAGENTä¸­å®žçŽ°äº†è¿™ä¸€æ¡†æž¶ï¼Œè¯¥ä»£ç†é€šè¿‡é¢„æµ‹-éªŒè¯å¾ªçŽ¯å®žçŽ°äº†6å€çš„æ”¶æ•›åŠ é€Ÿï¼Œå¹¶è¶…è¶Šäº†åŸºäºŽæ‰§è¡Œçš„åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04720",
            "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
            "url": "https://huggingface.co/papers/2601.04720",
            "abstract": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
            "score": 19,
            "issue_id": 524,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "f64f73630c899742",
            "authors": [
                "Mingxin Li",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Keqin Chen",
                "Sibo Song",
                "Shuai Bai",
                "Zhibo Yang",
                "Pengjun Xie",
                "An Yang",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04720.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#rag",
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "Ð•Ð´Ð¸Ð½Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð²Ð¾ Ð²ÑÐµÑ… Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑÑ…",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Qwen3-VL-Embedding Ð¸ Qwen3-VL-Reranker, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÑŽÑ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑ Ñ‚ÐµÐºÑÑ‚, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ, Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð² ÐµÐ´Ð¸Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾. ÐœÐ¾Ð´ÐµÐ»ÑŒ Qwen3-VL-Embedding Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ð°Ð¿Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¾Ð¹ Ð¸ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸ÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿ÐµÑ€ÐµÑ€Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð±Ð¾Ð³Ð°Ñ‚Ñ‹Ñ… Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹. Qwen3-VL-Reranker Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½ÑƒÑŽ Ð¾Ñ†ÐµÐ½ÐºÑƒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ-Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐºÑ€Ð¾ÑÑ-ÐºÐ¾Ð´ÐµÑ€Ð° Ð¸ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð¾Ð² ÐºÑ€Ð¾ÑÑ-Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ. ÐžÐ±Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ð±Ð¾Ð»ÐµÐµ 30 ÑÐ·Ñ‹ÐºÐ¾Ð², Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð² Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°Ñ… 2B Ð¸ 8B Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¸ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÑŽÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Search with Qwen3 Models",
                    "desc": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models create a powerful system for searching across different types of data, like text and images. They use advanced training techniques and cross-attention to improve how well they find relevant information. The embedding model generates detailed representations of data, while the reranker fine-tunes the results to ensure the best matches are highlighted. Together, they support multiple languages and have shown top performance in multimodal search tasks."
                },
                "zh": {
                    "title": "é«˜ç²¾åº¦å¤šæ¨¡æ€æœç´¢çš„æœªæ¥",
                    "desc": "Qwen3-VL-Embeddingå’ŒQwen3-VL-Rerankeræ¨¡åž‹æž„æˆäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æœç´¢ç®¡é“ï¼Œåˆ©ç”¨å¤šé˜¶æ®µè®­ç»ƒå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®žçŽ°é«˜ç²¾åº¦çš„æ£€ç´¢ã€‚è¿™äº›æ¨¡åž‹å°†æ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ã€‚Qwen3-VL-Embeddingæ¨¡åž‹é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒï¼Œä»Žå¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒåˆ°é‡æŽ’åºæ¨¡åž‹è’¸é¦ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„é«˜ç»´å‘é‡ã€‚Qwen3-VL-Rerankeråˆ™ä½¿ç”¨äº¤å‰ç¼–ç å™¨æž¶æž„è¿›è¡Œç»†ç²’åº¦ç›¸å…³æ€§ä¼°è®¡ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼Œé€‚åº”ä¸åŒçš„éƒ¨ç½²éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04786",
            "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
            "url": "https://huggingface.co/papers/2601.04786",
            "abstract": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
            "score": 18,
            "issue_id": 523,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "a8a1ae23f28f1d1a",
            "authors": [
                "Lang Feng",
                "Fuchao Yang",
                "Feng Chen",
                "Xin Cheng",
                "Haiyang Xu",
                "Zhenglin Wan",
                "Ming Yan",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04786.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ðŸ–¼ï¸",
                "ru": {
                    "title": "Ð’Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ…",
                    "desc": "AgentOCR â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ…, Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÑ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð² ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ…. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð½Ð¾Ðµ Ð¾Ð¿Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¿ÑƒÑ‚Ñ‘Ð¼ Ñ…ÐµÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹. ÐÐ³ÐµÐ½Ñ‚ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð¸ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ, Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ðº ÑÐ¶Ð°Ñ‚Ð¸ÑŽ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾ Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÑÐ¿ÐµÑ… Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð². ÐÐ° ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… AgentOCR ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð±Ð¾Ð»ÐµÐµ 95% Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¿Ñ€Ð¸ ÑÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ð¸ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð±Ð¾Ð»ÐµÐµ Ñ‡ÐµÐ¼ Ð½Ð° 50%."
                },
                "en": {
                    "title": "Efficient Interaction with Visual Tokens in Agentic Systems",
                    "desc": "AgentOCR is a novel framework designed to enhance the efficiency of agentic systems by minimizing token usage during multi-turn interactions. It achieves this by converting interaction history into visual tokens, which are more information-dense than text. The framework employs segment optical caching to avoid redundant rendering of similar histories, significantly speeding up processing. Additionally, AgentOCR incorporates self-compression techniques that allow the agent to adaptively manage its performance and token efficiency, leading to substantial reductions in token consumption while maintaining high performance levels."
                },
                "zh": {
                    "title": "AgentOCRï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°ä¸Žè‡ªæˆ‘åŽ‹ç¼©æŠ€æœ¯",
                    "desc": "AgentOCR æ˜¯ä¸€ç§æ–°æ¡†æž¶ï¼Œé€šè¿‡å°†äº¤äº’åŽ†å²è¡¨ç¤ºä¸ºè§†è§‰æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ä»£ç†ç³»ç»Ÿä¸­çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®ƒåˆ©ç”¨è§†è§‰ç¼“å­˜å’Œè‡ªæˆ‘åŽ‹ç¼©æŠ€æœ¯ï¼Œä½¿å¾—å¤šè½®äº¤äº’çš„å¤„ç†æ›´åŠ é«˜æ•ˆã€‚é€šè¿‡å°†åŽ†å²åˆ†è§£ä¸ºå¯å“ˆå¸Œçš„æ®µå¹¶ç»´æŠ¤è§†è§‰ç¼“å­˜ï¼ŒAgentOCR é¿å…äº†å†—ä½™çš„é‡æ–°æ¸²æŸ“ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAgentOCR åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œä»¤ç‰Œæ¶ˆè€—å‡å°‘è¶…è¿‡ 50%ï¼Œå¹¶å®žçŽ°äº†æ˜¾è‘—çš„å†…å­˜æ•ˆçŽ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05882",
            "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
            "url": "https://huggingface.co/papers/2601.05882",
            "abstract": "Preference tuning of language models shows varying generalization capabilities under domain shift, with pseudo-labeling adaptation strategies effectively reducing performance degradation in summarization and question-answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation",
            "score": 17,
            "issue_id": 534,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "807f9304c133b53f",
            "authors": [
                "Constantinos Karouzos",
                "Xingwei Tan",
                "Nikolaos Aletras"
            ],
            "affiliations": [
                "School of Computer Science University of Sheffield, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05882.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf",
                    "#transfer_learning"
                ],
                "emoji": "ðŸŽ¯",
                "ru": {
                    "title": "ÐÐ´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€Ð¸ ÑÐ¼ÐµÐ½Ðµ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð¿ÑÐµÐ²Ð´Ð¾Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÑƒ",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ, ÐºÐ°Ðº ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±Ñ‹Ð»Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ ÐºÐ¾Ð³Ð´Ð° Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ÑÑ‚ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ðµ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÑŽÑ‚ Ð¿ÑÑ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° Ñ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð¸ Ð¿ÑÐµÐ²Ð´Ð¾Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÑƒ. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾-Ñ€Ð°Ð·Ð½Ð¾Ð¼Ñƒ ÑÐ¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ñ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð¾Ð¼ Ð¼ÐµÐ¶Ð´Ñƒ Ð¾Ð±Ð»Ð°ÑÑ‚ÑÐ¼Ð¸, Ð¸ Ñ‡Ñ‚Ð¾ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿ÑÐµÐ²Ð´Ð¾Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¸ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¾ Ð½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ñ Ñ†ÐµÐ»ÑŒÑŽ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²."
                },
                "en": {
                    "title": "Mitigating Domain Shift in Language Model Preference Tuning",
                    "desc": "This paper investigates how preference tuning of language models can be affected by changes in the domain of the data. Preference tuning aims to align models with human preferences for quality and helpfulness, but it often leads to performance drops when applied to new domains. The authors explore various adaptation strategies, particularly focusing on pseudo-labeling, to see how they can help maintain performance in summarization and question-answering tasks. Their results indicate that certain adaptation methods can significantly lessen the negative impact of domain shifts on model performance."
                },
                "zh": {
                    "title": "ä¼ªæ ‡ç­¾ç­–ç•¥åŠ©åŠ›è¯­è¨€æ¨¡åž‹é¢†åŸŸé€‚åº”",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†è¯­è¨€æ¨¡åž‹çš„åå¥½è°ƒä¼˜åœ¨é¢†åŸŸè½¬ç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä¼ªæ ‡ç­¾é€‚åº”ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘åœ¨æ‘˜è¦å’Œé—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†äº”ç§æµè¡Œçš„å¯¹é½ç›®æ ‡å’Œå¤šç§é€‚åº”ç­–ç•¥ï¼Œå‘çŽ°ä¸åŒçš„å¯¹é½ç›®æ ‡åœ¨é¢†åŸŸè½¬ç§»ä¸‹çš„æ³›åŒ–è¡¨çŽ°å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ã€‚ç»“æžœè¡¨æ˜Žï¼ŒåŸºäºŽä¼ªæ ‡ç­¾çš„é€‚åº”ç­–ç•¥å¯ä»¥æ˜¾è‘—é™ä½Žé¢†åŸŸè½¬ç§»å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05966",
            "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
            "url": "https://huggingface.co/papers/2601.05966",
            "abstract": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
            "score": 13,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "e8ed88c8c20040fb",
            "authors": [
                "Longbin Ji",
                "Xiaoxiong Liu",
                "Junyuan Shang",
                "Shuohuan Wang",
                "Yu Sun",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "affiliations": [
                "Baidu"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05966.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#video",
                    "#training"
                ],
                "emoji": "ðŸŽ¬",
                "ru": {
                    "title": "ÐÐ²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾: ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð² ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸",
                    "desc": "VideoAR Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¿ÐµÑ€Ð²ÑƒÑŽ ÐºÑ€ÑƒÐ¿Ð½Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½ÑƒÑŽ Ð°Ð²Ñ‚Ð°Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ ÐºÐ°Ð´Ñ€Ð° Ñ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ñ‹Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ 3D Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾-Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐµÑ‚ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸. Ð”Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ð¹ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð², Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ð¹ Temporal RoPE Ð¸ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²ÐºÑƒ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¼ÐµÐ¶Ð´Ñƒ ÐºÐ°Ð´Ñ€Ð°Ð¼Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ½Ð¸Ð¶Ð°ÑŽÑ‚ Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº. VideoAR Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð², ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼Ñ‹Ñ… Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸, Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ñ‚Ñ€ÐµÐ±ÑƒÑ Ð² 10 Ñ€Ð°Ð· Ð¼ÐµÐ½ÑŒÑˆÐµ ÑˆÐ°Ð³Ð¾Ð² Ð¸ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð° Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾."
                },
                "en": {
                    "title": "VideoAR: Efficient and Consistent Video Generation with Autoregressive Modeling",
                    "desc": "VideoAR is a novel framework for generating videos using a visual autoregressive approach that enhances efficiency and temporal consistency. It integrates multi-scale next-frame prediction with autoregressive modeling to effectively manage spatial and temporal dependencies. The framework employs techniques like Multi-scale Temporal RoPE and Cross-Frame Error Correction to reduce errors and maintain coherence over time. VideoAR sets new benchmarks in video generation, outperforming previous autoregressive models while significantly cutting down on computational requirements."
                },
                "zh": {
                    "title": "VideoARï¼šé«˜æ•ˆä¸€è‡´çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æž¶",
                    "desc": "VideoARæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰è‡ªå›žå½’æ¡†æž¶ï¼Œç”¨äºŽè§†é¢‘ç”Ÿæˆï¼Œç»“åˆäº†å¤šå°ºåº¦çš„ä¸‹ä¸€å¸§é¢„æµ‹å’Œè‡ªå›žå½’å»ºæ¨¡ï¼Œå–å¾—äº†å…ˆè¿›çš„æ•ˆçŽ‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¸§å†…è‡ªå›žå½’å»ºæ¨¡ä¸Žå› æžœä¸‹ä¸€å¸§é¢„æµ‹ç›¸ç»“åˆï¼Œè§£è€¦äº†ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ã€‚ä¸ºäº†æé«˜é•¿æœŸä¸€è‡´æ€§ï¼ŒVideoARæå‡ºäº†å¤šå°ºåº¦æ—¶é—´RoPEã€è·¨å¸§è¯¯å·®ä¿®æ­£å’Œéšæœºå¸§æŽ©ç ç­‰æŠ€æœ¯ï¼Œå‡å°‘äº†é”™è¯¯ä¼ æ’­å¹¶ç¨³å®šäº†æ—¶é—´è¿žè´¯æ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVideoARåœ¨è‡ªå›žå½’æ¨¡åž‹ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æžœï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶å‡å°‘äº†æŽ¨ç†æ­¥éª¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05905",
            "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
            "url": "https://huggingface.co/papers/2601.05905",
            "abstract": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
            "score": 11,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "30111934330923b5",
            "authors": [
                "Haoming Xu",
                "Ningyuan Zhao",
                "Yunzhi Yao",
                "Weihong Xu",
                "Hongru Wang",
                "Xinle Deng",
                "Shumin Deng",
                "Jeff Z. Pan",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "NUS-NCS Joint Lab",
                "National University of Singapore",
                "University of Edinburgh",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05905.jpg",
            "data": {
                "categories": [],
                "emoji": "ðŸ—ï¸",
                "ru": {
                    "title": "Ð£ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð½Ð¾Ð¹ Ð½Ð°Ð´Ñ‘Ð¶Ð½Ð¾ÑÑ‚Ð¸ ÑƒÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð½Ð¾, Ñ‡Ñ‚Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ñ…Ñ€ÑƒÐ¿ÐºÐ¸Ðµ ÑƒÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ð»Ð¾Ñ…Ð¾ Ð¸Ð·Ð¼ÐµÑ€ÑÑŽÑ‚ÑÑ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸ Ð²Ñ€Ð¾Ð´Ðµ Self-Consistency. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ Neighbor-Consistency Belief (NCB), ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¾ÐºÑ€ÐµÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ñ„Ð°ÐºÑ‚Ð¾Ð². ÐžÐ½Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» ÑÑ‚Ñ€ÐµÑÑ-Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð²Ð»Ð¸ÑÐ½Ð¸ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ñ… Ð¿Ð¾Ð¼ÐµÑ…. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Structure-Aware Training (SAT) Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾-Ð¸Ð½Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð½ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ ÑƒÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ñ…Ñ€ÑƒÐ¿ÐºÐ¾ÑÑ‚ÑŒ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ 30%."
                },
                "en": {
                    "title": "Building Robust Beliefs in Language Models",
                    "desc": "This paper discusses the limitations of Large Language Models (LLMs) in maintaining accurate beliefs when faced with slight changes in context. It introduces a new metric called Neighbor-Consistency Belief (NCB) to measure the robustness of these beliefs by assessing how consistent responses are within a related conceptual framework. The authors demonstrate that traditional evaluation methods can overlook fragile beliefs that may collapse under minor contextual shifts. To improve the stability of LLMs, they propose Structure-Aware Training (SAT), which enhances the models' ability to maintain consistent beliefs, reducing knowledge brittleness significantly."
                },
                "zh": {
                    "title": "å¢žå¼ºå¤§åž‹è¯­è¨€æ¨¡åž‹çš„ä¿¡å¿µç¨³å®šæ€§",
                    "desc": "å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨é¢å¯¹ä¸Šä¸‹æ–‡å¹²æ‰°æ—¶è¡¨çŽ°å‡ºè„†å¼±çš„ä¿¡å¿µï¼Œè¿™ç§çŽ°è±¡å¯ä»¥é€šè¿‡ç»“æž„ä¸€è‡´æ€§æŒ‡æ ‡æ¥æ›´å¥½åœ°è¡¡é‡ã€‚çŽ°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºŽç‚¹å¯¹ç‚¹çš„è‡ªä¿¡åº¦ï¼Œå¯èƒ½æŽ©ç›–äº†ä¿¡å¿µçš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†é‚»å±…ä¸€è‡´æ€§ä¿¡å¿µï¼ˆNCBï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¯„ä¼°å“åº”ä¸€è‡´æ€§çš„ç»“æž„æ€§æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨æ¦‚å¿µé‚»åŸŸå†…è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡ç»“æž„æ„ŸçŸ¥è®­ç»ƒï¼ˆSATï¼‰ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ä¸å˜çš„ä¿¡å¿µç»“æž„ï¼Œæ˜¾è‘—å‡å°‘äº†çŸ¥è¯†çš„è„†å¼±æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05848",
            "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
            "url": "https://huggingface.co/papers/2601.05848",
            "abstract": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
            "score": 8,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "fedc48a7e2d815ae",
            "authors": [
                "Nate Gillman",
                "Yinghua Zhou",
                "Zitian Tang",
                "Evan Luo",
                "Arjan Chakravarthy",
                "Daksh Aggarwal",
                "Michael Freeman",
                "Charles Herrmann",
                "Chen Sun"
            ],
            "affiliations": [
                "Brown University",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05848.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#dataset",
                    "#synthetic",
                    "#video",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ðŸŽ¬",
                "ru": {
                    "title": "ÐžÑ‚ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð·Ð°ÐºÐ¾Ð½Ð¾Ð² Ñ„Ð¸Ð·Ð¸ÐºÐ¸ Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð³Ð¾ Ð¼Ð¸Ñ€Ð°",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Goal Force â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾-Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· ÑÐ²Ð½Ñ‹Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ ÑÐ¸Ð» Ð¸ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½ÑƒÑŽ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð·Ð°Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ñ†ÐµÐ»Ð¸ Ð² Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ñ… Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°Ð»Ð°ÑÑŒ Ð½Ð° ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð¼Ð¸Ñ‚Ð¸Ð²Ð°Ñ… Ñ„Ð¸Ð·Ð¸ÐºÐ¸ (ÑƒÐ¿Ñ€ÑƒÐ³Ð¸Ðµ ÑÑ‚Ð¾Ð»ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸Ñ, Ð¿Ð°Ð´Ð°ÑŽÑ‰Ð¸Ðµ ÐºÐ¾ÑÑ‚ÑÑˆÐºÐ¸ Ð´Ð¾Ð¼Ð¸Ð½Ð¾) Ð´Ð»Ñ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÑÐ¸Ð» Ð²Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ. ÐÐµÑÐ¼Ð¾Ñ‚Ñ€Ñ Ð½Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑÑ…, Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€Ð¾Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð° Ð¿Ð¾Ñ€Ð°Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð½ÑƒÐ»ÐµÐ²ÑƒÑŽ Ð¾Ð±Ð¾Ð±Ñ‰Ð°ÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð½Ð° ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»ÑÑ†Ð¸ÑŽ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð¸ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð½Ð¾-ÑÐ»ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð·Ð°Ð·ÐµÐ¼Ð»ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾-Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð² Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð·Ð°ÐºÐ¾Ð½Ð°Ñ… Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÐºÐ°Ðº Ð½ÐµÑÐ²Ð½Ñ‹Ðµ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ²Ñ‹Ðµ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ¸Ð¼ÑƒÐ»ÑÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ."
                },
                "en": {
                    "title": "Empowering Video Generation with Physics-Based Goal Setting",
                    "desc": "This paper presents a new framework called Goal Force for video generation models that allows users to set goals using explicit force vectors. By training on simple physics scenarios, the model learns to simulate how forces propagate over time and space. Remarkably, it can apply this knowledge to complex real-world tasks without needing additional training, demonstrating zero-shot generalization. This approach positions the model as an implicit neural physics simulator, enhancing its ability to plan and execute dynamic tasks based on physical interactions."
                },
                "zh": {
                    "title": "é€šè¿‡åŠ›å‘é‡å®žçŽ°ç›®æ ‡å®šä¹‰çš„åˆ›æ–°æ¡†æž¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGoal Forceçš„æ–°æ¡†æž¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ˜Žç¡®çš„åŠ›å‘é‡å’Œä¸­é—´åŠ¨æ€æ¥å®šä¹‰ç›®æ ‡ã€‚è¿™ç§æ–¹æ³•æ¨¡ä»¿äº†äººç±»å¯¹ç‰©ç†ä»»åŠ¡çš„æ¦‚å¿µåŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡å›¾åƒåœ¨åŠ¨æ€ä»»åŠ¡ä¸­éš¾ä»¥å…·ä½“åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬è®­ç»ƒçš„æ¨¡åž‹åœ¨ç®€å•çš„ç‰©ç†æ•°æ®ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„çŽ°å®žåœºæ™¯ä¸­å®žçŽ°é›¶-shotæ³›åŒ–ï¼ŒåŒ…æ‹¬å·¥å…·æ“ä½œå’Œå¤šç‰©ä½“å› æžœé“¾ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œé€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸ŽåŸºæœ¬ç‰©ç†äº¤äº’ç›¸ç»“åˆï¼Œæ¨¡åž‹å¯ä»¥ä½œä¸ºéšå¼ç¥žç»ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œå®žçŽ°ç²¾ç¡®çš„ç‰©ç†æ„ŸçŸ¥è§„åˆ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05573",
            "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
            "url": "https://huggingface.co/papers/2601.05573",
            "abstract": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
            "score": 7,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "6d51bd0357e81bbe",
            "authors": [
                "Zehan Wang",
                "Ziang Zhang",
                "Jiayang Xu",
                "Jialei Wang",
                "Tianyu Pang",
                "Chao Du",
                "HengShuang Zhao",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Sea AI Lab",
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05573.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#3d",
                    "#multimodal",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ðŸ”„",
                "ru": {
                    "title": "Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ñ‘Ñ…Ð¼ÐµÑ€Ð½Ð¾Ð¹ Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ð¸",
                    "desc": "Orient Anything V2 â€” ÑÑ‚Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð°Ñ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ð¼ÐµÑ€Ð½Ð¾Ð¹ Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÑ…. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ 3D-Ð°ÐºÑ‚Ð¸Ð²Ñ‹, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ðµ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸, Ð¸ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ñ€Ð¾Ñ‚Ð°Ñ†Ð¸Ð¾Ð½Ð½ÑƒÑŽ ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸ÑŽ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð´Ð»Ñ Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¹. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚Ñ‹ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÐºÐ°Ð´Ñ€Ð°Ñ… Ð¸ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° 11 Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸, Ð¿Ð¾Ð·Ñ‹ Ð¸ ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ð¸. ÐŸÐ¾Ð´Ñ…Ð¾Ð´ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¸Ð»ÑŒÐ½ÑƒÑŽ Ð¾Ð±Ð¾Ð±Ñ‰Ð°ÑŽÑ‰ÑƒÑŽ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð² Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð·Ñ€ÐµÐ½Ð¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Orientation Understanding with Orient Anything V2",
                    "desc": "Orient Anything V2 is a machine learning model designed to improve the understanding of 3D object orientation and rotation from images. It builds on its predecessor by allowing for the handling of objects with various rotational symmetries and estimating relative rotations directly. Key innovations include the use of generative models for creating diverse 3D assets, a robust annotation system for identifying valid orientations, and a multi-frame architecture for predicting rotations. The model has shown exceptional performance in orientation estimation and related tasks across multiple benchmarks, demonstrating its versatility in real-world applications."
                },
                "zh": {
                    "title": "å¢žå¼ºä¸‰ç»´æ–¹å‘ç†è§£çš„é©å‘½æ€§æ¨¡åž‹",
                    "desc": "Orient Anything V2 æ˜¯ä¸€ä¸ªå¢žå¼ºçš„åŸºç¡€æ¨¡åž‹ï¼Œæ—¨åœ¨ç»Ÿä¸€ç†è§£ç‰©ä½“çš„ä¸‰ç»´æ–¹å‘å’Œæ—‹è½¬ã€‚ä¸Žä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒV2 èƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒæ—‹è½¬å¯¹ç§°æ€§çš„ç‰©ä½“ï¼Œå¹¶ç›´æŽ¥ä¼°è®¡ç›¸å¯¹æ—‹è½¬ã€‚è¯¥æ¨¡åž‹é€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®žçŽ°äº†è¿™äº›æ”¹è¿›ï¼ŒåŒ…æ‹¬å¯æ‰©å±•çš„ä¸‰ç»´èµ„äº§åˆæˆã€æœ‰æ•ˆçš„æ ‡æ³¨ç³»ç»Ÿã€å¯¹ç§°æ„ŸçŸ¥çš„åˆ†å¸ƒæ‹Ÿåˆç›®æ ‡ä»¥åŠå¤šå¸§æž¶æž„ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒOrient Anything V2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ–¹å‘ä¼°è®¡çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05403",
            "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
            "url": "https://huggingface.co/papers/2601.05403",
            "abstract": "A comprehensive benchmark evaluates behavioral biases in large language models for multilingual financial misinformation detection across diverse economic scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.",
            "score": 7,
            "issue_id": 528,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "330579eb3db9e0c3",
            "authors": [
                "Zhiwei Liu",
                "Yupen Cao",
                "Yuechen Jiang",
                "Mohsinul Kabir",
                "Polydoros Giannouris",
                "Chen Xu",
                "Ziyang Xu",
                "Tianlei Zhu",
                "Tariquzzaman Faisal",
                "Triantafillos Papadopoulos",
                "Yan Wang",
                "Lingfei Qian",
                "Xueqing Peng",
                "Zhuohan Xie",
                "Ye Yuan",
                "Saeed Almheiri",
                "Abdulrazzaq Alnajjar",
                "Mingbin Chen",
                "Harry Stuart",
                "Paul Thompson",
                "Prayag Tiwari",
                "Alejandro Lopez-Lira",
                "Xue Liu",
                "Jimin Huang",
                "Sophia Ananiadou"
            ],
            "affiliations": [
                "Archimedes, Athena Research Center",
                "Athens University of Economics and Business",
                "Columbia University",
                "Dubai Police",
                "ELLIS Manchester",
                "Halmstad University",
                "Islamic University of Technology",
                "MBZUAI",
                "McGill University",
                "Mila - Quebec AI Institute",
                "Stevens Institute of Technology",
                "The FinAI",
                "The University of Manchester",
                "University of Florida",
                "University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05403.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#ethics",
                    "#low_resource",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ðŸ’°",
                "ru": {
                    "title": "Ð’Ñ‹ÑÐ²Ð»ÐµÐ½Ð¸Ðµ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¿Ñ€ÐµÐ´ÑƒÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ LLM Ð² Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð¹ Ð´ÐµÐ·Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€ÐµÐ´ÑƒÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð¹ Ð´ÐµÐ·Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ°Ñ…. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€Ð¾Ð»ÐµÐ¹, Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚ÐµÐ¹, Ð³ÐµÐ¾Ð³Ñ€Ð°Ñ„Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð², ÑÑ‚Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸ Ñ€ÐµÐ»Ð¸Ð³Ð¸Ð¾Ð·Ð½Ñ‹Ñ… Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ñ†ÐµÐ½Ð¸Ñ‚ÑŒ, ÐºÐ°Ðº LLM Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ…. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð¹ Ð´ÐµÐ·Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼, ÐºÐ¸Ñ‚Ð°Ð¹ÑÐºÐ¾Ð¼, Ð³Ñ€ÐµÑ‡ÐµÑÐºÐ¾Ð¼ Ð¸ Ð±ÐµÐ½Ð³Ð°Ð»ÑŒÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐ°Ñ… Ð¸ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ 22 Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ LLM. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ¼ÐµÑ‰ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÐºÐ°Ðº Ð² ÐºÐ¾Ð¼Ð¼ÐµÑ€Ñ‡ÐµÑÐºÐ¸Ñ…, Ñ‚Ð°Ðº Ð¸ Ð² Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…, Ñ‡Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ñ€Ð¸ÑÐº Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹."
                },
                "en": {
                    "title": "Unveiling Biases in Language Models for Financial Misinformation Detection",
                    "desc": "This paper introduces a benchmark called \textit{mfmdscen} to assess behavioral biases in large language models (LLMs) when detecting multilingual financial misinformation. The benchmark is designed to reflect complex real-world financial scenarios, incorporating factors like roles, regions, ethnicity, and religious beliefs. By evaluating 22 mainstream LLMs using this benchmark, the study highlights the persistence of behavioral biases in both commercial and open-source models. The findings emphasize the need for more nuanced approaches in training and deploying LLMs in high-stakes financial contexts."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§åž‹è¯­è¨€æ¨¡åž‹çš„é‡‘èžåå·®",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€é‡‘èžè™šå‡ä¿¡æ¯æ£€æµ‹ä¸­çš„è¡Œä¸ºåå·®ã€‚ç”±äºŽLLMsçš„è®­ç»ƒæ•°æ®ä¸»è¦æ¥æºäºŽäººç±»åˆ›ä½œçš„æ–‡æœ¬ï¼Œå®ƒä»¬å¯èƒ½ä¼šç»§æ‰¿å„ç§äººç±»åè§ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨å¤„ç†é‡‘èžä¿¡æ¯æ—¶å†³ç­–çš„ä¸ç¨³å®šæ€§å’Œä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬æž„å»ºäº†ä¸‰ç§å¤æ‚çš„é‡‘èžåœºæ™¯ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ¶µç›–å¤šç§è¯­è¨€çš„é‡‘èžè™šå‡ä¿¡æ¯æ•°æ®é›†ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°è¯„ä¼°22ç§ä¸»æµLLMsçš„è¡¨çŽ°ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œæ— è®ºæ˜¯å•†ä¸šæ¨¡åž‹è¿˜æ˜¯å¼€æºæ¨¡åž‹ï¼Œæ˜Žæ˜¾çš„è¡Œä¸ºåå·®åœ¨å„ç±»æ¨¡åž‹ä¸­æ™®éå­˜åœ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04888",
            "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
            "url": "https://huggingface.co/papers/2601.04888",
            "abstract": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
            "score": 5,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "1d9a399da8a29c5c",
            "authors": [
                "Tongyu Wen",
                "Guanting Dong",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04888.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "Ð£Ð¼Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ½Ñ‹Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð¸ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²",
                    "desc": "SmartSearch â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ LLM-based Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ„Ð¾ÐºÑƒÑÐ¸Ñ€ÑƒÐµÑ‚ÑÑ Ð½Ð° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð². Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ½Ñ‹Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ñ Ð´Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¾Ð¹ ÐºÑ€ÐµÐ´Ð¸Ñ‚Ð° Ð´Ð»Ñ Ñ‚Ð¾Ð½ÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ. ÐœÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿ÐµÑ€ÐµÐ´ÐµÐ»Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½Ð¸Ð·ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð¾Ð¸ÑÐºÐ¸ Ð¸ Ð¿ÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Ñ€Ð°ÑƒÐ½Ð´Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹. Ð¢Ñ€Ñ‘Ñ…ÑÑ‚Ð°Ð¿Ð½Ñ‹Ð¹ curriculum learning Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸ÑŽ, Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑÐ¼ Ð² ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²."
                },
                "en": {
                    "title": "Enhancing Search Quality with SmartSearch",
                    "desc": "SmartSearch is a framework designed to improve the performance of large language model (LLM)-based search agents by enhancing the quality of their intermediate search queries. It introduces two main mechanisms: process rewards, which provide detailed feedback on the quality of each query, and query refinement, which helps to improve low-quality queries through selective adjustments. The framework employs a three-stage curriculum learning approach that guides the search agent from basic imitation to more complex generalization of query improvement. Experimental results demonstrate that SmartSearch outperforms existing methods, leading to better search efficiency and higher quality queries."
                },
                "zh": {
                    "title": "SmartSearchï¼šæå‡æœç´¢ä»£ç†çš„æŸ¥è¯¢è´¨é‡",
                    "desc": "SmartSearch æ˜¯ä¸€ç§å¢žå¼ºåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰çš„æœç´¢ä»£ç†çš„æ¡†æž¶ã€‚å®ƒé€šè¿‡è¿‡ç¨‹å¥–åŠ±å’ŒæŸ¥è¯¢ä¼˜åŒ–æœºåˆ¶ï¼Œæé«˜ä¸­é—´æœç´¢æŸ¥è¯¢çš„è´¨é‡ã€‚è¯¥æ¡†æž¶é‡‡ç”¨ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œå¸®åŠ©æœç´¢ä»£ç†é€æ­¥æŽŒæ¡æ”¹è¿›æŸ¥è¯¢è´¨é‡çš„èƒ½åŠ›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒSmartSearch åœ¨æœç´¢æ•ˆçŽ‡å’ŒæŸ¥è¯¢è´¨é‡ä¸Šå‡ä¼˜äºŽçŽ°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02760",
            "title": "AnyDepth: Depth Estimation Made Easy",
            "url": "https://huggingface.co/papers/2601.02760",
            "abstract": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
            "score": 4,
            "issue_id": 524,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "a6ad530d65a90d73",
            "authors": [
                "Zeyu Ren",
                "Zeyu Zhang",
                "Wukai Li",
                "Qingxiang Liu",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai University of Engineering Science",
                "The University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02760.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#3d",
                    "#training",
                    "#data",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ðŸŽ¯",
                "ru": {
                    "title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð³Ð»ÑƒÐ±Ð¸Ð½Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Ð±Ð°Ð»Ð°Ð½Ñ Ð´Ð¸Ð·Ð°Ð¹Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð»Ñ‘Ð³ÐºÐ¸Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¼Ð¾Ð½Ð¾ÐºÑƒÐ»ÑÑ€Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð³Ð»ÑƒÐ±Ð¸Ð½Ñ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ DINOv3 Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð²Ñ‹ÑÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð². ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Simple Depth Transformer (SDT) â€” ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð½Ñ‹Ð¹ Ð´ÐµÐºÐ¾Ð´ÐµÑ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹ Ð½Ð° 85-89% Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ DPT, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð±Ð¾Ð»ÐµÐµ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ðº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð½Ð°Ð±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ñ ÐµÐ³Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ð° Ð¿ÑÑ‚Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… Ð´Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð° Ð² Ð·Ð°Ð´Ð°Ñ‡Ðµ zero-shot Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð³Ð»ÑƒÐ±Ð¸Ð½Ñ‹."
                },
                "en": {
                    "title": "Efficient Monocular Depth Estimation with DINOv3 and SDT",
                    "desc": "This paper presents a lightweight framework for monocular depth estimation that utilizes DINOv3 as a visual encoder and a compact transformer decoder called Simple Depth Transformer (SDT). The framework aims to improve accuracy while significantly reducing computational costs and the size of the dataset required for training. By employing a single-path feature fusion and upsampling process, the SDT minimizes the complexity of traditional decoders, achieving a reduction in parameters by 85%-89%. The authors also introduce a quality-based filtering strategy to enhance data quality, demonstrating that their approach outperforms existing methods in accuracy across multiple benchmarks."
                },
                "zh": {
                    "title": "è½»é‡çº§å•ç›®æ·±åº¦ä¼°è®¡çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å•ç›®æ·±åº¦ä¼°è®¡æ¡†æž¶ï¼Œä½¿ç”¨DINOv3ä½œä¸ºè§†è§‰ç¼–ç å™¨å’Œç´§å‡‘çš„å˜æ¢è§£ç å™¨ï¼Œä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚è¯¥æ¡†æž¶é€šè¿‡å•è·¯å¾„ç‰¹å¾èžåˆå’Œä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½Žäº†è·¨å°ºåº¦ç‰¹å¾èžåˆçš„è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶å‚æ•°æ•°é‡å‡å°‘çº¦85%-89%ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºŽè´¨é‡çš„è¿‡æ»¤ç­–ç•¥ï¼Œä»¥åŽ»é™¤æœ‰å®³æ ·æœ¬ï¼Œä»Žè€Œå‡å°‘æ•°æ®é›†å¤§å°å¹¶æé«˜æ•´ä½“è®­ç»ƒè´¨é‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„DPTæ¨¡åž‹ï¼Œå¼ºè°ƒäº†æ¨¡åž‹è®¾è®¡ä¸Žæ•°æ®è´¨é‡ä¹‹é—´çš„å¹³è¡¡å¯¹äºŽé«˜æ•ˆå’Œå¯æ³›åŒ–çš„é›¶-shotæ·±åº¦ä¼°è®¡çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05637",
            "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
            "url": "https://huggingface.co/papers/2601.05637",
            "abstract": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  \t\t\t\t\tAI-generated summary \t\t\t\t As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
            "score": 2,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "30ec0c4c1e6d1f97",
            "authors": [
                "Emily Cheng",
                "Carmen Amo Alonso",
                "Federico Danieli",
                "Arno Blaas",
                "Luca Zappella",
                "Pau Rodriguez",
                "Xavier Suau"
            ],
            "affiliations": [
                "Apple",
                "Stanford",
                "Universitat Pompeu Fabra"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05637.jpg",
            "data": {
                "categories": [],
                "emoji": "ðŸŽ®",
                "ru": {
                    "title": "Ð£Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð¾Ñ‚ Ð¸Ð»Ð»ÑŽÐ·Ð¸Ð¸ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ ÐµÐ³Ð¾ Ð³Ñ€Ð°Ð½Ð¸Ñ†",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ñ‚ÐµÐ¾Ñ€ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾ÑÑ‚Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð° Ð´Ð¾ÑÑ‚Ð¸Ð¶Ð¸Ð¼Ñ‹Ñ… ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð½Ð¾-Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… Ð³Ñ€Ð°Ð½Ð¸Ñ† Ð¾ÑˆÐ¸Ð±ÐºÐ¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€ÑƒÑŽÑ‚ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÐºÐ°Ðº Ð·Ð°Ð´Ð°Ñ‡Ñƒ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð² ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾ÑÑ‚Ð¸ Ð² Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ…. Ð“Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð´Ð»Ñ Ð»ÑŽÐ±Ñ‹Ñ… Ñ‡Ñ‘Ñ€Ð½Ñ‹Ñ… ÑÑ‰Ð¸ÐºÐ¾Ð² Ð½ÐµÐ»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±ÐµÐ· Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…, ÐºÑ€Ð¾Ð¼Ðµ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ð°. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ…Ñ€ÑƒÐ¿ÐºÐ° Ð¸ ÑÐ¸Ð»ÑŒÐ½Ð¾ Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°, Ñ‡Ñ‚Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ."
                },
                "en": {
                    "title": "Understanding the Fragility of Generative Model Controllability",
                    "desc": "This paper analyzes how controllable generative models really are, using a new theoretical framework. It introduces an algorithm that estimates the sets of outputs that can be controlled in dialogue systems, providing guarantees on the accuracy of these estimates. The findings reveal that the ability to control these models is often fragile and varies greatly depending on the context. This emphasizes the importance of understanding the limits of model controllability before trying to exert control over them."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ¨¡åž‹çš„å¯æŽ§æ€§ï¼šè„†å¼±è€Œä¾èµ–ä¸Šä¸‹æ–‡",
                    "desc": "æœ¬æ–‡é€šè¿‡ä¸€ä¸ªç†è®ºæ¡†æž¶åˆ†æžç”Ÿæˆæ¨¡åž‹çš„å¯æŽ§æ€§ï¼Œä¼°è®¡å¯æŽ§é›†å¹¶æä¾›æ— åˆ†å¸ƒç•Œé™çš„ä¿è¯ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œç”Ÿæˆæ¨¡åž‹çš„å¯æŽ§æ€§æ˜¯è„†å¼±çš„ï¼Œå¹¶ä¸”é«˜åº¦ä¾èµ–äºŽä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç”¨äºŽåœ¨å¯¹è¯è®¾ç½®ä¸­ä¼°è®¡æ¨¡åž‹çš„å¯æŽ§é›†ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å®žè¯éªŒè¯ã€‚ç»“æžœæ˜¾ç¤ºï¼Œæ¨¡åž‹çš„å¯æŽ§æ€§å¹¶ä¸å¦‚é¢„æœŸï¼Œå¼ºè°ƒäº†å¯¹å¯æŽ§æ€§è¿›è¡Œä¸¥æ ¼åˆ†æžçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04823",
            "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
            "url": "https://huggingface.co/papers/2601.04823",
            "abstract": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
            "score": 2,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "81ecc151e1820135",
            "authors": [
                "Guanzhi Deng",
                "Bo Li",
                "Ronghao Chen",
                "Huacan Wang",
                "Linqi Song",
                "Lijie Wen"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Peking University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04823.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training"
                ],
                "emoji": "ðŸŽ¯",
                "ru": {
                    "title": "Ð£Ð¼Ð½Ð°Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð²: Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€Ð°Ð½Ð³Ð¸ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ DR-LoRA Ð´Ð»Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ€Ð°Ð½Ð³Ð¾Ð² Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð¾Ð² LoRA Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Mixture-of-Experts Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ fine-tuning. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ð²Ð¾Ð´ÑÑ‚ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð·Ð½Ð°Ñ‡Ð¸Ð¼Ð¾ÑÑ‚Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð½Ð³Ð° Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð½Ð¾ÑÑ‚Ð¸ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð° Ð² Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸. ÐœÐµÑ‚Ð¾Ð´ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð³ÐµÑ‚ÐµÑ€Ð¾Ð³ÐµÐ½Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð½Ð³Ð¾Ð², Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€ÑƒÑÑÑŒ Ðº ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ðµ, Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð²ÑÐµÐ¼ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð°Ð¼. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ DR-LoRA Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¸ Ñ‚Ð¾Ð¹ Ð¶Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°, Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°Ñ Ð»ÑƒÑ‡ÑˆÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ñ Ð±Ð¾Ð»ÐµÐµ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²."
                },
                "en": {
                    "title": "Dynamic Expert Tuning for Efficient MoE Performance",
                    "desc": "The paper introduces DR-LoRA, a method that optimizes the use of parameters in Mixture-of-Experts (MoE) models by dynamically adjusting the LoRA ranks of different experts based on the specific needs of a task. Traditional approaches assign the same LoRA rank to all experts, which can lead to inefficiencies as some experts may be over-allocated while others are under-utilized. DR-LoRA addresses this issue by using an Expert Saliency Scoring mechanism that evaluates how much additional capacity each expert requires based on their relevance to the task. The results show that DR-LoRA significantly improves performance and parameter efficiency compared to standard LoRA methods and static rank assignments."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ä¸“å®¶ç­‰çº§ï¼Œæå‡æ¨¡åž‹æ€§èƒ½",
                    "desc": "DR-LoRAæ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´Mixture-of-Expertsæ¨¡åž‹ä¸­LoRAç­‰çº§çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚æé«˜å‚æ•°æ•ˆçŽ‡å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸ºæ‰€æœ‰ä¸“å®¶åˆ†é…ç›¸åŒçš„LoRAç­‰çº§ï¼Œå¿½è§†äº†ä¸“å®¶ä¹‹é—´çš„åŠŸèƒ½ä¸“é—¨åŒ–ï¼Œå¯¼è‡´èµ„æºåˆ†é…ä¸å‡ã€‚DR-LoRAé€šè¿‡ä¸“å®¶æ˜¾è‘—æ€§è¯„åˆ†æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ä¸“å®¶çš„LoRAç­‰çº§ï¼Œä¼˜å…ˆæ‰©å±•éœ€æ±‚æ›´é«˜çš„ä¸“å®¶ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDR-LoRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜äºŽæ ‡å‡†LoRAå’Œé™æ€åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹å®žçŽ°æ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04726",
            "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
            "url": "https://huggingface.co/papers/2601.04726",
            "abstract": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
            "score": 2,
            "issue_id": 526,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "156f14ff101f34b7",
            "authors": [
                "Yuyang Hu",
                "Jiongnan Liu",
                "Jiejun Tan",
                "Yutao Zhu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04726.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#graphs"
                ],
                "emoji": "ðŸ§­",
                "ru": {
                    "title": "Ð“Ñ€Ð°Ñ„ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ ÐºÐ°Ðº Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ°Ñ€Ñ‚Ð° Ð´Ð»Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²",
                    "desc": "CompassMem â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·ÑƒÐµÑ‚ Ð¾Ð¿Ñ‹Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð² Ð²Ð¸Ð´Ðµ Ð³Ñ€Ð°Ñ„Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ñ ÑÐ²Ð½Ñ‹Ð¼Ð¸ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑÐ²ÑÐ·ÑÐ¼Ð¸. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ñƒ, CompassMem Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ Ð½Ð°Ð²Ð¸Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð°Ð´ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ð¼Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑÐ¼Ð¸. Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ð¿Ñ‹Ñ‚ Ð½Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¸ ÑÐ²ÑÐ·Ñ‹Ð²Ð°ÐµÑ‚ Ð¸Ñ… Ñ‡ÐµÑ€ÐµÐ· ÑÐ²Ð½Ñ‹Ðµ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ, ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÑƒÑŽ ÐºÐ°Ñ€Ñ‚Ñƒ Ð´Ð»Ñ Ñ†ÐµÐ»ÐµÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ CompassMem ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÐºÐ°Ðº ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ñ‚Ð°Ðº Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹-Ð¾ÑÐ½Ð¾Ð²."
                },
                "en": {
                    "title": "Empowering Long-Horizon Reasoning with Event-Centric Memory",
                    "desc": "CompassMem is a novel memory framework designed for intelligent agents that enhances their ability to reason and plan over extended periods. It organizes experiences into an Event Graph, which captures the logical relationships between events, allowing for structured memory navigation. Unlike traditional flat memory systems that rely on simple similarity-based retrieval, CompassMem enables agents to access memories in a way that supports deeper reasoning and decision-making. Experiments show that this approach significantly improves both memory retrieval and reasoning capabilities in various tasks."
                },
                "zh": {
                    "title": "CompassMemï¼šç»“æž„åŒ–è®°å¿†å¯¼èˆªçš„æ–°æ–¹æ³•",
                    "desc": "CompassMemæ˜¯ä¸€ç§ä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„è®°å¿†æ¡†æž¶ï¼Œå®ƒå°†ç»éªŒç»„ç»‡æˆäº‹ä»¶å›¾ï¼Œä»¥ä¾¿å®žçŽ°ç»“æž„åŒ–çš„è®°å¿†å¯¼èˆªå’Œé•¿æ—¶é—´æŽ¨ç†ã€‚è¯¥æ¡†æž¶é€šè¿‡é€æ­¥å°†ç»éªŒåˆ†æ®µä¸ºäº‹ä»¶ï¼Œå¹¶é€šè¿‡æ˜Žç¡®çš„é€»è¾‘å…³ç³»å°†å®ƒä»¬è¿žæŽ¥èµ·æ¥ï¼Œä»Žè€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚ä¸ŽçŽ°æœ‰çš„æ‰å¹³è®°å¿†å­˜å‚¨æ–¹å¼ä¸åŒï¼ŒCompassMemèƒ½å¤Ÿæ•æ‰ç»éªŒä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæ”¯æŒæ›´æ·±å±‚æ¬¡çš„æŽ¨ç†ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCompassMemåœ¨å¤šä¸ªåŸºç¡€æ¨¡åž‹ä¸Šéƒ½æ˜¾è‘—æé«˜äº†æ£€ç´¢å’ŒæŽ¨ç†çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04544",
            "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2601.04544",
            "abstract": "A multi-agent system router that uses dynamic agent onboarding and natural language reasoning chains to improve routing accuracy and reduce conflicts in enterprise applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.",
            "score": 2,
            "issue_id": 528,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "50c6ddb04d917f2a",
            "authors": [
                "Jiuzhou Zhao",
                "Chunrong Chen",
                "Chenqi Qiao",
                "Lebin Zheng",
                "Minqi Han",
                "Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang"
            ],
            "affiliations": [
                "Tencent Cloud"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04544.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "ðŸš¦",
                "ru": {
                    "title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ Ð³Ð¸Ð±ÐºÐ¸Ñ… Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ TCAR, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð² Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð². Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸ÑŽ Ð½Ð¾Ð²Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð½Ð° ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ ÑÐ·Ñ‹ÐºÐµ Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð°Ð±Ð¾Ñ€Ð° Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð±Ð¾Ñ€Ð°. ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° TCAR Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ, Ð³Ð´Ðµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð°Ñ‚ÐµÐ¼ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð¸ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‚ÑÑ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ð¾Ð²Ñ‹ÑˆÐ°ÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸, ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ñ‹ Ð¸ Ð¾ÑÑ‚Ð°Ñ‘Ñ‚ÑÑ Ð½Ð°Ð´Ñ‘Ð¶Ð½Ñ‹Ð¼ Ð² Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ñ‹Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Routing for Enhanced Multi-Agent Collaboration",
                    "desc": "This paper presents TCAndon-Router (TCAR), a novel multi-agent system router designed to enhance routing accuracy and minimize conflicts in enterprise applications. TCAR introduces dynamic agent onboarding, allowing for the seamless integration of new agents as business needs evolve. It utilizes natural language reasoning chains to identify a set of candidate agents, rather than relying on static single-label decisions, which often lead to routing conflicts. Experimental results show that TCAR outperforms traditional routing methods by improving accuracy and robustness in complex scenarios."
                },
                "zh": {
                    "title": "åŠ¨æ€æ™ºèƒ½ä½“æŽ¥å…¥ï¼Œæå‡è·¯ç”±å‡†ç¡®æ€§ï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTCAndon-Routerï¼ˆTCARï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè·¯ç”±å™¨ï¼Œæ—¨åœ¨æé«˜ä¼ä¸šåº”ç”¨ä¸­çš„è·¯ç”±å‡†ç¡®æ€§å¹¶å‡å°‘å†²çªã€‚TCARæ”¯æŒåŠ¨æ€æ™ºèƒ½ä½“æŽ¥å…¥ï¼Œå¹¶åœ¨é¢„æµ‹å€™é€‰æ™ºèƒ½ä½“ä¹‹å‰ç”Ÿæˆè‡ªç„¶è¯­è¨€æŽ¨ç†é“¾ï¼Œä»Žè€Œæ›´å¥½åœ°å¤„ç†æŸ¥è¯¢ã€‚ä¸Žä¼ ç»Ÿçš„é™æ€å•æ ‡ç­¾å†³ç­–æ–¹æ³•ä¸åŒï¼ŒTCARèƒ½å¤Ÿçµæ´»åº”å¯¹ä¸šåŠ¡é¢†åŸŸçš„æ‰©å±•ï¼Œé¿å…äº†å› æ™ºèƒ½ä½“èƒ½åŠ›é‡å è€Œå¯¼è‡´çš„è·¯ç”±å†²çªã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTCARåœ¨å…¬å…±æ•°æ®é›†å’ŒçœŸå®žä¼ä¸šæ•°æ®ä¸Šæ˜¾è‘—æé«˜äº†è·¯ç”±å‡†ç¡®æ€§ï¼Œå¢žå¼ºäº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05960",
            "title": "Distilling Feedback into Memory-as-a-Tool",
            "url": "https://huggingface.co/papers/2601.05960",
            "abstract": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
            "score": 1,
            "issue_id": 527,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "eebae4efe4e25421",
            "authors": [
                "VÃ­ctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI Technologies"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05960.jpg",
            "data": {
                "categories": [],
                "emoji": "ðŸ’¾",
                "ru": {
                    "title": "Ð˜Ð· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¸ Ð² ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ: ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹ Ð¿Ñ€Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐµ",
                    "desc": "ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹ Ð¿Ñ€Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐµ, Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÑ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¸ Ð² ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼Ñ‹Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ñ„Ð°Ð¹Ð»Ð¾Ð²ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ñ‹Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð². ÐÐ° Ð½Ð¾Ð²Ð¾Ð¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ Rubric Feedback Bench Ð¾Ð½Ð¸ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÑŽÑ‚ Ð¼ÐµÑ‚Ð¾Ð´, Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ð¾ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ðµ LLM Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÑŽÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð¾Ð² Ñ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ñ‚Ñ€Ð°Ñ‚ Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ð¸Ð»Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð¢Ð°ÐºÐ¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÐµÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÑÑƒÑ€ÑÑ‹ Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð³Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²."
                },
                "en": {
                    "title": "Transforming Critiques into Cost-Effective Guidelines for LLMs",
                    "desc": "This paper presents a framework that enhances the efficiency of large language models (LLMs) by transforming temporary critiques into reusable guidelines. It utilizes a file-based memory system and agent-controlled tool calls to streamline the inference process. The framework is evaluated using the Rubric Feedback Bench, which is designed for rubric-based learning assessments. Results show that the modified LLMs achieve comparable performance to traditional test-time refinement methods while significantly lowering inference costs."
                },
                "zh": {
                    "title": "é™ä½ŽæŽ¨ç†æˆæœ¬ï¼Œæå‡æ¨¡åž‹æ•ˆçŽ‡çš„åˆ›æ–°æ¡†æž¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æž¶ï¼Œé€šè¿‡æ–‡ä»¶åŸºç¡€çš„è®°å¿†ç³»ç»Ÿå’Œä»£ç†æŽ§åˆ¶çš„å·¥å…·è°ƒç”¨ï¼Œå°†çž¬æ—¶æ‰¹è¯„è½¬åŒ–ä¸ºå¯æ£€ç´¢çš„æŒ‡å¯¼æ–¹é’ˆï¼Œä»Žè€Œé™ä½ŽæŽ¨ç†æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨ä¸€ä¸ªæ–°çš„åŸºäºŽè¯„åˆ†æ ‡å‡†çš„å­¦ä¹ æ•°æ®é›†Rubric Feedback Benchä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œå¢žå¼ºåž‹å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè¿…é€Ÿè¾¾åˆ°æµ‹è¯•æ—¶ç²¾ç»†åŒ–ç®¡é“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½ŽæŽ¨ç†æˆæœ¬ã€‚æ­¤æ¡†æž¶ä¸ºæé«˜æœºå™¨å­¦ä¹ æ¨¡åž‹çš„æ•ˆçŽ‡æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05899",
            "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
            "url": "https://huggingface.co/papers/2601.05899",
            "abstract": "A new tower defense-based environment called TowerMind is introduced for evaluating large language models' planning and decision-making capabilities with low computational requirements and multimodal observations.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
            "score": 1,
            "issue_id": 537,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "b20f557319791d13",
            "authors": [
                "Dawei Wang",
                "Chengming Zhou",
                "Di Zhao",
                "Xinyuan Liu",
                "Marci Chi Ma",
                "Gary Ushaw",
                "Richard Davison"
            ],
            "affiliations": [
                "Newcastle University, United Kingdom",
                "University of Auckland, New Zealand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05899.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#games",
                    "#open_source",
                    "#multimodal",
                    "#agents",
                    "#rl",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ðŸ›¡ï¸",
                "ru": {
                    "title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ LLM Ñ‡ÐµÑ€ÐµÐ· tower defense ÑÑ€ÐµÐ´Ñƒ",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° TowerMind â€” Ð½Ð¾Ð²Ð°Ñ ÑÑ€ÐµÐ´Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ tower defense Ð¸Ð³Ñ€ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð½Ð¸Ð·ÐºÐ¸Ð¼Ð¸ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸. Ð¡Ñ€ÐµÐ´Ð° Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ñ, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¿Ð¸ÐºÑÐµÐ»ÑŒÐ½Ñ‹Ðµ, Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸Ð³Ñ€Ð¾Ð²Ð¾Ð³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ LLM Ð² ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. ÐŸÑ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¸ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ñ€Ñ‹Ð² Ð² Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ LLM Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð°Ð¼Ð¸, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð² Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¿Ð»Ð°Ð½Ð¾Ð² Ð¸ Ð½ÐµÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹. TowerMind Ð´Ð¾Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ Ð»Ð°Ð½Ð´ÑˆÐ°Ñ„Ñ‚ ÑÑ€ÐµÐ´ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ AI-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼."
                },
                "en": {
                    "title": "TowerMind: A New Benchmark for Evaluating LLMs in Strategic Planning",
                    "desc": "TowerMind is a new environment designed to test the planning and decision-making skills of large language models (LLMs) using a tower defense game format. It allows for low computational costs while providing multimodal observations, including text and game states. The environment helps identify the strengths and weaknesses of LLMs, revealing gaps in their performance compared to human experts, particularly in planning and decision-making. Additionally, TowerMind offers customizable benchmarks to evaluate LLMs and classic reinforcement learning algorithms, contributing to the advancement of AI agent research."
                },
                "zh": {
                    "title": "TowerMindï¼šè¯„ä¼°è¯­è¨€æ¨¡åž‹çš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¡”é˜²æ¸¸æˆçŽ¯å¢ƒTowerMindï¼Œç”¨äºŽè¯„ä¼°å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚TowerMindå…·æœ‰ä½Žè®¡ç®—éœ€æ±‚å’Œå¤šæ¨¡æ€è§‚å¯Ÿç©ºé—´ï¼ŒåŒ…æ‹¬åƒç´ ã€æ–‡æœ¬å’Œç»“æž„åŒ–æ¸¸æˆçŠ¶æ€è¡¨ç¤ºã€‚é€šè¿‡è®¾è®¡äº”ä¸ªåŸºå‡†å…³å¡ï¼Œç ”ç©¶å‘çŽ°LLMsåœ¨èƒ½åŠ›å’Œå¹»è§‰ç»´åº¦ä¸Šä¸Žäººç±»ä¸“å®¶å­˜åœ¨æ˜Žæ˜¾å·®è·ï¼Œå¹¶æ­ç¤ºäº†LLMsåœ¨è§„åˆ’éªŒè¯å’Œå†³ç­–å¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ã€‚TowerMindä¸ºAIä»£ç†é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¿ƒè¿›äº†çŽ°æœ‰å®žæ—¶æˆ˜ç•¥æ¸¸æˆçŽ¯å¢ƒçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05851",
            "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
            "url": "https://huggingface.co/papers/2601.05851",
            "abstract": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "7a9a52f4b2a51b55",
            "authors": [
                "Sandeep Mishra",
                "Devichand Budagam",
                "Anubhab Mandal",
                "Bishal Santra",
                "Pawan Goyal",
                "Manish Gupta"
            ],
            "affiliations": [
                "IIT Kharagpur",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05851.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ðŸ’¬",
                "ru": {
                    "title": "Ð£Ð¼Ð½Ð¾Ðµ Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð° Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ (MAC), ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹ Ð² Ñ‡Ð°Ñ‚Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ Ð½Ð°Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð² MMDialog Ð¸ ImageChat, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ñ€Ð¾Ð²ÐµÐ»Ð¸ Ð¾Ñ†ÐµÐ½ÐºÑƒ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ñ… vision-language Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¸ Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼Ð¸ Ð±Ð°Ð·ÐµÐ»Ð°Ð¹Ð½Ð°Ð¼Ð¸. ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð²ÐºÐ»Ð°Ð´ â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Router-Suggest, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ð¸ VLM Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°, Ñ‡Ñ‚Ð¾ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð² 2.3-10x Ñ€Ð°Ð·. ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¾, Ñ‡Ñ‚Ð¾ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ð¾ ÑƒÐ´Ð¾Ð²Ð»ÐµÑ‚Ð²Ð¾Ñ€Ñ‘Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ ÑÐ½Ð¸Ð¶Ð°ÑŽÑ‚ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹ Ð½Ð° Ð¿ÐµÑ‡Ð°Ñ‚ÑŒ Ð² Ð¼Ð½Ð¾Ð³Ð¾Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ð½Ñ‹Ñ… Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°Ñ…."
                },
                "en": {
                    "title": "Enhancing Conversations with Multimodal Auto-Completion",
                    "desc": "This paper introduces Multimodal Auto-Completion (MAC), a method that enhances real-time predictions in conversational interfaces by utilizing both visual and textual information. Unlike traditional text-only auto-completion, MAC integrates visual cues to better understand user intent and improve prediction accuracy. The authors develop a router framework called Router-Suggest, which efficiently selects between vision-language models (VLMs) and textual models based on the context of the conversation. Their findings demonstrate that VLMs significantly outperform traditional models in user satisfaction and typing efficiency, highlighting the importance of multimodal context in creating smarter digital assistants."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼šæ™ºèƒ½åŠ©æ‰‹çš„æ–°æ–¹å‘",
                    "desc": "å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æé«˜å¯¹è¯ç•Œé¢çš„å®žæ—¶é¢„æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼ˆMACï¼‰ä»»åŠ¡ï¼Œé€šè¿‡éƒ¨åˆ†è¾“å…¥æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢é¢„æµ‹å³å°†è¾“å…¥çš„å­—ç¬¦ã€‚ä¸Žä¼ ç»Ÿçš„ä»…æ–‡æœ¬è‡ªåŠ¨è¡¥å…¨ï¼ˆTACï¼‰ä¸åŒï¼ŒMACåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé¢„æµ‹ï¼Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Router-Suggestæ¡†æž¶ï¼Œæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æ–‡æœ¬æ¨¡åž‹å’Œè§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆçŽ‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05741",
            "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
            "url": "https://huggingface.co/papers/2601.05741",
            "abstract": "ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.",
            "score": 1,
            "issue_id": 530,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "a3ae1020afe47c87",
            "authors": [
                "Guray Ozgur",
                "Eduarda Caldeira",
                "Tahar Chettaoui",
                "Jan Niklas Kolf",
                "Marco Huber",
                "Naser Damer",
                "Fadi Boutros"
            ],
            "affiliations": [
                "Fraunhofer IGD",
                "TU Darmstadt"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ðŸŽ­",
                "ru": {
                    "title": "Ð¡Ñ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² ÐºÐ°Ðº Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð»Ð¸Ñ† Ð² Vision Transformer",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ViTNT-FIQA â€” Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð»Ð¸Ñ† Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð¿Ð°Ñ‚Ñ‡ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· ÑÐ»Ð¾Ð¸ Vision Transformer. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸Ð·ÑƒÑŽÑ‚ÑÑ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸ÐµÐ¹ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð², Ð° Ð´ÐµÐ³Ñ€Ð°Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ñ…Ð°Ð¾Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ. ÐœÐµÑ‚Ð¾Ð´ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ ÐµÐ²ÐºÐ»Ð¸Ð´Ð¾Ð²Ñ‹ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼Ð¸ Ð¸Ð· Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð±Ð»Ð¾ÐºÐ¾Ð² Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð° Ð¸ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ñ… Ð² Ð¾Ñ†ÐµÐ½ÐºÑƒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. ÐŸÐ¾Ð´Ñ…Ð¾Ð´ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð° Ð±ÐµÐ· Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ð° Ð²Ð¾ÑÑŒÐ¼Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ…."
                },
                "en": {
                    "title": "Assessing Face Image Quality with Stability in Vision Transformers",
                    "desc": "ViTNT-FIQA is a novel method for assessing the quality of face images by analyzing the stability of patch embeddings through Vision Transformer (ViT) blocks. Unlike traditional methods that rely on final-layer outputs or require multiple passes, this approach is training-free and only needs a single forward pass. It measures the consistency of feature refinement by calculating Euclidean distances between normalized patch embeddings from consecutive transformer blocks. The results show that high-quality images maintain stable trajectories, while lower-quality images exhibit erratic changes, making ViTNT-FIQA efficient and effective for face image quality assessment."
                },
                "zh": {
                    "title": "ViTNT-FIQAï¼šé«˜æ•ˆçš„é¢éƒ¨å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•",
                    "desc": "ViTNT-FIQAæ˜¯ä¸€ç§é¢éƒ¨å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡åˆ†æžè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å—ä¸­è¡¥ä¸åµŒå…¥çš„ç¨³å®šæ€§æ¥è¡¡é‡å›¾åƒè´¨é‡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ä¸åŒï¼ŒViTNT-FIQAä¸éœ€è¦è®­ç»ƒï¼Œä¸”åªéœ€ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯å®Œæˆè¯„ä¼°ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ç›¸é‚»å˜æ¢å™¨å—ä¹‹é—´L2å½’ä¸€åŒ–è¡¥ä¸åµŒå…¥çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œç”Ÿæˆå›¾åƒçº§è´¨é‡è¯„åˆ†ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œé«˜è´¨é‡çš„é¢éƒ¨å›¾åƒåœ¨ç‰¹å¾æç‚¼è¿‡ç¨‹ä¸­è¡¨çŽ°å‡ºç¨³å®šçš„è½¨è¿¹ï¼Œè€ŒåŠ£è´¨å›¾åƒåˆ™æ˜¾ç¤ºå‡ºä¸è§„åˆ™çš„å˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05503",
            "title": "Over-Searching in Search-Augmented Large Language Models",
            "url": "https://huggingface.co/papers/2601.05503",
            "abstract": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "33cf12cd1c7618ab",
            "authors": [
                "Roy Xie",
                "Deepak Gopinath",
                "David Qiu",
                "Dong Lin",
                "Haitian Sun",
                "Saloni Potdar",
                "Bhuwan Dhingra"
            ],
            "affiliations": [
                "Apple",
                "Duke University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05503.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#hallucinations",
                    "#optimization"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "ÐšÐ¾Ð³Ð´Ð° Ð¿Ð¾Ð¸ÑÐº Ð²Ñ€ÐµÐ´Ð¸Ñ‚: ÐºÐ°Ðº Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ñ‡Ñ€ÐµÐ·Ð¼ÐµÑ€Ð½Ñ‹Ñ… Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð¿Ð¾Ð¸ÑÐº-Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… LLM",
                    "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ñ‡Ñ€ÐµÐ·Ð¼ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼, ÐºÐ¾Ð³Ð´Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ñ‰Ð°ÐµÑ‚ÑÑ Ðº Ð²Ð½ÐµÑˆÐ½Ð¸Ð¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº Ð¿ÑƒÑÑ‚Ñ€Ð°Ñ‚Ð°Ð¼ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² Ð¸ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸ÑÐ¼. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÑÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¾Ñ†ÐµÐ½ÐºÑƒ Ð½Ð°Ð´-Ð¿Ð¾Ð¸ÑÐºÐ°, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð¸ÑÐº ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÐ¼Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ñ…, Ð½Ð¾ Ð²Ñ€ÐµÐ´Ð¸Ñ‚ Ð¾Ñ‚ÐºÐ°Ð·Ñƒ Ð¾Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð½ÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð° ÑÑ„Ñ„ÐµÐºÑ‚ Ð±Ð¾Ð»ÐµÐµ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½ Ð² ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° ÑƒÑÐ¸Ð»Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ Ð·Ð°ÑˆÑƒÐ¼Ð»ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ñ… Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ Ð½Ð°ÐºÐ°Ð¿Ð»Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð² Ð¼Ð½Ð¾Ð³Ð¾Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ð½Ñ‹Ñ… Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°Ñ…, Ð½Ð¾ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚Ñ€Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð² ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¾Ñ‚ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ. Ð”Ð»Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ° Tokens Per Correctness, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ñ‚Ñ€Ð°Ð¶Ð°ÐµÑ‚ ÐºÐ¾Ð¼Ð¿Ñ€Ð¾Ð¼Ð¸ÑÑ Ð¼ÐµÐ¶Ð´Ñƒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¸ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ð°Ð¼Ð¸ Ð½Ð° Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ."
                },
                "en": {
                    "title": "Optimizing Search for Smarter Language Models",
                    "desc": "This paper addresses the issue of over-searching in search-augmented large language models (LLMs), which can lead to wasted computational resources and hallucinations. The authors systematically evaluate how over-searching affects different model types and conversation contexts, revealing that while search can improve accuracy for answerable queries, it can negatively impact responses for unanswerable ones. They introduce a new metric, Tokens Per Correctness (TPC), to measure the efficiency of search-augmented LLMs and highlight the importance of the quality of retrieved evidence. The study also explores strategies to mitigate over-searching at both the query and retrieval levels, contributing to the development of more efficient LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æœç´¢ï¼Œæå‡è¯­è¨€æ¨¡åž‹æ•ˆçŽ‡",
                    "desc": "æœ¬è®ºæ–‡æŽ¢è®¨äº†å¢žå¼ºæœç´¢çš„å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä½¿ç”¨æœç´¢å·¥å…·æ—¶çš„è¿‡åº¦æœç´¢è¡Œä¸ºï¼Œè¿™ç§è¡Œä¸ºä¼šæµªè´¹è®¡ç®—èµ„æºå¹¶å¼•å…¥å¹»è§‰ã€‚ç ”ç©¶å‘çŽ°ï¼Œæœç´¢é€šå¸¸èƒ½æé«˜å¯å›žç­”é—®é¢˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨æ— æ³•å›žç­”çš„é—®é¢˜ä¸Šå´ä¼šå¯¼è‡´é”™è¯¯çš„æ”¾å¼ƒã€‚è¿‡åº¦æœç´¢åœ¨å¤æ‚æŽ¨ç†æ¨¡åž‹å’Œæ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­æ›´ä¸ºæ˜Žæ˜¾ï¼Œå¹¶ä¸”åœ¨å¤šè½®å¯¹è¯ä¸­ä¼šåŠ å‰§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”æ¯ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä»¤ç‰Œæ•°ï¼ˆTPCï¼‰ï¼Œå¹¶æŽ¢è®¨äº†åœ¨æŸ¥è¯¢å’Œæ£€ç´¢å±‚é¢ä¸Šçš„ç¼“è§£æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05870",
            "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
            "url": "https://huggingface.co/papers/2601.05870",
            "abstract": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
            "score": 0,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "d2103f98d8252f93",
            "authors": [
                "Huilin Deng",
                "Hongchen Luo",
                "Yue Zhu",
                "Long Li",
                "Zhuoyue Chen",
                "Xinghao Zhao",
                "Ming Li",
                "Jihai Zhang",
                "Mengchang Wang",
                "Yang Cao",
                "Yu Kang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Northeastern University",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05870.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#rl",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ðŸŒ¿",
                "ru": {
                    "title": "Ð’ÐµÑ‚Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ ÑƒÐ·ÐºÐ¾Ðµ Ð¼ÐµÑÑ‚Ð¾",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ IIB-LPO Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ ÐºÐ¾Ð»Ð»Ð°Ð¿ÑÐ° Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€ÐµÑˆÐµÐ½Ð¸ÑŽ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ð¼Ð¸ Ð½Ð°Ð³Ñ€Ð°Ð´Ð°Ð¼Ð¸. Ð’Ð¼ÐµÑÑ‚Ð¾ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð² Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ð¸ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸, Ð¼ÐµÑ‚Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ ÑƒÐ·ÐºÐ¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ. Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ ÑƒÐ·ÐºÐ¾Ðµ Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ ÐºÐ°Ðº Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹ Ð¸ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ ÑÐ°Ð¼Ð¾Ð³Ñ€Ð°Ð´Ð°Ñ†Ð¸Ð¸, Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°Ñ Ð±Ð°Ð»Ð°Ð½Ñ Ð¼ÐµÐ¶Ð´Ñƒ ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚ÑŒÑŽ Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒÑŽ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ð° Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð½Ð° 5,3% Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ Ð½Ð° 7,4% Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¼Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸."
                },
                "en": {
                    "title": "Branching Out: Enhancing LLM Reasoning with IIB-LPO",
                    "desc": "This paper introduces Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO) to tackle the issue of exploration collapse in Large Language Model (LLM) reasoning. The authors highlight that traditional methods often lead to over-optimized behaviors due to semantic homogeneity in random rollouts. IIB-LPO innovatively shifts the focus from merely adjusting token distributions to creating diverse reasoning paths through topological branching at high-entropy states. The approach not only filters trajectories using the Information Bottleneck principle but also incorporates a self-reward mechanism, resulting in improved accuracy and diversity in reasoning tasks."
                },
                "zh": {
                    "title": "é€šè¿‡ä¿¡æ¯ç“¶é¢ˆå®žçŽ°æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼Œæ‰“ç ´æŽ¢ç´¢å´©æºƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¿­ä»£ä¿¡æ¯ç“¶é¢ˆä¸‹çš„æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼ˆIIB-LPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æŽ¨ç†ä¸­çš„æŽ¢ç´¢å´©æºƒé—®é¢˜ã€‚é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŽŸç†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨é«˜ç†µçŠ¶æ€ä¸‹è§¦å‘æ½œåœ¨åˆ†æ”¯ï¼Œä»Žè€Œå¤šæ ·åŒ–æŽ¨ç†è·¯å¾„ã€‚ä¸Žä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒIIB-LPOä¸å†ä¾èµ–äºŽç»Ÿè®¡æ‰°åŠ¨ï¼Œè€Œæ˜¯é€šè¿‡æ‹“æ‰‘åˆ†æ”¯æ¥å¢žå¼ºæŽ¢ç´¢èƒ½åŠ›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒIIB-LPOåœ¨å››ä¸ªæ•°å­¦æŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œå‡†ç¡®çŽ‡å’Œå¤šæ ·æ€§æŒ‡æ ‡å‡è¶…è¿‡äº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05699",
            "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
            "url": "https://huggingface.co/papers/2601.05699",
            "abstract": "Afri-MCQA benchmark demonstrates poor performance of open-weight LLMs in African languages, highlighting the need for culturally grounded pretraining and speech-first approaches in AI development.  \t\t\t\t\tAI-generated summary \t\t\t\t Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)",
            "score": 0,
            "issue_id": 531,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "41b7253b439939ec",
            "authors": [
                "Atnafu Lambebo Tonja",
                "Srija Anand",
                "Emilio Villa-Cueva",
                "Israel Abebe Azime",
                "Jesujoba Oluwadara Alabi",
                "Muhidin A. Mohamed",
                "Debela Desalegn Yadeta",
                "Negasi Haile Abadi",
                "Abigail Oppong",
                "Nnaemeka Casmir Obiefuna",
                "Idris Abdulmumin",
                "Naome A Etori",
                "Eric Peter Wairagala",
                "Kanda Patrick Tshinu",
                "Imanigirimbabazi Emmanuel",
                "Gabofetswe Malema",
                "Alham Fikri Aji",
                "David Ifeoluwa Adelani",
                "Thamar Solorio"
            ],
            "affiliations": [
                "AI4Bharat, Indian Institute of Technology, Madras",
                "Addis Ababa University",
                "Aston University",
                "Friedrich-Alexander University",
                "Independent",
                "Kabale University",
                "Lelapa AI",
                "Lesan AI",
                "MBZUAI",
                "Mila, McGill University & Canada CIFAR AI Chair",
                "Saarland University",
                "Tshwane University of Technology",
                "University of Botswana",
                "University of Minnesota - Twin Cities",
                "University of Pretoria"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05699.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#audio",
                    "#low_resource",
                    "#multimodal",
                    "#benchmark",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ðŸŒ",
                "ru": {
                    "title": "ÐÑ„Ñ€Ð¸ÐºÐ°Ð½ÑÐºÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð½Ð¾-ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Afri-MCQA â€” Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ½Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ Ð½Ð° 15 Ð°Ñ„Ñ€Ð¸ÐºÐ°Ð½ÑÐºÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ°Ñ…, ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‰Ð¸Ð¹ 7,5 Ñ‚Ñ‹ÑÑÑ‡Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð² Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼ Ð¸ Ð°Ñ„Ñ€Ð¸ÐºÐ°Ð½ÑÐºÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ°Ñ… Ð² Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼ Ð¸ Ñ€ÐµÑ‡ÐµÐ²Ð¾Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°Ñ…. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð² Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð½Ð¸Ð·ÐºÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ â€” Ð¿Ð¾Ñ‡Ñ‚Ð¸ Ð½ÑƒÐ»ÐµÐ²ÑƒÑŽ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð½Ð° Ñ€Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ°Ñ… Ð¸ Ð² Ñ€ÐµÑ‡ÐµÐ²Ð¾Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð¸Ñ Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¼ÐµÐ¶Ð´Ñƒ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼ Ð¸ Ð°Ñ„Ñ€Ð¸ÐºÐ°Ð½ÑÐºÐ¸Ð¼Ð¸ ÑÐ·Ñ‹ÐºÐ°Ð¼Ð¸, Ñ‡Ñ‚Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð° Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð»Ð¸Ð½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ¾Ð¼Ð¿ÐµÑ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾Ð´Ñ‡Ñ‘Ñ€ÐºÐ¸Ð²Ð°ÑŽÑ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ€ÐµÑ‡ÐµÐ¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð², ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð½Ð¾-Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² ÐºÑ€Ð¾ÑÑ-Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð½ÑÑ„ÐµÑ€Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹."
                },
                "en": {
                    "title": "Bridging the AI Gap: Empowering African Languages in Machine Learning",
                    "desc": "The Afri-MCQA benchmark reveals that open-weight large language models (LLMs) struggle significantly with African languages, indicating a lack of cultural understanding in their training. This benchmark includes 7.5k question-and-answer pairs in 15 African languages, created by native speakers, and assesses both text and speech modalities. Results show that these models achieve near-zero accuracy in open-ended visual question answering when using native languages, highlighting the need for culturally relevant pretraining. The study emphasizes the importance of speech-first approaches and cross-lingual cultural transfer to improve AI performance in diverse linguistic contexts."
                },
                "zh": {
                    "title": "æŽ¨åŠ¨éžæ´²è¯­è¨€çš„AIå‘å±•ï¼Œé‡è§†æ–‡åŒ–ä¸Žè¯­éŸ³ï¼",
                    "desc": "Afri-MCQAåŸºå‡†å±•ç¤ºäº†å¼€æ”¾æƒé‡çš„å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨éžæ´²è¯­è¨€ä¸Šçš„è¡¨çŽ°ä¸ä½³ï¼Œå¼ºè°ƒäº†åœ¨äººå·¥æ™ºèƒ½å‘å±•ä¸­éœ€è¦ä»¥æ–‡åŒ–ä¸ºåŸºç¡€çš„é¢„è®­ç»ƒå’Œä»¥è¯­éŸ³ä¸ºä¸»çš„æ–¹å¼ã€‚éžæ´²æ‹¥æœ‰è¶…è¿‡ä¸‰åˆ†ä¹‹ä¸€çš„ä¸–ç•Œè¯­è¨€ï¼Œä½†åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­ä»ç„¶ä»£è¡¨æ€§ä¸è¶³ã€‚æˆ‘ä»¬æŽ¨å‡ºäº†Afri-MCQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¶µç›–æ¥è‡ª12ä¸ªå›½å®¶çš„15ç§éžæ´²è¯­è¨€çš„å¤šè¯­è¨€æ–‡åŒ–é—®ç­”åŸºå‡†ï¼ŒåŒ…å«7500å¯¹é—®ç­”ã€‚åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå¼€æ”¾æƒé‡æ¨¡åž‹åœ¨è¯„ä¼°çš„æ–‡åŒ–ä¸­è¡¨çŽ°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ¯è¯­æˆ–è¯­éŸ³è¿›è¡Œå¼€æ”¾å¼è§†è§‰é—®ç­”æ—¶å‡ ä¹Žæ²¡æœ‰å‡†ç¡®çŽ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05376",
            "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
            "url": "https://huggingface.co/papers/2601.05376",
            "abstract": "Persona conditioning in clinical language models produces context-dependent effects on performance and safety that vary systematically with professional role and interaction style, challenging assumptions of monotonic improvement in expert behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to sim+20% in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's Îº= 0.43) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
            "score": 0,
            "issue_id": 534,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "f5cdba803ffd9dc3",
            "authors": [
                "Tassallah Abdullahi",
                "Shrestha Ghosh",
                "Hamish S Fraser",
                "Daniel LeÃ³n Tramontini",
                "Adeel Abbasi",
                "Ghada Bourjeily",
                "Carsten Eickhoff",
                "Ritambhara Singh"
            ],
            "affiliations": [
                "Brown University",
                "University of Tuebingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05376.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "âš•ï¸",
                "ru": {
                    "title": "ÐŸÐµÑ€ÑÐ¾Ð½Ñ‹ Ð²Ñ€Ð°Ñ‡ÐµÐ¹ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…: ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¸Ð»Ð»ÑŽÐ·Ð¸Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸?",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÐºÐ¾Ð½Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÑ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ñ‹ (Ñ€Ð¾Ð»Ð¸ Ð²Ñ€Ð°Ñ‡Ð°, Ð¼ÐµÐ´ÑÐµÑÑ‚Ñ€Ñ‹) Ð½Ðµ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð¼Ð¾Ð½Ð¾Ñ‚Ð¾Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ñ€Ð¾ÑÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°, Ð° Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾-Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ð¼ ÑÑ„Ñ„ÐµÐºÑ‚Ð°Ð¼. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¾Ñ†ÐµÐ½Ð¸Ð»Ð¸ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€Ð¾Ð»ÐµÐ¹ Ð¸ ÑÑ‚Ð¸Ð»ÐµÐ¹ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð½Ð° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ LLM Ð² ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…, Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð² ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð½Ð° ~20% Ð² Ð¸Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð½Ð¾Ð¹ Ñ‚ÐµÑ€Ð°Ð¿Ð¸Ð¸ Ð¸ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ðµ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð² Ð°Ð¼Ð±ÑƒÐ»Ð°Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ. ÐŸÐ¾Ð¼Ð¸Ð¼Ð¾ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸, ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¸Ð·Ð¼ÐµÑ€ÑÐ»Ð° Ð¾Ñ‚ÐºÐ°Ð»Ð¸Ð±Ñ€Ð¾Ð²ÐºÑƒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ, ÑÐ²ÑÐ·Ð°Ð½Ð½Ð¾Ðµ Ñ Ñ€Ð¸ÑÐºÐ¾Ð¼ Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð¿Ð°Ñ†Ð¸ÐµÐ½Ñ‚Ð¾Ð². Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ñ‹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð¸Ñ€ÑƒÑŽÑ‚ ÐºÐ°Ðº Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ñ‡ÐµÑÐºÐ¸Ðµ Ð°Ð¿Ñ€Ð¸Ð¾Ñ€Ð½Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼Ð¸ ÐºÐ¾Ð¼Ð¿Ñ€Ð¾Ð¼Ð¸ÑÑÐ°Ð¼Ð¸, Ð° Ð½Ðµ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸ÑÐ¼Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¸Ð·Ñ‹ Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸."
                },
                "en": {
                    "title": "Persona Conditioning: Context Matters in Clinical AI Performance",
                    "desc": "This paper investigates how persona conditioning in clinical language models affects their performance and safety in medical tasks. It reveals that the impact of different professional roles and interaction styles on model behavior is not straightforward, challenging the idea that expert personas always lead to better outcomes. The study finds that while medical personas can enhance accuracy in critical care tasks, they may actually hinder performance in primary care scenarios. Overall, the results highlight that persona conditioning introduces complex trade-offs rather than consistent improvements in safety and expertise."
                },
                "zh": {
                    "title": "è§’è‰²æ¡ä»¶åŒ–ï¼šåŒ»ç–—æ¨¡åž‹ä¸­çš„åŒåˆƒå‰‘",
                    "desc": "æœ¬ç ”ç©¶æŽ¢è®¨äº†åœ¨ä¸´åºŠè¯­è¨€æ¨¡åž‹ä¸­ä½¿ç”¨è§’è‰²æ¡ä»¶åŒ–å¯¹æ€§èƒ½å’Œå®‰å…¨æ€§çš„å½±å“ã€‚ç ”ç©¶å‘çŽ°ï¼Œä¸åŒçš„ä¸“ä¸šè§’è‰²å’Œäº’åŠ¨é£Žæ ¼ä¼šç³»ç»Ÿæ€§åœ°å½±å“æ¨¡åž‹åœ¨åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚å°½ç®¡åŒ»ç–—è§’è‰²åœ¨å±æ€¥æŠ¤ç†ä»»åŠ¡ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†åœ¨åˆçº§æŠ¤ç†çŽ¯å¢ƒä¸­å´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„ç»“æžœè¡¨æ˜Žï¼Œè§’è‰²æ¡ä»¶åŒ–å¹¶ä¸æ€»æ˜¯èƒ½ä¿è¯å®‰å…¨æˆ–ä¸“ä¸šæ€§ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¾èµ–äºŽä¸Šä¸‹æ–‡çš„æƒè¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04175",
            "title": "Legal Alignment for Safe and Ethical AI",
            "url": "https://huggingface.co/papers/2601.04175",
            "abstract": "Legal alignment explores how legal principles and methods can guide AI system design to ensure safety, ethics, and compliance through three research directions involving rule adherence, legal reasoning methods, and structural blueprints for AI reliability and trust.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.",
            "score": 0,
            "issue_id": 531,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑÐ½Ð²Ð°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "ca62024488ec38f0",
            "authors": [
                "Noam Kolt",
                "Nicholas Caputo",
                "Jack Boeglin",
                "Cullen O'Keefe",
                "Rishi Bommasani",
                "Stephen Casper",
                "Mariano-Florentino CuÃ©llar",
                "Noah Feldman",
                "Iason Gabriel",
                "Gillian K. Hadfield",
                "Lewis Hammond",
                "Peter Henderson",
                "Atoosa Kasirzadeh",
                "Seth Lazar",
                "Anka Reuel",
                "Kevin L. Wei",
                "Jonathan Zittrain"
            ],
            "affiliations": [
                "Australian National University",
                "Berkman Klein Center for Internet & Society",
                "Carnegie Endowment for International Peace",
                "Carnegie Mellon University",
                "Centre for the Governance of AI",
                "Cooperative AI Foundation",
                "Harvard University",
                "Hebrew University",
                "Institute for Law & AI",
                "Johns Hopkins University",
                "MIT CSAIL",
                "Oxford Martin AI Governance Initiative",
                "Princeton University",
                "School of Advanced Study University of London",
                "Stanford University",
                "University of Oxford",
                "University of Pennsylvania",
                "Vector Institute for Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04175.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ÐŸÑ€Ð°Ð²Ð¾Ð²Ð¾Ðµ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ: Ð¾Ñ‚ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð¾Ð² Ðº Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¼ AI ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ð¼",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¿Ñ€Ð°Ð²Ð¾Ð²Ð¾Ðµ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ (legal alignment) â€” Ð½Ð¾Ð²Ð¾Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ AI, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð¸ ÑÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ AI ÑÐ¸ÑÑ‚ÐµÐ¼. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ñ‚Ñ€Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ñ… Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ: Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° AI ÑÐ¸ÑÑ‚ÐµÐ¼, ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ Ð¿Ñ€Ð°Ð²Ð¾Ð²Ñ‹Ñ… Ð½Ð¾Ñ€Ð¼, Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ‚Ð¾Ð»ÐºÐ¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ AI, Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð°Ð²Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¹ ÐºÐ°Ðº ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð½Ð¾Ð¹ Ð¾ÑÐ½Ð¾Ð²Ñ‹ Ð´Ð»Ñ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð´Ð¾Ð²ÐµÑ€Ð¸Ñ. ÐŸÐ¾Ð´Ñ…Ð¾Ð´ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¼ÐµÐ¶Ð´Ð¸ÑÑ†Ð¸Ð¿Ð»Ð¸Ð½Ð°Ñ€Ð½Ð¾Ð³Ð¾ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð°Ð¼Ð¸ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð¿Ñ€Ð°Ð²Ð° Ð¸ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ñ‹Ñ… Ð½Ð°ÑƒÐº Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñæ¡†æž¶ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ AI ÑÐ¸ÑÑ‚ÐµÐ¼ Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¼Ñƒ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ñƒ. Ð­Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ñ€Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ ÐºÐ°Ðº ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð·Ð°ÐºÐ¾Ð½Ñ‹, Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ñ‹Ðµ Ðº ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ AI ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ð¼, Ñ‚Ð°Ðº Ð¸ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ Ð¸Ñ… Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸."
                },
                "en": {
                    "title": "Bridging Law and AI for Ethical Alignment",
                    "desc": "This paper introduces the concept of legal alignment, which integrates legal principles into the design of AI systems to ensure they operate safely and ethically. It identifies three key research directions: ensuring AI compliance with legal rules, applying legal reasoning methods to AI decision-making, and using legal frameworks to enhance AI reliability and trust. The authors argue that leveraging legal knowledge can help address the normative and technical challenges of AI alignment. This interdisciplinary approach encourages collaboration between legal experts and computer scientists to create AI systems that adhere to established legal standards."
                },
                "zh": {
                    "title": "æ³•å¾‹ä¸Žäººå·¥æ™ºèƒ½çš„å®Œç¾Žç»“åˆ",
                    "desc": "æ³•å¾‹å¯¹é½ç ”ç©¶å¦‚ä½•åˆ©ç”¨æ³•å¾‹åŽŸåˆ™å’Œæ–¹æ³•æŒ‡å¯¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¾è®¡ï¼Œä»¥ç¡®ä¿å…¶å®‰å…¨æ€§ã€ä¼¦ç†æ€§å’Œåˆè§„æ€§ã€‚è¯¥ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸‰ä¸ªæ–¹å‘ï¼šä¸€æ˜¯è®¾è®¡ç¬¦åˆåˆæ³•æœºæž„å’Œç¨‹åºåˆ¶å®šçš„æ³•å¾‹è§„åˆ™çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼›äºŒæ˜¯å€Ÿé‰´æ³•å¾‹è§£é‡Šçš„æ–¹æ³•æ¥æŒ‡å¯¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æŽ¨ç†å’Œå†³ç­–ï¼›ä¸‰æ˜¯åˆ©ç”¨æ³•å¾‹æ¦‚å¿µä½œä¸ºåº”å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿå¯é æ€§ã€ä¿¡ä»»å’Œåˆä½œæŒ‘æˆ˜çš„ç»“æž„è“å›¾ã€‚è¿™ä¸€æ–°å…´é¢†åŸŸä¸ºæ³•å¾‹ã€è®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªå­¦ç§‘çš„åˆä½œæä¾›äº†æœºä¼šï¼Œä»¥å…±åŒè®¾è®¡æ›´å¥½çš„äººå·¥æ™ºèƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-09.html",
    "link_next": "2026-01-13.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 1,
        "#benchmark": 13,
        "#agents": 6,
        "#cv": 5,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 8,
        "#math": 2,
        "#multilingual": 3,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 3,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 2
    }
}