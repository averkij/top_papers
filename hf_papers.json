{
    "date": {
        "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 27",
        "zh": "11æœˆ27æ—¥"
    },
    "time_utc": "2024-11-27 03:27",
    "weekday": 2,
    "issue_id": 803,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17116",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "url": "https://huggingface.co/papers/2411.17116",
            "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.",
            "score": 4,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "12194d270104d50f",
            "authors": [
                "Shantanu Acharya",
                "Fei Jia",
                "Boris Ginsburg"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17116.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "â­",
                "ru": {
                    "title": "Ğ—Ğ²ĞµĞ·Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Star Attention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Star Attention ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 11 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 95-100% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Boosting Efficiency in Long Sequence Processing with Star Attention",
                    "desc": "This paper presents Star Attention, a new method designed to enhance the efficiency of Transformer-based Large Language Models (LLMs) when processing long sequences. It addresses the high computational cost and slow inference times caused by the traditional self-attention mechanism's quadratic complexity. Star Attention operates in two phases: first, it uses blockwise-local attention to process context in parallel across multiple hosts, and then it applies sequence-global attention for query and response tokens. This approach significantly reduces memory usage and inference time by up to 11 times while maintaining a high level of accuracy, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "æ˜Ÿé™…æ³¨æ„åŠ›ï¼šæå‡é•¿åºåˆ—æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæ˜Ÿé™…æ³¨æ„åŠ›ï¼ˆStar Attentionï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºå˜æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿åºåˆ—ä¸Šçš„æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å¤šä¸ªä¸»æœºä¹‹é—´åˆ†ç‰‡æ³¨æ„åŠ›ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„å—ç¨€ç–è¿‘ä¼¼ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å—å±€éƒ¨æ³¨æ„åŠ›å¹¶è¡Œå¤„ç†ä¸Šä¸‹æ–‡ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡å…¨å±€æ³¨æ„åŠ›å¤„ç†æŸ¥è¯¢å’Œå“åº”æ ‡è®°ã€‚æ˜Ÿé™…æ³¨æ„åŠ›ä¸å¤§å¤šæ•°ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹æ— ç¼é›†æˆï¼Œèƒ½å¤Ÿå°†å†…å­˜éœ€æ±‚å’Œæ¨ç†æ—¶é—´å‡å°‘å¤šè¾¾11å€ï¼ŒåŒæ—¶ä¿æŒ95-100%çš„å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17465",
            "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
            "url": "https://huggingface.co/papers/2411.17465",
            "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI.",
            "score": 2,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "372a78043d62af12",
            "authors": [
                "Kevin Qinghong Lin",
                "Linjie Li",
                "Difei Gao",
                "Zhengyuan Yang",
                "Shiwei Wu",
                "Zechen Bai",
                "Weixian Lei",
                "Lijuan Wang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Microsoft",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17465.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#games",
                    "#optimization",
                    "#cv",
                    "#agents",
                    "#graphs",
                    "#dataset"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ShowUI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ShowUI - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² (GUI) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ShowUI Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ GUI. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 75.1% Ğ¿Ñ€Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 256 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering GUI Assistants with Visual Intelligence",
                    "desc": "This paper introduces ShowUI, a vision-language-action model designed to improve GUI assistant capabilities by better understanding visual elements. It innovates with UI-Guided Visual Token Selection to optimize computational efficiency by treating screenshots as connected graphs, which helps in selecting relevant visual tokens. Additionally, it employs Interleaved Vision-Language-Action Streaming to manage visual-action history effectively, enhancing the model's ability to handle complex GUI tasks. The model demonstrates strong performance with a 75.1% accuracy in zero-shot screenshot grounding and significantly reduces redundant visual tokens during training, showcasing its potential in advancing GUI visual agents."
                },
                "zh": {
                    "title": "æå‡GUIåŠ©æ‰‹æ•ˆç‡çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œåä¸ºShowUIï¼Œæ—¨åœ¨æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åŠ©æ‰‹çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹é€šè¿‡UIå¼•å¯¼çš„è§†è§‰æ ‡è®°é€‰æ‹©ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­ä¼˜åŒ–äº†æ ‡è®°é€‰æ‹©è¿‡ç¨‹ã€‚ShowUIè¿˜å®ç°äº†äº¤é”™çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æµï¼Œçµæ´»å¤„ç†GUIä»»åŠ¡ä¸­çš„å¤šæ ·éœ€æ±‚ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ•´ç†å’Œé‡é‡‡æ ·ç­–ç•¥ï¼ŒShowUIåœ¨é›¶æ ·æœ¬æˆªå›¾å®šä½ä¸­è¾¾åˆ°äº†75.1%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨GUIè§†è§‰ä»£ç†é¢†åŸŸçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17223",
            "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
            "url": "https://huggingface.co/papers/2411.17223",
            "abstract": "Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce a disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github.com/mycfhs/DreamMix.",
            "score": 2,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "56eb0ccceda40f61",
            "authors": [
                "Yicheng Yang",
                "Pengxiang Li",
                "Lu Zhang",
                "Liqian Ma",
                "Ping Hu",
                "Siyu Du",
                "Yunzhi Zhuge",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "University of Electronic Science and Technology of China",
                "ZMO AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17223.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DreamMix: Ğ’ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "DreamMix - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ² DreamMix Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "DreamMix: Seamless Object Insertion and Attribute Editing in Images",
                    "desc": "This paper presents DreamMix, a novel diffusion-based generative model designed for subject-driven image inpainting. Unlike previous methods that prioritize identity preservation, DreamMix allows users to insert objects into images at specified locations while also enabling modifications to their attributes through text prompts. The model employs a disentangled local-global inpainting framework to ensure that inserted objects blend well with the overall scene. Additionally, it introduces mechanisms for attribute decoupling and textual attribute substitution to enhance the diversity and effectiveness of text-based guidance for editing."
                },
                "zh": {
                    "title": "DreamMixï¼šæ™ºèƒ½å›¾åƒä¿®å¤ä¸å±æ€§ç¼–è¾‘çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamMixçš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ç›®æ ‡å¯¹è±¡åœ¨æŒ‡å®šåœºæ™¯ä¸­çš„æ’å…¥ï¼ŒåŒæ—¶å…è®¸ç”¨æˆ·å¯¹å…¶å±æ€§è¿›è¡Œæ–‡æœ¬é©±åŠ¨çš„ä¿®æ”¹ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒDreamMixä¸ä»…å…³æ³¨èº«ä»½ä¿ç•™ï¼Œè¿˜èƒ½ä¿æŒæ’å…¥å¯¹è±¡çš„å¯ç¼–è¾‘æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å…ˆè¿›çš„åŸºç¡€ä¿®å¤æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†å±€éƒ¨-å…¨å±€åˆ†ç¦»ä¿®å¤æ¡†æ¶ï¼Œä»¥å¹³è¡¡ç²¾ç¡®çš„å±€éƒ¨å¯¹è±¡æ’å…¥å’Œæœ‰æ•ˆçš„å…¨å±€è§†è§‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å±æ€§è§£è€¦æœºåˆ¶å’Œæ–‡æœ¬å±æ€§æ›¿æ¢æ¨¡å—ï¼Œä»¥æé«˜åŸºäºæ–‡æœ¬çš„å±æ€§æŒ‡å¯¼çš„å¤šæ ·æ€§å’ŒåŒºåˆ†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15411",
            "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
            "url": "https://huggingface.co/papers/2411.15411",
            "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions.   However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning.   Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "54aae052c8dc586b",
            "authors": [
                "Hang Hua",
                "Qing Liu",
                "Lingzhi Zhang",
                "Jing Shi",
                "Zhifei Zhang",
                "Yilin Wang",
                "Jianming Zhang",
                "Jiebo Luo"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15411.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#games",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FINECAPTION, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ COMPOSITIONCAP Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Compositional Understanding in Vision-Language Models",
                    "desc": "This paper introduces FINECAPTION, a new Vision-Language Model (VLM) designed to enhance compositional image captioning by accurately recognizing and processing arbitrary segmentation masks. The model addresses the challenge of aligning image regions with their corresponding semantics, which is crucial for generating coherent captions. To facilitate this, the authors present COMPOSITIONCAP, a dataset that focuses on multi-grained region compositional image captioning, allowing for a deeper understanding of visual attributes. Empirical results show that FINECAPTION outperforms existing VLMs, while also identifying areas for further improvement in VLM capabilities."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç»„åˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹FINECAPTIONï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å›¾åƒåŒºåŸŸç»„åˆä¿¡æ¯æ„ŸçŸ¥èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘æè¿°ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç²¾ç¡®å¯¹é½åˆ†å‰²æ©ç å’Œè¯­ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚FINECAPTIONèƒ½å¤Ÿè¯†åˆ«ä»»æ„æ©ç ä½œä¸ºå‚è€ƒè¾“å…¥ï¼Œå¹¶å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä»¥å®ç°ä¸åŒç²’åº¦çº§åˆ«çš„ç»„åˆå›¾åƒæè¿°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†COMPOSITIONCAPï¼Œä»¥æ”¯æŒå¤šç²’åº¦åŒºåŸŸç»„åˆå›¾åƒæè¿°ä»»åŠ¡ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17467",
            "title": "Learning 3D Representations from Procedural 3D Programs",
            "url": "https://huggingface.co/papers/2411.17467",
            "abstract": "Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations.   Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current self-supervised learning methods primarily capture geometric structures rather than high-level semantics.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "de7061420b319a85",
            "authors": [
                "Xuweiyi Chen",
                "Zezhou Cheng"
            ],
            "affiliations": [
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17467.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ÑĞ¼Ñ‹ÑĞ»Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒÑÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ° Ğ½Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ."
                },
                "en": {
                    "title": "Unlocking 3D Learning with Procedural Generation",
                    "desc": "This paper discusses a new method for self-supervised learning to create useful 3D representations from unlabeled 3D point clouds. The authors highlight the difficulty of obtaining 3D data due to the need for specialized equipment and the associated copyright issues. They propose using procedural 3D programs that generate shapes from basic building blocks, allowing for scalable data generation. The results show that these representations, although generated without semantic meaning, perform comparably to those derived from labeled 3D models in various tasks like shape classification and segmentation."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å­¦ä¹ ï¼šä»ç¨‹åºåŒ–ç”Ÿæˆä¸­è·å–3Dè¡¨ç¤º",
                    "desc": "è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æœªæ ‡è®°çš„3Dç‚¹äº‘ä¸­è·å–å¯è½¬ç§»çš„3Dè¡¨ç¤ºã€‚ä¸2Då›¾åƒä¸åŒï¼Œè·å–3Dèµ„äº§éœ€è¦ä¸“ä¸šçŸ¥è¯†æˆ–ä¸“ä¸šçš„3Dæ‰«æè®¾å¤‡ï¼Œè¿™ä½¿å¾—å…¶éš¾ä»¥æ‰©å±•å¹¶å¼•å‘ç‰ˆæƒé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä»ç¨‹åºåŒ–3Dç¨‹åºä¸­å­¦ä¹ 3Dè¡¨ç¤ºï¼Œè¿™äº›ç¨‹åºä½¿ç”¨ç®€å•çš„åŸå§‹ä½“å’Œå¢å¼ºæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆ3Då½¢çŠ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ç¼ºä¹è¯­ä¹‰å†…å®¹ï¼Œä½†ä»åˆæˆæ•°æ®é›†ä¸­å­¦ä¹ çš„3Dè¡¨ç¤ºåœ¨å„ç§ä¸‹æ¸¸3Dä»»åŠ¡ä¸­è¡¨ç°ä¸ä»è¯­ä¹‰å¯è¯†åˆ«çš„3Dæ¨¡å‹ï¼ˆå¦‚é£æœºï¼‰å­¦ä¹ çš„æœ€å…ˆè¿›è¡¨ç¤ºç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14740",
            "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
            "url": "https://huggingface.co/papers/2411.14740",
            "abstract": "While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis. Project page is at http://cvmi-lab.github.io/TEXGen/.",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "79b09429e72c3c18",
            "authors": [
                "Xin Yu",
                "Ze Yuan",
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "JianHui Liu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Ding Liang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, Hong Kong",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14740.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² UV-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ² UV-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° UV-ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ñ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Revolutionizing 3D Textures: Direct Learning in UV Space",
                    "desc": "This paper presents a novel approach to generating high-quality texture maps for 3D assets by directly learning in the UV texture space. Unlike traditional methods that use pre-trained 2D models, the authors introduce a large diffusion model that generates textures in a feed-forward manner. They propose a scalable network architecture that combines convolutional operations on UV maps with attention mechanisms on point clouds, enabling efficient learning. The resulting model, with 700 million parameters, can create UV texture maps based on text prompts and single-view images, and supports various applications like texture inpainting and synthesis."
                },
                "zh": {
                    "title": "ç›´æ¥åœ¨UVçº¹ç†ç©ºé—´ä¸­ç”Ÿæˆé«˜è´¨é‡çº¹ç†å›¾",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç›´æ¥åœ¨UVçº¹ç†ç©ºé—´ä¸­å­¦ä¹ é«˜è´¨é‡çš„çº¹ç†å›¾ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥å‰é¦ˆæ–¹å¼ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„çº¹ç†å›¾ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·æœ‰7äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå’Œå•è§†å›¾å›¾åƒç”ŸæˆUVçº¹ç†å›¾ã€‚æˆ‘ä»¬çš„æ¶æ„è®¾è®¡ç»“åˆäº†å·ç§¯å’Œæ³¨æ„åŠ›å±‚ï¼Œä½¿å¾—åœ¨é«˜åˆ†è¾¨ç‡UVç©ºé—´ä¸­çš„å­¦ä¹ æ›´åŠ é«˜æ•ˆï¼Œå¹¶æ”¯æŒå¤šç§æ‰©å±•åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17383",
            "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
            "url": "https://huggingface.co/papers/2411.17383",
            "abstract": "The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Additionally, we introduce the HOI-region reweighting loss, a training objective that enhances the learning of object details. Extensive experiments demonstrate that our proposed system outperforms existing methods in preserving object appearance and shape awareness, while simultaneously maintaining consistency in human appearance and motion. Project page: https://cangcz.github.io/Anchor-Crafter/",
            "score": 0,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "8deec5510490c901",
            "authors": [
                "Ziyi Xu",
                "Ziyao Huang",
                "Juan Cao",
                "Yong Zhang",
                "Xiaodong Cun",
                "Qing Shuai",
                "Yuchen Wang",
                "Linchao Bao",
                "Jintao Li",
                "Fan Tang"
            ],
            "affiliations": [
                "Great Bay University",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "Meituan",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17383.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "AnchorCrafter: Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AnchorCrafter - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ·. AnchorCrafter Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ HOI-appearance perception Ğ¸ HOI-motion injection Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing Product Promotion with AnchorCrafter!",
                    "desc": "This paper presents AnchorCrafter, a new system for generating promotional videos that feature a person interacting with a product. It focuses on improving how humans and objects are represented together in videos by using advanced techniques in pose-guided video generation. The system introduces two main innovations: one that improves how objects are recognized from different angles and another that allows for realistic interactions between humans and objects. The results show that AnchorCrafter produces videos with better object appearance and human motion consistency compared to existing methods."
                },
                "zh": {
                    "title": "é”šç‚¹é£æ ¼è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAnchorCrafterçš„æ–°å‹æ‰©æ•£ç³»ç»Ÿï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé”šç‚¹é£æ ¼çš„äº§å“æ¨å¹¿è§†é¢‘ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆäºº-ç‰©äº¤äº’ï¼ˆHOIï¼‰æ¥æå‡åŸºäºå§¿æ€çš„äººç±»è§†é¢‘ç”Ÿæˆæ•ˆæœï¼Œè§£å†³äº†å¤æ‚çš„äºº-ç‰©äº¤äº’é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†HOI-å¤–è§‚æ„ŸçŸ¥å’ŒHOI-è¿åŠ¨æ³¨å…¥ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼Œå‰è€…æ”¹å–„äº†ç‰©ä½“å¤–è§‚çš„è¯†åˆ«ï¼Œåè€…åˆ™å¢å¼ºäº†äºº-ç‰©äº¤äº’çš„å¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnchorCrafteråœ¨ç‰©ä½“å¤–è§‚å’Œå½¢çŠ¶ä¿æŒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†äººç±»å¤–è§‚å’Œè¿åŠ¨çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-26.html",
    "link_next": "2024-11-28.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬å±•ç¤ºäº†Material Anythingï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–ã€ç»Ÿä¸€çš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º3Dç‰©ä½“ç”ŸæˆåŸºäºç‰©ç†çš„æè´¨ã€‚ä¸ä¾èµ–å¤æ‚æµæ°´çº¿æˆ–ç‰¹å®šæ¡ˆä¾‹ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒMaterial Anythingæä¾›äº†ä¸€ä¸ªé€‚åº”å¤šç§å…‰ç…§æ¡ä»¶ä¸‹ç‰©ä½“çš„å¥å£®ã€ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆä¸‰å¤´æ¶æ„å’Œæ¸²æŸ“æŸå¤±ï¼Œä»¥æé«˜ç¨³å®šæ€§å’Œæè´¨è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡æ©ç ä½œä¸ºæ‰©æ•£æ¨¡å‹å†…çš„åŠ¨æ€å¼€å…³ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœ‰çº¹ç†å’Œæ— çº¹ç†çš„ç‰©ä½“ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›ç½®ä¿¡æ©ç æŒ‡å¯¼çš„æ¸è¿›æè´¨ç”Ÿæˆç­–ç•¥ï¼Œä»¥åŠUVç©ºé—´æè´¨ç²¾ç‚¼å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ä¸€è‡´çš„ã€UVå‡†å¤‡å¥½çš„æè´¨è¾“å‡ºã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ç‰©ä½“ç±»åˆ«å’Œå…‰ç…§æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "pinyin": "WÇ’men zhÇnshÃ¬le Material Anything, yÄ«gÃ¨ wÃ¡nquÃ¡n zÃ¬dÃ²nghuÃ , tÇ’ngyÄ« de kuÃ²sÃ n kuÃ ngjiÃ , zhÇzÃ i wÃ¨i 3D wÃ¹tÇ shÄ“ngchÃ©ng jÄ«yÇ wÃ¹lÇ de cÃ¡izhÃ¬. YÇ” yÄ«lÃ i fÃ¹zÃ¡ xiÃ¹shuÇxiÃ n huÃ² tÃ¨dÃ¬ng Ã nliÃ o yÅuhuÃ  de xiÃ n yÇ’u fÄngfÇ bÃ¹tÃ³ng, Material Anything tÃ­gÅngle yÄ«gÃ¨ shÃ¬yÃ¬ng duÅzhÇ’ng guÄngzhÃ o tiÃ¡ojiÃ n xiÃ  wÃ¹tÇ de jiÃ nkÄng, duÄn dÃ o duÄn de jiÄ›juÃ© fÄng'Ã n. WÇ’men de fÄngfÇ lÃ¬yÃ²ngle yÃ¹ xÃ¹nliÃ n de tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng, jiÃ©hÃ© sÄn tÃ³u jiÃ gÃ²u hÃ© xuÃ nshÄ“i sÇ”nshÄ«, yÇ tÃ­gÄo wÄ›ndÃ¬ngxÃ¬ng hÃ© cÃ¡izhÃ¬ zhÃ¬liÃ ng. CÇwÃ i, wÇ’men yÇn rÃ¹le zhÃ¬xÃ¬n mÃ³zhÃ o zuÃ²wÃ©i kuÃ²sÃ n mÃ³xÃ­ng nÃ¨i de dÃ²ngtÃ i kÄiguÄn, shÇ qÃ­ nÃ©nggÃ²u yÇ’uxiÃ o chÇ”lÇ yÇ’u wÃ©nlÇ hÃ© wÃº wÃ©nlÇ de wÃ¹tÇ. TÅngguÃ² shÇyÃ²ng zhÃ¨xiÄ“ zhÃ¬xÃ¬n mÃ³zhÃ o zhÇdÇo de jiÃ njÃ¬n cÃ¡izhÃ¬ shÄ“ngchÃ©ng cÃ¨lÃ¼Ã¨, yÇjiÇ UV kÅngjiÄn cÃ¡izhÃ¬ jÄ«ngliÃ nqÃ¬, wÇ’men de fÄngfÇ quÃ¨bÇole yÄ«zhÃ¬ de, UV zhÇ”nbÃ¨i hÇo de cÃ¡izhÃ¬ shÅ«chÅ«. GuÇngfÃ n de shÃ¬yÃ n biÇomÃ­ng, wÇ’men de fÄngfÇ zÃ i gÃ¨zhÇ’ng wÃ¹tÇ lÃ¨ibiÃ© hÃ© guÄngzhÃ o tiÃ¡ojiÃ n xiÃ  yÅu yÃº xiÃ n yÇ’u fÄngfÇ.",
        "vocab": "[{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'display'},\n{'word': 'Material', 'pinyin': 'MÃ©itÃ¨riÇl', 'trans': 'Material'},\n{'word': 'Anything', 'pinyin': 'Ä’nÃ¬thÃ¬ng', 'trans': 'Anything'},\n{'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬dÃ²nghuÃ ', 'trans': 'automated'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'ç‰©ç†', 'pinyin': 'wÃ¹lÇ', 'trans': 'physics'},\n{'word': 'æè´¨', 'pinyin': 'cÃ¡izhÃ¬', 'trans': 'material'},\n{'word': 'ä¾èµ–', 'pinyin': 'yÄ«lÃ i', 'trans': 'rely on'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'},\n{'word': 'æµæ°´çº¿', 'pinyin': 'liÃºshuÇxiÃ n', 'trans': 'pipeline'},\n{'word': 'ç‰¹å®š', 'pinyin': 'tÃ¨dÃ¬ng', 'trans': 'specific'},\n{'word': 'æ¡ˆä¾‹', 'pinyin': 'Ã nlÃ¬', 'trans': 'case'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimization'},\n{'word': 'å¥å£®', 'pinyin': 'jiÃ nzhuÃ ng', 'trans': 'robust'},\n{'word': 'ç«¯åˆ°ç«¯', 'pinyin': 'duÄndÃ oduÄn', 'trans': 'end-to-end'},\n{'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ›juÃ© fÄngÃ n', 'trans': 'solution'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'},\n{'word': 'ç»“åˆ', 'pinyin': 'jiÃ©hÃ©', 'trans': 'combine'},\n{'word': 'ä¸‰å¤´æ¶æ„', 'pinyin': 'sÄntÃ³u jiÃ gÃ²u', 'trans': 'three-headed architecture'},\n{'word': 'æ¸²æŸ“', 'pinyin': 'xuÃ nrÃ¡n', 'trans': 'rendering'},\n{'word': 'æŸå¤±', 'pinyin': 'sÇ”nshÄ«', 'trans': 'loss'},\n{'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›ndÃ¬ngxÃ¬ng', 'trans': 'stability'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬liÃ ng', 'trans': 'quality'},\n{'word': 'ç½®ä¿¡', 'pinyin': 'zhÃ¬xÃ¬n', 'trans': 'confidence'},\n{'word': 'æ©ç ', 'pinyin': 'yÇnmÇ', 'trans': 'mask'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ngtÃ i', 'trans': 'dynamic'},\n{'word': 'å¼€å…³', 'pinyin': 'kÄiguÄn', 'trans': 'switch'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’uxiÃ o', 'trans': 'effective'},\n{'word': 'çº¹ç†', 'pinyin': 'wÃ©nlÇ', 'trans': 'texture'},\n{'word': 'æ¸è¿›', 'pinyin': 'jiÃ njÃ¬n', 'trans': 'progressive'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'UVç©ºé—´', 'pinyin': 'UV kÅngjiÄn', 'trans': 'UV space'},\n{'word': 'ç²¾ç‚¼å™¨', 'pinyin': 'jÄ«ngliÃ nqÃ¬', 'trans': 'refiner'},\n{'word': 'ç¡®ä¿', 'pinyin': 'quÃ¨bÇo', 'trans': 'ensure'},\n{'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'},\n{'word': 'å‡†å¤‡å¥½', 'pinyin': 'zhÇ”nbÃ¨i hÇo', 'trans': 'ready'},\n{'word': 'å¹¿æ³›', 'pinyin': 'guÇngfÃ n', 'trans': 'extensive'},\n{'word': 'ç±»åˆ«', 'pinyin': 'lÃ¨ibiÃ©', 'trans': 'category'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅuyÃº', 'trans': 'superior to'}]",
        "trans": "We presented Material Anything, a fully automated, unified diffusion framework aimed at generating physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to various lighting conditions for objects. Our approach leverages pre-trained image diffusion models, combined with a tri-head architecture and rendering loss, to enhance stability and material quality. Additionally, we introduced confidence masks as dynamic switches within the diffusion model, enabling it to effectively handle both textured and non-textured objects. By employing a progressive material generation strategy guided by these confidence masks, along with a UV space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate that our method outperforms existing approaches across various object categories and lighting conditions.",
        "update_ts": "2024-11-26 09:11"
    }
}