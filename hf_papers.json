{
    "date": {
        "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 12",
        "zh": "11æœˆ12æ—¥"
    },
    "time_utc": "2024-11-12 08:16",
    "weekday": 1,
    "issue_id": 523,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07140",
            "title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models",
            "url": "https://huggingface.co/papers/2411.07140",
            "abstract": "New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.",
            "score": 17,
            "issue_id": 522,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "ffca97b13123516b",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Chinese SimpleQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ¸ 99 Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼, Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Chinese SimpleQA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ API OpenAI Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Empowering Chinese LLMs with SimpleQA Factuality Benchmark",
                    "desc": "This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context."
                },
                "zh": {
                    "title": "ä¸­æ–‡SimpleQAï¼šæå‡è¯­è¨€æ¨¡å‹äº‹å®èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡SimpleQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹å›ç­”çŸ­é—®é¢˜çš„äº‹å®èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºä¸­æ–‡ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ä¸»é¢˜å’Œ99ä¸ªå¤šæ ·åŒ–çš„å­ä¸»é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶è¿‡ç¨‹ï¼Œç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆçš„é«˜è´¨é‡ï¼Œå¹¶ä¸”å‚è€ƒç­”æ¡ˆæ˜¯é™æ€çš„ï¼Œä¸ä¼šéšæ—¶é—´å˜åŒ–ã€‚å¸Œæœ›ä¸­æ–‡SimpleQAèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å…¶æ¨¡å‹çš„ä¸­æ–‡äº‹å®èƒ½åŠ›ï¼Œä¿ƒè¿›åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07232",
            "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
            "url": "https://huggingface.co/papers/2411.07232",
            "abstract": "Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed \"Additing Affordance Benchmark\" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.",
            "score": 9,
            "issue_id": 523,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "5e344b551de578a9",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Add-it: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Add-it Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑÑ†ĞµĞ½Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Add-it Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 80% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Seamless Object Insertion with Add-it: No Fine-Tuning Needed!",
                    "desc": "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."
                },
                "zh": {
                    "title": "æ— ç¼å›¾åƒç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAdd-itçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æŒ‡ä»¤å°†ç‰©ä½“æ·»åŠ åˆ°å›¾åƒä¸­ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåœºæ™¯å›¾åƒã€æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå›¾åƒçš„ä¿¡æ¯ï¼Œä»¥å®ç°è‡ªç„¶çš„ç‰©ä½“æ”¾ç½®ã€‚Add-itåœ¨ä¸è¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†å›¾åƒæ’å…¥åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœï¼Œå¹¶åœ¨80%ä»¥ä¸Šçš„æƒ…å†µä¸‹è¢«äººç±»è¯„ä¼°è€…æ‰€åå¥½ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç»“æ„ä¸€è‡´æ€§å’Œç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿äº†ç‰©ä½“çš„è‡ªç„¶ä½ç½®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07199",
            "title": "OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision",
            "url": "https://huggingface.co/papers/2411.07199",
            "abstract": "Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/",
            "score": 9,
            "issue_id": 522,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "89d7bedc1b5241ac",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "OmniEdit: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniEdit, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. OmniEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞµĞ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ EditNet Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OmniEdit: The All-in-One Image Editing Solution",
                    "desc": "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."
                },
                "zh": {
                    "title": "å…¨èƒ½å›¾åƒç¼–è¾‘ï¼Œæ‰“ç ´ç°å®åº”ç”¨çš„é™åˆ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º\textit{omniedit}çš„å…¨èƒ½å›¾åƒç¼–è¾‘å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åè§åˆæˆè¿‡ç¨‹å¯¼è‡´çš„ç¼–è¾‘èƒ½åŠ›æœ‰é™ã€è®­ç»ƒæ•°æ®é›†ä¸­çš„å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œä»¥åŠæ•°æ®é›†çš„ä½åˆ†è¾¨ç‡å’Œå›ºå®šå®½é«˜æ¯”é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨ä¸ƒä¸ªä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£ï¼Œ\textit{omniedit}èƒ½å¤Ÿå¤„ç†ä¸ƒç§ä¸åŒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶ä¸”æ”¯æŒä»»æ„å®½é«˜æ¯”ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{omniedit}åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07126",
            "title": "Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models",
            "url": "https://huggingface.co/papers/2411.07126",
            "abstract": "We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.",
            "score": 9,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "a7486a925b416669",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Edify Image - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ»Ğ°Ğ¿Ğ»Ğ°ÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. Edify Image Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ´Ğ¾ 4K, ControlNets Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ HDR 360Â°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Edify Image: Revolutionizing Photorealistic Image Generation with Precision",
                    "desc": "Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks."
                },
                "zh": {
                    "title": "Edify Imageï¼šç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„æ–°çªç ´",
                    "desc": "Edify Imageæ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆåƒç´ çº§ç²¾ç¡®çš„çœŸå®æ„Ÿå›¾åƒå†…å®¹ã€‚å®ƒé‡‡ç”¨çº§è”åƒç´ ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„æ‹‰æ™®æ‹‰æ–¯æ‰©æ•£è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥ä¸åŒçš„é€Ÿç‡è¡°å‡ä¸åŒé¢‘ç‡å¸¦çš„å›¾åƒä¿¡å·ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€4Kè¶…åˆ†è¾¨ç‡ã€ControlNetsã€360 HDRå…¨æ™¯ç”Ÿæˆä»¥åŠå›¾åƒå®šåˆ¶çš„å¾®è°ƒã€‚Edify Imageåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°äº†å¼ºå¤§çš„çµæ´»æ€§å’Œé«˜è´¨é‡çš„è¾“å‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06176",
            "title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
            "url": "https://huggingface.co/papers/2411.06176",
            "abstract": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.",
            "score": 7,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "950719af940fd8d0",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "M-LongDoc: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M-LongDoc - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 851 Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Multimodal Document Understanding with M-LongDoc",
                    "desc": "This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ•ˆç‡ä¸æ•ˆæœ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºM-LongDocçš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«851ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£å’Œé—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç”±äºæ–‡æ¡£é€šå¸¸åŒ…å«æ–‡æœ¬ã€å›¾å½¢å’Œè¡¨æ ¼ç­‰å¤šç§å†…å®¹ï¼Œäººå·¥é˜…è¯»è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éœ€è¦å¼€å‘æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ³•æ¥è¾…åŠ©äººç±»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€ç´¢æ„ŸçŸ¥çš„è°ƒä¼˜æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ–‡æ¡£é˜…è¯»çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿å¼€æºæ¨¡å‹æœ‰4.6%çš„ç›¸å¯¹æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06208",
            "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
            "url": "https://huggingface.co/papers/2411.06208",
            "abstract": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
            "score": 5,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "de83b5a8e14da36e",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "TRACE Ğ¸ IOPO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TRACE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IOPO (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TRACE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 1000 Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs with TRACE and IOPO for Complex Instructions",
                    "desc": "This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO."
                },
                "zh": {
                    "title": "æå‡å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ¨¡å‹å‡†ç¡®éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤å¤æ‚æ€§è¿…é€Ÿå¢åŠ çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†TRACEï¼Œä¸€ä¸ªç”¨äºæé«˜å’Œè¯„ä¼°å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«12ä¸‡æ¡è®­ç»ƒæ•°æ®å’Œ1000æ¡è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†IOPOï¼ˆè¾“å…¥-è¾“å‡ºåå¥½ä¼˜åŒ–ï¼‰å¯¹é½æ–¹æ³•ï¼Œè€ƒè™‘äº†è¾“å…¥å’Œè¾“å‡ºåå¥½å¯¹ï¼Œå¸®åŠ©LLMså¿«é€Ÿå¯¹é½å“åº”åå¥½å¹¶æ·±å…¥æ¢ç´¢æŒ‡ä»¤åå¥½ã€‚é€šè¿‡åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†IOPOçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨é¢†åŸŸå†…æ•°æ®ä¸Šåˆ†åˆ«æé«˜äº†8.15%å’Œ2.18%ï¼Œåœ¨é¢†åŸŸå¤–æ•°æ®ä¸Šæé«˜äº†6.29%å’Œ3.13%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07180",
            "title": "Counterfactual Generation from Language Models",
            "url": "https://huggingface.co/papers/2411.07180",
            "abstract": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.",
            "score": 2,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "6b57fa07bdf242ce",
            "data": {
                "categories": [
                    "#math",
                    "#interpretability",
                    "#reasoning",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€ÑĞº Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ-Ğ¼Ğ°ĞºÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº."
                },
                "en": {
                    "title": "Harnessing Counterfactuals for Better Control of Language Models",
                    "desc": "This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques."
                },
                "zh": {
                    "title": "æŒæ¡è¯­è¨€æ¨¡å‹çš„å› æœç”Ÿæˆæœºåˆ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ç†è§£å’Œæ“æ§å› æœç”Ÿæˆæœºåˆ¶çš„é‡è¦æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè¡¨ç¤ºæ‰‹æœ¯ç­‰æŠ€æœ¯æ¥å¹²é¢„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¼ºè°ƒåäº‹å®æ¨ç†ä¸å¹²é¢„æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡å°†è¯­è¨€æ¨¡å‹é‡æ„ä¸ºå¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼Œç”ŸæˆçœŸå®çš„å­—ç¬¦ä¸²åäº‹å®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„åäº‹å®ï¼ŒåŒæ—¶æ­ç¤ºäº†å¸¸ç”¨å¹²é¢„æŠ€æœ¯çš„æ˜¾è‘—å‰¯ä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05990",
            "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games",
            "url": "https://huggingface.co/papers/2411.05990",
            "abstract": "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "aae23469f2886f4c",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#rl",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ ĞÑÑˆĞ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing LLM Rationality in Strategic Decision-Making",
                    "desc": "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åšå¼ˆç†æ€§",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„ç†æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åšå¼ˆè®ºæ¡†æ¶ä¸‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„LLMsåœ¨å®Œå…¨ä¿¡æ¯å’Œä¸å®Œå…¨ä¿¡æ¯åšå¼ˆä¸­çš„è¡¨ç°ï¼Œå‘ç°éšç€åšå¼ˆå¤æ‚æ€§çš„å¢åŠ ï¼ŒLLMså¸¸å¸¸åç¦»ç†æ€§ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šç§åšå¼ˆè®ºå·¥ä½œæµç¨‹ï¼Œä»¥æŒ‡å¯¼LLMsçš„æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶è®¡ç®—çº³ä»€å‡è¡¡å’Œåœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹åšå‡ºç†æ€§é€‰æ‹©çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¿™äº›å·¥ä½œæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨åšå¼ˆä»»åŠ¡ä¸­çš„ç†æ€§å’Œç¨³å¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06272",
            "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
            "url": "https://huggingface.co/papers/2411.06272",
            "abstract": "As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.",
            "score": 1,
            "issue_id": 520,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "2559c023f673c9b4",
            "data": {
                "categories": [
                    "#low_resource",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ ÑĞµĞºÑ‚Ğ¾Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ \"Golden Touchstone\", Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ NLP. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Touchstone-GPT, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Golden Touchstone: Elevating Financial LLM Evaluation",
                    "desc": "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."
                },
                "zh": {
                    "title": "é‡‘èé¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·â€”â€”é‡‘è‰²åŸºå‡†",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œè¯„ä¼°å…¶æ€§èƒ½çš„æ ‡å‡†åŒ–æ–¹æ³•å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„é‡‘èåŸºå‡†æµ‹è¯•å­˜åœ¨è¯­è¨€å’Œä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€æ•°æ®é›†è´¨é‡ä½ä»¥åŠé€‚åº”æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé‡‘è‰²åŸºå‡†â€ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŒè¯­é‡‘èåŸºå‡†ï¼Œæ¶µç›–äº†ä¸­è‹±æ–‡çš„å…«ä¸ªæ ¸å¿ƒé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹ä¸»è¦æ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚é‡‘èä¿¡æ¯æ—¶çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼€æºäº†Touchstone-GPTæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å’Œä¼˜åŒ–ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-11.html",
    "link_next": "2024-11-13.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le StdGEN, yÄ« zhÇ’ng chuÃ ng xÄ«n de cÃ³ng dÄn zhÄng tÃº xiÃ ng shÄ“ng chÃ©ng gÄo zhÃ¬ liÃ ng 3D juÃ© sÃ¨ de fÄng fÇ. tÄ nÃ©ng zÃ i sÄn fÄ“n zhÅng nÃ¨i shÄ“ng chÃ©ng jÃ¹ yÇ’u fÄ“n lÃ¬ de yÇ” yÃ¬ zÇ” jÃ¬n (rÃº shÄ“n tÇ, yÄ« fÃº hÃ© tÃ³u fÃ ) de xiÃ¡ng xÃ¬ 3D juÃ© sÃ¨. StdGEN de hÃ© xÄ«n shÃ¬ tÃ­ chÅ« de yÇ” yÃ¬ gÇn juÃ© dÃ  xÃ­ng chÃ³ng jiÃ n mÃ³ xÃ­ng (S-LRM), nÃ©ng gÃ²u cÃ³ng duÅ shÃ¬ jÃ¬ tÃº xiÃ ng zhÅng chÃ³ng jiÃ n jÇ hÃ©, yÃ¡n sÃ¨ hÃ© yÇ” yÃ¬. shÃ­ yÃ n zhÃ¨ng mÃ­ng, StdGEN zÃ i 3D dÃ²ng mÃ n juÃ© sÃ¨ shÄ“ng chÃ©ng fÄng miÃ n biÇo xiÇn chÅ« sÃ¨, yÅu yÃº xiÃ n yÇ’u fÄng fÇ. tÄ wÃ¨i xÅ« nÇ xiÃ n shÃ­, yÃ³u xÃ¬ hÃ© diÃ n yÇng zhÃ¬ zuÃ² dÄ›ng ti gÅng gÄ›i le lÃ­nghuÃ³ de dÃ¬ng zhÃ¬ huÃ  3D juÃ© sÃ¨.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sÄ«tÄ«dÄ« jÄ«n\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ngxÄ«n\", \"trans\": \"innovative\"},\n    {\"word\": \"è§’è‰²\", \"pinyin\": \"juÃ©sÃ¨\", \"trans\": \"character\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"ç»„ä»¶\", \"pinyin\": \"zÇ”jiÃ n\", \"trans\": \"component\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇhÃ©\", \"trans\": \"geometry\"},\n    {\"word\": \"é‡å»º\", \"pinyin\": \"chÃ³ngjiÃ n\", \"trans\": \"reconstruct\"},\n    {\"word\": \"å¤šè§†å›¾\", \"pinyin\": \"duÅshÃ¬tÃº\", \"trans\": \"multi-view\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"çµæ´»\", \"pinyin\": \"lÃ­nghuÃ³\", \"trans\": \"flexible\"},\n    {\"word\": \"å®šåˆ¶åŒ–\", \"pinyin\": \"dÃ¬ngzhÃ¬huÃ \", \"trans\": \"customized\"},\n    {\"word\": \"è™šæ‹Ÿç°å®\", \"pinyin\": \"xÅ«nÇ xiÃ nshÃ­\", \"trans\": \"virtual reality\"},\n    {\"word\": \"æ¸¸æˆ\", \"pinyin\": \"yÃ³uxÃ¬\", \"trans\": \"game\"},\n    {\"word\": \"ç”µå½±åˆ¶ä½œ\", \"pinyin\": \"diÃ nyÇng zhÃ¬zuÃ²\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    }
}