{
    "date": {
        "ru": "8 сентября",
        "en": "September 8",
        "zh": "9月8日"
    },
    "time_utc": "2025-09-08 04:14",
    "weekday": 0,
    "issue_id": 5762,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.04744",
            "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
            "url": "https://huggingface.co/papers/2509.04744",
            "abstract": "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.",
            "score": 4,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "44d75a7c2c61a026",
            "authors": [
                "Gagan Mundada",
                "Yash Vishe",
                "Amit Namburi",
                "Xin Xu",
                "Zachary Novack",
                "Julian McAuley",
                "Junda Wu"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04744.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#survey"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "WildScore: новый рубеж в понимании музыки искусственным интеллектом",
                    "desc": "Статья представляет WildScore - первый бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в области анализа и рассуждений о символической музыке. Бенчмарк состоит из реальных музыкальных партитур и вопросов пользователей, охватывая сложности практического музыкального анализа. Авторы предлагают систематическую таксономию и формулируют задачу как ответы на вопросы с множественным выбором. Эмпирическое тестирование современных MLLM на WildScore выявляет как перспективные направления, так и сохраняющиеся проблемы в области рассуждений о символической музыке."
                },
                "en": {
                    "title": "WildScore: Unlocking MLLMs' Music Reasoning Potential",
                    "desc": "WildScore is a benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in the context of symbolic music. It evaluates how well these models can interpret real-world music scores and respond to complex questions about music. The benchmark includes genuine musical compositions and user-generated queries, providing a realistic setting for analysis. By framing music reasoning as multiple-choice questions, WildScore allows for systematic evaluation of MLLMs' understanding of music, revealing both their strengths and areas needing improvement."
                },
                "zh": {
                    "title": "WildScore：音乐推理的新基准",
                    "desc": "WildScore是一个评估多模态大型语言模型（MLLMs）在符号音乐推理能力的基准测试。它通过真实的音乐乐谱和用户生成的查询，揭示了这些模型在音乐分析中的优势和挑战。该基准测试采用了系统的分类法，涵盖了高层次和细粒度的音乐学本体。通过将复杂的音乐推理框架化为多项选择题回答，WildScore为MLLMs的符号音乐理解提供了可控和可扩展的评估方式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05263",
            "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
            "url": "https://huggingface.co/papers/2509.05263",
            "abstract": "LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18",
            "score": 1,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "027e61af3a0f5c1a",
            "authors": [
                "Yinglin Duan",
                "Zhengxia Zou",
                "Tongwei Gu",
                "Wei Jia",
                "Zhan Zhao",
                "Luyi Xu",
                "Xinzhu Liu",
                "Hao Jiang",
                "Kang Chen",
                "Shuang Qiu"
            ],
            "affiliations": [
                "Beihang University, China",
                "City University of Hong Kong, China",
                "NetEase, Inc., China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05263.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Генерация 3D-миров на основе ИИ: быстро, точно, реалистично",
                    "desc": "LatticeWorld - это фреймворк для создания трёхмерных миров, использующий облегчённые языковые модели и Unreal Engine 5. Система принимает текстовые и визуальные инструкции для генерации динамических интерактивных сред. LatticeWorld достигает высокой точности в создании планировки сцен и визуальной достоверности. Фреймворк значительно повышает эффективность промышленного производства по сравнению с традиционными ручными методами."
                },
                "en": {
                    "title": "Revolutionizing 3D World Generation with LatticeWorld",
                    "desc": "LatticeWorld is a 3D world generation framework that utilizes lightweight large language models (LLMs) and Unreal Engine 5 to create interactive environments from both textual and visual inputs. This framework aims to enhance the realism of simulations, bridging the gap between simulated and real-world scenarios, which is crucial for applications like autonomous driving and embodied AI. By employing generative methods, LatticeWorld can produce large-scale 3D worlds with dynamic agents, showcasing high-fidelity physics and real-time rendering capabilities. The results demonstrate a significant increase in production efficiency, achieving over 90 times faster output compared to traditional modeling techniques while maintaining high visual quality."
                },
                "zh": {
                    "title": "LatticeWorld：高效生成动态3D世界的创新框架",
                    "desc": "LatticeWorld是一个使用轻量级大语言模型和虚幻引擎5的3D世界生成框架。它能够根据文本和视觉输入创建动态、互动的环境，具有高准确性和效率。该框架通过多模态输入生成大规模的3D互动世界，支持动态代理和高保真物理模拟。与传统手动建模方法相比，LatticeWorld在工业生产效率上提高了90倍，同时保持了高创意质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03800",
            "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
            "url": "https://huggingface.co/papers/2509.03800",
            "abstract": "MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.",
            "score": 1,
            "issue_id": 5762,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "ad0922456cbd778e",
            "authors": [
                "Yuheng Li",
                "Yenho Chen",
                "Yuxiang Lai",
                "Jike Zhong",
                "Vanessa Wildman",
                "Xiaofeng Yang"
            ],
            "affiliations": [
                "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA",
                "Department of Computer Science, University of Southern California, Los Angeles, CA",
                "Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA",
                "Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03800.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#healthcare",
                    "#transfer_learning",
                    "#3d"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Улучшение анализа КТ с помощью многомасштабного обучения компьютерного зрения и обработки естественного языка",
                    "desc": "MedVista3D - это фреймворк для предварительного обучения многомасштабных семантически обогащенных моделей компьютерного зрения и обработки естественного языка для анализа 3D КТ-изображений. Он решает проблемы локально-глобального понимания и вариативности медицинских отчетов. MedVista3D использует выравнивание изображения и текста на локальном и глобальном уровнях для детального представления в контексте полного объема. Фреймворк достигает передовых результатов в классификации заболеваний, поиске отчетов и медицинских вопросно-ответных системах."
                },
                "en": {
                    "title": "Revolutionizing 3D CT Analysis with MedVista3D",
                    "desc": "MedVista3D is a new framework designed to improve the analysis of 3D CT scans by combining vision and language understanding. It tackles common problems in radiology, such as missing details and inconsistent report language, by enhancing local and global context in image analysis. The framework uses advanced techniques for aligning images with text, allowing for better disease detection and interpretation of medical reports. MedVista3D has shown to outperform existing models in tasks like disease classification and report retrieval, making it a significant advancement in medical imaging technology."
                },
                "zh": {
                    "title": "MedVista3D：提升3D CT分析的智能框架",
                    "desc": "MedVista3D是一个多尺度语义增强的视觉-语言预训练框架，专门用于3D CT分析。它解决了局部与全局理解、报告变异性等问题，并在疾病分类、报告检索和医学视觉问答中达到了最先进的性能。该框架通过局部和全局图像-文本对齐，实现了细粒度的表示学习，并引入了语义匹配库来处理报告的变异性。MedVista3D在零样本疾病分类和器官分割等任务中表现优异，展示了其在医学影像分析中的潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-09-05.html",
    "link_next": "2025-09-09.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9月5日"
    },
    "short_date_next": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9月9日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}