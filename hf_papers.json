{
    "date": {
        "ru": "24 апреля",
        "en": "April 24",
        "zh": "4月24日"
    },
    "time_utc": "2025-04-24 02:25",
    "weekday": 3,
    "issue_id": 3403,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15843",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "url": "https://huggingface.co/papers/2504.15843",
            "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.",
            "score": 3,
            "issue_id": 3403,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "e8cb4456a20efe2a",
            "authors": [
                "Junshu Pan",
                "Wei Shen",
                "Shulin Huang",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Engineering, Westlake University",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15843.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Pre-DPO: Эффективная оптимизация языковых моделей с учетом предпочтений",
                    "desc": "Эта статья представляет Pre-DPO - новый подход к обучению языковых моделей на основе предпочтений человека. Pre-DPO улучшает существующие методы DPO и SimPO, используя ориентирующую эталонную модель для оптимизации весов данных. Этот метод позволяет более эффективно использовать обучающие данные и достигать лучших результатов. Эксперименты на бенчмарках AlpacaEval 2.0 и Arena-Hard v0.1 показывают преимущества Pre-DPO без необходимости во внешних моделях или дополнительных данных."
                },
                "en": {
                    "title": "Enhancing Preference Learning with Pre-DPO",
                    "desc": "This paper introduces Pre-DPO, a new training method for large language models that enhances Direct Preference Optimization (DPO) by using a guiding reference model. The reference model helps adjust the importance of training data, allowing the model to focus on more relevant samples and improve learning efficiency. The authors highlight that traditional methods can lead to poor performance due to identical initialization of models and lack of robustness in simpler approaches. Through experiments, they show that Pre-DPO outperforms existing methods without needing extra data or external models."
                },
                "zh": {
                    "title": "提升偏好优化性能的Pre-DPO方法",
                    "desc": "直接偏好优化（DPO）通过直接优化人类偏好，简化了大型语言模型（LLM）的强化学习过程。研究发现，在DPO训练中，参考模型充当数据权重调整器，但常见的将策略模型和参考模型初始化为相同的做法可能导致数据利用效率低下。我们提出的Pre-DPO训练范式，通过利用指导性参考模型，增强了偏好优化的性能，能够自适应地为更适合模型的样本分配更高的权重。实验结果表明，Pre-DPO在多个基准测试中持续提升了DPO和SimPO的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-04-23.html",
    "link_next": "2025-04-25.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4月23日"
    },
    "short_date_next": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4月25日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。这提供了一种成本效益高的替代方案，避免了大规模重新训练。",
        "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
        "pinyin": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。\nZhè piān wénzhāng jièshào le yī zhǒng xīn fāngfǎ, jiāng xīn yǔyán zhěnghé dào dàxíng yǔyán móxíng zhōng.\n\n该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。\nGǎi fāngfǎ chénggōng de jiāng zhīqián wèijiàn de mùbiāo yǔyán jiārù xiànyǒu móxíng, bù yǐngxiǎng qí yuányǒu zhīshi.\n\n研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。\nYánjiū tuánduì yòng 15 yì cānshǔ xùnliàn le yīgè míngwèi Kuwain de xiǎo móxíng, jiāng Ālābóyǔ jiārù zhǔyào yòng Yīngyǔ xùnliàn de kāiyuán móxíng.\n\n结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。\nJiégǔo xiǎnshì, Ālābóyǔ xìngnéng tígāo le 8%, tóngshí bǎoliú le yuányǒu zhīshi.\n\n这提供了一种成本效益高的替代方案，避免了大规模重新训练。\nZhè tígōng le yī zhǒng chéngběn xiàoyì gāo de tìdài fāng'àn, bìmiǎn le dàguīmó chóngxīn xùnliàn.",
        "vocab": "[\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integrate\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"未见\", \"pinyin\": \"wèijiàn\", \"trans\": \"unseen\"},\n    {\"word\": \"目标\", \"pinyin\": \"mùbiāo\", \"trans\": \"target\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐngxiǎng\", \"trans\": \"affect\"},\n    {\"word\": \"原有\", \"pinyin\": \"yuányǒu\", \"trans\": \"original\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhīshi\", \"trans\": \"knowledge\"},\n    {\"word\": \"研究\", \"pinyin\": \"yánjiū\", \"trans\": \"research\"},\n    {\"word\": \"团队\", \"pinyin\": \"tuánduì\", \"trans\": \"team\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"},\n    {\"word\": \"名为\", \"pinyin\": \"míngwéi\", \"trans\": \"named\"},\n    {\"word\": \"小模型\", \"pinyin\": \"xiǎo móxíng\", \"trans\": \"small model\"},\n    {\"word\": \"加入\", \"pinyin\": \"jiārù\", \"trans\": \"add\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔyào\", \"trans\": \"main\"},\n    {\"word\": \"用英语\", \"pinyin\": \"yòng yīngyǔ\", \"trans\": \"using English\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎoliú\", \"trans\": \"retain\"},\n    {\"word\": \"提供\", \"pinyin\": \"tígōng\", \"trans\": \"provide\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéngběn\", \"trans\": \"cost\"},\n    {\"word\": \"效益\", \"pinyin\": \"xiàoyì\", \"trans\": \"benefit\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"替代\", \"pinyin\": \"tìdài\", \"trans\": \"alternative\"},\n    {\"word\": \"方案\", \"pinyin\": \"fāngàn\", \"trans\": \"solution\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dàguīmó\", \"trans\": \"large-scale\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóngxīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"}\n]",
        "trans": "This article introduces a new method for integrating new languages into large language models. The method successfully incorporates previously unseen target languages into existing models without affecting their original knowledge. The research team trained a small model named Kuwain with 1.5 billion parameters, adding Arabic to a primarily English-trained open-source model. The results showed an 8% improvement in Arabic performance while retaining the original knowledge. This provides a cost-effective alternative, avoiding the need for large-scale retraining.",
        "update_ts": "2025-04-23 09:13"
    }
}