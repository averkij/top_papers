{
    "date": {
        "ru": "17 июля",
        "en": "July 17",
        "zh": "7月17日"
    },
    "time_utc": "2025-07-17 04:31",
    "weekday": 3,
    "issue_id": 4862,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.12465",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "url": "https://huggingface.co/papers/2507.12465",
            "abstract": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
            "score": 11,
            "issue_id": 4861,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "ece62f7e4ecd0487",
            "authors": [
                "Ziang Cao",
                "Zhaoxi Chen",
                "Linag Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧱",
                "ru": {
                    "title": "Физически достоверная генерация 3D-объектов",
                    "desc": "PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет PhysXNet с аннотациями физических характеристик 3D-моделей и разработали фреймворк PhysXGen для интеграции физических знаний в процесс генерации. PhysXGen использует двухветвевую архитектуру для моделирования связей между 3D-структурами и физическими свойствами. Эксперименты показали превосходную производительность и способность к обобщению предложенного метода."
                },
                "en": {
                    "title": "Bridging 3D Generation with Real-World Physics",
                    "desc": "PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations."
                },
                "zh": {
                    "title": "物理驱动的3D资产生成新方法",
                    "desc": "PhysX提出了一种新的方法来解决3D生成模型中缺乏物理属性的问题。它引入了PhysXNet，这是一个物理注释的数据集，系统地标注了五个基础维度，包括绝对尺度、材料、可用性、运动学和功能描述。通过PhysXGen框架，物理知识被整合到3D资产生成中，利用双分支架构建模3D结构与物理属性之间的潜在关联。实验结果表明，该框架在生成具有可信物理预测的3D资产方面表现优越，具有良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11527",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "url": "https://huggingface.co/papers/2507.11527",
            "abstract": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
            "score": 5,
            "issue_id": 4861,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "e6f20729b2c748f9",
            "authors": [
                "Yinsheng Li",
                "Zhen Dong",
                "Yi Shao"
            ],
            "affiliations": [
                "Department of Civil Engineering McGill University",
                "NVIDIA",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании",
                    "desc": "DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в задаче проверки технических чертежей. Он оценивает способности агентов в понимании структурированных данных, выполнении функций, следовании инструкциям и критическом мышлении. Бенчмарк содержит 12 типов задач, основанных на реальных чертежах, с 46 специальными функциями и инструментами, всего 1920 заданий. DrafterBench предлагает детальный анализ точности выполнения задач и статистики ошибок, чтобы глубже понять возможности агентов и определить цели для улучшения интеграции LLM в инженерные приложения."
                },
                "en": {
                    "title": "DrafterBench: Evaluating LLMs for Technical Drawing Mastery",
                    "desc": "DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications."
                },
                "zh": {
                    "title": "DrafterBench：评估LLM代理的技术图纸修订能力",
                    "desc": "DrafterBench是一个开源基准，用于评估大型语言模型（LLM）代理在技术图纸修订中的能力。它涵盖了结构化数据理解、功能执行、指令遵循和批判性推理等多个方面。该基准包含来自真实绘图文件的十二种任务，提供了46种定制功能和1920个任务。DrafterBench旨在深入分析代理的能力，帮助识别在工程应用中整合LLM的改进目标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11949",
            "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
            "url": "https://huggingface.co/papers/2507.11949",
            "abstract": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.",
            "score": 2,
            "issue_id": 4862,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "60ad5621a3842ae5",
            "authors": [
                "Shuyang Xu",
                "Zhiyang Dou",
                "Mingyi Shi",
                "Liang Pan",
                "Leo Ho",
                "Jingbo Wang",
                "Yuan Liu",
                "Cheng Lin",
                "Yuexin Ma",
                "Wenping Wang",
                "Taku Komura"
            ],
            "affiliations": [
                "Macau University of Science and Technology",
                "Shanghai AI Lab",
                "ShanghaiTech University",
                "Texas A&M University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Реалистичная анимация движений под пространственное аудио",
                    "desc": "Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в ответ на пространственное аудио. Для обучения модели был создан новый набор данных SAM, содержащий разнообразные пространственные аудио и соответствующие им движения. MOSPA использует эффективный механизм слияния для захвата взаимосвязи между движениями тела и пространственным звуком. Модель достигла лучших результатов в этой задаче по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Bridging Sound and Motion: MOSPA for Realistic Human Animation",
                    "desc": "The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area."
                },
                "zh": {
                    "title": "空间音频驱动的人类运动生成新突破",
                    "desc": "本文介绍了一种基于扩散的生成框架MOSPA，用于建模人类在空间音频刺激下的运动。我们创建了首个综合性的空间音频驱动人类运动（SAM）数据集，包含多样化和高质量的空间音频与运动数据。MOSPA通过有效的融合机制，准确捕捉身体运动与空间音频之间的关系，能够生成多样且真实的人类运动。经过广泛的实验验证，我们的方法在这一任务上达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09477",
            "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
            "url": "https://huggingface.co/papers/2507.09477",
            "abstract": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
            "score": 2,
            "issue_id": 4862,
            "pub_date": "2025-07-13",
            "pub_date_card": {
                "ru": "13 июля",
                "en": "July 13",
                "zh": "7月13日"
            },
            "hash": "da4aa711048f0a7f",
            "authors": [
                "Yangning Li",
                "Weizhi Zhang",
                "Yuyao Yang",
                "Wei-Chieh Huang",
                "Yaozu Wu",
                "Junyu Luo",
                "Yuanchen Bei",
                "Henry Peng Zou",
                "Xiao Luo",
                "Yusheng Zhao",
                "Chunkit Chan",
                "Yankai Chen",
                "Zhongfen Deng",
                "Yinghui Li",
                "Hai-Tao Zheng",
                "Dongyuan Li",
                "Renhe Jiang",
                "Ming Zhang",
                "Yangqiu Song",
                "Philip S. Yu"
            ],
            "affiliations": [
                "HKUST",
                "Peking University",
                "The University of Tokyo",
                "Tsinghua University",
                "University of California, Los Angeles",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09477.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Объединение извлечения информации и рассуждений для создания более мощных языковых моделей",
                    "desc": "Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LLM) для улучшения фактической точности и многоступенчатого вывода. В статье рассматриваются подходы, сочетающие Retrieval-Augmented Generation (RAG) и методы рассуждений, что позволяет преодолеть ограничения каждого из них по отдельности. Авторы выделяют синергетические фреймворки RAG-Reasoning, демонстрирующие передовые результаты на задачах, требующих обширных знаний. Обзор также очерчивает направления будущих исследований в этой области."
                },
                "en": {
                    "title": "Enhancing LLMs with Synergized Retrieval and Reasoning",
                    "desc": "This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs."
                },
                "zh": {
                    "title": "推理与检索的协同提升大型语言模型的能力",
                    "desc": "这篇论文调查了大型语言模型中的推理与检索的结合，以提高事实准确性和多步推理能力。检索增强生成（RAG）通过引入外部知识来提升大型语言模型的事实性，但在需要多步推理的问题上表现不足。论文提出了统一的推理-检索视角，展示了如何通过先进的推理优化RAG的每个阶段，并强调了新兴的协同RAG-推理框架。最后，论文分类了方法、数据集和开放挑战，并概述了未来研究方向，以构建更有效、适应多模态、可信赖和以人为本的RAG-推理系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09025",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "url": "https://huggingface.co/papers/2507.09025",
            "abstract": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
            "score": 2,
            "issue_id": 4861,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "3490901c2a32da3d",
            "authors": [
                "Chien Van Nguyen",
                "Ruiyi Zhang",
                "Hanieh Deilamsalehy",
                "Puneet Mathur",
                "Viet Dac Lai",
                "Haoliang Wang",
                "Jayakumar Subramanian",
                "Ryan A. Rossi",
                "Trung Bui",
                "Nikos Vlassis",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🦎",
                "ru": {
                    "title": "Lizard: эффективные языковые модели с бесконечным контекстом",
                    "desc": "Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратичные архитектуры для эффективной генерации с бесконечным контекстом. Он использует гибридный механизм внимания, сочетающий линейное внимание с гейтингом и оконное внимание с мета-памятью. Lizard позволяет адаптивно управлять памятью, поддерживает вывод с постоянным объемом памяти и обеспечивает сильную обобщаемость по длине. Эксперименты показывают, что Lizard почти без потерь восстанавливает производительность исходной модели на стандартных задачах языкового моделирования."
                },
                "en": {
                    "title": "Lizard: Efficient Infinite-Context Generation for Transformers",
                    "desc": "Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks."
                },
                "zh": {
                    "title": "Lizard：高效无限上下文生成的新框架",
                    "desc": "Lizard是一个线性化框架，旨在将基于Transformer的大型语言模型（LLMs）转变为灵活的亚二次架构，以实现高效的无限上下文生成。该框架通过引入一种亚二次注意力机制，克服了传统softmax注意力在上下文长度增加时的内存和计算瓶颈，同时保持输出质量。Lizard还结合了门控模块，支持自适应内存控制和常量内存推理，增强了模型设计的灵活性。通过混合门控线性注意力和滑动窗口注意力，Lizard能够有效捕捉长距离依赖和细粒度的局部交互，显著提升了模型性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-16.html",
    "link_next": "2025-07-18.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7月16日"
    },
    "short_date_next": {
        "ru": "18.07",
        "en": "07/18",
        "zh": "7月18日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}