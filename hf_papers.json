{
    "date": {
        "ru": "22 января",
        "en": "January 22",
        "zh": "1月22日"
    },
    "time_utc": "2025-01-22 05:10",
    "weekday": 2,
    "issue_id": 1797,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12380",
            "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
            "url": "https://huggingface.co/papers/2501.12380",
            "abstract": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains.",
            "score": 17,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "dcb04aaca349cc32",
            "authors": [
                "Yilun Zhao",
                "Lujing Xie",
                "Haowei Zhang",
                "Guo Gan",
                "Yitao Long",
                "Zhiyuan Hu",
                "Tongyan Hu",
                "Weiyuan Chen",
                "Chuhan Li",
                "Junyang Song",
                "Zhijian Xu",
                "Chengye Wang",
                "Weifeng Pan",
                "Ziyao Shangguan",
                "Xiangru Tang",
                "Zhenwen Liang",
                "Yixin Liu",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "affiliations": [
                "Yale NLP"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12380.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#benchmark",
                    "#video",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Новый рубеж в понимании видео: от базового восприятия к экспертному анализу",
                    "desc": "Статья представляет MMVU - многодисциплинарный экспертный бенчмарк для оценки фундаментальных моделей в понимании видео. MMVU включает 3000 вопросов по 27 предметам в четырех основных дисциплинах, требующих применения специализированных знаний и экспертного анализа. Бенчмарк отличается высоким качеством данных, аннотированных экспертами, и включает обоснования и релевантные знания для каждого примера. Оценка 32 мультимодальных моделей на MMVU показала, что даже лучшие модели пока не достигают уровня человека-эксперта в этой задаче."
                },
                "en": {
                    "title": "MMVU: Elevating Video Understanding to Expert Levels",
                    "desc": "The paper presents MMVU, a new benchmark designed to evaluate foundation models specifically in video understanding across various expert domains. It includes 3,000 questions that require advanced reasoning and domain-specific knowledge, moving beyond simple visual recognition tasks. Each question is meticulously annotated by human experts, ensuring high data quality and providing reasoning rationales to enhance analysis. The evaluation of 32 advanced multimodal models reveals that while some perform well, they still do not reach the level of human expertise, highlighting areas for future improvement in this field."
                },
                "zh": {
                    "title": "MMVU：视频理解的新标准",
                    "desc": "我们介绍了MMVU，这是一个全面的专家级多学科基准，用于评估基础模型在视频理解方面的表现。MMVU包含3000个专家注释的问题，涵盖科学、医疗、人文学科与社会科学和工程四个核心学科。与之前的基准相比，MMVU在三个关键方面有所改进，包括要求模型应用领域特定知识进行专家级推理，确保数据集的高质量，以及为每个示例提供专家注释的推理依据和相关领域知识。我们对32个前沿多模态基础模型在MMVU上的表现进行了广泛评估，发现最新的系统2能力模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳，但仍未能达到人类专家的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12273",
            "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
            "url": "https://huggingface.co/papers/2501.12273",
            "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.",
            "score": 6,
            "issue_id": 1796,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "10499c8b820d5368",
            "authors": [
                "Maosong Cao",
                "Taolin Zhang",
                "Mo Li",
                "Chuyu Zhang",
                "Yunxin Liu",
                "Haodong Duan",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12273.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "Condor: прорыв в создании синтетических данных для обучения языковых моделей",
                    "desc": "В статье представлен Condor - новый фреймворк для генерации синтетических данных для обучения больших языковых моделей (LLM). Он использует дерево мировых знаний и самоанализ для создания высококачественных обучающих данных. Эксперименты показали, что модель, обученная на 20 тысячах сгенерированных Condor примеров, превосходит аналоги. Исследование также выявило потенциал для улучшения производительности LLM при масштабировании синтетических данных."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Synthetic Data Generation",
                    "desc": "This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data."
                },
                "zh": {
                    "title": "合成数据生成，提升对话能力的关键",
                    "desc": "本论文探讨了监督微调（SFT）数据的质量对大型语言模型（LLMs）对话能力的重要性。随着LLMs的进步，高质量的人类标注SFT数据变得稀缺，因此需要更多依赖合成训练数据。我们提出了一种名为Condor的两阶段合成数据生成框架，结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。实验结果表明，仅用20K个Condor生成的样本微调的基础模型，其性能优于其他模型，验证了我们方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12390",
            "title": "GPS as a Control Signal for Image Generation",
            "url": "https://huggingface.co/papers/2501.12390",
            "abstract": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.",
            "score": 3,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "11d289e8a895bedd",
            "authors": [
                "Chao Feng",
                "Ziyang Chen",
                "Aleksander Holynski",
                "Alexei A. Efros",
                "Andrew Owens"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12390.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "GPS-метки открывают новые горизонты в генерации изображений и 3D-моделировании",
                    "desc": "Исследователи демонстрируют, как GPS-метки в метаданных фотографий могут использоваться для улучшения генерации изображений. Они обучают модели диффузии, генерирующие изображения на основе GPS-координат и текста, что позволяет точно отображать особенности различных районов и достопримечательностей. Авторы также извлекают 3D-модели из 2D GPS-моделей с помощью методики score distillation sampling. Результаты показывают, что GPS-обусловленные модели успешно генерируют изображения, варьирующиеся в зависимости от местоположения, и улучшают оценку 3D-структуры."
                },
                "en": {
                    "title": "Harnessing GPS Data for Location-Aware Image Generation",
                    "desc": "This paper explores the use of GPS data embedded in photo metadata as a control signal for generating images. The authors develop GPS-to-image models, particularly a diffusion model, that can create images based on both GPS coordinates and textual descriptions. The model effectively captures the unique characteristics of various urban environments, such as neighborhoods and landmarks. Additionally, they demonstrate the ability to extract 3D models from these images, enhancing the accuracy of 3D reconstructions by using GPS information to guide the process."
                },
                "zh": {
                    "title": "利用GPS标签生成城市图像的创新方法",
                    "desc": "本文展示了照片元数据中的GPS标签可以作为图像生成的有用控制信号。我们训练了GPS到图像的模型，并将其应用于需要细致理解城市中图像变化的任务。特别地，我们训练了一个扩散模型，生成同时依赖于GPS和文本的图像。评估结果表明，我们的GPS条件模型成功学习了基于位置生成变化图像，并且GPS条件改善了估计的3D结构。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11873",
            "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
            "url": "https://huggingface.co/papers/2501.11873",
            "abstract": "This paper revisits the implementation of Load-balancing Loss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i represents the frequency of expert i being selected, and p_i denotes the average gating score of the expert i. Existing MoE training frameworks usually employ the parallel training strategy so that f_i and the LBL are calculated within a micro-batch and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence (e.g., code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a global-batch to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize f_i across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to 42.8B total parameters and 400B tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
            "score": 1,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "370d057fec504963",
            "authors": [
                "Zihan Qiu",
                "Zeyu Huang",
                "Bo Zheng",
                "Kaiyue Wen",
                "Zekun Wang",
                "Rui Men",
                "Ivan Titov",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "Stanford University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11873.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Глобальный подход к балансировке нагрузки экспертов в MoE моделях",
                    "desc": "Статья предлагает новый подход к реализации функции потерь балансировки нагрузки (LBL) при обучении моделей Mixture-of-Experts (MoE). Авторы предлагают вычислять LBL на уровне глобального батча, а не микро-батча, что позволяет ослабить ограничения на распределение токенов между экспертами. Эксперименты на крупномасштабных языковых моделях показывают, что этот метод улучшает перплексию при предобучении и результаты на задачах downstream. Анализ также демонстрирует улучшение специализации экспертов по доменам."
                },
                "en": {
                    "title": "Enhancing Expert Specialization with Global-Batch Load-Balancing",
                    "desc": "This paper focuses on improving the Load-balancing Loss (LBL) in training Mixture-of-Experts (MoEs) models. The authors highlight that traditional methods use micro-batches, which limit the diversity of sequences and hinder expert specialization. They propose a new approach that utilizes global-batches, allowing for a broader range of sequences and better load balancing across the entire dataset. Experimental results show that this global-batch LBL method significantly enhances model performance and expert specialization in large language models."
                },
                "zh": {
                    "title": "全局批次提升混合专家模型的负载均衡与专业化",
                    "desc": "本文重新审视了在训练混合专家模型（MoEs）时的负载均衡损失（LBL）实现。我们提出使用全局批次来计算LBL，以打破微批次的严格约束，从而在语料库层面上促进负载均衡。通过在训练中引入额外的通信步骤来同步专家选择频率，实验结果显示全局批次LBL策略在预训练困惑度和下游任务中均显著提升了性能。我们的分析表明，全局批次LBL还大大改善了MoE专家的领域专业化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12326",
            "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
            "url": "https://huggingface.co/papers/2501.12326",
            "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
            "score": 0,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "1f98d8f49b073983",
            "authors": [
                "Yujia Qin",
                "Yining Ye",
                "Junjie Fang",
                "Haoming Wang",
                "Shihao Liang",
                "Shizuo Tian",
                "Junda Zhang",
                "Jiahao Li",
                "Yunxin Li",
                "Shijue Huang",
                "Wanjun Zhong",
                "Kuanye Li",
                "Jiale Yang",
                "Yu Miao",
                "Woyu Lin",
                "Longxiang Liu",
                "Xu Jiang",
                "Qianli Ma",
                "Jingyu Li",
                "Xiaojun Xiao",
                "Kai Cai",
                "Chuang Li",
                "Yaowei Zheng",
                "Chaolin Jin",
                "Chen Li",
                "Xiao Zhou",
                "Minchao Wang",
                "Haoli Chen",
                "Zhaojian Li",
                "Haihua Yang",
                "Haifeng Liu",
                "Feng Lin",
                "Tao Peng",
                "Xin Liu",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "UI-TARS: Революция в мире GUI-агентов",
                    "desc": "Статья представляет UI-TARS - модель графического агента, которая воспринимает только скриншоты и выполняет операции, подобные человеческим. UI-TARS превосходит существующие фреймворки агентов, достигая лучших результатов в более чем 10 бенчмарках для GUI-агентов. Модель включает в себя несколько ключевых инноваций: улучшенное восприятие, унифицированное моделирование действий, рассуждение по системе-2 и итеративное обучение с рефлексивными онлайн-трассами. UI-TARS постоянно учится на своих ошибках и адаптируется к непредвиденным ситуациям с минимальным вмешательством человека."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with UI-TARS: The End-to-End Agent Model",
                    "desc": "UI-TARS is a novel GUI agent model that processes screenshots to perform tasks like a human would, using keyboard and mouse actions. Unlike existing models that rely on complex commercial frameworks and pre-defined prompts, UI-TARS operates end-to-end and shows superior performance in various benchmarks. It achieves state-of-the-art results in GUI task execution by utilizing enhanced perception, unified action modeling, and system-2 reasoning for better decision-making. Additionally, its iterative training approach allows it to learn from past interactions, improving its adaptability with minimal human input."
                },
                "zh": {
                    "title": "UI-TARS：革新图形用户界面代理的全新模型",
                    "desc": "本文介绍了UI-TARS，这是一种原生的图形用户界面（GUI）代理模型，能够仅通过屏幕截图进行人类般的交互。与依赖复杂商业模型的现有代理框架不同，UI-TARS是一个端到端的模型，在多个GUI代理基准测试中表现优异，尤其在感知、定位和任务执行方面。UI-TARS通过增强感知、统一动作建模、系统-2推理和反思在线追踪等创新，显著提高了其性能。通过迭代训练和反思调优，UI-TARS能够不断学习并适应新的情况，减少对人类干预的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11223",
            "title": "Reasoning Language Models: A Blueprint",
            "url": "https://huggingface.co/papers/2501.11223",
            "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and experimentation.",
            "score": 0,
            "issue_id": 1797,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "f554416ad9af3344",
            "authors": [
                "Maciej Besta",
                "Julia Barth",
                "Eric Schreiber",
                "Ales Kubicek",
                "Afonso Catarino",
                "Robert Gerstenberger",
                "Piotr Nyczyk",
                "Patrick Iff",
                "Yueling Li",
                "Sam Houliston",
                "Tomasz Sternal",
                "Marcin Copik",
                "Grzegorz Kwaśniewski",
                "Jürgen Müller",
                "Łukasz Flis",
                "Hannes Eberhard",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "affiliations": [
                "BASF SE",
                "Cledar",
                "Cyfronet AGH",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11223.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#math",
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Демократизация искусственного интеллекта: модульный подход к созданию моделей рассуждений",
                    "desc": "Статья представляет комплексный подход к созданию моделей рассуждений (RLM), объединяющих языковые модели с механизмами продвинутых рассуждений. Авторы предлагают модульную структуру, включающую различные стратегии рассуждений, концепции обучения с подкреплением и схемы обучения. Они демонстрируют применимость этой структуры на примере существующих моделей и представляют x1 - модульную реализацию для быстрого прототипирования RLM. Исследование направлено на демократизацию возможностей продвинутых рассуждений в ИИ и снижение барьеров для разработки RLM."
                },
                "en": {
                    "title": "Democratizing Advanced Reasoning in AI",
                    "desc": "This paper introduces a modular framework for Reasoning Language Models (RLMs), which enhance traditional Large Language Models (LLMs) with advanced reasoning capabilities. The authors address the challenges of high costs and complex architectures by organizing RLM components into a comprehensive blueprint that includes various reasoning structures and strategies. They provide mathematical formulations and algorithmic specifications to facilitate easier implementation of RLMs. Additionally, the paper presents x1, a tool for rapid prototyping, and discusses how RLMs can be integrated into the larger LLM ecosystem to promote accessibility and innovation in AI development."
                },
                "zh": {
                    "title": "简化推理语言模型，促进AI创新",
                    "desc": "推理语言模型（RLMs）通过结合强化学习、搜索启发式和大型语言模型（LLMs），重新定义了人工智能的解决问题能力。尽管它们具有强大的推理机制，但高成本和复杂架构使得其可访问性和可扩展性面临挑战。为了解决这些问题，我们提出了一个模块化框架，组织RLM组件，并提供详细的数学公式和算法规范，以简化RLM的实现。我们的工作旨在降低RLM开发和实验的门槛，促进创新，缩小“富有AI”和“贫穷AI”之间的差距。"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1月23日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wèi GameFactory de kuàng jià, zhǐ zài tōng guò shēng chéng yòu xí yǐn qíng lái gé xīn yòu xí kāi fā. tā shǐ yòng yù xùn liàn de shì pín kuò sàn mó xíng, néng gòu chuàng jiàn quán xīn qiě duō yàng huà de yòu xí. wèi le jiě jué xiàn yǒu fāng fǎ zài chǎng jīng shēng chéng shàng de jú xiàn, zuò zhě tí chū le yī zhǒng duō jiē duàn xùn liàn cè lüè. tā men hái fā bù le yī gè jī yú Minecraft de gāo zhì liàng shì pín shù jù jí, bìng zhàn shì le kuàng jià néng gòu shēng chéng kāi fàng yù, duō yàng huà hé kě kòng de yòu xí shì pín.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '革新', 'pinyin': 'géxīn', 'trans': 'innovate'},\n{'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'},\n{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-trained'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '多样化', 'pinyin': 'duōyànghuà', 'trans': 'diversified'},\n{'word': '局限', 'pinyin': 'júxiàn', 'trans': 'limitation'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'},\n{'word': '数据集', 'pinyin': 'shùjù jí', 'trans': 'dataset'},\n{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'},\n{'word': '开放域', 'pinyin': 'kāifàng yù', 'trans': 'open domain'},\n{'word': '可控', 'pinyin': 'kěkòng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}