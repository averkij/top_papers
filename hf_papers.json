{
    "date": {
        "ru": "12 мая",
        "en": "May 12",
        "zh": "5月12日"
    },
    "time_utc": "2025-05-12 07:12",
    "weekday": 0,
    "issue_id": 3705,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.05026",
            "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
            "url": "https://huggingface.co/papers/2505.05026",
            "abstract": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.",
            "score": 3,
            "issue_id": 3705,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "41e61eccd430ea55",
            "authors": [
                "Jaehyun Jeon",
                "Jang Han Yoon",
                "Min Soo Kim",
                "Sumin Shim",
                "Yejin Choi",
                "Hanbin Kim",
                "Youngjae Yu"
            ],
            "affiliations": [
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05026.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Оценка убедительности UI без A/B-тестов",
                    "desc": "Авторы статьи представляют новый подход к оценке эффективности пользовательских интерфейсов с точки зрения их убедительности. Они вводят бенчмарк WiserUI-Bench для сравнительной оценки дизайна интерфейсов, содержащий 300 пар реальных UI-изображений с результатами A/B-тестов. Исследователи также предлагают стратегию G-FOCUS для улучшения оценки убедительности интерфейсов с помощью мультимодальных языковых моделей. Эксперименты показывают, что G-FOCUS превосходит существующие методы по согласованности и точности оценки пользовательских интерфейсов."
                },
                "en": {
                    "title": "Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench",
                    "desc": "This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization."
                },
                "zh": {
                    "title": "提升用户界面设计的说服力评估",
                    "desc": "本论文探讨了用户界面（UI）设计的有效性评估，强调设计的说服力对用户行为的影响。传统的A/B测试方法虽然常用，但成本高且耗时。我们提出了WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对真实的UI图像及其A/B测试结果和专家理由。此外，我们还提出了G-FOCUS，这是一种新颖的推理策略，能够提高基于视觉语言模型的说服力评估的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06111",
            "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
            "url": "https://huggingface.co/papers/2505.06111",
            "abstract": "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.",
            "score": 1,
            "issue_id": 3704,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 мая",
                "en": "May 9",
                "zh": "5月9日"
            },
            "hash": "bf19981dd100b8fb",
            "authors": [
                "Qingwen Bu",
                "Yanting Yang",
                "Jisong Cai",
                "Shenyuan Gao",
                "Guanghui Ren",
                "Maoqing Yao",
                "Ping Luo",
                "Hongyang Li"
            ],
            "affiliations": [
                "AgiBot",
                "OpenDriveLab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06111.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальное обучение роботов через видео и язык",
                    "desc": "UniVLA - это новый фреймворк для обучения универсальных политик взаимодействия робота с окружающей средой на основе зрения, языка и действий. Ключевая инновация заключается в использовании скрытой модели действий для извлечения представлений из видео, что позволяет использовать разнородные данные из различных воплощений и перспектив. Фреймворк демонстрирует превосходные результаты на нескольких бенчмарках по манипуляции и навигации, а также в реальных роботизированных системах. UniVLA достигает лучшей производительности по сравнению с OpenVLA, используя при этом значительно меньше вычислительных ресурсов и данных."
                },
                "en": {
                    "title": "UniVLA: Empowering Robots with Cross-Embodiment Learning",
                    "desc": "The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches."
                },
                "zh": {
                    "title": "UniVLA：提升通用机器人学习效率的新框架",
                    "desc": "本文提出了一种新的框架UniVLA，用于学习跨体现的视觉-语言-动作（VLA）策略，以提高通用机器人在不同环境中的表现。我们通过视频中的潜在动作模型提取以任务为中心的动作表示，从而利用广泛的多样化数据。为了减少与任务无关的动态影响，我们结合了语言指令，并在DINO特征空间中建立了潜在动作模型。实验结果表明，UniVLA在多个操作和导航基准测试中表现优异，且在预训练计算和下游数据方面的需求显著低于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-05-09.html",
    "link_next": "2025-05-13.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5月9日"
    },
    "short_date_next": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了推理在人工智能中的重要性，特别是在开放、不确定和多模态环境中。大型多模态推理模型（LMRMs）结合了文本、图像、音频和视频等模态，支持复杂推理能力。研究从模块化、感知驱动的流水线发展到统一的、以语言为中心的框架。文章回顾了早期任务特定模块的努力，并检查了最近将推理统一到多模态LLMs的方法，最后讨论了未来的方向。",
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
        "pinyin": "这篇文章讨论了推理在人工智能中的重要性，特别是在开放、不确定和多模态环境中。大型多模态推理模型（LMRMs）结合了文本、图像、音频和视频等模态，支持复杂推理能力。研究从模块化、感知驱动的流水线发展到统一的、以语言为中心的框架。文章回顾了早期任务特定模块的努力，并检查了最近将推理统一到多模态LLMs的方法，最后讨论了未来的方向。\n\nZhè piān wénzhāng tǎolùn le tuīlǐ zài réngōng zhìnéng zhōng de zhòngyàoxìng, tèbié shì zài kāifàng, bù quèdìng hé duō móshì huánjìng zhōng. Dàxíng duō móshì tuīlǐ móxíng (LMRMs) jiéhé le wénběn, túxiàng, yīnpiàn hé shìpín děng móshì, zhīchí fùzá tuīlǐ nénglì. Yánjiū cóng mókùhuà, gǎnjué qūdòng de liúshuǐxiàn fāzhǎn dào tǒngyī de, yǐ yǔyán wéi zhōngxīn de kuàngjià. Wénzhāng huígù le zǎoqī rènwù tèdìng mókù de nǔlì, bìng jiǎnchá le zuìjìn jiāng tuīlǐ tǒngyī dào duō móshì LLMs de fāngfǎ, zuìhòu tǎolùn le wèilái de fāngxiàng.",
        "vocab": "[\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"人工智能\", \"pinyin\": \"rén gōng zhì néng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"不确定\", \"pinyin\": \"bù què dìng\", \"trans\": \"uncertain\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fù zá\", \"trans\": \"complex\"},\n    {\"word\": \"模块化\", \"pinyin\": \"mó kuài huà\", \"trans\": \"modularization\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎn zhī\", \"trans\": \"perception\"},\n    {\"word\": \"驱动\", \"pinyin\": \"qū dòng\", \"trans\": \"drive\"},\n    {\"word\": \"流水线\", \"pinyin\": \"liú shuǐ xiàn\", \"trans\": \"pipeline\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒng yī\", \"trans\": \"unified\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"回顾\", \"pinyin\": \"huí gù\", \"trans\": \"review\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"特定\", \"pinyin\": \"tè dìng\", \"trans\": \"specific\"},\n    {\"word\": \"检查\", \"pinyin\": \"jiǎn chá\", \"trans\": \"examine\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"方向\", \"pinyin\": \"fāng xiàng\", \"trans\": \"direction\"}\n]",
        "trans": "This article discusses the importance of reasoning in artificial intelligence, particularly in open, uncertain, and multimodal environments. Large Multimodal Reasoning Models (LMRMs) integrate modalities such as text, images, audio, and video, supporting complex reasoning capabilities. Research has evolved from modular, perception-driven pipelines to unified, language-centric frameworks. The article reviews early efforts with task-specific modules and examines recent methods that unify reasoning into multimodal LLMs, concluding with a discussion on future directions.",
        "update_ts": "2025-05-11 18:30"
    }
}