{
    "date": {
        "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 26",
        "zh": "8æœˆ26æ—¥"
    },
    "time_utc": "2025-08-26 23:10",
    "weekday": 1,
    "issue_id": 5560,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.18265",
            "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
            "url": "https://huggingface.co/papers/2508.18265",
            "abstract": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
            "score": 109,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "f9e73f2c4171c881",
            "authors": [
                "Weiyun Wang",
                "Zhangwei Gao",
                "Lixin Gu",
                "Hengjun Pu",
                "Long Cui",
                "Xingguang Wei",
                "Zhaoyang Liu",
                "Linglin Jing",
                "Shenglong Ye",
                "Jie Shao",
                "Zhaokai Wang",
                "Zhe Chen",
                "Hongjie Zhang",
                "Ganlin Yang",
                "Haomin Wang",
                "Qi Wei",
                "Jinhui Yin",
                "Wenhao Li",
                "Erfei Cui",
                "Guanzhou Chen",
                "Zichen Ding",
                "Changyao Tian",
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Yuchen Duan",
                "Xuehui Wang",
                "Songze Li",
                "Xiangyu Zhao",
                "Haodong Duan",
                "Nianchen Deng",
                "Bin Fu",
                "Yinan He",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Junjun He",
                "Yingtong Xiong",
                "Han Lv",
                "Lijun Wu",
                "Wenqi Shao",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Biqing Qi",
                "Jiaye Ge",
                "Qipeng Guo",
                "Wenwei Zhang",
                "Wanli Ouyang",
                "Limin Wang",
                "Min Dou",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Jifeng Dai",
                "Bowen Zhou",
                "Weijie Su",
                "Kai Chen",
                "Yu Qiao",
                "Wenhai Wang",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18265.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "InternVL 3.5: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "InternVL 3.5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Cascade Reinforcement Learning (Cascade RL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Visual Resolution Router (ViR), Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Reasoning with InternVL 3.5",
                    "desc": "InternVL 3.5 is a new set of open-source multimodal models that improve reasoning, efficiency, and performance. It introduces Cascade Reinforcement Learning (Cascade RL), which uses a two-stage training process to enhance reasoning capabilities. The Visual Resolution Router (ViR) optimizes the resolution of visual inputs dynamically, while the Decoupled Vision-Language Deployment (DvD) strategy balances the computational load across GPUs. These innovations lead to significant performance gains in reasoning tasks and faster inference speeds compared to previous models."
                },
                "zh": {
                    "title": "æå‡æ¨ç†ä¸æ•ˆç‡çš„å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "InternVL 3.5 æ˜¯ä¸€æ¬¾æ–°å‹çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å…¶å¤šæ ·æ€§ã€æ¨ç†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯çº§è”å¼ºåŒ–å­¦ä¹ ï¼ˆCascade RLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ä¼˜åŒ–æ•ˆç‡ï¼Œæå‡ºäº†è§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼ˆViRï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¸å½±å“æ€§èƒ½ã€‚ç»“åˆè§£è€¦è§†è§‰-è¯­è¨€éƒ¨ç½²ï¼ˆDvDï¼‰ç­–ç•¥ï¼ŒInternVL 3.5 åœ¨æ¨ç†æ€§èƒ½ä¸Šæå‡äº† 16.0%ï¼Œå¹¶å®ç°äº† 4.05 å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18032",
            "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2508.18032",
            "abstract": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.",
            "score": 34,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "19b061daf44e7f58",
            "authors": [
                "Yaqi Li",
                "Peng Chen",
                "Mingyang Han",
                "Bu Pi",
                "Haoxiang Shi",
                "Runzhou Zhao",
                "Yang Yao",
                "Xuan Zhang",
                "Jun Song"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18032.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Visual-Chain of Guidance (Visual-CoG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Stage-Aware Rewards for Enhanced Image Generation",
                    "desc": "The Visual-Chain of Guidance (Visual-CoG) paradigm improves text-to-image generation by introducing stage-aware rewards that enhance the model's performance. Traditional models often provide feedback only at the end of the generation process, making it hard to understand which parts of the process are effective. Visual-CoG breaks down the generation into three stages: semantic reasoning, process refining, and outcome evaluation, allowing for immediate feedback at each stage. This approach leads to significant performance improvements across various benchmarks, demonstrating its effectiveness in handling complex prompts."
                },
                "zh": {
                    "title": "è§†è§‰å¼•å¯¼é“¾ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰å¼•å¯¼é“¾ï¼ˆVisual-CoGï¼‰çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡æä¾›é˜¶æ®µæ€§å¥–åŠ±ï¼Œæ”¹å–„äº†æ¨¡å‹åœ¨å¤„ç†å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºæ—¶çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„ä»…åœ¨ç”Ÿæˆç»“æŸæ—¶æä¾›å¥–åŠ±çš„æ–¹å¼ä¸åŒï¼ŒVisual-CoGåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¯ä¸ªé˜¶æ®µéƒ½ç»™äºˆå³æ—¶æŒ‡å¯¼ï¼Œä»è€Œä¼˜åŒ–ç”Ÿæˆç­–ç•¥ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼ŒVisual-CoGæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16577",
            "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
            "url": "https://huggingface.co/papers/2508.16577",
            "abstract": "MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.",
            "score": 28,
            "issue_id": 5545,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "346f008a32848d96",
            "authors": [
                "Yosef Dayani",
                "Omer Benishu",
                "Sagie Benaim"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.16577.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#diffusion",
                    "#3d",
                    "#multimodal",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MV-RAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… 2D-ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. MV-RAG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing 3D Generation with 2D Image Retrieval",
                    "desc": "MV-RAG is a new method for generating 3D models from text descriptions by first retrieving relevant 2D images from a large database. It uses a multiview diffusion model that conditions on these images to create outputs that are more consistent and accurate, especially for rare or out-of-domain concepts. The training process combines multiview data with diverse 2D images, allowing the model to learn how to predict different views from a set of images. Experiments show that MV-RAG outperforms existing methods in producing high-quality, photorealistic 3D outputs that adhere closely to the input text, even for challenging prompts."
                },
                "zh": {
                    "title": "MV-RAGï¼šæå‡æ–‡æœ¬åˆ°3Dç”Ÿæˆçš„ä¸€è‡´æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "MV-RAGæ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆç®¡é“ï¼Œé€šè¿‡ä»å¤§å‹2Dæ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³çš„2Då›¾åƒï¼Œå¹¶å°†å…¶ç”¨äºå¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶åŒ–ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŸŸå¤–æ¦‚å¿µæ—¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç§æ–°é¢–çš„æ··åˆç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†ç»“æ„åŒ–çš„å¤šè§†è§’æ•°æ®å’Œå¤šæ ·åŒ–çš„2Då›¾åƒé›†åˆã€‚æ¨¡å‹åœ¨å¤šè§†è§’æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨å¢å¼ºçš„æ¡ä»¶è§†å›¾æ¥æ¨¡æ‹Ÿæ£€ç´¢å˜å¼‚ï¼Œä»¥å®ç°è§†å›¾ç‰¹å®šçš„é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-RAGåœ¨å¤„ç†åŸŸå¤–æˆ–ç¨€æœ‰æ¦‚å¿µæ—¶ï¼Œæ˜¾è‘—æé«˜äº†3Dä¸€è‡´æ€§ã€ç…§ç‰‡çœŸå®æ„Ÿå’Œæ–‡æœ¬éµå¾ªæ€§ï¼ŒåŒæ—¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17472",
            "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2508.17472",
            "abstract": "T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.",
            "score": 20,
            "issue_id": 5541,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "26d1dcc01fb66261",
            "authors": [
                "Kaiyue Sun",
                "Rongyao Fang",
                "Chengqi Duan",
                "Xian Liu",
                "Xihui Liu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17472.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾ÑÑ‚Ğ¸ text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "T2I-ReasonBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ: Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ´Ğ¸Ğ¾Ğ¼, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ T2I, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Evaluating Reasoning in Text-to-Image Models",
                    "desc": "T2I-ReasonBench is a benchmark designed to assess the reasoning abilities of text-to-image (T2I) models. It evaluates these models across four key dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning, and Scientific-Reasoning. The evaluation follows a two-stage protocol that measures both the accuracy of reasoning and the quality of the generated images. By benchmarking multiple T2I models, the study provides a detailed analysis of their performance in reasoning tasks."
                },
                "zh": {
                    "title": "è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "T2I-ReasonBenchæ˜¯ä¸€ä¸ªè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº†å››ä¸ªç»´åº¦ï¼šæˆè¯­è§£é‡Šã€æ–‡æœ¬å›¾åƒè®¾è®¡ã€å®ä½“æ¨ç†å’Œç§‘å­¦æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯„ä¼°åè®®ï¼Œä»¥è¯„ä¼°æ¨ç†å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ã€‚é€šè¿‡å¯¹å¤šç§T2Iç”Ÿæˆæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹å…¶æ€§èƒ½çš„å…¨é¢åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16949",
            "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
            "url": "https://huggingface.co/papers/2508.16949",
            "abstract": "RuscaRL, a novel instructional scaffolding framework, enhances LLM reasoning by using rubrics for exploration and verifiable rewards, significantly improving performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3.",
            "score": 17,
            "issue_id": 5541,
            "pub_date": "2025-08-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 23",
                "zh": "8æœˆ23æ—¥"
            },
            "hash": "d0852bed475aa719",
            "authors": [
                "Yang Zhou",
                "Sunzhu Li",
                "Shunyu Liu",
                "Wenkai Fang",
                "Jiale Zhao",
                "Jingwen Yang",
                "Jianwei Lv",
                "Kongcheng Zhang",
                "Yihe Zhou",
                "Hengtong Lu",
                "Wei Chen",
                "Yan Xie",
                "Mingli Song"
            ],
            "affiliations": [
                "Li Auto Inc.",
                "Nanyang Technological University",
                "The Chinese University of Hong Kong, Shenzhen",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16949.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RuscaRL: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "RuscaRL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (RL). ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RuscaRL Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RuscaRL Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning in LLMs with RuscaRL",
                    "desc": "RuscaRL is a new framework that improves the reasoning abilities of Large Language Models (LLMs) by using structured rubrics to guide exploration and provide clear rewards. It addresses the challenge of limited exploration in LLMs, which hinders their learning from high-quality samples. By implementing checklist-style rubrics, RuscaRL helps models generate diverse and high-quality responses while gradually encouraging them to learn reasoning patterns independently. Experimental results show that RuscaRL significantly enhances performance on reasoning tasks, outperforming existing models like GPT-4.1."
                },
                "zh": {
                    "title": "RuscaRLï¼šæ‰“ç ´æ¨ç†ç“¶é¢ˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "RuscaRLæ˜¯ä¸€ç§æ–°é¢–çš„æ•™å­¦æ”¯æ¶æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨è¯„åˆ†æ ‡å‡†æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†æ¢ç´¢æ ·æœ¬çš„ç“¶é¢ˆé—®é¢˜ï¼Œæä¾›äº†æ¸…å•å¼çš„è¯„åˆ†æ ‡å‡†ä½œä¸ºå¤–éƒ¨æŒ‡å¯¼ï¼Œå¸®åŠ©æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å“åº”ã€‚RuscaRLè¿˜å¼•å…¥äº†å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è¿™äº›è¯„åˆ†æ ‡å‡†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRuscaRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16745",
            "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory\n  and Test-Time Compute Scaling",
            "url": "https://huggingface.co/papers/2508.16745",
            "abstract": "Models trained on random Boolean functions in a cellular automata framework show that increasing depth, recurrence, memory, and test-time compute scaling enhances multi-step reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities.",
            "score": 15,
            "issue_id": 5546,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "a385e4e24a0f85fb",
            "authors": [
                "Ivan Rodkin",
                "Daniil Orel",
                "Konstantin Smirnov",
                "Arman Bolatov",
                "Bilal Elbouardi",
                "Besher Hassan",
                "Yuri Kuratov",
                "Aydar Bulatov",
                "Preslav Nakov",
                "Timothy Baldwin",
                "Artem Shelmanov",
                "Mikhail Burtsev"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "London Institute for Mathematical Sciences, London, UK",
                "MBZUAI, Abu Dhabi, UAE",
                "Neural Networks and Deep Learning Lab, MIPT, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16745.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ±ÑƒĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multi-Step Reasoning in Neural Networks",
                    "desc": "This paper investigates how different neural network architectures and training methods influence the ability of models to perform multi-step reasoning. Using a cellular automata framework, the authors train models on sequences generated by random Boolean functions to prevent memorization. They find that while models can predict the next state accurately, their performance drops significantly when multi-step reasoning is needed. The study highlights that increasing model depth, along with incorporating recurrence, memory, and scaling during testing, significantly improves the reasoning capabilities of these models."
                },
                "zh": {
                    "title": "æ·±åº¦ä¸é€’å½’æå‡å¤šæ­¥æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒæ¶æ„å’Œè®­ç»ƒæ–¹æ³•å¦‚ä½•å½±å“æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨äº†ç»†èƒè‡ªåŠ¨æœºæ¡†æ¶ã€‚é€šè¿‡åœ¨éšæœºå¸ƒå°”å‡½æ•°ç”Ÿæˆçš„çŠ¶æ€åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬æ’é™¤äº†è®°å¿†çš„å½±å“ï¼Œå‘ç°å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¶æ„èƒ½å¤ŸæŠ½è±¡å‡ºåŸºæœ¬è§„åˆ™ã€‚å°½ç®¡æ¨¡å‹åœ¨ä¸‹ä¸€çŠ¶æ€é¢„æµ‹ä¸­è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥æ¨ç†æ—¶ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬ç¡®è®¤å¢åŠ æ¨¡å‹æ·±åº¦å¯¹é¡ºåºè®¡ç®—è‡³å…³é‡è¦ï¼Œå¹¶ä¸”é€šè¿‡é€’å½’ã€è®°å¿†å’Œæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•æœ‰æ•ˆæ·±åº¦ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18264",
            "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
            "url": "https://huggingface.co/papers/2508.18264",
            "abstract": "A multimodal method leverages both vision and text tokens to optimize vision token selection, improving inference efficiency in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as a maximum coverage problem. Afterward, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87x speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection.",
            "score": 14,
            "issue_id": 5556,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "c6557c55c3475786",
            "authors": [
                "Sixun Dong",
                "Juhua Hu",
                "Mian Zhang",
                "Ming Yin",
                "Yanjie Fu",
                "Qi Qian"
            ],
            "affiliations": [
                "Arizona State University",
                "Duke University",
                "University of Texas at Dallas",
                "University of Washington",
                "Zoom Communications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18264.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… VLM."
                },
                "en": {
                    "title": "Maximizing Efficiency with Multimodal Token Selection",
                    "desc": "This paper introduces a multimodal approach called MMTok that enhances the efficiency of vision-language models (VLMs) by optimizing the selection of vision tokens using both visual and textual information. The authors identify that redundancy in vision tokens hampers inference efficiency and propose a method that formulates the token selection as a maximum coverage problem. By leveraging the complementary nature of vision and text tokens, the method improves the quality of selected vision tokens while maintaining high performance. Experimental results demonstrate significant speed improvements and performance retention, showcasing the effectiveness of the proposed multimodal token selection strategy."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ä¿¡æ¯æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ–¹æ³•ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ¥ä¼˜åŒ–è§†è§‰æ ‡è®°çš„é€‰æ‹©ï¼Œä»è€Œæé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚é€šè¿‡å°†å­é›†é€‰æ‹©é—®é¢˜å½¢å¼åŒ–ä¸ºæœ€å¤§è¦†ç›–é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–è§†è§‰æ ‡è®°ä»¥è¦†ç›–æ–‡æœ¬æ ‡è®°å’ŒåŸå§‹è§†è§‰æ ‡è®°é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ˜¯äº’è¡¥çš„ï¼Œç»“åˆå¤šæ¨¡æ€ä¿¡æ¯å¯ä»¥æ˜¾è‘—è¶…è¶Šå•æ¨¡æ€åŸºçº¿ã€‚æˆ‘ä»¬çš„MMTokæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ¨ç†é€Ÿåº¦çš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17188",
            "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
            "url": "https://huggingface.co/papers/2508.17188",
            "abstract": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.",
            "score": 9,
            "issue_id": 5539,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "fb29b368892da03a",
            "authors": [
                "Zhilin Zhang",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "New York University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17188.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#agi",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PosterGen: Ğ˜Ğ˜-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²",
                    "desc": "PosterGen - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¼Ğ°ĞºĞµÑ‚, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚ĞµÑ€. PosterGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ñ‹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM)."
                },
                "en": {
                    "title": "Automating Academic Posters with PosterGen: Design Meets Efficiency",
                    "desc": "PosterGen is a multi-agent framework that automates the process of creating academic posters from research papers using large language models. It consists of four specialized agents that work together: Parser and Curator agents extract and organize content, the Layout agent arranges this content spatially, the Stylist agents enhance the visual design, and the Renderer composes the final poster. This system not only ensures that the posters are semantically accurate but also visually appealing, addressing the shortcomings of previous automation methods. The framework is evaluated using a vision-language model to ensure high design quality, demonstrating superior performance in generating ready-to-present posters with minimal manual adjustments."
                },
                "zh": {
                    "title": "PosterGenï¼šè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å­¦æœ¯æµ·æŠ¥çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "PosterGenæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å­¦æœ¯æµ·æŠ¥ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å››ä¸ªåä½œçš„ä¸“é—¨ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£å†…å®¹æå–ã€å¸ƒå±€è®¾è®¡ã€è§†è§‰é£æ ¼åº”ç”¨å’Œæœ€ç»ˆæµ·æŠ¥åˆæˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPosterGenåœ¨è®¾è®¡ç¾å­¦å’Œå†…å®¹å‡†ç¡®æ€§ä¸Šè¡¨ç°æ›´ä½³ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡ ä¹æ— éœ€äººå·¥ä¿®æ”¹çš„æµ·æŠ¥ã€‚é€šè¿‡å¼•å…¥è§†è§‰-è¯­è¨€æ¨¡å‹è¯„ä¼°è®¾è®¡è´¨é‡ï¼ŒPosterGenç¡®ä¿äº†æµ·æŠ¥çš„å¯è¯»æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18255",
            "title": "Hermes 4 Technical Report",
            "url": "https://huggingface.co/papers/2508.18255",
            "abstract": "Hermes 4, a hybrid reasoning model, integrates structured multi-turn reasoning with broad instruction-following, evaluated across various benchmarks including math, coding, knowledge, comprehension, and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Hermes 4, a family of hybrid reasoning models that combine structured, multi-turn reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728",
            "score": 8,
            "issue_id": 5558,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "92414d7ee9f2292c",
            "pdf_title_img": "assets/pdf/title_img/2508.18255.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#alignment",
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Hermes 4, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ¿Ğ¸ÑĞ°Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºÑˆĞ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Hermes 4: Bridging Structured Reasoning and Instruction-Following",
                    "desc": "Hermes 4 is a hybrid reasoning model that combines structured multi-turn reasoning with the ability to follow a wide range of instructions. The paper discusses the challenges faced in data curation, synthesis, training, and evaluation of the model, and presents solutions to these issues. It evaluates the model's performance across various benchmarks, including mathematical reasoning, coding tasks, knowledge comprehension, and alignment with human values. The authors also provide both quantitative metrics and qualitative analyses of the model's behavior, making the model weights publicly available for further research."
                },
                "zh": {
                    "title": "Hermes 4ï¼šæ··åˆæ¨ç†çš„æœªæ¥",
                    "desc": "Hermes 4æ˜¯ä¸€ç§æ··åˆæ¨ç†æ¨¡å‹ï¼Œç»“åˆäº†ç»“æ„åŒ–çš„å¤šè½®æ¨ç†å’Œå¹¿æ³›çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†ã€ç¼–ç¨‹ã€çŸ¥è¯†ç†è§£å’Œå¯¹é½ç­‰ã€‚æˆ‘ä»¬æè¿°äº†åœ¨æ•°æ®æ•´ç†ã€åˆæˆã€è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­é‡åˆ°çš„æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜è€Œé‡‡å–çš„è§£å†³æ–¹æ¡ˆã€‚æ‰€æœ‰æ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒå¼€æ”¾ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17580",
            "title": "UQ: Assessing Language Models on Unsolved Questions",
            "url": "https://huggingface.co/papers/2508.17580",
            "abstract": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.",
            "score": 8,
            "issue_id": 5539,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "9f8a8ba45f2a8eca",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#survey",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "UQ: ĞÑ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "UQ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Stack Exchange, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ğ½Ñ‚Ğ°ÑÑ‚Ğ¸ĞºĞ¸. UQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "UQ: Evaluating AI on Real-World Unsolved Questions",
                    "desc": "The paper introduces UQ, a new benchmark for evaluating AI models on unsolved questions, which combines difficulty and realism to better assess capabilities like reasoning and factuality. Unlike traditional benchmarks that often present artificially difficult questions, UQ focuses on real-world challenges that arise from genuine human inquiries. The UQ framework includes a dataset of 500 diverse questions, validation strategies, and a collaborative platform for community verification. This innovative approach aims to push the boundaries of AI performance by addressing open-ended challenges that reflect actual knowledge gaps."
                },
                "zh": {
                    "title": "UQï¼šè¯„ä¼°AIæ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "UQæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æœªè§£å†³é—®é¢˜ä¸Šçš„åŸºå‡†ï¼Œç»“åˆäº†éš¾åº¦å’Œç°å®æ€§ï¼Œä»¥è¯„ä¼°æ¨ç†ã€äº‹å®æ€§å’Œæµè§ˆç­‰èƒ½åŠ›ã€‚å½“å‰çš„åŸºå‡†å¾€å¾€é¢ä¸´éš¾åº¦ä¸ç°å®æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œè€ŒUQé€šè¿‡è¯„ä¼°æœªè§£å†³çš„é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°èŒƒå¼ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«500ä¸ªæ¥è‡ªStack Exchangeçš„æŒ‘æˆ˜æ€§é—®é¢˜çš„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†éªŒè¯è€…è¾…åŠ©ç­›é€‰å’Œç¤¾åŒºéªŒè¯çš„æ–¹æ³•ã€‚UQä¸ºè¯„ä¼°å‰æ²¿æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°æä¾›äº†æ–°çš„è·¯å¾„ï¼Œæ¨åŠ¨äººç±»çŸ¥è¯†çš„å‰æ²¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17290",
            "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment",
            "url": "https://huggingface.co/papers/2508.17290",
            "abstract": "MEENA, a Persian-English dataset, evaluates vision-language models across scientific, reasoning, and human-level understanding tasks, enhancing capabilities beyond English.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large vision-language models (VLMs) have primarily focused on English, with limited attention given to other languages. To address this gap, we introduce MEENA (also known as PersianMMMU), the first dataset designed to evaluate Persian VLMs across scientific, reasoning, and human-level understanding tasks. Our dataset comprises approximately 7,500 Persian and 3,000 English questions, covering a wide range of topics such as reasoning, mathematics, physics, diagrams, charts, and Persian art and literature. Key features of MEENA include: (1) diverse subject coverage spanning various educational levels, from primary to upper secondary school, (2) rich metadata, including difficulty levels and descriptive answers, (3) original Persian data that preserves cultural nuances, (4) a bilingual structure to assess cross-linguistic performance, and (5) a series of diverse experiments assessing various capabilities, including overall performance, the model's ability to attend to images, and its tendency to generate hallucinations. We hope this benchmark contributes to enhancing VLM capabilities beyond English.",
            "score": 6,
            "issue_id": 5545,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "f8b5adba597a583e",
            "authors": [
                "Omid Ghahroodi",
                "Arshia Hemmat",
                "Marzia Nouri",
                "Seyed Mohammad Hadi Hosseini",
                "Doratossadat Dastgheib",
                "Mohammad Vali Sanian",
                "Alireza Sahebi",
                "Reihaneh Zohrabi",
                "Mohammad Hossein Rohban",
                "Ehsaneddin Asgari",
                "Mahdieh Soleymani Baghshah"
            ],
            "affiliations": [
                "Computer Engineering Department, Sharif University of Technology, Iran",
                "Computer Engineering Department, University of Isfahan, Iran",
                "Qatar Computing Research Institute, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17290.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#reasoning",
                    "#hallucinations",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "MEENA: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MEENA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¸Ğ´ÑĞºĞ¾-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 7500 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¸Ğ´ÑĞºĞ¾Ğ¼ Ğ¸ 3000 Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ñ‚ĞµĞ¼. MEENA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµÑ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¸Ğ´ÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models Beyond English with MEENA",
                    "desc": "The paper introduces MEENA, a dataset specifically designed to evaluate vision-language models (VLMs) in Persian and English. It addresses the lack of resources for non-English languages in the field of machine learning, particularly in tasks requiring scientific reasoning and human-level understanding. MEENA includes a diverse set of questions across various subjects and educational levels, along with rich metadata to aid in performance assessment. The dataset aims to improve the capabilities of VLMs by providing a bilingual framework that highlights cross-linguistic performance and cultural nuances."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³¢æ–¯è¯­èƒ½åŠ›",
                    "desc": "MEENAæ˜¯ä¸€ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³¢æ–¯è¯­-è‹±è¯­æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥éè‹±è¯­è¯­è¨€çš„ç ”ç©¶ç©ºç™½ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦7500ä¸ªæ³¢æ–¯è¯­å’Œ3000ä¸ªè‹±è¯­é—®é¢˜ï¼Œæ¶µç›–ç§‘å­¦ã€æ¨ç†å’Œäººç±»ç†è§£ç­‰ä»»åŠ¡ã€‚MEENAçš„ç‰¹ç‚¹åŒ…æ‹¬å¤šæ ·çš„ä¸»é¢˜è¦†ç›–ã€ä¸°å¯Œçš„å…ƒæ•°æ®å’Œä¿ç•™æ–‡åŒ–ç»†èŠ‚çš„åŸå§‹æ³¢æ–¯è¯­æ•°æ®ã€‚é€šè¿‡è¿™ä¸€åŸºå‡†ï¼Œæˆ‘ä»¬å¸Œæœ›æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­è¯­è¨€ä¸Šçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17298",
            "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
            "url": "https://huggingface.co/papers/2508.17298",
            "abstract": "A comprehensive survey of compositional visual reasoning from 2023 to 2025 reviews advancements in multimodal AI, highlighting architectural designs, benchmarks, and future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.",
            "score": 2,
            "issue_id": 5542,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "e2e4d94059c3bca1",
            "authors": [
                "Fucai Ke",
                "Joy Hsu",
                "Zhixi Cai",
                "Zixian Ma",
                "Xin Zheng",
                "Xindi Wu",
                "Sukai Huang",
                "Weiqing Wang",
                "Pari Delir Haghighi",
                "Gholamreza Haffari",
                "Ranjay Krishna",
                "Jiajun Wu",
                "Hamid Rezatofighi"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Griffith University",
                "Monash University",
                "Princeton University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17298.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#hallucinations",
                    "#survey",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ 2023 Ğ¿Ğ¾ 2025 Ğ³Ğ¾Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 260 Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Advancing AI: A Deep Dive into Compositional Visual Reasoning",
                    "desc": "This paper presents a comprehensive survey of compositional visual reasoning in multimodal AI, covering advancements from 2023 to 2025. It reviews over 260 research papers, formalizes key definitions, and discusses the benefits of compositional approaches, such as improved cognitive alignment and interpretability. The authors outline a five-stage evolution in the field, highlighting various architectural designs and their respective strengths and weaknesses. Additionally, the survey identifies open challenges and proposes future research directions to enhance the capabilities of AI in visual reasoning tasks."
                },
                "zh": {
                    "title": "ç»„åˆè§†è§‰æ¨ç†çš„æœªæ¥æ¢ç´¢",
                    "desc": "è¿™ç¯‡è®ºæ–‡å¯¹2023å¹´è‡³2025å¹´é—´çš„ç»„åˆè§†è§‰æ¨ç†è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„è¿›å±•ã€‚å®ƒç³»ç»Ÿåœ°å›é¡¾äº†260å¤šç¯‡æ¥è‡ªé¡¶çº§ä¼šè®®çš„è®ºæ–‡ï¼Œæ¢è®¨äº†ç»„åˆæ–¹æ³•åœ¨è®¤çŸ¥å¯¹é½ã€è¯­ä¹‰ä¿çœŸåº¦ã€é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚è®ºæ–‡è¿˜è¿½è¸ªäº†ä»è¯­è¨€ä¸­å¿ƒç®¡é“åˆ°å·¥å…·å¢å¼ºçš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„äº”é˜¶æ®µèŒƒå¼è½¬å˜ï¼Œå¹¶åˆ—å‡ºäº†60å¤šä¸ªåŸºå‡†å’Œç›¸åº”çš„æŒ‡æ ‡ã€‚æœ€åï¼Œä½œè€…æ€»ç»“äº†å…³é”®è§è§£ï¼Œè¯†åˆ«äº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16790",
            "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
            "url": "https://huggingface.co/papers/2508.16790",
            "abstract": "TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 1) dependence on multi-layer residual vector quantization structures or high frame rates, 2) reliance on auxiliary pre-trained models for semantic distillation, and 3) requirements for complex two-stage training processes. In this work, we introduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a novel approach designed to overcome these challenges. TaDiCodec employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression. TaDiCodec achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS). Notably, TaDiCodec employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models. We also validate the compatibility of TaDiCodec in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small reconstruction-generation gap. We will open source our code and model checkpoints. Audio samples are are available at https:/tadicodec.github.io/. We release code and model checkpoints at https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
            "score": 2,
            "issue_id": 5540,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "566e852623f03412",
            "authors": [
                "Yuancheng Wang",
                "Dekun Chen",
                "Xueyao Zhang",
                "Junan Zhang",
                "Jiaqi Li",
                "Zhizheng Wu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16790.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "TaDiCodec: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "TaDiCodec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² (6,25 Ğ“Ñ†) Ğ¸ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ° (0,0875 ĞºĞ±Ğ¸Ñ‚/Ñ) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. TaDiCodec Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, ĞºĞ°Ğº Word Error Rate, ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Speech Generation with TaDiCodec",
                    "desc": "TaDiCodec is a new speech codec that uses a Text-aware Diffusion Transformer to improve speech generation while keeping low frame rates and bitrates. It addresses limitations of existing speech tokenizers by using end-to-end optimization for quantization and reconstruction, eliminating the need for complex training processes and auxiliary models. The codec achieves a low frame rate of 6.25 Hz and a bitrate of 0.0875 kbps, while still performing well on important metrics like Word Error Rate and speech quality. Additionally, TaDiCodec supports zero-shot text-to-speech applications, showcasing its versatility in speech language modeling."
                },
                "zh": {
                    "title": "æ–‡æœ¬æ„ŸçŸ¥çš„é«˜æ•ˆè¯­éŸ³ç¼–è§£ç å™¨",
                    "desc": "TaDiCodecæ˜¯ä¸€ç§æ–‡æœ¬æ„ŸçŸ¥çš„æ‰©æ•£å˜æ¢å™¨è¯­éŸ³ç¼–è§£ç å™¨ï¼Œæ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å’Œæ–‡æœ¬æŒ‡å¯¼æ¥å®ç°ä½å¸§ç‡å’Œä½æ¯”ç‰¹ç‡çš„ä¼˜è¶Šè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚è¯¥æ¨¡å‹å…‹æœäº†ç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨å¤šå±‚æ®‹å·®å‘é‡é‡åŒ–ã€ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹å’Œå¤æ‚çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚TaDiCodecé‡‡ç”¨æ‰©æ•£è‡ªç¼–ç å™¨è¿›è¡Œé‡åŒ–å’Œé‡å»ºï¼Œå¹¶é€šè¿‡æ‰©æ•£è§£ç å™¨é›†æˆæ–‡æœ¬æŒ‡å¯¼ï¼Œä»¥æé«˜é‡å»ºè´¨é‡å’Œå‹ç¼©æ•ˆç‡ã€‚æœ€ç»ˆï¼ŒTaDiCodecåœ¨è¯­éŸ³ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å®ç°äº†æä½çš„å¸§ç‡å’Œæ¯”ç‰¹ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18190",
            "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
            "url": "https://huggingface.co/papers/2508.18190",
            "abstract": "ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.",
            "score": 1,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "e7ede69164051787",
            "authors": [
                "Zirui Tang",
                "Boyu Niu",
                "Xuanhe Zhou",
                "Boxiu Li",
                "Wei Zhou",
                "Jiannan Wang",
                "Guoliang Li",
                "Xinyi Zhang",
                "Fan Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai Jiao Tong University",
                "Simon Fraser University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18190.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#data",
                    "#interpretability"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ST-Raptor: Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "ST-Raptor - ÑÑ‚Ğ¾ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞÑ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ”ĞµÑ€ĞµĞ²Ğ¾ (HO-Tree) Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ñ€ĞµĞ²ĞµÑĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ST-Raptor Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸: Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ST-Raptor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾ 20% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "ST-Raptor: Revolutionizing Table Question Answering with Hierarchical Trees",
                    "desc": "ST-Raptor is a novel framework designed to enhance question answering from semi-structured tables using large language models (LLMs). It introduces a Hierarchical Orthogonal Tree (HO-Tree) to effectively represent complex table layouts, allowing for better interpretation of the data. The framework employs a two-stage verification mechanism to ensure the accuracy of answers by validating execution steps and reconstructing queries. Experimental results demonstrate that ST-Raptor significantly improves answer accuracy compared to existing methods, making it a valuable tool for automating table-based question answering."
                },
                "zh": {
                    "title": "ST-Raptorï¼šåŠç»“æ„åŒ–è¡¨æ ¼é—®ç­”çš„æ–°çªç ´",
                    "desc": "ST-Raptoræ˜¯ä¸€ä¸ªåŸºäºæ ‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»åŠç»“æ„åŒ–è¡¨æ ¼ä¸­å›ç­”é—®é¢˜çš„æŒ‘æˆ˜ã€‚å®ƒå¼•å…¥äº†å±‚æ¬¡æ­£äº¤æ ‘ï¼ˆHO-Treeï¼‰æ¥æ•æ‰å¤æ‚çš„è¡¨æ ¼å¸ƒå±€ï¼Œå¹¶é€šè¿‡åŸºæœ¬æ ‘æ“ä½œæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡Œå¸¸è§çš„é—®ç­”ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨äº†ä¸¤é˜¶æ®µéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿æ‰§è¡Œæ­¥éª¤çš„æ­£ç¡®æ€§å’Œç­”æ¡ˆçš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒST-Raptoråœ¨ç­”æ¡ˆå‡†ç¡®æ€§ä¸Šæ¯”ä¹ä¸ªåŸºçº¿æ–¹æ³•æé«˜äº†å¤šè¾¾20%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18159",
            "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
            "url": "https://huggingface.co/papers/2508.18159",
            "abstract": "SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visually-guided image editing, where edits are conditioned on both visual cues and textual prompts, has emerged as a powerful paradigm for fine-grained, controllable content generation. Although recent generative models have shown remarkable capabilities, existing evaluations remain simple and insufficiently representative of real-world editing challenges. We present SpotEdit, a comprehensive benchmark designed to systematically assess visually-guided image editing methods across diverse diffusion, autoregressive, and hybrid generative models, uncovering substantial performance disparities. To address a critical yet underexplored challenge, our benchmark includes a dedicated component on hallucination, highlighting how leading models, such as GPT-4o, often hallucinate the existence of a visual cue and erroneously perform the editing task. Our code and benchmark are publicly released at https://github.com/SaraGhazanfari/SpotEdit.",
            "score": 1,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "7e9e7873459fbab7",
            "authors": [
                "Sara Ghazanfari",
                "Wei-An Lin",
                "Haitong Tian",
                "Ersin Yumer"
            ],
            "affiliations": [
                "Adobe Inc.",
                "New York University, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18159.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#hallucinations",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "SpotEdit: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "SpotEdit â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. ĞĞ½ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. SpotEdit Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "SpotEdit: A New Standard for Evaluating Image Editing Models",
                    "desc": "SpotEdit is a new benchmark created to evaluate how well different image editing methods work when guided by visual cues and text prompts. It highlights the differences in performance among various generative models, including diffusion, autoregressive, and hybrid types. The benchmark also focuses on a significant issue called hallucination, where models mistakenly believe a visual cue exists and make incorrect edits. By providing a more thorough evaluation framework, SpotEdit aims to improve the understanding and development of visually-guided image editing techniques."
                },
                "zh": {
                    "title": "SpotEditï¼šè¯„ä¼°è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘çš„åŸºå‡†",
                    "desc": "SpotEditæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘æ–¹æ³•çš„åŸºå‡†ï¼Œæ­ç¤ºäº†æ‰©æ•£ã€è‡ªåŠ¨å›å½’å’Œæ··åˆç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å’Œå¹»è§‰é—®é¢˜ã€‚è§†è§‰å¼•å¯¼å›¾åƒç¼–è¾‘ç»“åˆäº†è§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æç¤ºï¼Œæˆä¸ºä¸€ç§å¼ºå¤§çš„ç»†ç²’åº¦å¯æ§å†…å®¹ç”Ÿæˆæ–¹å¼ã€‚å°½ç®¡æœ€è¿‘çš„ç”Ÿæˆæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¿‡äºç®€å•ï¼Œæ— æ³•å……åˆ†ä»£è¡¨ç°å®ä¸–ç•Œçš„ç¼–è¾‘æŒ‘æˆ˜ã€‚SpotEditåŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨å¹»è§‰é—®é¢˜ï¼ŒæŒ‡å‡ºä¸€äº›é¢†å…ˆæ¨¡å‹åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­å¸¸å¸¸é”™è¯¯åœ°å‡è®¾å­˜åœ¨è§†è§‰çº¿ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17973",
            "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German",
            "url": "https://huggingface.co/papers/2508.17973",
            "abstract": "A large-scale German dataset and model for readability-controlled paraphrasing are introduced, achieving state-of-the-art performance in text simplification.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to paraphrase texts across different complexity levels is essential for creating accessible texts that can be tailored toward diverse reader groups. Thus, we introduce German4All, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases. It spans five readability levels and comprises over 25,000 samples. The dataset is automatically synthesized using GPT-4 and rigorously evaluated through both human and LLM-based judgments. Using German4All, we train an open-source, readability-controlled paraphrasing model that achieves state-of-the-art performance in German text simplification, enabling more nuanced and reader-specific adaptations. We opensource both the dataset and the model to encourage further research on multi-level paraphrasing",
            "score": 1,
            "issue_id": 5546,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "b12e2be4bd1806ae",
            "authors": [
                "Miriam AnschÃ¼tz",
                "Thanh Mai Pham",
                "Eslam Nasrallah",
                "Maximilian MÃ¼ller",
                "Cristian-George Craciun",
                "Georg Groh"
            ],
            "affiliations": [
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17973.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#data",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: German4All Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ German4All - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 25 000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ German4All Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Text Accessibility with German4All",
                    "desc": "This paper presents German4All, a comprehensive dataset designed for readability-controlled paraphrasing in the German language. It includes over 25,000 paragraph-level samples across five different readability levels, allowing for tailored text simplification. The dataset is generated using GPT-4 and is evaluated through both human assessments and large language model (LLM) judgments. An open-source model trained on this dataset achieves state-of-the-art results, promoting further research in the field of multi-level paraphrasing."
                },
                "zh": {
                    "title": "å¯è¯»æ€§æ§åˆ¶çš„å¾·è¯­æ”¹å†™æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§å‹å¾·è¯­æ•°æ®é›†å’Œæ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å¯è¯»æ€§æ§åˆ¶çš„æ”¹å†™ï¼Œè¾¾åˆ°æ–‡æœ¬ç®€åŒ–çš„æœ€å…ˆè¿›æ°´å¹³ã€‚è¯¥æ•°æ®é›†åä¸ºGerman4Allï¼ŒåŒ…å«è¶…è¿‡25,000ä¸ªæ ·æœ¬ï¼Œæ¶µç›–äº”ä¸ªå¯è¯»æ€§ç­‰çº§ï¼Œèƒ½å¤Ÿä¸ºä¸åŒè¯»è€…ç¾¤ä½“æä¾›å®šåˆ¶åŒ–çš„æ–‡æœ¬ã€‚æ•°æ®é›†é€šè¿‡GPT-4è‡ªåŠ¨åˆæˆï¼Œå¹¶ç»è¿‡äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†è¯¥æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å¤šå±‚æ¬¡æ”¹å†™çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17821",
            "title": "Limitations of Normalization in Attention Mechanism",
            "url": "https://huggingface.co/papers/2508.17821",
            "abstract": "Theoretical and empirical analysis of softmax normalization in attention mechanisms reveals limitations in token selection and gradient sensitivity, highlighting the need for improved normalization strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures.",
            "score": 1,
            "issue_id": 5547,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "309afcc613e927f2",
            "authors": [
                "Timur Mudarisov",
                "Mikhail Burtsev",
                "Tatiana Petrova",
                "Radu State"
            ],
            "affiliations": [
                "London Institute for Mathematical Sciences",
                "State University of Luxembourg",
                "University of Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17821.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-2 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ… Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Rethinking Softmax: Enhancing Token Selection in Attention Mechanisms",
                    "desc": "This paper explores the limitations of softmax normalization in attention mechanisms used in machine learning models. It provides a theoretical framework to analyze how well models can select important tokens and the geometric relationships between them. The authors present empirical evidence from experiments with the GPT-2 model, showing that as more tokens are selected, the model struggles to differentiate between them, leading to a uniform selection pattern. Additionally, the paper highlights issues with gradient sensitivity during training, suggesting that better normalization strategies are needed for future attention models."
                },
                "zh": {
                    "title": "æå‡æ³¨æ„åŠ›æœºåˆ¶çš„å½’ä¸€åŒ–ç­–ç•¥",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æ³¨æ„åŠ›æœºåˆ¶ä¸­softmaxå½’ä¸€åŒ–çš„å±€é™æ€§ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œä»¥è¯†åˆ«æ¨¡å‹çš„é€‰æ‹©èƒ½åŠ›å’Œä»¤ç‰Œé€‰æ‹©ä¸­çš„å‡ ä½•åˆ†ç¦»ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„GPT-2æ¨¡å‹è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†ç†è®ºç»“æœï¼Œå¹¶åˆ†æäº†æ³¨æ„åŠ›æœºåˆ¶çš„å…³é”®è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œéšç€é€‰æ‹©çš„ä»¤ç‰Œæ•°é‡å¢åŠ ï¼Œæ¨¡å‹åŒºåˆ†æœ‰ç”¨ä»¤ç‰Œçš„èƒ½åŠ›ä¸‹é™ï¼Œä¸”softmaxå½’ä¸€åŒ–ä¸‹çš„æ¢¯åº¦æ•æ„Ÿæ€§åœ¨è®­ç»ƒä¸­ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17811",
            "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian\n  Splatting",
            "url": "https://huggingface.co/papers/2508.17811",
            "abstract": "MeshSplat uses Gaussian Splatting and a feed-forward network to reconstruct surfaces from sparse views, improving accuracy with a Weighted Chamfer Distance Loss and normal prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web",
            "score": 1,
            "issue_id": 5550,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "0c22f3ea15ee787c",
            "authors": [
                "Hanzhi Chang",
                "Ruijie Zhu",
                "Wenjie Chang",
                "Mulin Yu",
                "Yanzhe Liang",
                "Jiahao Lu",
                "Zhuoyuan Li",
                "Tianzhu Zhang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17811.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "MeshSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ¼Ñƒ Ñ‡Ğ¸ÑĞ»Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Gaussian Splatting Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Weighted Chamfer Distance Loss Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². MeshSplat Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ¼Ñƒ Ñ‡Ğ¸ÑĞ»Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Surface Reconstruction from Sparse Views with MeshSplat",
                    "desc": "MeshSplat is a novel framework designed for reconstructing surfaces from very few input views using Gaussian Splatting. It employs a feed-forward network to predict 2D Gaussian splats that help in synthesizing new view images without needing direct 3D ground-truth data. To enhance the accuracy of the reconstruction, it introduces a Weighted Chamfer Distance Loss that focuses on improving depth predictions in overlapping view areas, along with a normal prediction network for better alignment with surface normals. The results show that MeshSplat outperforms existing methods in sparse-view surface reconstruction tasks, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "MeshSplatï¼šç¨€ç–è§†å›¾é‡å»ºçš„æ–°çªç ´",
                    "desc": "MeshSplat æ˜¯ä¸€ä¸ªç”¨äºç¨€ç–è§†å›¾è¡¨é¢é‡å»ºçš„æ¡†æ¶ï¼Œé‡‡ç”¨é«˜æ–¯ç‚¹äº‘å’Œå‰é¦ˆç½‘ç»œæ¥æé«˜é‡å»ºç²¾åº¦ã€‚å®ƒé€šè¿‡åŠ æƒçš„ Chamfer è·ç¦»æŸå¤±å’Œæ³•çº¿é¢„æµ‹æ¥ä¼˜åŒ–æ·±åº¦å›¾çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥è§†å›¾é‡å çš„åŒºåŸŸã€‚è¯¥æ–¹æ³•åˆ©ç”¨ 2D é«˜æ–¯ç‚¹äº‘ä½œä¸ºæ¡¥æ¢ï¼Œå°†æ–°è§†å›¾åˆæˆä¸å­¦ä¹ çš„å‡ ä½•å…ˆéªŒè¿æ¥èµ·æ¥ï¼Œä»è€Œå®ç°è¡¨é¢é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMeshSplat åœ¨ç¨€ç–è§†å›¾ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18076",
            "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
            "url": "https://huggingface.co/papers/2508.18076",
            "abstract": "The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "0e751250907ca130",
            "authors": [
                "Khaoula Chehbouni",
                "Mohammed Haddou",
                "Jackie Chi Kit Cheung",
                "Golnoosh Farnadi"
            ],
            "affiliations": [
                "McGill University",
                "Mila - Quebec AI Institute",
                "Statistics Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18076.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#alignment",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑÑƒĞ´ÑŒĞ¸: Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ½Ñ‚ÑƒĞ·Ğ¸Ğ°Ğ·Ğ¼?",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ñ… Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM ĞºĞ°Ğº ÑÑƒĞ´ĞµĞ¹: ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Rethinking Large Language Models as Evaluators in NLG",
                    "desc": "This paper critiques the use of large language models (LLMs) as judges for evaluating natural language generation (NLG) systems. It questions their reliability, capabilities, scalability, and cost-effectiveness, suggesting that the excitement around using LLMs as evaluators may be premature. The authors analyze four key assumptions about LLMs acting as proxies for human judgment and their effectiveness in evaluation tasks. They call for more responsible evaluation practices to ensure that the integration of LLMs in NLG evaluation supports meaningful progress in the field."
                },
                "zh": {
                    "title": "å®¡æ…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç”Ÿæˆç³»ç»Ÿ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ‰¹è¯„äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆç³»ç»Ÿçš„è¯„åˆ¤è€…çš„åšæ³•ï¼Œè´¨ç–‘å…¶å¯é æ€§ã€èƒ½åŠ›ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ä½œè€…è®¤ä¸ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLJsï¼‰è¢«è§†ä¸ºä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡çš„æœ‰å¸Œæœ›æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚è®ºæ–‡åˆ†æäº†LLJsä½¿ç”¨çš„å››ä¸ªæ ¸å¿ƒå‡è®¾ï¼ŒåŒ…æ‹¬å…¶ä½œä¸ºäººç±»åˆ¤æ–­ä»£ç†çš„èƒ½åŠ›ã€è¯„ä¼°èƒ½åŠ›ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œå¹¶æŒ‡å‡ºè¿™äº›å‡è®¾å¯èƒ½å—åˆ°LLMsçš„å›ºæœ‰é™åˆ¶çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒéœ€è¦æ›´è´Ÿè´£ä»»çš„è¯„ä¼°å®è·µï¼Œä»¥ç¡®ä¿LLJsåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä½œç”¨èƒ½å¤Ÿä¿ƒè¿›è€Œä¸æ˜¯é˜»ç¢è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17326",
            "title": "Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing",
            "url": "https://huggingface.co/papers/2508.17326",
            "abstract": "A semantic-guided diffusion-based dehazing algorithm improves echocardiography image quality by integrating a pixel-wise noise model and a generative prior.  \t\t\t\t\tAI-generated summary \t\t\t\t Echocardiography plays a central role in cardiac imaging, offering dynamic views of the heart that are essential for diagnosis and monitoring. However, image quality can be significantly degraded by haze arising from multipath reverberations, particularly in difficult-to-image patients. In this work, we propose a semantic-guided, diffusion-based dehazing algorithm developed for the MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method integrates a pixel-wise noise model, derived from semantic segmentation of hazy inputs into a diffusion posterior sampling framework guided by a generative prior trained on clean ultrasound data. Quantitative evaluation on the challenge dataset demonstrates strong performance across contrast and fidelity metrics. Code for the submitted algorithm is available at https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.",
            "score": 0,
            "issue_id": 5559,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "85b923b583d24ac1",
            "authors": [
                "Tristan S. W. Stevens",
                "OisÃ­n Nolan",
                "Ruud J. G. van Sloun"
            ],
            "affiliations": [
                "Eindhoven University of Technology, the Netherlands"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17326.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#healthcare",
                    "#diffusion"
                ],
                "emoji": "ğŸ«€",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ñ‹Ğ¼ĞºĞ¸ Ğ´Ğ»Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ°Ğ¹Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´Ñ‹Ğ¼ĞºÑƒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ»ÑƒÑ‡ĞµĞ²Ñ‹Ñ… Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Echocardiography with Semantic-Guided Dehazing",
                    "desc": "This paper presents a new algorithm for improving the quality of echocardiography images that are affected by haze. The proposed method uses a diffusion-based approach that incorporates a pixel-wise noise model, which is informed by semantic segmentation of the hazy images. By leveraging a generative prior trained on clean ultrasound data, the algorithm effectively enhances image clarity and detail. The results show significant improvements in contrast and fidelity, making it a valuable tool for cardiac imaging."
                },
                "zh": {
                    "title": "è¯­ä¹‰å¼•å¯¼çš„æ‰©æ•£å»é›¾ç®—æ³•æå‡è¶…å£°å›¾åƒè´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰å¼•å¯¼çš„æ‰©æ•£å»é›¾ç®—æ³•ï¼Œæ—¨åœ¨æé«˜è¶…å£°å¿ƒåŠ¨å›¾çš„å›¾åƒè´¨é‡ã€‚è¯¥ç®—æ³•ç»“åˆäº†é€åƒç´ å™ªå£°æ¨¡å‹å’ŒåŸºäºå¹²å‡€è¶…å£°æ•°æ®è®­ç»ƒçš„ç”Ÿæˆå…ˆéªŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå»é™¤ç”±äºå¤šè·¯å¾„åå°„å¼•èµ·çš„é›¾éœ¾ã€‚é€šè¿‡å¯¹æŒ‘æˆ˜æ•°æ®é›†çš„å®šé‡è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¯¹æ¯”åº¦å’Œä¿çœŸåº¦æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ­¤ç®—æ³•çš„ä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17061",
            "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage\n  Generative Network Framework",
            "url": "https://huggingface.co/papers/2508.17061",
            "abstract": "A dual-stage generative network framework enhances photorealism in real-time video game rendering by improving inference speed and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired image-to-image translation task. This enables training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN.",
            "score": 0,
            "issue_id": 5551,
            "pub_date": "2025-08-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 23",
                "zh": "8æœˆ23æ—¥"
            },
            "hash": "be26e5f42a32f781",
            "authors": [
                "Stefanos Pasios",
                "Nikos Nikolaidis"
            ],
            "affiliations": [
                "Aristotle University of Thessaloniki, Informatics, 54124, Thessaloniki, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17061.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#cv",
                    "#games",
                    "#video",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ÑĞ²ĞµÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (GAN). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº REGEN, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ³Ñ€Ğµ Grand Theft Auto V Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 32-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Real-Time Game Rendering with REGEN",
                    "desc": "This paper introduces a dual-stage generative network framework called REGEN, designed to enhance photorealism in real-time video game rendering. By utilizing generative adversarial networks (GANs), the framework improves both the visual quality and inference speed of rendered game frames. The approach simplifies the task of image translation by transforming it into a paired image-to-image translation problem, allowing for efficient training. The results demonstrate a significant performance boost, achieving a 32.14 times faster inference speed while maintaining high visual fidelity, particularly in the context of Grand Theft Auto V."
                },
                "zh": {
                    "title": "æå‡æ¸¸æˆæ¸²æŸ“çœŸå®æ„Ÿçš„åŒé˜¶æ®µç”Ÿæˆç½‘ç»œ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé˜¶æ®µç”Ÿæˆç½‘ç»œæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å®æ—¶è§†é¢‘æ¸¸æˆæ¸²æŸ“ä¸­çš„ç…§ç‰‡çœŸå®æ„Ÿã€‚å°½ç®¡ç°ä»£ç¡¬ä»¶å’Œæ¸²æŸ“æŠ€æœ¯å·²ç»æ˜¾è‘—æå‡äº†æ¸¸æˆçš„è§†è§‰çœŸå®æ„Ÿï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°çœŸæ­£çš„ç…§ç‰‡çœŸå®æ„Ÿä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œé€šè¿‡æ— é…å¯¹å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ¨¡å‹ï¼Œç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„ç…§ç‰‡çœŸå®æ„Ÿå¸§ï¼Œä»è€Œç®€åŒ–äº†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ã€Šä¾ ç›—çŒè½¦æ‰‹Vã€‹ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†32.14å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16838",
            "title": "If We May De-Presuppose: Robustly Verifying Claims through\n  Presupposition-Free Question Decomposition",
            "url": "https://huggingface.co/papers/2508.16838",
            "abstract": "A structured claim verification framework reduces prompt sensitivity and presupposition issues in large language models, improving performance by 2-5%.  \t\t\t\t\tAI-generated summary \t\t\t\t Prior work has shown that presupposition in generated questions can introduce unverified assumptions, leading to inconsistencies in claim verification. Additionally, prompt sensitivity remains a significant challenge for large language models (LLMs), resulting in performance variance as high as 3-6%. While recent advancements have reduced this gap, our study demonstrates that prompt sensitivity remains a persistent issue. To address this, we propose a structured and robust claim verification framework that reasons through presupposition-free, decomposed questions. Extensive experiments across multiple prompts, datasets, and LLMs reveal that even state-of-the-art models remain susceptible to prompt variance and presupposition. Our method consistently mitigates these issues, achieving up to a 2-5% improvement.",
            "score": 0,
            "issue_id": 5555,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "8a5151f170f92247",
            "authors": [
                "Shubhashis Roy Dipta",
                "Francis Ferraro"
            ],
            "affiliations": [
                "Department of Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, MD 21250, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16838.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#multimodal",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµÑÑƒĞ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµÑÑƒĞ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµÑÑƒĞ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2-5%."
                },
                "en": {
                    "title": "Enhancing Claim Verification by Reducing Prompt Sensitivity",
                    "desc": "This paper presents a new framework for verifying claims that minimizes the impact of prompt sensitivity and presupposition in large language models (LLMs). Previous research highlighted that unverified assumptions in generated questions can lead to inconsistencies, while prompt sensitivity causes performance fluctuations. The proposed framework uses structured, presupposition-free questions to enhance reasoning and improve verification accuracy. Experimental results show that this approach can lead to a performance boost of 2-5% across various models and datasets, addressing persistent challenges in the field."
                },
                "zh": {
                    "title": "ç»“æ„åŒ–å£°æ˜éªŒè¯æ¡†æ¶æå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„å£°æ˜éªŒè¯æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æç¤ºæ•æ„Ÿæ€§å’Œå‰æå‡è®¾é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆé—®é¢˜ä¸­çš„å‰æå‡è®¾ä¼šå¼•å…¥æœªç»éªŒè¯çš„å‡è®¾ï¼Œä»è€Œå¯¼è‡´å£°æ˜éªŒè¯ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚å°½ç®¡è¿‘æœŸçš„è¿›å±•æœ‰æ‰€æ”¹å–„ï¼Œä½†æç¤ºæ•æ„Ÿæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹å¤šä¸ªæç¤ºã€æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæé«˜äº†2-5%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-25.html",
    "link_next": "2025-08-27.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 5,
        "#benchmark": 13,
        "#agents": 2,
        "#cv": 7,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 4,
        "#reasoning": 11,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 4,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}