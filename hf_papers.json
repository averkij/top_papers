{
    "date": {
        "ru": "22 Ğ¸ÑĞ»Ñ",
        "en": "July 22",
        "zh": "7æœˆ22æ—¥"
    },
    "time_utc": "2025-07-22 04:35",
    "weekday": 1,
    "issue_id": 4938,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.15061",
            "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
            "url": "https://huggingface.co/papers/2507.15061",
            "abstract": "A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.",
            "score": 18,
            "issue_id": 4936,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "16ab84cfe7ace89e",
            "authors": [
                "Zhengwei Tao",
                "Jialong Wu",
                "Wenbiao Yin",
                "Junkai Zhang",
                "Baixuan Li",
                "Haiyang Shen",
                "Kuan Li",
                "Liwen Zhang",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15061.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "WebShaper - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸ ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ—Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. WebShaper ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebShaper Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GAIA Ğ¸ WebWalkerQA."
                },
                "en": {
                    "title": "Enhancing LLM Agents with Structured Data Synthesis",
                    "desc": "WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks."
                },
                "zh": {
                    "title": "WebShaperï¼šæå‡ä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "WebShaperæ˜¯ä¸€ä¸ªåŸºäºå½¢å¼åŒ–é©±åŠ¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨é›†åˆè®ºå’ŒçŸ¥è¯†æŠ•å½±æŠ€æœ¯åˆæˆä¿¡æ¯æ£€ç´¢æ•°æ®é›†ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»ŸåŒ–çš„å½¢å¼åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿ä¿¡æ¯ç»“æ„ä¸æ¨ç†ç»“æ„çš„ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å¸¸è§çš„æ•°æ®ä¸ä¸€è‡´é—®é¢˜ã€‚WebShaperçš„æ ¸å¿ƒæ˜¯çŸ¥è¯†æŠ•å½±ï¼ˆKPï¼‰æ¦‚å¿µï¼Œé€šè¿‡KPæ“ä½œç»„åˆå®ç°å¯¹æ¨ç†ç»“æ„çš„ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebShaperåœ¨GAIAå’ŒWebWalkerQAåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°äº†å¼€æºä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14683",
            "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
            "url": "https://huggingface.co/papers/2507.14683",
            "abstract": "The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.",
            "score": 15,
            "issue_id": 4937,
            "pub_date": "2025-07-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ»Ñ",
                "en": "July 19",
                "zh": "7æœˆ19æ—¥"
            },
            "hash": "47799c3d5002f685",
            "authors": [
                "Xingxuan Li",
                "Yao Xiao",
                "Dianwen Ng",
                "Hai Ye",
                "Yue Deng",
                "Xiang Lin",
                "Bin Wang",
                "Zhanfeng Mo",
                "Chong Zhang",
                "Yueyi Zhang",
                "Zonglin Yang",
                "Ruilin Li",
                "Lei Lei",
                "Shihao Xu",
                "Han Zhao",
                "Weiling Chen",
                "Feng Ji",
                "Lidong Bing"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.14683.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "MiroMind-M1 - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 719 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 62 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Context-Aware Multi-Stage Policy Optimization Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ÑĞµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Open-Source Models for Superior Mathematical Reasoning",
                    "desc": "The MiroMind-M1 series introduces open-source reasoning language models that excel in mathematical reasoning tasks through a two-stage training approach. The first stage involves supervised fine-tuning (SFT) on a large dataset of math problems, while the second stage employs reinforcement learning with verified responses (RLVR) to refine the model's reasoning capabilities. To improve training efficiency, the authors propose a novel Context-Aware Multi-Stage Policy Optimization algorithm that adapts training based on context and problem complexity. By providing complete access to models, datasets, and training configurations, this work aims to enhance transparency and reproducibility in the development of reasoning language models."
                },
                "zh": {
                    "title": "å¼€æºæ¨ç†æ¨¡å‹çš„é€æ˜æ€§ä¸å…ˆè¿›æ€§",
                    "desc": "MiroMind-M1ç³»åˆ—æ˜¯ä¸€ä¸ªå¼€æºæ¨ç†è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šé˜¶æ®µç­–ç•¥ä¼˜åŒ–ï¼Œåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚è¿™äº›æ¨¡å‹é¦–å…ˆåœ¨ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„719Kæ•°å­¦æ¨ç†é—®é¢˜ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶ååœ¨62Kå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ éªŒè¯ã€‚ä¸ºäº†æé«˜å¼ºåŒ–å­¦ä¹ éªŒè¯è¿‡ç¨‹çš„é²æ£’æ€§å’Œæ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç»“åˆäº†é•¿åº¦æ¸è¿›è®­ç»ƒå’Œè‡ªé€‚åº”é‡å¤æƒ©ç½šã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å‘å¸ƒå®Œæ•´çš„æ¨¡å‹ã€æ•°æ®é›†å’Œè®­ç»ƒé…ç½®ï¼Œä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§å’Œç¤¾åŒºçš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15846",
            "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
            "url": "https://huggingface.co/papers/2507.15846",
            "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.",
            "score": 14,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "d36bacfa3f66add9",
            "authors": [
                "Fei Tang",
                "Zhangxuan Gu",
                "Zhengxi Lu",
                "Xuyang Liu",
                "Shuheng Shen",
                "Changhua Meng",
                "Wen Wang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15846.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ GUI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ (GUI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ GUI Gaussian Grounding Rewards (GUI-G^2), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-G^2 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 24.7% Ğ½Ğ° ScreenSpot-Pro."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with Continuous Gaussian Rewards",
                    "desc": "This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks."
                },
                "zh": {
                    "title": "é«˜æ–¯å¥–åŠ±æ¡†æ¶æå‡GUIäº¤äº’ç²¾åº¦",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¡†æ¶ï¼Œç§°ä¸ºGUI Gaussian Grounding Rewardsï¼ˆGUI-G^2ï¼‰ï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç²¾ç¡®ä½ç½®ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒå¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒGUI-G^2é€šè¿‡å°†GUIå…ƒç´ å»ºæ¨¡ä¸ºè¿ç»­çš„é«˜æ–¯åˆ†å¸ƒï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„æ¢¯åº¦ä¿¡å·ï¼Œä¿ƒè¿›äº†æ¨¡å‹çš„ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é«˜æ–¯ç‚¹å¥–åŠ±å’Œè¦†ç›–å¥–åŠ±ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ä¸åŒå…ƒç´ çš„å°ºåº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹åœ¨ç•Œé¢å˜åŒ–ä¸­çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUI-G^2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­çš„æ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15493",
            "title": "GR-3 Technical Report",
            "url": "https://huggingface.co/papers/2507.15493",
            "abstract": "A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.",
            "score": 13,
            "issue_id": 4938,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "5e91567240893b65",
            "authors": [
                "Chilam Cheang",
                "Sijin Chen",
                "Zhongren Cui",
                "Yingdong Hu",
                "Liqun Huang",
                "Tao Kong",
                "Hang Li",
                "Yifeng Li",
                "Yuxiao Liu",
                "Xiao Ma",
                "Hao Niu",
                "Wenxuan Ou",
                "Wanli Peng",
                "Zeyu Ren",
                "Haixin Shi",
                "Jiawen Tian",
                "Hongtao Wu",
                "Xin Xiao",
                "Yuyang Xiao",
                "Jiafeng Xu",
                "Yichu Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.15493.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#agents",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "GR-3: Ğ¨Ğ°Ğ³ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GR-3 - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸. GR-3 Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "GR-3: A Leap Towards Generalist Robots for Everyday Tasks",
                    "desc": "The paper presents GR-3, a large-scale vision-language-action model that excels in generalizing across various robotic tasks. It can adapt quickly to new environments and instructions with minimal human input, making it efficient for fine-tuning. GR-3 is particularly effective in performing complex tasks that require dexterity and coordination, such as bi-manual manipulation. The model's training combines web-scale data and imitation learning, leading to superior performance compared to existing methods."
                },
                "zh": {
                    "title": "GR-3ï¼šé€šç”¨æœºå™¨äººæ”¿ç­–çš„æœªæ¥",
                    "desc": "GR-3æ˜¯ä¸€ä¸ªå¤§å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”æ–°ç‰©ä½“ã€æ–°ç¯å¢ƒå’ŒæŠ½è±¡æ¦‚å¿µçš„æŒ‡ä»¤ã€‚è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡å°‘é‡çš„äººç±»è½¨è¿¹æ•°æ®è¿›è¡Œé«˜æ•ˆçš„å¾®è°ƒï¼Œå¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒã€‚é€šè¿‡ä¸ç½‘ç»œè§„æ¨¡çš„è§†è§‰-è¯­è¨€æ•°æ®å…±åŒè®­ç»ƒï¼ŒGR-3åœ¨é•¿æ—¶é—´å’Œçµå·§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¾…åŠ©äººç±»çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15778",
            "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
            "url": "https://huggingface.co/papers/2507.15778",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.",
            "score": 7,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "8e0f7bdfedf50691",
            "authors": [
                "Jiakang Wang",
                "Runze Liu",
                "Fuzheng Zhang",
                "Xiu Li",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15778.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Archer. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Archer Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±ÑƒÑ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Archer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLVR Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Archer: Smart Token Training for Better Reasoning in LLMs",
                    "desc": "This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„åŒé‡ä»¤ç‰Œå¼ºåŒ–å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºArcherï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚Archeré€šè¿‡åŒé‡ä»¤ç‰Œçº¦æŸå’ŒåŒæ­¥æ›´æ–°ï¼Œåˆ†åˆ«å¯¹çŸ¥è¯†ç›¸å…³çš„ä½ç†µä»¤ç‰Œå’Œæ¨ç†ç›¸å…³çš„é«˜ç†µä»¤ç‰Œæ–½åŠ ä¸åŒçš„è®­ç»ƒä¿¡å·ã€‚ä¸ä»¥å¾€çš„ç®—æ³•ä¸åŒï¼ŒArcheråœ¨æ¨ç†ä»¤ç‰Œä¸Šä½¿ç”¨è¾ƒå¼±çš„KLæ­£åˆ™åŒ–ï¼Œä»¥é¼“åŠ±æ¢ç´¢ï¼ŒåŒæ—¶å¯¹çŸ¥è¯†ä»¤ç‰Œæ–½åŠ æ›´å¼ºçš„çº¦æŸï¼Œä»¥ä¿æŒäº‹å®çŸ¥è¯†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArcheråœ¨å¤šä¸ªæ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„RLVRæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15028",
            "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding",
            "url": "https://huggingface.co/papers/2507.15028",
            "abstract": "Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.",
            "score": 6,
            "issue_id": 4938,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "7f71d09a9b276de8",
            "authors": [
                "Yuanhan Zhang",
                "Yunice Chew",
                "Yuhao Dong",
                "Aria Leo",
                "Bo Hu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Independent Researcher",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15028.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#security",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM: Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Video Thinking Test (Video-TT) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Video-TT ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1000 ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… YouTube-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ². Ğ¢ĞµÑÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Video-TT Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Video LLMs with the Video Thinking Test",
                    "desc": "This paper discusses the importance of correctness and robustness in video understanding, which are essential for mimicking human intelligence. It highlights that current benchmarks do not adequately measure how well video large language models (LLMs) interpret videos compared to humans. To address this, the authors introduce the Video Thinking Test (Video-TT), designed to evaluate the performance of video LLMs on real-world videos. The test includes 1,000 YouTube Shorts videos with questions that challenge the models' understanding of complex visual narratives, revealing a significant performance gap between the models and human interpreters."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼šäººç±»ä¸æ¨¡å‹çš„å·®è·",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘ç†è§£ä¸­çš„æ­£ç¡®æ€§å’Œé²æ£’æ€§é—®é¢˜ã€‚å°½ç®¡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆè§†é¢‘LLMsï¼‰å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†åæ˜ è¿™äº›æ¨¡å‹ä¸äººç±»æ™ºèƒ½åœ¨è§†é¢‘è§£é‡Šä¸­çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†è§†é¢‘æ€ç»´æµ‹è¯•ï¼ˆVideo-TTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘LLMsæ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·æœ‰æ•ˆåœ°ç†è§£ç°å®ä¸–ç•Œçš„è§†é¢‘ã€‚æµ‹è¯•åŒ…å«1000ä¸ªYouTube Shortsè§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘é…æœ‰ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜å’Œå››ä¸ªé’ˆå¯¹è§†è§‰å’Œå™äº‹å¤æ‚æ€§çš„å¯¹æŠ—æ€§é—®é¢˜ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºè§†é¢‘LLMsä¸äººç±»è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11539",
            "title": "Streaming 4D Visual Geometry Transformer",
            "url": "https://huggingface.co/papers/2507.11539",
            "abstract": "A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.",
            "score": 4,
            "issue_id": 4937,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "03e472d31e5edcaf",
            "authors": [
                "Dong Zhuo",
                "Wenzhao Zheng",
                "Jiahe Guo",
                "Yuqi Wu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11539.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 4D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ 4D-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Real-Time 4D Reconstruction with Streaming Transformers",
                    "desc": "This paper presents a streaming 4D visual geometry transformer that utilizes causal attention and knowledge distillation for real-time 4D reconstruction from video data. The model processes input sequences in an online manner, leveraging a causal transformer architecture to maintain high spatial consistency while integrating historical information. By employing temporal causal attention and caching past data, the system achieves efficient long-term reconstruction. The approach is validated through extensive experiments, showing improved inference speed and competitive performance, making it suitable for interactive 4D vision applications."
                },
                "zh": {
                    "title": "å®æ—¶4Dé‡å»ºçš„åˆ›æ–°å˜æ¢å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æµå¼4Dè§†è§‰å‡ ä½•å˜æ¢å™¨ï¼Œåˆ©ç”¨å› æœæ³¨æ„åŠ›å’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå®ç°å®æ—¶çš„4Dé‡å»ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨å› æœå˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨çº¿å¤„ç†è¾“å…¥åºåˆ—ï¼Œå¹¶é€šè¿‡ç¼“å­˜å†å²ä¿¡æ¯æ¥æé«˜é‡å»ºæ•ˆç‡ã€‚é€šè¿‡ä»å¯†é›†åŒå‘è§†è§‰å‡ ä½•å˜æ¢å™¨ä¸­è’¸é¦çŸ¥è¯†ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾—ä»¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜ç©ºé—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†åœ¨çº¿æ¨ç†é€Ÿåº¦ï¼Œé€‚ç”¨äºå¯æ‰©å±•çš„äº¤äº’å¼4Dè§†è§‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15375",
            "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models",
            "url": "https://huggingface.co/papers/2507.15375",
            "abstract": "Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.",
            "score": 2,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "2a7d1e1e1882f002",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15375.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Stitch: Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Stitch. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Stitch Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Stitch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 15% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ°ĞºÑƒÑ Ğ¶Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Stitch: Simultaneous Thinking and Talking for Enhanced Spoken Language Models",
                    "desc": "This paper introduces Stitch, a new method for Spoken Language Models (SLMs) that allows them to think internally while responding to speech. Unlike traditional SLMs that generate responses without prior reasoning, Stitch alternates between generating unspoken reasoning chunks and spoken responses. This approach minimizes latency by utilizing the time taken to play audio responses to continue generating reasoning. As a result, Stitch not only matches the response time of existing models but also improves performance on math reasoning tasks by 15%."
                },
                "zh": {
                    "title": "åŒæ­¥æ€è€ƒä¸è¡¨è¾¾çš„å£è¯­æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å£è¯­è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼Œåä¸ºStitchã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿ç”Ÿæˆæ— å£°æ¨ç†ç‰‡æ®µå’Œå£è¯­å“åº”ç‰‡æ®µï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å›åº”å‰ç¼ºä¹å†…åœ¨æ€è€ƒè¿‡ç¨‹çš„é—®é¢˜ã€‚Stitchåˆ©ç”¨å£è¯­å“åº”çš„éŸ³é¢‘æŒç»­æ—¶é—´ï¼Œå……åˆ†åˆ©ç”¨å‰©ä½™æ—¶é—´ç”Ÿæˆæ¨ç†å†…å®¹ï¼Œä»è€Œå®ç°æ€è€ƒä¸è¡¨è¾¾çš„åŒæ­¥è¿›è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStitchåœ¨æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šæ¯”åŸºçº¿æ¨¡å‹æé«˜äº†15%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨éæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°ä¹Ÿä¸åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15815",
            "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
            "url": "https://huggingface.co/papers/2507.15815",
            "abstract": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.",
            "score": 1,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "ad03ed3ae6e4256b",
            "authors": [
                "Seth Karten",
                "Wenzhe Li",
                "Zihan Ding",
                "Samuel Kleiner",
                "Yu Bai",
                "Chi Jin"
            ],
            "affiliations": [
                "Princeton University",
                "Salesforce Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15815.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#multimodal",
                    "#rl",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ĞºĞ°Ğº ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑÑ‚: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'LLM Economist', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹-Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ° Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºÑƒÑĞ¾Ñ‡Ğ½Ğ¾-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ²Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸ÑĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ ÑĞ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¡Ğ°ĞµĞ·Ğ°."
                },
                "en": {
                    "title": "Harnessing AI for Smarter Economic Policy Design",
                    "desc": "The LLM Economist is a new framework that combines agent-based modeling with large language models to evaluate economic policies in complex decision-making environments. It features two levels of agents: lower-level worker agents that optimize their labor supply based on learned utility functions, and an upper-level planner agent that uses reinforcement learning to create tax schedules. This approach allows for realistic simulations of diverse populations and effective mechanism design, all expressed in natural language. The framework shows promising results in improving social welfare through strategic interactions among agents, making it a valuable tool for testing economic policies."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–ç»æµæ”¿ç­–",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLLM Economistçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºä»£ç†çš„å»ºæ¨¡æ¥è®¾è®¡å’Œè¯„ä¼°å…·æœ‰å±‚çº§å†³ç­–çš„ç»æµæ”¿ç­–ã€‚åœ¨ä½å±‚æ¬¡ï¼Œæœ‰é™ç†æ€§çš„å·¥äººä»£ç†æ ¹æ®ç¾å›½äººå£æ™®æŸ¥çš„æ”¶å…¥å’Œäººå£ç»Ÿè®¡æ•°æ®é€‰æ‹©åŠ³åŠ¨ä¾›ç»™ï¼Œä»¥æœ€å¤§åŒ–åŸºäºæ–‡æœ¬çš„æ•ˆç”¨å‡½æ•°ã€‚åœ¨é«˜å±‚æ¬¡ï¼Œè§„åˆ’è€…ä»£ç†ä½¿ç”¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ æå‡ºä¸å½“å‰ç¾å›½è”é‚¦ç¨ç‡ç›¸ç»“åˆçš„åˆ†æ®µçº¿æ€§è¾¹é™…ç¨ç‡ã€‚è¿™ç§æ„å»ºä½¿ç»æµæ¨¡æ‹Ÿå…·å¤‡äº†ä¼˜åŒ–å¼‚è´¨æ•ˆç”¨ã€ç”Ÿæˆå¤§è§„æ¨¡äººå£å’Œæœºåˆ¶è®¾è®¡ç­‰ä¸‰ç§èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ä¸­è¿›è¡Œæœ‰æ•ˆçš„è´¢æ”¿å®éªŒã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-21.html",
    "link_next": "2025-07-23.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 4,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}