{
    "date": {
        "ru": "22 октября",
        "en": "October 22",
        "zh": "10月22日"
    },
    "time_utc": "2025-10-22 19:10",
    "weekday": 2,
    "issue_id": 6561,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.18866",
            "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
            "url": "https://huggingface.co/papers/2510.18866",
            "abstract": "LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.",
            "score": 78,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "cbff2ef26dc05177",
            "authors": [
                "Jizhan Fang",
                "Xinle Deng",
                "Haoming Xu",
                "Ziyan Jiang",
                "Yuqi Tang",
                "Ziwen Xu",
                "Shumin Deng",
                "Yunzhi Yao",
                "Mengru Wang",
                "Shuofei Qiao",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18866.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#data",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Человеческая память для AI: быстрее, точнее, эффективнее",
                    "desc": "LightMem — это система памяти для LLM, вдохновлённая моделью человеческой памяти Аткинсона-Шиффрина. Система организует память в три этапа: сенсорная память быстро фильтрует нерелевантную информацию, кратковременная память группирует данные по темам, а долговременная память консолидирует информацию в офлайн-режиме. Эксперименты показывают улучшение точности до 10.9% при одновременном сокращении использования токенов в 117 раз и времени выполнения более чем в 12 раз. LightMem решает проблему эффективного использования исторической информации из взаимодействий без существенных вычислительных затрат."
                },
                "en": {
                    "title": "LightMem: Enhancing LLMs with Efficient Memory Management",
                    "desc": "LightMem is a novel memory system designed to enhance Large Language Models (LLMs) by efficiently managing historical interaction data. It incorporates a three-stage memory architecture inspired by human memory, which includes sensory memory for filtering information, short-term memory for organizing topics, and long-term memory for offline updates. This approach allows LLMs to utilize past interactions more effectively, leading to improved accuracy and reduced computational costs. Experimental results demonstrate that LightMem significantly outperforms existing memory systems, achieving notable gains in accuracy while drastically lowering resource usage."
                },
                "zh": {
                    "title": "LightMem：高效的记忆系统提升LLMs性能",
                    "desc": "LightMem是一种受人类记忆启发的内存系统，旨在提高大型语言模型（LLMs）的性能。它通过有效管理历史交互信息，帮助模型在动态复杂的环境中更好地利用这些信息。LightMem将内存组织为三个互补的阶段：感知记忆、短期记忆和长期记忆，从而实现信息的快速过滤、组织和总结。实验结果表明，LightMem在准确性上优于强基线，同时显著减少了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18135",
            "title": "World-in-World: World Models in a Closed-Loop World",
            "url": "https://huggingface.co/papers/2510.18135",
            "abstract": "World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.",
            "score": 62,
            "issue_id": 6545,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "d41a575be6992504",
            "authors": [
                "Jiahan Zhang",
                "Muqing Jiang",
                "Nanru Dai",
                "Taiming Lu",
                "Arda Uzunoglu",
                "Shunchi Zhang",
                "Yana Wei",
                "Jiahao Wang",
                "Vishal M. Patel",
                "Paul Pu Liang",
                "Daniel Khashabi",
                "Cheng Peng",
                "Rama Chellappa",
                "Tianmin Shu",
                "Alan Yuille",
                "Yilun Du",
                "Jieneng Chen"
            ],
            "affiliations": [
                "Harvard",
                "JHU",
                "MIT",
                "PKU",
                "Princeton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18135.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Когда красивая картинка не помогает роботу: важна управляемость, а не визуальное качество",
                    "desc": "Исследователи создали платформу World-in-World для оценки генеративных world models в замкнутом цикле взаимодействия с окружением, где важнее успех в задаче, чем визуальное качество. Выяснилось, что высокое визуальное качество симуляций не гарантирует успешного выполнения задач агентами — ключевой фактор это управляемость модели. Дообучение на данных действий и наблюдений оказалось эффективнее, чем улучшение предобученных видео-генераторов. Также показано, что выделение большего количества вычислений на этапе инференса существенно повышает производительность world models в замкнутом цикле."
                },
                "en": {
                    "title": "Prioritizing Task Success Over Visual Quality in Generative World Models",
                    "desc": "The paper introduces World-in-World, a new platform for evaluating generative world models (WMs) in closed-loop environments, focusing on their effectiveness in helping agents complete tasks rather than just their visual quality. It highlights that traditional benchmarks often overlook the practical utility of WMs in real-world interactions, which this study aims to address. The research reveals that controllability is more crucial than visual fidelity for task success, and that scaling data post-training is more beneficial than simply enhancing visual generators. Additionally, it shows that increasing computational resources during inference can significantly boost the performance of WMs in these environments."
                },
                "zh": {
                    "title": "闭环环境中的生成世界模型评估",
                    "desc": "本文介绍了World-in-World，这是一个评估生成世界模型的开放平台，专注于在闭环环境中进行任务成功的评估，而非单纯的视觉质量。研究表明，视觉质量并不能保证任务的成功，控制能力更为重要。此外，使用行动-观察数据进行后期训练的效果优于升级预训练的视频生成器。最后，增加推理时间的计算资源可以显著提升生成世界模型在闭环环境中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18701",
            "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2510.18701",
            "abstract": "UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.",
            "score": 57,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "a88009f51e46a1e5",
            "authors": [
                "Yibin Wang",
                "Zhimin Li",
                "Yuhang Zang",
                "Jiazi Bu",
                "Yujie Zhou",
                "Yi Xin",
                "Junjun He",
                "Chunyu Wang",
                "Qinglin Lu",
                "Cheng Jin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Hunyuan, Tencent",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18701.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multilingual",
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Всесторонняя оценка генерации изображений по тексту",
                    "desc": "UniGenBench++ — это комплексный бенчмарк для оценки text-to-image генерации, который проверяет семантическую согласованность моделей в различных сценариях и на разных языках. Он включает 600 промптов, организованных иерархически по 5 основным темам и 20 подтемам, покрывая 10 первичных и 27 дополнительных критериев оценки. Для проверки робастности модели промпты представлены на английском и китайском языках в короткой и длинной форме. Для оценки используется мультимодальная LLM Gemini-2.5-Pro, а также обученная модель для офлайн-анализа результатов text-to-image моделей."
                },
                "en": {
                    "title": "UniGenBench++: Elevating Text-to-Image Evaluation with Semantic Precision",
                    "desc": "UniGenBench++ is a new benchmark designed to evaluate text-to-image (T2I) generation models by measuring how well the generated images match the meanings of their corresponding text prompts. It addresses previous benchmarks' shortcomings by including a wide variety of scenarios and supporting multiple languages, specifically English and Chinese. The benchmark features a hierarchical structure with 600 prompts that cover 5 main themes and 20 subthemes, allowing for detailed assessments across 10 primary and 27 sub-evaluation criteria. Additionally, it utilizes a Multi-modal Large Language Model to create a reliable evaluation pipeline, enabling thorough analysis of both open- and closed-source T2I models."
                },
                "zh": {
                    "title": "UniGenBench++：文本到图像生成的全面评估基准",
                    "desc": "UniGenBench++ 是一个全面的文本到图像生成基准，旨在评估生成图像与文本提示之间的语义一致性。该基准采用分层提示结构，涵盖多种场景和语言，解决了现有基准缺乏多样性和细粒度评估的问题。它包含600个提示，组织成5个主要主题和20个子主题，能够全面探测T2I模型在10个主要和27个子评估标准上的表现。通过使用多模态大型语言模型的能力，UniGenBench++ 提供了一个有效的评估管道，帮助社区更好地评估和比较不同的T2I模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16880",
            "title": "Chem-R: Learning to Reason as a Chemist",
            "url": "https://huggingface.co/papers/2510.16880",
            "abstract": "Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.",
            "score": 43,
            "issue_id": 6544,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "1cb1fbcf81423b67",
            "authors": [
                "Weida Wang",
                "Benteng Chen",
                "Di Zhang",
                "Wanhao Liu",
                "Shuchen Pu",
                "Ben Gao",
                "Jin Zeng",
                "Lei Bai",
                "Wanli Ouyang",
                "Xiaoyong Wei",
                "Tianshu Yu",
                "Tianfan Fu",
                "Shuzhou Sun",
                "Jiatong Li",
                "Zifu Wang",
                "Yuqiang Li",
                "Shufei Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Shanghai AI Lab",
                "The Chinese University of Hong Kong, Shenzhen",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16880.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "⚗️",
                "ru": {
                    "title": "Chem-R: LLM, которая рассуждает как химик",
                    "desc": "Chem-R - это модель для химического рассуждения, обученная в три этапа для решения задач в химии. Сначала модель получает базовые химические знания, затем учится рассуждать как эксперт-химик через дистилляцию структурированных reasoning траекторий, и наконец оптимизируется для работы с различными задачами через Multi-task Group Relative Policy Optimization. Модель превосходит ведущие LLM, включая Gemini-2.5-Pro и DeepSeek-R1, на 46% в молекулярных задачах и на 66% в задачах с химическими реакциями. Chem-R демонстрирует сильную генерализацию и интерпретируемость, открывая путь к новому поколению AI-систем для химических исследований."
                },
                "en": {
                    "title": "Chem-R: Revolutionizing Chemical Reasoning with Expert Insights",
                    "desc": "Chem-R is a novel Chemical Reasoning model that enhances chemical task performance by integrating essential chemical knowledge and expert reasoning. It employs a three-phase training approach: first, it builds a solid foundation of chemical knowledge; second, it distills expert reasoning protocols to improve problem-solving reliability; and third, it optimizes performance across various tasks through multi-task learning. This structured training allows Chem-R to outperform existing large language models and chemical foundation models significantly. The results demonstrate Chem-R's strong generalization capabilities and its potential to drive advancements in AI-assisted chemical discovery."
                },
                "zh": {
                    "title": "Chem-R：化学推理的新纪元",
                    "desc": "Chem-R是一种经过三阶段训练的化学推理模型，能够在化学任务中表现出色。它通过整合核心知识、专家推理和多任务优化，模拟化学家的思维过程。该模型的训练包括建立化学基础知识、引入结构化的专家推理轨迹以及优化多任务性能。Chem-R在多个基准测试中超越了现有的大型语言模型，展示了其强大的泛化能力和解释性，成为下一代AI驱动化学发现的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18692",
            "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
            "url": "https://huggingface.co/papers/2510.18692",
            "abstract": "Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.",
            "score": 30,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "c596d2b417fc08c8",
            "authors": [
                "Weinan Jia",
                "Yuning Lu",
                "Mengqi Huang",
                "Hualiang Wang",
                "Binyuan Huang",
                "Nan Chen",
                "Mu Liu",
                "Jidong Jiang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "FanqieAI, ByteDance China",
                "Hong Kong University of Science and Technology",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18692.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умное внимание для длинных видео",
                    "desc": "Статья представляет Mixture-of-Groups Attention (MoGA) — новый механизм внимания для эффективной генерации длинных видео с помощью Diffusion Transformers. Вместо квадратичного роста вычислений при увеличении длины последовательности, MoGA использует обучаемый роутер для точного сопоставления токенов без блочной оценки. Метод легко интегрируется с современными техниками вроде FlashAttention и позволяет генерировать минутные видео в разрешении 480p при 24 fps с контекстной длиной около 580 тысяч токенов. Эксперименты подтверждают эффективность подхода для различных задач генерации видео."
                },
                "en": {
                    "title": "Efficient Long Video Generation with MoGA",
                    "desc": "The paper presents Mixture-of-Groups Attention (MoGA), a novel approach to improve long video generation using Diffusion Transformers by solving the quadratic scaling problem of full attention. MoGA reduces redundancy in attention mechanisms by employing a learnable token router that accurately matches tokens, eliminating the need for blockwise estimation. This method allows for effective long-range interactions while maintaining efficiency, integrating well with existing attention frameworks. The results demonstrate that MoGA can generate high-quality, long videos at a significant scale, showcasing its potential in video generation tasks."
                },
                "zh": {
                    "title": "高效长视频生成的新方法",
                    "desc": "Mixture-of-Groups Attention（MoGA）是一种高效的稀疏注意力机制，旨在解决扩散变换器中全注意力的二次扩展问题，从而实现长视频生成。由于注意力机制存在高度冗余，输出主要由少量查询-键对主导，MoGA通过轻量级的可学习令牌路由器精确匹配令牌，避免了块状估计的限制。该方法通过语义感知路由实现了有效的长距离交互，并且作为无核方法，MoGA能够与现代注意力堆栈无缝集成。基于MoGA，我们开发了一种高效的长视频生成模型，能够以24帧每秒的速度生成分钟级、480p的多镜头视频，验证了该方法在各种视频生成任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18876",
            "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
            "url": "https://huggingface.co/papers/2510.18876",
            "abstract": "Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.",
            "score": 25,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "f12731f1d8090440",
            "authors": [
                "Haochen Wang",
                "Yuhao Wang",
                "Tao Zhang",
                "Yikang Zhou",
                "Yanwei Li",
                "Jiacong Wang",
                "Ye Tian",
                "Jiahao Meng",
                "Zilong Huang",
                "Guangcan Mai",
                "Anran Wang",
                "Yunhai Tong",
                "Zhuochen Wang",
                "Xiangtai Li",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "NLPR, MAIS, CASIA",
                "PKU",
                "UCAS",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18876.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Понимание любых регионов изображения с учётом глобального контекста",
                    "desc": "Статья представляет модель GAR (Grasp Any Region), которая улучшает понимание отдельных регионов на изображениях, интегрируя глобальный контекст и моделируя взаимодействия между объектами. В отличие от предыдущих подходов, которые анализировали регионы изолированно, GAR использует технику RoI-aligned feature replay для точного восприятия с учётом окружающего контекста. Модель способна не только описывать регионы, но и отвечать на свободные вопросы о них, демонстрируя продвинутое композиционное рассуждение. GAR-1B показывает state-of-the-art результаты в задачах генерации описаний и превосходит даже гораздо более крупные модели в понимании взаимосвязей между несколькими регионами, а также легко переносится на видео."
                },
                "en": {
                    "title": "Grasp Any Region: Enhancing Visual Understanding through Global Contexts and Interactions",
                    "desc": "The paper introduces Grasp Any Region (GAR), a model designed to improve region-level visual understanding by integrating global contexts and modeling interactions between different visual prompts. Unlike previous models that focused on isolated regions, GAR utilizes a RoI-aligned feature replay technique to enhance perception and reasoning capabilities. This allows GAR to answer complex questions about specific regions while considering their relationships with other regions. The model has shown superior performance in tasks like captioning and video reference, demonstrating its effectiveness in both single-region comprehension and multi-region interactions."
                },
                "zh": {
                    "title": "掌握任何区域，提升视觉理解！",
                    "desc": "Grasp Any Region (GAR) 是一种增强区域级视觉理解的方法，它通过整合全局上下文和建模交互来实现更高级的推理。GAR 采用有效的 RoI 对齐特征重放技术，支持精确感知和多提示之间的交互建模。通过这些功能，GAR 能够回答关于任何区域的具体自由形式问题，从被动描述转变为主动对话。实验结果表明，GAR 在图像描述和视频参考任务中均超越了现有模型，展现出强大的理解能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18726",
            "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
            "url": "https://huggingface.co/papers/2510.18726",
            "abstract": "A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.",
            "score": 23,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "02c47b36a2a8b616",
            "authors": [
                "Shihao Li",
                "Yuanxing Zhang",
                "Jiangtao Wu",
                "Zhide Lei",
                "Yiwen He",
                "Runzhe Wen",
                "Chenxi Liao",
                "Chengkang Jiang",
                "An Ping",
                "Shuo Gao",
                "Suhan Wang",
                "Zhaozhou Bian",
                "Zijun Zhou",
                "Jingyi Xie",
                "Jiayi Zhou",
                "Jing Wang",
                "Yifan Yao",
                "Weihao Xie",
                "Yingshui Tan",
                "Yanghai Wang",
                "Qianqian Xie",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Shanghai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18726.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Следование инструкциям важнее полноты описания видео",
                    "desc": "Исследователи представили новый бенчмарк IF-VidCap для оценки способности моделей генерировать описания видео согласно конкретным инструкциям пользователя. Бенчмарк содержит 1400 образцов и оценивает модели по двум критериям: корректность формата и корректность содержания. Тестирование более 20 моделей показало, что лучшие open-source решения почти сравнялись с проприетарными моделями. Модели, специализированные на детальном описании видео, уступают универсальным LLM при работе со сложными инструкциями."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Instruction-Following in Video Captioning",
                    "desc": "The paper introduces IF-VidCap, a new benchmark designed to evaluate video captioning models based on their ability to follow user instructions. Unlike existing benchmarks that focus mainly on descriptive accuracy, IF-VidCap assesses both format correctness and content correctness in captions. The study reveals that while proprietary models still lead in performance, top open-source models are rapidly closing the gap. Additionally, it highlights that models optimized for dense captioning struggle with complex instructions, suggesting a need for future advancements in both descriptive and instruction-following capabilities."
                },
                "zh": {
                    "title": "新基准IF-VidCap：提升视频字幕生成的指令遵循能力",
                    "desc": "本文介绍了一个新的基准测试IF-VidCap，用于评估视频字幕生成模型的指令遵循能力。现有的基准主要关注描述的全面性，而忽视了模型在遵循用户特定指令方面的表现。IF-VidCap包含1400个高质量样本，评估字幕的格式正确性和内容正确性两个维度。研究发现，尽管专有模型仍占主导地位，但顶尖的开源模型正在缩小与专有模型的性能差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18121",
            "title": "Efficient Long-context Language Model Training by Core Attention\n  Disaggregation",
            "url": "https://huggingface.co/papers/2510.18121",
            "abstract": "CAD, a technique for long-context large language model training, improves throughput and balance by decoupling and distributing core attention computations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.",
            "score": 22,
            "issue_id": 6559,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "19a6d500453c7e85",
            "authors": [
                "Yonghao Zhuang",
                "Junda Chen",
                "Bo Pang",
                "Yi Gu",
                "Yibo Zhu",
                "Yimin Jiang",
                "Ion Stoica",
                "Eric Xing",
                "Hao Zhang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "MBZUAI",
                "StepFun",
                "UC Berkeley",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18121.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Разделяй внимание и властвуй над длинным контекстом",
                    "desc": "Статья представляет технику CAD (core attention disaggregation), которая ускоряет обучение больших языковых моделей на длинных контекстах. Ключевая идея — вынести вычисление core attention (операцию softmax(QK^T)V) на отдельный пул устройств, так как при длинных контекстах квадратичный рост вычислений создаёт дисбаланс нагрузки. Система DistCA разбивает attention на задачи уровня токенов и динамически распределяет их между специальными серверами, полностью перекрывая коммуникацию вычислениями. На 512 GPU H200 с контекстами до 512 тысяч токенов достигнуто ускорение обучения в 1.35 раза и практически идеальный баланс нагрузки."
                },
                "en": {
                    "title": "Boosting Training Efficiency with Core Attention Disaggregation",
                    "desc": "The paper introduces Core Attention Disaggregation (CAD), a method designed to enhance the training of long-context large language models by separating core attention computations from other model components. This separation allows for more efficient use of resources, as core attention, which is stateless and has no trainable parameters, can be processed on dedicated devices without causing load imbalances. By partitioning core attention into token-level tasks and dynamically rebatching them, CAD ensures that computational tasks are balanced, leading to improved throughput. The implementation of CAD in the DistCA system demonstrates significant performance gains, achieving up to 1.35 times faster training while maintaining optimal compute and memory usage."
                },
                "zh": {
                    "title": "核心注意力分解：提升大语言模型训练效率的关键",
                    "desc": "CAD（核心注意力分解）是一种用于长上下文大语言模型训练的技术，通过将核心注意力计算与模型的其他部分解耦并分配到不同的设备上，从而提高了吞吐量和负载平衡。该技术利用核心注意力的无状态特性和可组合性，将其分解为基于令牌的任务，并将这些任务分派给专用的注意力服务器。通过动态重新批处理任务，CAD能够在不牺牲内核效率的情况下平衡计算负载。我们在名为DistCA的系统中实现了CAD，显著提高了训练的吞吐量，并消除了数据和管道并行中的延迟问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18855",
            "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
            "url": "https://huggingface.co/papers/2510.18855",
            "abstract": "Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.",
            "score": 19,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "8c4e98c6e6663cb4",
            "authors": [
                "Ling Team",
                "Anqi Shen",
                "Baihui Li",
                "Bin Hu",
                "Bin Jing",
                "Cai Chen",
                "Chao Huang",
                "Chao Zhang",
                "Chaokun Yang",
                "Cheng Lin",
                "Chengyao Wen",
                "Congqi Li",
                "Deng Zhao",
                "Dingbo Yuan",
                "Donghai You",
                "Fagui Mao",
                "Fanzhuang Meng",
                "Feng Xu",
                "Guojie Li",
                "Guowei Wang",
                "Hao Dai",
                "Haonan Zheng",
                "Hong Liu",
                "Jia Guo",
                "Jiaming Liu",
                "Jian Liu",
                "Jianhao Fu",
                "Jiannan Shi",
                "Jianwen Wang",
                "Jianxin Lai",
                "Jin Yang",
                "Jun Mei",
                "Jun Zhou",
                "Junbo Zhao",
                "Junping Zhao",
                "Kuan Xu",
                "Le Su",
                "Lei Chen",
                "Li Tang",
                "Liang Jiang",
                "Liangcheng Fu",
                "Lianhao Xu",
                "Linfeng Shi",
                "Lisha Liao",
                "Longfei Zheng",
                "Meng Li",
                "Mingchun Chen",
                "Qi Zuo",
                "Qiang Cheng",
                "Qianggang Cao",
                "Qitao Shi",
                "Quanrui Guo",
                "Senlin Zhu",
                "Shaofei Wang",
                "Shaomian Zheng",
                "Shuaicheng Li",
                "Shuwei Gu",
                "Siba Chen",
                "Tao Wu",
                "Tao Zhang",
                "Tianyu Zhang",
                "Tianyu Zhou",
                "Tiwei Bie",
                "Tongkai Yang",
                "Wang Hong",
                "Wang Ren",
                "Weihua Chen",
                "Wenbo Yu",
                "Wengang Zheng",
                "Xiangchun Wang",
                "Xiaodong Yan",
                "Xiaopei Wan",
                "Xin Zhao",
                "Xinyu Kong",
                "Xinyu Tang",
                "Xudong Han",
                "Xudong Wang",
                "Xuemin Yang",
                "Xueyu Hu",
                "Yalin Zhang",
                "Yan Sun",
                "Yicheng Shan",
                "Yilong Wang",
                "Yingying Xu",
                "Yongkang Liu",
                "Yongzhen Guo",
                "Yuanyuan Wang",
                "Yuchen Yan",
                "Yuefan Wang",
                "Yuhong Guo",
                "Zehuan Li",
                "Zhankai Xu",
                "Zhe Li",
                "Zhenduo Zhang",
                "Zhengke Gui",
                "Zhenxuan Pan",
                "Zhenyu Huang",
                "Zhenzhong Lan",
                "Zhiqiang Ding",
                "Zhiqiang Zhang",
                "Zhixun Li",
                "Zhizhen Liu",
                "Zihao Wang",
                "Zujie Wen"
            ],
            "affiliations": [
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18855.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Триллион параметров для всех: демократизация мощного AI-мышления",
                    "desc": "Ring-1T — это первая открытая thinking-модель с триллионом параметров, которая использует около 50 миллиардов параметров на токен. Для обучения такой масштабной модели разработаны три ключевые технологии: IcePop для стабилизации RL-обучения, C3PO++ для эффективной обработки длинных последовательностей и ASystem — высокопроизводительный фреймворк для reinforcement learning. Модель показывает выдающиеся результаты на сложных бенчмарках, включая математические олимпиады и программирование, достигая серебряной медали на IMO-2025. Полный релиз модели делает продвинутые reasoning-способности доступными для всего исследовательского сообщества."
                },
                "en": {
                    "title": "Democratizing Intelligence with a Trillion-Parameter Model",
                    "desc": "Ring-1T is a groundbreaking open-source thinking model that boasts a trillion parameters, making it one of the largest models available. It tackles significant training challenges through three innovative techniques: IcePop for stabilizing reinforcement learning, C3PO++ for optimizing resource use during long rollouts, and ASystem to eliminate bottlenecks in training. These advancements enable Ring-1T to achieve impressive benchmark scores, demonstrating its superior reasoning abilities. By making this model accessible to the research community, it aims to democratize advanced reasoning intelligence and set a new standard for open-source models."
                },
                "zh": {
                    "title": "一万亿参数模型，推动推理智能的民主化",
                    "desc": "Ring-1T是一个开源的思维模型，拥有一万亿个参数，旨在解决训练中的挑战。它通过三项创新技术IcePop、C3PO++和ASystem，克服了训练推理不一致、资源利用效率低下和系统瓶颈等问题。Ring-1T在多个基准测试中取得了优异的成绩，展示了其卓越的推理能力。通过向社区发布完整的一万亿参数模型，我们为研究人员提供了前沿推理能力的直接访问，推动了大规模推理智能的民主化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18849",
            "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.18849",
            "abstract": "A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.",
            "score": 18,
            "issue_id": 6545,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "d16733b11aed5d27",
            "authors": [
                "Chenghao Zhu",
                "Meiling Tao",
                "Tiannan Wang",
                "Dongyi Ding",
                "Yuchen Eleanor Jiang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "South China Agricultural University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18849.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "✍️",
                "ru": {
                    "title": "Критика и редактирование: новый путь к персонализации LLM",
                    "desc": "Статья представляет фреймворк Critique-Post-Edit для улучшения персонализации больших языковых моделей под индивидуальные предпочтения пользователей. Метод использует Generative Reward Model, которая даёт многомерные оценки и текстовые критические замечания вместо простых скалярных значений, что предотвращает reward hacking. Модель самостоятельно редактирует свои ответы на основе полученной критики, что делает обучение более целенаправленным и эффективным. Эксперименты показывают значительное превосходство над стандартным PPO, а персонализированная Qwen2.5-14B превосходит GPT-4.1."
                },
                "en": {
                    "title": "Revolutionizing Personalization in Language Models with Critique-Post-Edit",
                    "desc": "This paper introduces the Critique-Post-Edit framework, which enhances the personalization of large language models (LLMs) by using a multi-dimensional reward model and a self-revision mechanism. Traditional methods like supervised fine-tuning and reinforcement learning from human feedback often fail to capture the complexities of individual user preferences. The proposed framework includes a Personalized Generative Reward Model that provides detailed feedback to prevent reward hacking, and a Critique-Post-Edit mechanism that allows the model to improve its outputs based on this feedback. The results show significant improvements in personalization performance, outperforming standard methods and even surpassing the capabilities of existing models like GPT-4.1."
                },
                "zh": {
                    "title": "提升个性化的Critique-Post-Edit框架",
                    "desc": "本文提出了一种名为Critique-Post-Edit的框架，旨在增强大型语言模型的个性化能力。该框架结合了多维度奖励模型和自我修正机制，克服了传统方法在个性化方面的局限性。通过引入个性化生成奖励模型和批评后编辑机制，模型能够更有效地学习并生成符合用户偏好的响应。实验结果表明，该方法在个性化基准测试中显著优于标准的强化学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17699",
            "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver",
            "url": "https://huggingface.co/papers/2510.17699",
            "abstract": "The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
            "score": 18,
            "issue_id": 6555,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "e6632849b69d7e40",
            "authors": [
                "Aleksandr Oganov",
                "Ilya Bykov",
                "Eva Neudachina",
                "Mishan Aliev",
                "Alexander Tolmachev",
                "Alexander Sidorov",
                "Aleksandr Zuev",
                "Andrey Okhotin",
                "Denis Rakitin",
                "Aibek Alanov"
            ],
            "affiliations": [
                "HSE University, Russia",
                "Lomonosov Moscow State University, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17699.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Быстрое семплирование диффузионных моделей через adversarial обучение солвера",
                    "desc": "Статья представляет Generalized Adversarial Solver — метод для ускорения семплирования диффузионных моделей. Авторы предлагают простую параметризацию ODE-солвера, которая не требует сложных техник обучения и позволяет сократить количество вычислений с десятков до нескольких шагов. Ключевая идея заключается в комбинировании дистилляции с adversarial обучением, что помогает избежать артефактов и лучше сохранять детали изображений. Метод демонстрирует превосходство над существующими подходами при сопоставимых вычислительных ресурсах."
                },
                "en": {
                    "title": "Efficient and Detailed Sampling with Generalized Adversarial Solver",
                    "desc": "The Generalized Adversarial Solver enhances the efficiency and quality of sampling in diffusion models by integrating a straightforward ODE solver parameterization with adversarial training techniques. This approach addresses the high computational costs associated with traditional sampling methods by significantly reducing the number of function evaluations needed. Unlike previous methods that require complex training strategies, our method maintains fine-grained detail without additional training complications. The combination of distillation loss and adversarial training in the Generalized Adversarial Solver leads to improved performance and reduced artifacts in generated outputs."
                },
                "zh": {
                    "title": "提升扩散模型采样效率与质量的广义对抗求解器",
                    "desc": "本文提出了一种名为广义对抗求解器的方法，旨在提高扩散模型的采样效率和质量。该方法结合了简单的常微分方程（ODE）求解器参数化和对抗训练，避免了复杂的训练技巧。通过这种方式，广义对抗求解器在保持细节的同时，减少了计算成本。实验结果表明，该方法在资源限制下的表现优于现有的求解器训练方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18019",
            "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple\n  Back-Translation Solution",
            "url": "https://huggingface.co/papers/2510.18019",
            "abstract": "STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.",
            "score": 14,
            "issue_id": 6554,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "34ec82f1e218baec",
            "authors": [
                "Asim Mohamed",
                "Martin Gubri"
            ],
            "affiliations": [
                "African Institute for Mathematical Sciences",
                "Parameter Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18019.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#watermarking",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Водяные знаки для LLM работают на всех языках с обратным переводом",
                    "desc": "Существующие методы водяных знаков для LLM утверждают, что работают на разных языках, но на практике теряют надёжность при атаках с переводом на языки со средними и низкими ресурсами. Проблема кроется в семантической кластеризации, которая плохо работает когда в словаре токенизатора мало полных слов для конкретного языка. Метод STEAM использует обратный перевод для детектирования водяных знаков и восстанавливает их надёжность после перевода текста. Решение совместимо с любыми методами водяных знаков, улучшает качество детектирования на 17 языках и обеспечивает более справедливую защиту текстов независимо от языка."
                },
                "en": {
                    "title": "STEAM: Strengthening Multilingual Watermarking with Back-Translation",
                    "desc": "This paper introduces STEAM, a new method for improving the robustness of multilingual watermarking in AI-generated text. Current watermarking techniques struggle with medium- and low-resource languages, often failing due to issues with semantic clustering and limited tokenizer vocabularies. STEAM uses back-translation to enhance the detection of watermarks, ensuring they remain effective even after translation. The method shows significant performance improvements across 17 languages, making watermarking more reliable and fair in diverse linguistic contexts."
                },
                "zh": {
                    "title": "STEAM：提升多语言水印鲁棒性的创新方法",
                    "desc": "STEAM是一种基于回译的检测方法，旨在增强多语言水印的鲁棒性。现有的多语言水印方法在中低资源语言上表现不佳，主要是因为语义聚类失败。STEAM通过恢复翻译过程中丢失的水印强度，解决了这一问题。该方法与任何水印技术兼容，适用于不同的分词器和语言，且易于扩展到新语言。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17722",
            "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
            "url": "https://huggingface.co/papers/2510.17722",
            "abstract": "MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
            "score": 13,
            "issue_id": 6544,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "4f3ffcd4b03ee354",
            "authors": [
                "Yaning Pan",
                "Zekun Wang",
                "Qianqian Xie",
                "Yongqian Wen",
                "Yuanxing Zhang",
                "Guohui Zhang",
                "Haoxuan Hu",
                "Zhiyu Pan",
                "Yibing Huang",
                "Zhidong Gan",
                "Yonghong Lin",
                "An Ping",
                "Tianhao Peng",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Kuaishou Technology",
                "Nanjing University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17722.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Многоходовые диалоги: новый рубеж в понимании видео для AI",
                    "desc": "Статья представляет MT-Video-Bench — новый бенчмарк для оценки мультимодальных языковых моделей (MLLM) в многоходовых диалогах о видеоконтенте. Существующие методы оценки ограничиваются односложными вопросами и ответами, игнорируя сложность реальных диалоговых сценариев. Бенчмарк включает 987 тщательно составленных многоходовых диалогов из различных областей и оценивает шесть ключевых компетенций, фокусируясь на восприятии и интерактивности. Тестирование современных open-source и closed-source моделей выявило значительные различия в производительности и ограничения при работе с многоходовыми видео-диалогами."
                },
                "en": {
                    "title": "Evaluating AI's Dialogue Skills in Video Contexts",
                    "desc": "The paper introduces MT-Video-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in the context of multi-turn video dialogues. Unlike previous benchmarks that focused on single-turn interactions, MT-Video-Bench assesses the models' abilities to perceive and interact across complex dialogues. It includes 987 carefully curated dialogues from various domains, ensuring relevance to real-world applications like sports analysis and intelligent tutoring. The evaluation reveals notable performance differences among MLLMs, highlighting their strengths and weaknesses in handling multi-turn interactions in video content."
                },
                "zh": {
                    "title": "多轮视频对话的评估新标准",
                    "desc": "MT-Video-Bench 是一个评估多模态大型语言模型（MLLMs）在多轮视频对话中的能力的基准。它关注感知能力和互动性，涵盖了来自不同领域的987个精心策划的多轮对话。该基准旨在填补现有评估工具在多轮对话复杂性方面的空白，特别是在实际应用中，如互动体育分析和基于视频的智能辅导。通过对多种最先进的 MLLMs 进行评估，MT-Video-Bench 揭示了它们在处理多轮视频对话时的显著性能差异和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18250",
            "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning",
            "url": "https://huggingface.co/papers/2510.18250",
            "abstract": "ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.",
            "score": 11,
            "issue_id": 6546,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "b1590a8489248052",
            "authors": [
                "Xiaohan Qin",
                "Xiaoxing Wang",
                "Ning Liao",
                "Cancheng Zhang",
                "Xiangdong Zhang",
                "Mingquan Feng",
                "Jingzhi Wang",
                "Junchi Yan"
            ],
            "affiliations": [
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18250.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умный выбор токенов: самомодуляция и семантика для эффективного fine-tuning LLM",
                    "desc": "Статья представляет ssToken - новый подход к выбору токенов для supervised fine-tuning больших языковых моделей. Метод использует историю обучения самой модели вместо дополнительной reference модели, вычисляя разницу loss между текущей и предыдущей версией модели. Помимо loss-based метрики, авторы предлагают semantic-aware метрику на основе механизма attention, которая помогает сохранять семантически важные токены. Эксперименты показывают, что комбинация обоих подходов превосходит существующие методы token-level селекции и обычный fine-tuning при сохранении эффективности обучения."
                },
                "en": {
                    "title": "Enhancing Token Selection with ssToken for Better Language Model Fine-Tuning",
                    "desc": "The paper introduces ssToken, a novel approach for selecting tokens during the supervised fine-tuning of large language models. It addresses limitations of existing methods by using a self-modulated signal derived from the model's own history, allowing for adaptive token selection without needing an additional reference model. Additionally, ssToken incorporates a semantic-aware metric that evaluates token importance based on their meaning, rather than just loss metrics. The results show that ssToken significantly improves performance and efficiency compared to traditional token selection methods."
                },
                "zh": {
                    "title": "自我调节与语义感知的令牌选择新方法",
                    "desc": "ssToken是一种自我调节和语义感知的令牌选择方法，旨在增强大型语言模型的监督微调。它通过自适应选择令牌并提供补充的语义信息，克服了现有方法的局限性。与传统方法不同，ssToken利用历史模型计算每个令牌的损失差异，作为自我调节信号，优化令牌选择过程。此外，ssToken引入了一种基于注意力的语义感知令牌重要性评估指标，进一步提高了选择的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18775",
            "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
            "url": "https://huggingface.co/papers/2510.18775",
            "abstract": "UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
            "score": 8,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "04f7bc0adba95d76",
            "authors": [
                "Teng Hu",
                "Jiangning Zhang",
                "Zihan Su",
                "Ran Yi"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18775.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Нативная генерация видео в 4K через разделение внимания",
                    "desc": "UltraGen - это новый фреймворк для генерации видео, который впервые позволяет эффективно создавать видео в высоком разрешении (1080P/2K/4K) напрямую, без промежуточных этапов. Основная проблема существующих diffusion transformer моделей - квадратичная вычислительная сложность attention механизма, которая делает генерацию высокого разрешения непрактичной. UltraGen решает эту проблему через иерархическую dual-branch архитектуру, разделяющую полное внимание на локальную ветвь для детальной проработки регионов и глобальную ветвь со сжатием для общей семантической согласованности. Эксперименты показывают, что UltraGen превосходит существующие методы и двухэтапные пайплайны с super-resolution по качеству генерации."
                },
                "en": {
                    "title": "UltraGen: High-Resolution Video Generation Made Efficient",
                    "desc": "UltraGen is a new framework for generating high-resolution videos efficiently. It uses a hierarchical dual-branch attention architecture that separates local and global attention, allowing for detailed regional content and overall coherence. This approach, combined with a spatially compressed global modeling strategy, reduces computational costs while maintaining high-quality outputs. As a result, UltraGen can produce videos at resolutions up to 4K, surpassing previous models in both quality and performance."
                },
                "zh": {
                    "title": "UltraGen：高效生成高分辨率视频的新框架",
                    "desc": "UltraGen是一种新的视频生成框架，能够高效地合成高分辨率视频。它采用了分层双分支注意力架构，结合了全局和局部注意力机制，以提高视频生成的质量和效率。通过空间压缩的全局建模策略，UltraGen能够有效学习全局依赖关系，同时降低计算成本。实验结果表明，UltraGen在生成1080P和4K分辨率视频方面表现优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17519",
            "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
            "url": "https://huggingface.co/papers/2510.17519",
            "abstract": "A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, large-scale generative models for visual content (e.g., images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in https://github.com/Shopee-MUG/MUG-V{our webpage}.",
            "score": 8,
            "issue_id": 6545,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "8d54b8afe2e3fec0",
            "authors": [
                "Yongshun Zhang",
                "Zhongyi Fan",
                "Yonghang Zhang",
                "Zhangzikang Li",
                "Weifeng Chen",
                "Zhongwei Feng",
                "Chaoyue Wang",
                "Peng Hou",
                "Anxiang Zeng"
            ],
            "affiliations": [
                "LLM Team, Shopee Pte. Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17519.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#data",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективное обучение огромных моделей для генерации видео",
                    "desc": "Исследователи представили комплексный фреймворк для обучения больших моделей генерации видео, оптимизирующий обработку данных, архитектуру, стратегию обучения и инфраструктуру. Их модель MUG-V 10B достигает state-of-the-art результатов и особенно хороша в генерации видео для e-commerce задач. Ключевая особенность — использование Megatron-Core для высокой эффективности обучения и почти линейного масштабирования на множестве узлов. Команда выложила в открытый доступ веса модели, код для обучения и inference pipeline, что делает их работу первым публичным релизом полноценного стека для обучения крупномасштабных video generation моделей."
                },
                "en": {
                    "title": "Optimizing Video Generation: MUG-V 10B Unleashed!",
                    "desc": "This paper presents a new training framework designed for large-scale video generation models, addressing the challenges of data processing, model architecture, training strategy, and infrastructure. The framework enhances efficiency and performance by optimizing data preprocessing, video compression, and training techniques, leading to the development of the MUG-V 10B model. This model not only matches state-of-the-art performance but also excels in specific tasks like e-commerce video generation, outperforming existing open-source models in human evaluations. Additionally, the authors have made their training code and model weights publicly available, promoting further research and development in the field."
                },
                "zh": {
                    "title": "优化大规模视频生成的训练框架",
                    "desc": "本文提出了一种针对大规模视频生成模型的训练框架，旨在优化数据处理、模型架构、训练策略和基础设施。通过这些优化，模型在数据预处理、视频压缩、参数缩放、基于课程的预训练和对齐后训练等各个阶段都实现了显著的效率提升和性能改进。最终生成的模型MUG-V 10B在视频生成任务中达到了最新的最先进水平，并在电商导向的视频生成任务中超越了领先的开源基线。更重要的是，研究团队开源了完整的训练代码和模型权重，推动了大规模视频生成技术的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18795",
            "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
            "url": "https://huggingface.co/papers/2510.18795",
            "abstract": "ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP",
            "score": 7,
            "issue_id": 6545,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "c294c8d308c352e1",
            "authors": [
                "Xiaoxing Hu",
                "Kaicheng Yang",
                "Ziyong Feng",
                "Qi Ming",
                "Zonghao Guo",
                "Xiang An",
                "Ziyong Feng",
                "Junchi Yan",
                "Xue Yang"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Beijing University of Technology",
                "DeepGlint",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18795.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#alignment",
                    "#training",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Постепенное выравнивание CLIP с LLM через curriculum learning",
                    "desc": "ProCLIP улучшает текстовые возможности модели CLIP, заменяя её текстовый энкодер на эмбеддер на основе LLM для обработки длинных текстов и многоязычности. Прямое выравнивание через контрастное обучение нарушает предобученные связи между изображениями и текстом в CLIP, поэтому авторы предлагают постепенный подход с curriculum learning. Сначала знания из оригинального текстового энкодера CLIP дистиллируются в LLM-эмбеддер, затем применяется контрастная настройка с регуляризацией через self-distillation. Метод использует дополнительные функции потерь для выравнивания семантики и структуры эмбеддингов, сохраняя при этом предобученные знания CLIP."
                },
                "en": {
                    "title": "Enhancing CLIP with ProCLIP: Aligning Vision and Language for Better Understanding",
                    "desc": "ProCLIP is a framework designed to improve the text processing abilities of CLIP by aligning its image encoder with a large language model (LLM) embedder. It addresses the limitations of CLIP's original text encoder, which struggles with long texts and multilingual inputs. By using curriculum learning and contrastive tuning, ProCLIP preserves the pretrained knowledge of CLIP while enhancing its semantic understanding. The framework employs techniques like self-distillation and specific loss functions to ensure effective alignment and prevent overfitting during training."
                },
                "zh": {
                    "title": "ProCLIP：提升 CLIP 的文本处理能力",
                    "desc": "ProCLIP 是一种增强 CLIP 文本处理能力的方法，通过课程学习和对比调优将其图像编码器与基于 LLM 的嵌入器对齐，同时保留 CLIP 的预训练知识。原始 CLIP 文本编码器的输入长度限制为 77 个标记，影响了其处理长文本和细粒度语义理解的能力。此外，CLIP 文本编码器不支持多语言输入，这限制了其在更广泛任务中的应用。ProCLIP 通过知识蒸馏和对比调优，逐步实现 CLIP 图像编码器与 LLM 嵌入器的有效对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18234",
            "title": "DeepSeek-OCR: Contexts Optical Compression",
            "url": "https://huggingface.co/papers/2510.18234",
            "abstract": "DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR.",
            "score": 7,
            "issue_id": 6555,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "e6398e83f14115e9",
            "authors": [
                "Haoran Wei",
                "Yaofeng Sun",
                "Yukun Li"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18234.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#open_source",
                    "#cv",
                    "#training",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Сжатие документов в 20 раз с сохранением качества распознавания",
                    "desc": "DeepSeek-OCR представляет новый подход к сжатию длинных контекстов через оптическое 2D-отображение. Система состоит из DeepEncoder для компрессии изображений и декодера DeepSeek3B-MoE-A570M. При степени сжатия менее 10x модель достигает точности OCR 97%, а при 20x сохраняет около 60% точности. На бенчмарке OmniDocBench система превосходит GOT-OCR2.0 и MinerU2.0, используя значительно меньше vision tokens и обрабатывая свыше 200 тысяч страниц в день на одной A100."
                },
                "en": {
                    "title": "Efficient Context Compression for High-Precision OCR",
                    "desc": "DeepSeek-OCR is a novel approach that utilizes optical 2D mapping to effectively compress long contexts in document processing. It features two main components: DeepEncoder, which optimizes the processing of high-resolution inputs while minimizing activations, and DeepSeek3B-MoE-A570M as the decoder. The model achieves impressive OCR precision, reaching 97% accuracy with a compression ratio of less than 10x, and maintains around 60% accuracy even at 20x compression. This method not only enhances OCR performance but also shows potential for applications in historical document analysis and large language model training."
                },
                "zh": {
                    "title": "DeepSeek-OCR：高效压缩与精准识别的结合",
                    "desc": "DeepSeek-OCR 是一种利用光学二维映射来压缩长文本上下文的技术，旨在提高光学字符识别（OCR）的精度。该模型由两个主要部分组成：DeepEncoder 作为核心引擎，能够在高分辨率输入下保持低激活，同时实现高压缩比，从而减少视觉标记的数量。实验表明，当文本标记数量不超过视觉标记的十倍时，模型的解码精度可达到 97%。此外，DeepSeek-OCR 在实际应用中表现出色，能够以较少的视觉标记生成大量训练数据，显示出其在文档处理中的实际价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18873",
            "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
            "url": "https://huggingface.co/papers/2510.18873",
            "abstract": "DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.",
            "score": 6,
            "issue_id": 6547,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "9706b668238005bf",
            "authors": [
                "Ziang Zhang",
                "Zehan Wang",
                "Guanghao Zhang",
                "Weilong Dai",
                "Yan Xia",
                "Ziang Yan",
                "Minjie Hong",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai AI Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18873.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Когда AI теряется в движении: тест на понимание динамического пространства",
                    "desc": "Статья представляет DSI-Bench — бенчмарк для оценки пространственного мышления AI-моделей в динамических сценариях. Датасет содержит около 1000 видео и более 1700 вопросов, охватывающих девять паттернов движения наблюдателя и объектов. Тестирование 14 vision-language моделей и экспертных систем выявило серьёзные ограничения: модели путают движение наблюдателя с движением объектов, демонстрируют семантические смещения и плохо понимают относительные пространственные отношения. Исследование показывает критические пробелы в понимании динамического 3D-пространства современными VLM и указывает направления для их развития."
                },
                "en": {
                    "title": "Evaluating Dynamic Spatial Reasoning in AI Models",
                    "desc": "The paper introduces DSI-Bench, a benchmark designed to assess the dynamic spatial reasoning abilities of vision-language models (VLMs) and visual expertise models. It highlights the challenges these models face in understanding complex scenarios where both observers and objects are in motion. The benchmark includes nearly 1,000 dynamic videos and over 1,700 annotated questions that focus on various motion patterns. The evaluation of 14 models reveals significant limitations, such as confusion between observer and object motion and difficulties in accurately interpreting relative relationships in dynamic environments."
                },
                "zh": {
                    "title": "动态空间智能的评估与挑战",
                    "desc": "本论文介绍了DSI-Bench，这是一个评估视觉语言模型和视觉专家模型在动态空间推理能力的基准。该基准包含近1000个动态视频和1700多个手动标注的问题，涵盖了观察者和物体的九种解耦运动模式。研究发现，现有模型在理解自我运动、物体运动和相对关系方面存在显著局限性，常常混淆观察者和物体的运动。DSI-Bench为未来动态空间智能模型的发展提供了重要的发现和见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14264",
            "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement\n  Learning Framework for Stock Trading",
            "url": "https://huggingface.co/papers/2510.14264",
            "abstract": "AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter",
            "score": 6,
            "issue_id": 6549,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 октября",
                "en": "October 16",
                "zh": "10月16日"
            },
            "hash": "6dd32561751df799",
            "authors": [
                "Zheye Deng",
                "Jiashu Wang"
            ],
            "affiliations": [
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14264.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#agents",
                    "#interpretability"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Один умный агент вместо хаоса: RL-трейдер с прозрачной логикой",
                    "desc": "AlphaQuanter — это фреймворк для автоматизированной торговли на основе единого AI-агента, который использует reinforcement learning для обучения динамической стратегии принятия решений. В отличие от многоагентных систем, которые страдают от неэффективности и противоречивых сигналов, AlphaQuanter автономно управляет инструментами и проактивно собирает необходимую информацию о рынке. Система обеспечивает прозрачный и проверяемый процесс принятия решений, достигая лучших результатов по ключевым финансовым метрикам. Интерпретируемые стратегии AlphaQuanter предоставляют ценные инсайты для человеческих трейдеров, демонстрируя сложные торговые паттерны."
                },
                "en": {
                    "title": "Revolutionizing Automated Trading with AlphaQuanter's Reinforcement Learning",
                    "desc": "AlphaQuanter is a single-agent framework that utilizes reinforcement learning to enhance automated trading strategies. It learns dynamic policies that allow it to adapt to changing market conditions while proactively gathering relevant information. Unlike multi-agent systems, AlphaQuanter provides a transparent decision-making process, enabling better end-to-end optimization and consistent trading signals. The framework's performance on financial metrics is state-of-the-art, and its interpretable strategies offer valuable insights for human traders."
                },
                "zh": {
                    "title": "AlphaQuanter：自动化交易的新纪元",
                    "desc": "AlphaQuanter是一个基于强化学习的单代理框架，专注于自动化交易。它通过学习动态策略和主动获取信息，达到了卓越的交易表现。与多代理框架相比，AlphaQuanter在效率和一致性上表现更佳，并且能够从市场反馈中学习出连贯的策略。实验结果表明，AlphaQuanter在关键金融指标上达到了最先进的性能，并为人类交易者提供了有价值的洞察。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18632",
            "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
            "url": "https://huggingface.co/papers/2510.18632",
            "abstract": "3DThinker is a framework that enhances multimodal reasoning by integrating 3D spatial understanding from images without requiring 3D prior input or labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.",
            "score": 5,
            "issue_id": 6557,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "cbfd7c79f6a76874",
            "authors": [
                "Zhangquan Chen",
                "Manyuan Zhang",
                "Xinlei Yu",
                "Xufang Luo",
                "Mingze Sun",
                "Zihao Pan",
                "Yan Feng",
                "Peng Pei",
                "Xunliang Cai",
                "Ruqi Huang"
            ],
            "affiliations": [
                "Meituan",
                "National University of Singapore",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18632.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Мышление в 3D: AI учится представлять пространство без явных трёхмерных данных",
                    "desc": "Статья представляет 3DThinker — фреймворк для улучшения мультимодального рассуждения путём интеграции трёхмерного пространственного понимания из изображений. Система не требует 3D-данных на входе или размеченных 3D-датасетов для обучения, а создаёт внутренние 3D-представления в процессе рассуждения подобно человеку. Обучение проходит в два этапа: сначала выравнивание 3D-латентов от VLM с латентами 3D foundation модели, затем оптимизация всей траектории рассуждения на основе финальных результатов. Эксперименты показывают превосходство над базовыми методами и открывают новый подход к объединению 3D-представлений в мультимодальное рассуждение."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Reasoning with 3DThinker",
                    "desc": "3DThinker is a novel framework designed to improve multimodal reasoning by incorporating 3D spatial understanding from images without needing prior 3D data or labeled examples. It addresses the challenge of interpreting 3D relationships from limited views, which traditional methods struggle with due to their reliance on 2D cues or text-based reasoning. The framework utilizes a two-stage training process, first aligning 3D latent representations from vision-language models with those from a 3D foundation model, and then refining the reasoning process based on outcome signals. Experimental results demonstrate that 3DThinker significantly outperforms existing methods, paving the way for better integration of 3D representations in multimodal tasks."
                },
                "zh": {
                    "title": "3DThinker：无标记数据的3D推理新框架",
                    "desc": "3DThinker是一个增强多模态推理的框架，它通过整合图像中的3D空间理解来实现，而无需3D先验输入或标记数据。该框架解决了从有限视角理解3D空间关系的挑战，克服了以往方法对文本或2D视觉线索的依赖。3DThinker通过两阶段训练，首先对齐推理过程中生成的3D潜在表示，然后基于结果信号优化推理轨迹，从而实现3D思维。实验结果表明，3DThinker在多个基准测试中表现优于强基线，为多模态推理中的3D表示统一提供了新视角。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17045",
            "title": "Video Reasoning without Training",
            "url": "https://huggingface.co/papers/2510.17045",
            "abstract": "The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning using Large Multimodal Models (LMMs) relies on costly reinforcement learning (RL) and verbose chain-of-thought, resulting in substantial computational overhead during both training and inference. Moreover, the mechanisms that control the thinking process in these reasoning models are very limited. In this paper, using entropy of the model's output as a signal, we discover that the high-quality models go through a series of micro-explorations and micro-exploitations which keep the reasoning process grounded (i.e., avoid excessive randomness while the model is exploring or thinking through an answer). We further observe that once this \"thinking\" process is over, more accurate models demonstrate a better convergence by reducing the entropy significantly via a final exploitation phase (i.e., a more certain convergence towards a solution trajectory). We then use these novel, theoretically-grounded insights to tune the model's behavior directly at inference, without using any RL or supervised fine-tuning. Specifically, during inference, our proposed approach called V-Reason (Video-Reason) adapts the value cache of the LMM via a few optimization steps on a small, trainable controller using an entropy-based objective, i.e., no supervision from any dataset or RL is necessary. This tuning improves the model's micro-exploration and exploitation behavior during inference. Our experiments show that our proposed method achieves significant improvements over the base instruction-tuned models across several video reasoning datasets, narrowing the gap with RL-trained models to within 0.6% average accuracy without any training, while offering massive efficiency benefits: output tokens are reduced by 58.6% compared to the RL model.",
            "score": 4,
            "issue_id": 6545,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "568871e71cf8c80e",
            "authors": [
                "Deepak Sridhar",
                "Kartikeya Bhardwaj",
                "Jeya Pradha Jeyaraj",
                "Nuno Vasconcelos",
                "Ankita Nayak",
                "Harris Teague"
            ],
            "affiliations": [
                "Qualcomm AI Research",
                "UCSD"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17045.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#training",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Управление рассуждениями через энтропию без обучения",
                    "desc": "Статья представляет V-Reason — метод улучшения рассуждений Large Multimodal Models над видео через оптимизацию на основе энтропии во время инференса. Авторы обнаружили, что качественные модели чередуют фазы микро-исследования и микро-эксплуатации, контролируя случайность через энтропию выходных данных. Предложенный подход настраивает value cache модели с помощью небольшого контроллера без reinforcement learning или supervised fine-tuning. Метод достигает результатов, близких к RL-моделям (разрыв 0.6%), при этом сокращая количество выходных токенов на 58.6%."
                },
                "en": {
                    "title": "Optimizing Video Reasoning with V-Reason: Efficiency Meets Accuracy",
                    "desc": "The paper introduces V-Reason, a novel method that enhances the performance of Large Multimodal Models (LMMs) in video reasoning tasks by optimizing their inference behavior using entropy-based techniques. Unlike traditional approaches that rely on reinforcement learning or extensive supervised fine-tuning, V-Reason directly tunes the model's output during inference, leading to improved accuracy and efficiency. The method leverages insights from the model's output entropy to balance exploration and exploitation, ensuring that the reasoning process remains grounded and converges effectively towards accurate solutions. Experimental results demonstrate that V-Reason significantly narrows the performance gap with RL-trained models while drastically reducing computational costs."
                },
                "zh": {
                    "title": "V-Reason：高效视频推理的新方法",
                    "desc": "本文提出了一种名为V-Reason的方法，通过基于熵的优化来调整大型多模态模型在推理过程中的行为，从而提高视频推理的准确性和效率，而无需使用强化学习或监督微调。研究发现，高质量模型在推理过程中会经历一系列微观探索和微观利用，保持推理过程的稳定性。我们进一步观察到，推理结束后，更准确的模型通过显著降低熵值来实现更好的收敛。V-Reason方法通过少量优化步骤直接在推理阶段调整模型行为，显著提高了模型的推理效率和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16505",
            "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
            "url": "https://huggingface.co/papers/2510.16505",
            "abstract": "PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants.",
            "score": 3,
            "issue_id": 6549,
            "pub_date": "2025-10-18",
            "pub_date_card": {
                "ru": "18 октября",
                "en": "October 18",
                "zh": "10月18日"
            },
            "hash": "98b154fc7678bf38",
            "authors": [
                "Lukas Selch",
                "Yufang Hou",
                "M. Jehanzeb Mirza",
                "Sivan Doveh",
                "James Glass",
                "Rogerio Feris",
                "Wei Lin"
            ],
            "affiliations": [
                "Interdisciplinary Transformation University Austria",
                "Johannes Kepler University Linz",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#science"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Проверка AI на понимание несоответствий в научных статьях",
                    "desc": "Исследователи создали бенчмарк PRISMM-Bench для оценки способности больших мультимодальных моделей находить и исправлять противоречия в научных статьях. Датасет содержит 262 реальных несоответствия из рецензий на научные работы, охватывающие текст, графики, таблицы и формулы. Бенчмарк включает три задачи: обнаружение противоречий, их исправление и сопоставление пар элементов. Тестирование 21 ведущей LMM показало очень низкую производительность (26-54%), что демонстрирует серьёзные проблемы современных моделей с мультимодальным научным рассуждением."
                },
                "en": {
                    "title": "Evaluating Multimodal Models for Scientific Consistency",
                    "desc": "PRISMM-Bench is a benchmark designed to evaluate how well large multimodal models (LMMs) can identify, correct, and reason about inconsistencies in scientific papers. It highlights the difficulties these models face when dealing with complex information presented in various formats, such as text, figures, and tables. The benchmark is based on real inconsistencies flagged by reviewers, making it more relevant than previous tests that used synthetic errors. The results show that current LMMs struggle significantly with these tasks, indicating a need for improvement in their ability to support scientific research effectively."
                },
                "zh": {
                    "title": "多模态科学推理的挑战与机遇",
                    "desc": "PRISMM-Bench 是一个评估大型多模态模型在科学论文中检测、纠正和推理不一致性的能力的基准。该研究揭示了多模态科学推理中的重大挑战，尤其是在文本、图形、表格和方程之间的细微不一致性。通过从真实的审稿人标记的不一致性中构建基准，PRISMM-Bench 设计了三个任务来评估模型的能力。实验结果显示，当前的多模态模型在处理这些不一致性时表现不佳，强调了在科学研究中建立可信赖助手的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18554",
            "title": "Extracting alignment data in open models",
            "url": "https://huggingface.co/papers/2510.18554",
            "abstract": "Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of 10times) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset.",
            "score": 2,
            "issue_id": 6549,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "01465adab5a272af",
            "authors": [
                "Federico Barbero",
                "Xiangming Gu",
                "Christopher A. Choquette-Choo",
                "Chawin Sitawarin",
                "Matthew Jagielski",
                "Itay Yona",
                "Petar Veličković",
                "Ilia Shumailov",
                "Jamie Hayes"
            ],
            "affiliations": [
                "AI Sequrity Company",
                "Anthropic",
                "Google DeepMind",
                "MentaLeap",
                "National University of Singapore",
                "OpenAI",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18554.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#rlhf",
                    "#long_context",
                    "#training",
                    "#alignment",
                    "#data"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Извлечение данных выравнивания через эмбеддинги: скрытые риски дистилляции",
                    "desc": "Исследователи показали, что из пост-обученных моделей можно извлечь значительные объёмы данных, использованных для alignment-тренировки (улучшения безопасности, следования инструкциям, математических способностей). Вместо традиционного сравнения строк они применили embedding модели, которые выявляют семантическое сходство и обнаруживают в 10 раз больше совпадений. Оказалось, что модели легко воспроизводят данные из этапов SFT и RL, и эти данные можно использовать для обучения базовой модели с восстановлением производительности. Работа выявляет риски утечки alignment-данных и показывает, что дистилляция моделей фактически означает косвенное обучение на оригинальном датасете учительской модели."
                },
                "en": {
                    "title": "Unlocking Hidden Data: Risks in Model Distillation Practices",
                    "desc": "This paper discusses how to extract alignment training data from models that have already been trained, using embedding models to find semantic similarities. The authors argue that traditional methods like string matching are less effective than embedding models for this purpose. They demonstrate that using embedding models can reveal much more data than previously thought, highlighting a significant risk in current distillation practices. The findings suggest that distillation may inadvertently lead to models reusing parts of their original training data, raising important questions about the implications of this process."
                },
                "zh": {
                    "title": "从后训练模型中提取对齐数据的潜在风险与机遇",
                    "desc": "本研究展示了如何从后训练模型中提取大量对齐训练数据，这些数据可以用于提升模型在长文本推理、安全性、遵循指令和数学等方面的能力。我们认为，嵌入模型比传统的字符串匹配方法更适合我们的目标，因为它能够更好地捕捉字符串之间的语义相似性。我们的调查表明，使用近似字符串匹配的方法会严重低估可提取数据的数量，可能低估了十倍以上。最后，我们的研究揭示了提取对齐数据的潜在风险，并引发了关于蒸馏实践下游影响的有趣讨论。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18489",
            "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos",
            "url": "https://huggingface.co/papers/2510.18489",
            "abstract": "A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
            "score": 2,
            "issue_id": 6548,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "d788eababe82cd47",
            "authors": [
                "Jinfeng Liu",
                "Lingtong Kong",
                "Mi Zhou",
                "Jinwen Chen",
                "Dan Xu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology (HKUST)",
                "vivo Mobile Communication Co., Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18489.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Реконструкция 4D HDR сцен из LDR видео без поз камеры",
                    "desc": "В статье представлена система Mono4DGS-HDR для реконструкции 4D HDR сцен из неупорядоченных LDR видео. Используется двухэтапная оптимизация на основе Gaussian Splatting, что позволяет обойтись без поз камеры и улучшить качество реконструкции. Первый этап обучает представление HDR видео в ортографической системе координат камеры, а второй этап преобразует его в мировое пространство. Также предложена стратегия временной регуляризации яркости для улучшения согласованности HDR изображения во времени."
                },
                "en": {
                    "title": "Revolutionizing HDR Scene Reconstruction from LDR Videos",
                    "desc": "This paper presents Mono4DGS-HDR, a novel system designed to reconstruct 4D high dynamic range (HDR) scenes from unposed low dynamic range (LDR) videos. The approach utilizes Gaussian Splatting within a two-stage optimization framework, allowing for effective HDR video reconstruction without requiring camera pose information. The first stage focuses on creating a Gaussian representation of the HDR video, while the second stage refines this representation in world space, incorporating camera poses. Additionally, a temporal luminance regularization technique is introduced to ensure consistent HDR appearance across frames, leading to superior rendering quality and speed compared to existing methods."
                },
                "zh": {
                    "title": "从LDR视频重建4D HDR场景的创新系统",
                    "desc": "我们介绍了Mono4DGS-HDR，这是第一个从未标定的单目低动态范围（LDR）视频中重建可渲染的4D高动态范围（HDR）场景的系统。该系统采用基于高斯点云的两阶段优化方法，第一阶段在正交相机坐标空间中学习视频HDR高斯表示，消除了对相机姿态的需求。第二阶段将视频高斯转换为世界空间，并联合优化世界高斯与相机姿态。此外，我们提出了一种时间亮度正则化策略，以增强HDR外观的时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18081",
            "title": "Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to\n  Any-Depth",
            "url": "https://huggingface.co/papers/2510.18081",
            "abstract": "Any-Depth Alignment (ADA) is an inference-time defense that enhances the safety of Large Language Models (LLMs) by reintroducing alignment tokens mid-stream, ensuring robust protection against adversarial attacks without altering the model's parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).",
            "score": 2,
            "issue_id": 6559,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "ac4035aee5616974",
            "authors": [
                "Jiawei Zhang",
                "Andrew Estornell",
                "David D. Baek",
                "Bo Li",
                "Xiaojun Xu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Massachusetts Institute of Technology",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18081.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#inference",
                    "#security"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Восстановление безопасности LLM на любой глубине генерации",
                    "desc": "Исследователи обнаружили, что LLM обладают «поверхностным выравниванием» (shallow alignment) — они отказываются отвечать на вредные запросы только в начале ответа, но теряют защиту после начала генерации вредного контента. Предложен метод Any-Depth Alignment (ADA), который работает во время инференса и повторно вставляет специальные токены-заголовки ассистента в середину генерации, заставляя модель переоценить безопасность запроса. Метод не требует изменения параметров модели и достигает почти 100% отказов на adversarial prefill атаках, снижая успешность популярных adversarial атак (GCG, AutoDAN, PAIR, TAP) до менее 3%. ADA сохраняет производительность на безопасных задачах с минимальным over-refusal и остаётся эффективным даже после дополнительного файнтюнинга базовой модели."
                },
                "en": {
                    "title": "Unlocking Safety at Any Depth with ADA",
                    "desc": "Any-Depth Alignment (ADA) is a novel defense mechanism designed to enhance the safety of Large Language Models (LLMs) during inference by reintroducing alignment tokens at various points in the text generation process. This approach addresses the issue of shallow alignment, where LLMs can initially refuse harmful queries but may fail to maintain that refusal as the conversation progresses. By leveraging the strong alignment properties of these tokens, ADA allows the model to reassess and reject harmful content at any stage of generation, significantly improving safety without modifying the model's parameters. The effectiveness of ADA is demonstrated across multiple model families, achieving high refusal rates against adversarial attacks while maintaining performance on benign tasks."
                },
                "zh": {
                    "title": "任何深度的对齐，确保安全生成",
                    "desc": "Any-Depth Alignment (ADA) 是一种推理时防御机制，旨在增强大型语言模型（LLMs）的安全性。它通过在生成过程中重新引入对齐标记，确保模型在面对对抗性攻击时能够保持强大的拒绝能力，而无需修改模型参数。研究表明，LLMs 在浅层拒绝训练中对齐标记的使用集中，这些标记具有强大的对齐先验。ADA 能够在生成的任意深度重新评估有害性，从而实现几乎 100% 的拒绝率，确保模型在面对复杂的对抗性攻击时依然安全。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15600",
            "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism",
            "url": "https://huggingface.co/papers/2510.15600",
            "abstract": "Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.",
            "score": 2,
            "issue_id": 6547,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "604d7aca35063798",
            "authors": [
                "Haoran Sun",
                "Yankai Jiang",
                "Zhenyu Tang",
                "Yaning Pan",
                "Shuang Gu",
                "Zekai Lin",
                "Lilong Wang",
                "Wenjie Lou",
                "Lei Liu",
                "Lei Bai",
                "Xiaosong Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15600.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#data",
                    "#science",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Научные протоколы от идеи до эксперимента",
                    "desc": "Исследователи представили Thoth — большую языковую модель для генерации воспроизводимых научных протоколов. Модель обучена по парадигме «Sketch-and-Fill» (набросок и заполнение), которая разделяет анализ, структурирование и формулировку на отдельные этапы. Для обучения создан датасет SciRecipe с 12 тысячами структурированных протоколов из 27 областей биологии. Thoth превосходит существующие LLM в точности последовательности шагов, логической упорядоченности и семантической корректности научных инструкций."
                },
                "en": {
                    "title": "Thoth: Revolutionizing Scientific Protocol Generation with Precision and Reliability",
                    "desc": "Thoth is a large language model designed to generate reliable scientific protocols using the Sketch-and-Fill paradigm and a structured reward mechanism. This model addresses the common issues of incomplete and inconsistent protocol generation found in existing models by utilizing a comprehensive dataset called SciRecipe, which includes over 12,000 structured protocols. The Sketch-and-Fill approach breaks down the protocol generation process into clear steps, ensuring that each part is explicit and verifiable. As a result, Thoth demonstrates superior performance in generating executable protocols, making it a valuable tool for enhancing reproducibility in scientific research."
                },
                "zh": {
                    "title": "Thoth：提升科学协议生成的可靠性与执行力",
                    "desc": "Thoth是一种大型语言模型，采用Sketch-and-Fill范式和结构化组件奖励机制，能够生成更可靠和可执行的科学协议。该模型通过自然语言查询自动生成精确、有序的协议，从而提高科学重现的效率。为了解决现有模型生成不完整或不一致协议的问题，研究者们引入了SciRecipe数据集，包含超过12000个结构化协议。Thoth在多个基准测试中表现优异，超越了现有的语言模型，推动了科学助手的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07581",
            "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
            "url": "https://huggingface.co/papers/2510.07581",
            "abstract": "Expanded Action space (ExpA) with ExpA Reinforcement Learning (EARL) enhances Large Language Models (LLMs) by decoupling environment interactions from language, improving performance in multi-turn interactions and contingent planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are powerful reasoners in natural language, but their actions are typically confined to outputting vocabulary tokens. As a result, interactions with external environments -- such as symbolic operators or simulators -- must be expressed through text in predefined formats, parsed, and routed to external interfaces. This overloads the model's language with both reasoning and control duties, and requires a hand-crafted parser, external to the LLM. To address this, we decouple environment interactions from language by internalizing them in an Expanded Action space (ExpA), beyond the vocabulary. The model starts reasoning in the default language environment, but may trigger routing actions and switch to an external environment at any time. From there, the model can only invoke environment-specific actions, receive feedback from the environment, and potentially route back to language as a result. To promote effective exploration of the expanded action space and new environments, we introduce ExpA Reinforcement Learning (EARL) with counterfactual policy optimization. On tasks requiring multi-turn interactions and contingent planning, EARL outperforms strong baselines with vocabulary-constrained actions. It performs robustly across calculator-based multi-task learning and, in the partially observed sorting problem, achieves perfect Sort-4 accuracy while self-discovering an efficient algorithm competitive with classical designs.",
            "score": 2,
            "issue_id": 6558,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 октября",
                "en": "October 8",
                "zh": "10月8日"
            },
            "hash": "8ebd04bed3af1659",
            "authors": [
                "Zhongqi Yue",
                "Weishi Wang",
                "Yundaichuan Zhan",
                "Juncheng Li",
                "Daniel Dahlmeier",
                "Fredrik D. Johansson"
            ],
            "affiliations": [
                "Chalmers University of Technology and University of Gothenburg",
                "SAP",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07581.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Расширенное пространство действий: когда LLM управляет не только словами",
                    "desc": "Исследователи предложили метод ExpA, который расширяет возможности больших языковых моделей за счёт отделения взаимодействия с внешними средами от языковой генерации. Вместо того чтобы заставлять модель выражать все действия через текст, система позволяет переключаться между языковым режимом и специальными действиями для конкретных сред. Для обучения используется метод EARL — reinforcement learning с контрфактуальной оптимизацией политик, который помогает эффективно исследовать новое пространство действий. На задачах многошагового взаимодействия и условного планирования метод превосходит базовые подходы, достигая идеальной точности в сортировке и самостоятельно обнаруживая эффективные алгоритмы."
                },
                "en": {
                    "title": "Decoupling Language and Action for Enhanced AI Interaction",
                    "desc": "This paper introduces Expanded Action space (ExpA) and ExpA Reinforcement Learning (EARL) to enhance Large Language Models (LLMs) by separating language processing from environment interactions. By internalizing environment actions within an expanded action space, the model can switch between reasoning in natural language and executing environment-specific tasks without relying on external parsers. This decoupling allows for more efficient exploration and interaction in complex tasks, particularly in multi-turn dialogues and contingent planning scenarios. The results show that EARL significantly improves performance on various tasks, achieving high accuracy and discovering efficient algorithms autonomously."
                },
                "zh": {
                    "title": "解耦语言与环境交互，提升模型表现",
                    "desc": "本文提出了一种扩展动作空间（ExpA）和扩展动作空间强化学习（EARL）的方法，以增强大型语言模型（LLMs）的能力。通过将环境交互与语言解耦，模型能够在多轮交互和应急规划任务中表现更好。ExpA允许模型在默认语言环境中推理，并在需要时切换到外部环境，执行特定的环境动作。EARL通过反事实策略优化，促进了对扩展动作空间的有效探索，显著提高了在多任务学习中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18087",
            "title": "Planned Diffusion",
            "url": "https://huggingface.co/papers/2510.18087",
            "abstract": "Planned diffusion combines autoregressive and diffusion models to achieve faster text generation with minimal quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produce high-quality text but generate tokens sequentially. Diffusion models can generate tokens in parallel but often need many iterations to match the same quality. We propose planned diffusion, a hybrid method that combines the strengths of both paradigms. Planned diffusion works in two stages: first, the model creates a short autoregressive plan that breaks the output into smaller, independent spans. Second, the model generates these spans simultaneously using diffusion. This approach expands the speed-quality Pareto frontier and provides a practical path to faster, high-quality text generation. On AlpacaEval, a suite of 805 instruction-following prompts, planned diffusion achieves Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x speedup over autoregressive generation with only 0.87\\% to 5.4\\% drop in win rate, respectively. Our sensitivity analysis shows that the planning mechanism of planned diffusion is minimal and reliable, and simple runtime knobs exist to provide flexible control of the quality-latency trade-off.",
            "score": 1,
            "issue_id": 6559,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "9a081414a12b50a8",
            "authors": [
                "Daniel Israel",
                "Tian Jin",
                "Ellie Cheng",
                "Guy Van den Broeck",
                "Aditya Grover",
                "Suvinay Subramanian",
                "Michael Carbin"
            ],
            "affiliations": [
                "Google",
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18087.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Быстрая генерация текста через планирование и параллельную диффузию",
                    "desc": "Статья представляет planned diffusion — гибридный метод генерации текста, объединяющий авторегрессионные модели и диффузионные модели. Сначала модель создаёт короткий авторегрессионный план, разбивающий выходной текст на независимые фрагменты, затем генерирует эти фрагменты параллельно с помощью диффузии. На бенчмарке AlpacaEval метод достигает ускорения в 1.27-1.81 раза по сравнению с обычной авторегрессией при падении качества всего на 0.87-5.4%. Подход обеспечивает оптимальный баланс между скоростью и качеством генерации текста в LLM."
                },
                "en": {
                    "title": "Planned Diffusion: Speed Meets Quality in Text Generation",
                    "desc": "Planned diffusion is a novel approach that merges autoregressive and diffusion models to enhance text generation speed while maintaining high quality. Autoregressive models excel in producing coherent text but do so sequentially, which slows down the process. In contrast, diffusion models can generate text in parallel but often require multiple iterations to achieve comparable quality. By first creating a short autoregressive plan and then generating text spans simultaneously, planned diffusion optimizes the balance between speed and quality, demonstrating significant improvements in generation efficiency with minimal quality loss."
                },
                "zh": {
                    "title": "计划扩散：快速高质量文本生成的新方法",
                    "desc": "计划扩散结合了自回归模型和扩散模型的优点，以实现更快的文本生成，同时保持较小的质量损失。自回归模型生成高质量文本，但需要逐个生成标记；而扩散模型可以并行生成标记，但通常需要多次迭代才能达到相同的质量。计划扩散采用两阶段的方法，首先生成一个短的自回归计划，将输出分解为较小的独立部分，然后使用扩散模型同时生成这些部分。该方法在速度和质量之间实现了最佳平衡，显著提高了文本生成的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15710",
            "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis",
            "url": "https://huggingface.co/papers/2510.15710",
            "abstract": "A unified multimodal medical model integrates image understanding and generation, enhancing performance across various medical vision-language tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot generate visual outputs, while medical image generation models synthesize images but cannot provide textual explanations. This leads to gaps in data representation, feature integration, and task-level multimodal capabilities. To this end, we propose a multi-level framework that draws inspiration from diagnostic workflows through the Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation level, we construct UniMed-5M, a dataset comprising over 5.6M samples that reformat diverse unimodal data into multimodal pairs for foundational observation. At the knowledge level, we propose Progressive Curriculum Learning that systematically introduces medical multimodal knowledge. At the analysis level, we introduce UniMedVL, the first medical unified multimodal model for the simultaneous analysis of image understanding and generation tasks within a single architecture. UniMedVL achieves superior performance on five medical image understanding benchmarks, while matching specialized models in generation quality across eight medical imaging modalities. Crucially, our unified architecture enables bidirectional knowledge sharing: generation tasks enhance visual understanding features, demonstrating that integrating traditionally separate capabilities within a single medical framework unlocks improvements across diverse medical vision-language tasks. Code is available at https://github.com/uni-medical/UniMedVL.",
            "score": 1,
            "issue_id": 6558,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "4092a61fb87f2079",
            "authors": [
                "Junzhi Ning",
                "Wei Li",
                "Cheng Tang",
                "Jiashi Lin",
                "Chenglong Ma",
                "Chaoyang Zhang",
                "Jiyao Liu",
                "Ying Chen",
                "Shujian Gao",
                "Lihao Liu",
                "Yuandong Pu",
                "Huihui Xu",
                "Chenhui Gou",
                "Ziyan Huang",
                "Yi Xin",
                "Qi Qin",
                "Zhongying Deng",
                "Diping Song",
                "Bin Fu",
                "Guang Yang",
                "Yuanfeng Ji",
                "Tianbin Li",
                "Yanzhou Su",
                "Jin Ye",
                "Shixiang Tang",
                "Ming Hu",
                "Junjun He"
            ],
            "affiliations": [
                "Fudan University",
                "Fuzhou University",
                "Imperial College London",
                "Monash University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Institute of Optics and Fine Mechanics",
                "Shanghai Jiao Tong University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15710.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#optimization",
                    "#science",
                    "#dataset",
                    "#benchmark",
                    "#healthcare"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Единая медицинская AI-модель: от понимания снимков до генерации диагностических изображений",
                    "desc": "Исследователи представили UniMedVL — первую унифицированную мультимодальную модель для медицины, которая одновременно понимает и генерирует медицинские изображения. Модель обучена на новом датасете UniMed-5M из 5.6 миллионов образцов и использует парадигму Observation-Knowledge-Analysis, вдохновлённую процессом медицинской диагностики. UniMedVL показывает превосходные результаты на пяти бенчмарках по пониманию изображений и сравнимое качество со специализированными моделями в генерации для восьми медицинских модальностей. Ключевое открытие: объединение задач понимания и генерации в единой архитектуре позволяет моделям обмениваться знаниями, улучшая качество работы в обоих направлениях."
                },
                "en": {
                    "title": "Integrating Image Understanding and Generation for Enhanced Medical Diagnostics",
                    "desc": "This paper presents a unified multimodal medical model that combines image understanding and generation to improve performance in medical vision-language tasks. The model, called UniMedVL, addresses the limitations of existing systems that either interpret images or generate visual outputs but not both. By utilizing a new dataset, UniMed-5M, and a Progressive Curriculum Learning approach, the model enhances the integration of multimodal data. The results show that UniMedVL outperforms traditional image understanding benchmarks and matches the quality of specialized generation models, demonstrating the benefits of a unified framework in medical diagnostics."
                },
                "zh": {
                    "title": "统一多模态医疗模型，提升医疗AI能力",
                    "desc": "这篇论文提出了一种统一的多模态医疗模型，旨在整合图像理解和生成，从而提升医疗视觉语言任务的表现。现有的医疗AI系统往往无法同时处理图像和文本，导致数据表示和特征整合的缺失。为了解决这个问题，研究者们构建了UniMed-5M数据集，并提出了渐进式课程学习和UniMedVL模型，以实现图像理解和生成的同时分析。该模型在多个医疗图像理解基准测试中表现优异，并在生成质量上与专业模型相匹配，展示了统一架构的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13982",
            "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2510.13982",
            "abstract": "Emerging architectures combining LLMs with multi-agent dynamics offer new possibilities for modeling complex, open-ended environments, but require addressing challenges like stability, diversity, and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.",
            "score": 1,
            "issue_id": 6559,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 октября",
                "en": "October 15",
                "zh": "10月15日"
            },
            "hash": "a0b73a0e30db62b7",
            "authors": [
                "Jinkun Chen",
                "Sher Badshah",
                "Xuemin Yu",
                "Sijia Han"
            ],
            "affiliations": [
                "Faculty of Computer Science, Dalhousie University, Halifax, Canada",
                "Meta, Vancouver, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13982.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#alignment",
                    "#games",
                    "#agi",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🌱",
                "ru": {
                    "title": "От статичных симуляций к живым AI-экосистемам",
                    "desc": "Статья исследует новые архитектуры, объединяющие LLM с мультиагентными системами для моделирования сложных, открытых сред. Авторы критикуют существующие симуляции за их статичность, предопределённые задачи и ограниченную динамику, которые не отражают реальную сложность социальных систем. В работе представлена новая таксономия подходов и выделены ключевые проблемы: стабильность, разнообразие поведения агентов и масштабируемость. Исследователи предлагают дорожную карту для создания адаптивных AI-экосистем с непрерывной коэволюцией агентов и социальной выравненностью."
                },
                "en": {
                    "title": "Evolving Multi-Agent Systems for Dynamic Real-World Simulations",
                    "desc": "This paper discusses the integration of large language models (LLMs) with multi-agent systems to create more dynamic and adaptable simulations of complex environments. It highlights the limitations of current models, which often rely on static tasks and rigid evaluation methods, failing to reflect the complexities of real-world interactions. The authors propose a new framework that emphasizes the need for stability, diversity, and scalability in these systems, while also introducing a taxonomy to better categorize the evolving landscape of multi-agent dynamics. They advocate for a shift towards open-ended simulations that allow for continuous adaptation and co-evolution among agents, aiming to foster resilient and socially aligned AI ecosystems."
                },
                "zh": {
                    "title": "超越静态范式，塑造自适应智能体模拟的未来",
                    "desc": "本论文探讨了将大型语言模型（LLM）与多智能体动态结合的新架构，这为建模复杂和开放的环境提供了新的可能性。当前的模拟大多局限于静态的沙盒环境，无法有效捕捉现实社会的复杂性。我们认为，静态的任务特定基准是不够的，必须重新思考。论文还提出了一个新的分类法，并制定了以开放性、持续共演和发展社会对齐的人工智能生态系统为中心的研究路线图。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17928",
            "title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable\n  Learning",
            "url": "https://huggingface.co/papers/2510.17928",
            "abstract": "An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.",
            "score": 0,
            "issue_id": 6548,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "cc690abb2b8c7630",
            "authors": [
                "He Du",
                "Bowen Li",
                "Aijun Yang",
                "Siyang He",
                "Qipeng Guo",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Fudan University",
                "Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17928.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#synthetic",
                    "#training",
                    "#reinforcement_learning",
                    "#agents",
                    "#data",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Эволюционный синтез проверяемых данных для обучения языковых моделей",
                    "desc": "Исследователи предложили эволюционный фреймворк для синтеза проверяемых данных, которые можно использовать для обучения языковых моделей. Система автоматически генерирует задачи, различные варианты решений и артефакты для их верификации, используя оценщик на основе согласованности проверок. Подход универсален и не требует специфичных для домена эвристик, что позволяет применять его для разных типов задач — от программирования до агентских систем. Эксперименты показали значительное улучшение результатов при обучении с подкреплением (RLVR) и дистилляции моделей на бенчмарках LiveCodeBench и AgentBench-OS."
                },
                "en": {
                    "title": "Revolutionizing Data Synthesis for Language Models",
                    "desc": "This paper presents a new framework for creating reliable synthetic data for training language models, which is crucial for improving their performance in various tasks. The proposed method uses an evolutionary approach to generate problems, solutions, and verification artifacts that can be checked for accuracy. By focusing on consistency between human evaluations and automated checks, the framework enhances the quality of training data without relying on specific task rules. Experiments show that this approach leads to better results in reinforcement learning and model distillation, demonstrating its effectiveness across different applications."
                },
                "zh": {
                    "title": "进化框架：提升语言模型的可验证性与泛化能力",
                    "desc": "这篇论文提出了一种进化框架，用于合成可验证的数据，以提升语言模型在强化学习和蒸馏过程中的表现。该框架能够从最小的种子监督中，联合合成问题、多样的候选解决方案和验证工件，并通过一致性评估器迭代发现策略。与现有方法不同，这种方法不依赖于特定任务的启发式规则，而是提供了一种通用的可验证性评估机制。实验结果表明，使用合成数据进行训练在多个任务上显著提高了模型的性能，展示了该框架的强大泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15862",
            "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
            "url": "https://huggingface.co/papers/2510.15862",
            "abstract": "PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.",
            "score": 0,
            "issue_id": 6554,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "68061c7c799ffe9d",
            "authors": [
                "Yi Wan",
                "Jiuqi Wang",
                "Liam Li",
                "Jinsong Liu",
                "Ruihao Zhu",
                "Zheqing Zhu"
            ],
            "affiliations": [
                "Pokee-AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15862.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Исследовательский агент на 7B параметров достигает топовых результатов через reinforcement learning",
                    "desc": "В статье представлен PokeeResearch-7B — исследовательский AI-агент с 7 миллиардами параметров, который использует reinforcement learning для улучшения качества работы с внешними инструментами и данными. Модель обучена с помощью RLAIF (обучение с подкреплением на основе обратной связи от AI) без необходимости ручной разметки, оптимизируя точность фактов, корректность цитирования и следование инструкциям. Система использует chain-of-thought рассуждения с множественными вызовами инструментов, самопроверкой и адаптивным восстановлением после ошибок. На 10 популярных бенчмарках для исследовательских агентов модель показала лучшие результаты среди моделей масштаба 7B параметров."
                },
                "en": {
                    "title": "Empowering Research with Robust AI: Introducing PokeeResearch-7B",
                    "desc": "PokeeResearch-7B is a 7 billion parameter deep research agent that utilizes reinforcement learning and chain-of-thought reasoning to improve its performance and reliability. It addresses limitations in existing AI agents by employing a novel Reinforcement Learning from AI Feedback (RLAIF) approach, which enhances factual accuracy and adherence to instructions without requiring extensive annotations. The model's multi-call reasoning framework allows it to verify its own outputs and recover from errors, making it more robust in handling complex queries. With state-of-the-art results on various benchmarks, PokeeResearch-7B demonstrates the effectiveness of combining advanced reinforcement learning techniques with structured reasoning in AI research applications."
                },
                "zh": {
                    "title": "强化学习与推理设计的完美结合",
                    "desc": "PokeeResearch-7B 是一个拥有 70 亿参数的深度研究代理，采用强化学习和思维链推理来提高其鲁棒性和对齐性。该模型通过无注释的 AI 反馈强化学习框架进行训练，优化策略以捕捉事实准确性和引用忠实度。它还通过多次推理的思维链驱动，增强了自我验证和从工具失败中自适应恢复的能力。PokeeResearch-7B 在 10 个流行的深度研究基准测试中表现出色，展示了精心设计的强化学习和推理可以产生高效且具有研究级别的 AI 代理。"
                }
            }
        }
    ],
    "link_prev": "2025-10-21.html",
    "link_next": "2025-10-23.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10月21日"
    },
    "short_date_next": {
        "ru": "23.10",
        "en": "10/23",
        "zh": "10月23日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 7,
        "#benchmark": 14,
        "#agents": 6,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 11,
        "#healthcare": 1,
        "#training": 18,
        "#robotics": 0,
        "#agi": 3,
        "#games": 6,
        "#interpretability": 2,
        "#reasoning": 9,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 16,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 6,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 6,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 6,
        "#low_resource": 1,
        "#watermarking": 1,
        "#reinforcement_learning": 1
    }
}