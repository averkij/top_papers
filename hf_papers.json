{
    "date": {
        "ru": "5 августа",
        "en": "August 5",
        "zh": "8月5日"
    },
    "time_utc": "2025-08-05 05:22",
    "weekday": 1,
    "issue_id": 5178,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.02276",
            "title": "CellForge: Agentic Design of Virtual Cell Models",
            "url": "https://huggingface.co/papers/2508.02276",
            "abstract": "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.",
            "score": 18,
            "issue_id": 5176,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "04238c0793ca08e5",
            "authors": [
                "Xiangru Tang",
                "Zhuoyun Yu",
                "Jiapeng Chen",
                "Yan Cui",
                "Daniel Shao",
                "Weixu Wang",
                "Fang Wu",
                "Yuchen Zhuang",
                "Wenqi Shi",
                "Zhi Huang",
                "Arman Cohan",
                "Xihong Lin",
                "Fabian Theis",
                "Smita Krishnaswamy",
                "Mark Gerstein"
            ],
            "affiliations": [
                "Google DeepMind",
                "Harvard University",
                "Helmholtz Zentrum Munchen",
                "Stanford University",
                "University of Pennsylvania",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02276.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#science",
                    "#open_source",
                    "#architecture",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "CellForge: ИИ-агенты создают виртуальные клетки из одноклеточных данных",
                    "desc": "CellForge - это агентная система, использующая мультиагентный фреймворк для преобразования необработанных одноклеточных мультиомных данных в оптимизированные вычислительные модели виртуальных клеток. Система состоит из трех основных модулей: анализа задач, разработки методов и выполнения экспериментов. CellForge превосходит современные методы в прогнозировании одноклеточных возмущений на шести разнообразных наборах данных. Это демонстрирует, как итеративное взаимодействие между агентами с различными перспективами обеспечивает лучшие решения, чем прямой подход к задаче моделирования."
                },
                "en": {
                    "title": "Transforming Biology with Collaborative AI Models",
                    "desc": "CellForge is an innovative system that uses a multi-agent framework to create computational models for virtual cells from raw single-cell multi-omics data. It addresses the complexities of biological systems by employing specialized agents that collaborate to analyze tasks, design methods, and execute experiments. This approach allows CellForge to generate optimized model architectures and executable code, significantly improving predictions for single-cell perturbations. By integrating diverse perspectives from its agents, CellForge consistently outperforms existing state-of-the-art methods in the field."
                },
                "zh": {
                    "title": "CellForge：优化虚拟细胞建模的智能系统",
                    "desc": "CellForge 是一个基于多智能体框架的系统，能够将原始的单细胞多组学数据转化为优化的虚拟细胞计算模型。该系统通过分析任务和数据集，自动生成可执行的代码，显著提高了单细胞扰动预测的准确性。CellForge 的设计模块由不同专业的智能体协作开发建模策略，确保了模型的优化。实验结果表明，CellForge 在多种数据集上均优于现有的最先进方法，展示了多智能体协作的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01059",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
            "url": "https://huggingface.co/papers/2508.01059",
            "abstract": "Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
            "score": 11,
            "issue_id": 5176,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "897c594bfa5630a6",
            "authors": [
                "Sajana Weerawardhena",
                "Paul Kassianik",
                "Blaine Nelson",
                "Baturay Saglam",
                "Anu Vellore",
                "Aman Priyanshu",
                "Supriti Vijay",
                "Massimo Aufiero",
                "Arthur Goldblatt",
                "Fraser Burch",
                "Ed Li",
                "Jianliang He",
                "Dhruv Kedia",
                "Kojin Oshiba",
                "Zhouran Yang",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Cisco Systems Inc.",
                "Foundation AI",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01059.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#alignment",
                    "#security",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Интеллектуальный помощник по кибербезопасности на базе ИИ",
                    "desc": "Foundation-Sec-8B-Instruct - это языковая модель, специализирующаяся на кибербезопасности и предназначенная для диалогового взаимодействия. Модель сочетает в себе специализированные знания в области кибербезопасности с возможностями следования инструкциям и ведения разговора. Оценки показывают, что Foundation-Sec-8B-Instruct превосходит другие модели в задачах кибербезопасности, сохраняя при этом способность следовать инструкциям. Авторы предполагают, что эта модель станет незаменимым помощником в повседневной работе специалистов по кибербезопасности."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Conversational AI",
                    "desc": "Foundation-Sec-8B-Instruct is a large language model (LLM) specifically designed for cybersecurity applications, enhancing chat-style interactions and instruction-following capabilities. It builds upon the previous Foundation-Sec-8B model, which was tailored for fine-tuning on cybersecurity tasks but lacked conversational features. This new model integrates domain-specific knowledge with the ability to follow instructions and engage in dialogue, resulting in high-quality responses relevant to cybersecurity. Evaluations demonstrate that it surpasses other models like Llama 3.1-8B-Instruct in cybersecurity tasks while maintaining competitive performance in instruction-following."
                },
                "zh": {
                    "title": "网络安全对话的智能助手",
                    "desc": "Foundation-Sec-8B-Instruct 是一个专注于网络安全的语言模型，旨在进行对话式交互和遵循指令。该模型在网络安全任务上表现优于其他模型，同时在遵循指令的能力上与之相匹配。它结合了特定领域的知识和人类偏好的对齐，能够生成高质量和相关的响应。我们希望 Foundation-Sec-8B-Instruct 能成为网络安全专业人员日常工作中不可或缺的助手。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17520",
            "title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation",
            "url": "https://huggingface.co/papers/2507.17520",
            "abstract": "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.",
            "score": 4,
            "issue_id": 5176,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "13d868fd7ad8ea42",
            "authors": [
                "Shuai Yang",
                "Hao Li",
                "Yilun Chen",
                "Bin Wang",
                "Yang Tian",
                "Tai Wang",
                "Hanqing Wang",
                "Feng Zhao",
                "Yiyi Liao",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17520.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "InstructVLA: Мост между интуитивным управлением роботами и эффективным обучением",
                    "desc": "InstructVLA - это модель обработки зрения, языка и действий, которая улучшает манипуляционные способности роботов, сохраняя при этом рассуждения на основе зрения и языка. Модель использует мультимодальное обучение и адаптацию на основе смеси экспертов. InstructVLA демонстрирует значительное улучшение производительности по сравнению с существующими моделями на различных задачах, включая задачи в симулированной и реальной среде. Эта модель открывает возможности для более интуитивного и управляемого взаимодействия человека с роботом при эффективном обучении политикам."
                },
                "en": {
                    "title": "Bridging Vision, Language, and Action for Smarter Robots",
                    "desc": "InstructVLA is a new model that combines vision, language, and action to improve how robots perform tasks. It uses a special training method called Vision-Language-Action Instruction Tuning (VLA-IT) to enhance both understanding and action capabilities without losing previous knowledge. This model outperforms existing systems in various tasks, showing significant improvements in manipulation and reasoning. By integrating multimodal training and expert adaptation, InstructVLA enables better human-robot interaction and efficient learning of tasks."
                },
                "zh": {
                    "title": "提升机器人操作与推理的完美结合",
                    "desc": "InstructVLA是一种端到端的视觉-语言-动作模型，旨在提高机器人操作性能，同时保持视觉-语言推理能力。该模型通过多模态训练和专家混合适应，解决了现有模型在任务特定数据上的局限性和灾难性遗忘问题。InstructVLA引入了一种新的训练范式，称为视觉-语言-动作指令调优（VLA-IT），在标准视觉-语言模型数据集和一个包含65万样本的VLA-IT数据集上共同优化文本推理和动作生成。实验结果表明，InstructVLA在多个任务上表现优异，展示了其在高效政策学习和人机交互中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01151",
            "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
            "url": "https://huggingface.co/papers/2508.01151",
            "abstract": "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/.",
            "score": 3,
            "issue_id": 5176,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 августа",
                "en": "August 2",
                "zh": "8月2日"
            },
            "hash": "22d777cd71f123c6",
            "authors": [
                "Yu Lei",
                "Jinbin Bai",
                "Qingyu Shi",
                "Aosong Feng",
                "Kaidong Yu"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "TeleAI, China Telecom",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01151.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#alignment",
                    "#diffusion",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Персонализированная безопасность в генеративных моделях изображений",
                    "desc": "Предложена новая система Personalized Safety Alignment (PSA) для настройки генеративных моделей изображений под индивидуальные предпочтения безопасности пользователей. PSA интегрирует персонализированные профили пользователей в процесс диффузии, адаптируя поведение модели к индивидуальным требованиям безопасности. Авторы представили новый датасет Sage, capturing пользовательские предпочтения безопасности. Эксперименты показали, что PSA превосходит существующие методы в подавлении вредного контента и лучше соответствует ограничениям пользователей."
                },
                "en": {
                    "title": "Personalized Safety for Safer AI-Generated Images",
                    "desc": "This paper presents a Personalized Safety Alignment (PSA) framework that enhances text-to-image diffusion models by incorporating individual user profiles. Current models apply a one-size-fits-all approach to safety, which does not consider the unique safety preferences shaped by personal factors. The PSA framework allows for user-specific adjustments in the generative process, ensuring that the generated images align with individual safety standards while maintaining high image quality. The authors also introduce a new dataset, Sage, to effectively capture and integrate these personalized safety preferences, demonstrating improved performance in harmful content suppression compared to existing methods."
                },
                "zh": {
                    "title": "个性化安全对齐：让生成内容更符合你的安全偏好",
                    "desc": "这篇论文提出了一种个性化安全对齐框架（PSA），旨在将用户特定的个人资料整合到文本到图像的扩散模型中，以更好地符合个体的安全偏好。当前的安全机制通常采用统一标准，无法考虑用户的多样化安全边界，如年龄、心理健康和个人信仰等因素。PSA通过在扩散过程中整合个性化用户资料，调整模型行为以匹配个体安全偏好，同时保持图像质量。实验结果表明，PSA在有害内容抑制和生成内容与用户约束的对齐方面优于现有方法，取得了更高的胜率和通过率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02317",
            "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
            "url": "https://huggingface.co/papers/2508.02317",
            "abstract": "A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.",
            "score": 2,
            "issue_id": 5176,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "2e96724e612a0eb6",
            "authors": [
                "Qianli Ma",
                "Yaowei Zheng",
                "Zhelun Shi",
                "Zhongkai Zhao",
                "Bin Jia",
                "Ziyue Huang",
                "Zhiqi Lin",
                "Youjie Li",
                "Jiacheng Yang",
                "Yanghua Peng",
                "Zhi Zhang",
                "Xin Liu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02317.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускоряем обучение омни-модальных LLM с помощью модульной архитектуры",
                    "desc": "Эта статья представляет новую модульную систему обучения для омни-модальных языковых моделей (LLM). Система предлагает эффективное 3D-распараллеливание и гибкую конфигурацию, что ускоряет разработку омни-модальных LLM. Ключевые особенности включают разделение коммуникации и вычислений, а также простую интеграцию новых модальностей. Результаты показывают высокую эффективность и масштабируемость при обучении крупных омни-модальных моделей."
                },
                "en": {
                    "title": "Accelerating Omni-Modal LLMs with Modular Training",
                    "desc": "This paper introduces \textit{veomni}, a modular training framework designed to enhance the development of omni-modal large language models (LLMs). It addresses the challenges of training these models by separating model architecture from parallel processing logic, which allows for efficient 3D parallelism. The framework supports easy integration of new modalities, reducing the need for extensive code modifications. With \textit{veomni}, a mixture-of-experts model with 30 billion parameters can achieve high throughput and scalability, demonstrating its effectiveness in training large omni-modal LLMs."
                },
                "zh": {
                    "title": "模块化训练框架，提升全模态LLM开发效率",
                    "desc": "这篇论文介绍了一种名为\\veomni的模块化训练框架，旨在加速全模态大语言模型（LLM）的开发。该框架通过高效的三维并行处理和灵活的配置，解决了训练全模态LLM时面临的挑战。\\veomni将模型定义与并行逻辑解耦，使得在多种模态上进行大规模训练变得更加高效。使用\\veomni，研究人员能够以极高的速度训练具有30亿参数的全模态专家模型，展示了其在训练大型全模态LLM方面的优越效率和可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01959",
            "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
            "url": "https://huggingface.co/papers/2508.01959",
            "abstract": "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.",
            "score": 1,
            "issue_id": 5178,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 августа",
                "en": "August 3",
                "zh": "8月3日"
            },
            "hash": "063b1bca074ff85f",
            "authors": [
                "Junjie Wu",
                "Jiangnan Li",
                "Yuqing Li",
                "Lemao Liu",
                "Liyan Xu",
                "Jiwei Li",
                "Dit-Yan Yeung",
                "Jie Zhou",
                "Mo Yu"
            ],
            "affiliations": [
                "HKUST",
                "IIE-CAS",
                "WeChat AI, Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01959.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#rag",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Контекст имеет значение: ситуативные эмбеддинги для точного информационного поиска",
                    "desc": "Статья представляет новый подход к улучшению производительности информационного поиска путем контекстуализации коротких текстовых фрагментов в более широком контексте. Авторы предлагают новую парадигму обучения и модели ситуативных эмбеддингов (SitEmb), которые превосходят современные модели, имея меньше параметров. Метод особенно эффективен для задач, требующих понимания локального контекста в рамках более широкого документа. Результаты показывают значительное улучшение производительности на специально созданном наборе данных для оценки ситуативного поиска."
                },
                "en": {
                    "title": "Enhancing Retrieval with Contextual Short Text Chunks",
                    "desc": "This paper introduces a new training method and a model called situated embedding models (SitEmb) that improve the retrieval of information from text. By conditioning short text chunks on a broader context, the model captures the meaning of each chunk more effectively. The authors demonstrate that existing models struggle with this task due to their limitations in encoding context. Their SitEmb model outperforms leading models with significantly fewer parameters, showing better retrieval performance across various languages and applications."
                },
                "zh": {
                    "title": "情境嵌入，提升检索性能！",
                    "desc": "本文提出了一种新的训练范式和情境嵌入模型（SitEmb），通过将短文本片段与更广泛的上下文窗口相结合，提升了检索性能。传统的检索方法往往将长文档拆分为小块，但这些小块的理解需要上下文信息。我们的方法通过在更广泛的上下文中对短片段进行编码，来增强检索效果。实验结果表明，SitEmb模型在参数更少的情况下，显著超越了现有的最先进模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01415",
            "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
            "url": "https://huggingface.co/papers/2508.01415",
            "abstract": "RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.",
            "score": 1,
            "issue_id": 5177,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 августа",
                "en": "August 2",
                "zh": "8月2日"
            },
            "hash": "ea0bbd88dcba8948",
            "authors": [
                "Mingcong Lei",
                "Honghao Cai",
                "Zezhou Cui",
                "Liangchen Tan",
                "Junkun Hong",
                "Gehan Hu",
                "Shuangyu Zhu",
                "Yimou Wu",
                "Shaohan Jiang",
                "Ge Wang",
                "Zhen Li",
                "Shuguang Cui",
                "Yiming Zhao",
                "Yatong Han"
            ],
            "affiliations": [
                "FNii-Shenzhen",
                "Harbin Engineering University",
                "Harbin Institute of Technology, Shenzhen",
                "Infused Synapse AI",
                "SSE",
                "The Chinese University of Hong Kong, Shengzhen",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01415.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#agi",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RoboMemory: Мозгоподобная память для непрерывного обучения роботов",
                    "desc": "RoboMemory - это мультимодульная система памяти для непрерывного обучения роботов, вдохновленная принципами работы мозга. Она включает четыре основных модуля, имитирующих функции различных отделов мозга: препроцессор информации, систему долговременной памяти, модуль замкнутого планирования и исполнитель низкого уровня. Система показала значительное улучшение производительности по сравнению с существующими решениями на бенчмарке EmbodiedBench. RoboMemory решает проблемы высокой латентности и масштабируемости, что делает ее перспективной основой для интеграции мультимодальных систем памяти в физических роботах."
                },
                "en": {
                    "title": "RoboMemory: Revolutionizing Lifelong Learning in Robots",
                    "desc": "RoboMemory is a new framework designed to help robots learn continuously over time, inspired by how the human brain works. It uses four main components that mimic brain functions to improve memory and planning in robots, allowing them to handle complex tasks better. The framework addresses issues like slow memory access and the need for robots to remember different types of information effectively. Tests show that RoboMemory significantly outperforms existing systems in real-world scenarios, making it a promising advancement in robotic learning."
                },
                "zh": {
                    "title": "RoboMemory：提升机器人终身学习的多记忆框架",
                    "desc": "RoboMemory是一个受大脑启发的多记忆框架，旨在提高物理机器人在终身学习中的表现。它结合了认知神经科学的原理，解决了现实环境中的关键挑战，如持续学习和任务相关性捕捉。该框架包含四个核心模块，分别模拟大脑的不同部分，以实现长期规划和累积学习。通过在复杂记忆框架中并行更新和检索，RoboMemory显著提高了推理速度，并在实际任务中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00910",
            "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime",
            "url": "https://huggingface.co/papers/2508.00910",
            "abstract": "Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.",
            "score": 1,
            "issue_id": 5177,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "181a31b28dfe8e6a",
            "authors": [
                "Terry Yue Zhuo",
                "Dingmin Wang",
                "Hantian Ding",
                "Varun Kumar",
                "Zijian Wang"
            ],
            "affiliations": [
                "Amazon",
                "Monash University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00910.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Синтез траекторий без среды выполнения для обучения передовых LLM в кибербезопасности",
                    "desc": "Cyber-Zero - это первая система для синтеза траекторий агентов без использования среды выполнения для обучения языковых моделей в кибербезопасности. Она использует общедоступные отчеты CTF и симуляцию на основе LLM для создания реалистичных последовательностей взаимодействий. Обученные на синтезированных траекториях агенты на основе LLM достигают значительного улучшения производительности на трех ключевых бенчмарках CTF. Лучшая модель Cyber-Zero-32B устанавливает новый state-of-the-art среди открытых моделей, соответствуя возможностям проприетарных систем."
                },
                "en": {
                    "title": "Revolutionizing Cybersecurity AI with Runtime-Free Trajectory Synthesis",
                    "desc": "Cyber-Zero introduces a novel framework for training cybersecurity large language models (LLMs) without the need for executable runtime environments. It synthesizes agent trajectories from Capture The Flag (CTF) writeups, allowing the generation of realistic interaction sequences that mimic runtime behaviors. This approach enables the training of LLM-based agents that outperform existing models on key CTF benchmarks. By achieving state-of-the-art performance with a cost-effective solution, Cyber-Zero demonstrates the potential of runtime-free trajectory synthesis in advancing cybersecurity AI."
                },
                "zh": {
                    "title": "Cyber-Zero：无运行时环境的网络安全代理训练新方法",
                    "desc": "Cyber-Zero 是一个创新的框架，旨在通过合成高质量的代理轨迹来训练网络安全领域的语言模型（LLM），而无需实际的运行时环境。该框架利用公开的CTF（Capture The Flag）写作材料，采用基于角色的LLM模拟，逆向工程运行时行为，生成真实的长时间交互序列。通过使用Cyber-Zero合成的轨迹，我们训练的LLM代理在三个主要的CTF基准测试中，性能提升达13.1%。Cyber-Zero-32B模型在开放权重模型中创造了新的性能记录，展示了无运行时轨迹合成在网络安全代理开发中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00890",
            "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks",
            "url": "https://huggingface.co/papers/2508.00890",
            "abstract": "AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.",
            "score": 1,
            "issue_id": 5178,
            "pub_date": "2025-07-26",
            "pub_date_card": {
                "ru": "26 июля",
                "en": "July 26",
                "zh": "7月26日"
            },
            "hash": "359ff54230f7e0ba",
            "authors": [
                "Fali Wang",
                "Hui Liu",
                "Zhenwei Dai",
                "Jingying Zeng",
                "Zhiwei Zhang",
                "Zongyu Wu",
                "Chen Luo",
                "Zhen Li",
                "Xianfeng Tang",
                "Qi He",
                "Suhang Wang"
            ],
            "affiliations": [
                "Amazon, Palo Alto, CA, USA",
                "The Pennsylvania State University, University Park, PA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00890.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rl",
                    "#agents",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Умное распределение ресурсов для сложных задач искусственного интеллекта",
                    "desc": "AgentTTS - это фреймворк на основе LLM-агентов для оптимизации распределения вычислительных ресурсов в многоэтапных сложных задачах. Он использует итеративные взаимодействия с обратной связью для поиска оптимальных распределений моделей и бюджетов для каждой подзадачи. AgentTTS превосходит традиционные методы по эффективности поиска, устойчивости к размерам обучающих выборок и интерпретируемости. Фреймворк решает проблемы комбинаторного пространства поиска и взаимозависимости оптимальных распределений между подзадачами."
                },
                "en": {
                    "title": "Optimizing Compute Allocation for Complex Multi-Stage Tasks with AgentTTS",
                    "desc": "AgentTTS is a framework that optimizes how computing resources are allocated for complex tasks that involve multiple stages. It focuses on improving the performance of large language models (LLMs) by dynamically adjusting resource allocation during inference, especially for tasks that require different capabilities at each stage. The framework addresses challenges such as the combinatorial nature of model selection and budget allocation, which makes traditional search methods inefficient. Through extensive experiments, AgentTTS has shown to be more efficient and robust compared to existing methods, providing better performance across various datasets and tasks."
                },
                "zh": {
                    "title": "AgentTTS：优化多阶段任务的计算分配",
                    "desc": "AgentTTS是一个基于大语言模型（LLM）代理的框架，旨在优化多阶段复杂任务的计算资源分配。与传统方法相比，它在性能和鲁棒性上有显著提升。该框架通过迭代反馈与执行环境进行交互，自动搜索计算最优的分配方案。实验结果表明，AgentTTS在搜索效率上显著优于传统方法和其他基于LLM的基线，并且对训练集大小的变化表现出更好的鲁棒性和可解释性。"
                }
            }
        }
    ],
    "link_prev": "2025-08-04.html",
    "link_next": "2025-08-06.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "04.08",
        "en": "08/04",
        "zh": "8月4日"
    },
    "short_date_next": {
        "ru": "06.08",
        "en": "08/06",
        "zh": "8月6日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 4,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 2,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}