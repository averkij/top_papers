{
    "date": {
        "ru": "6 июня",
        "en": "June 6",
        "zh": "6月6日"
    },
    "time_utc": "2025-06-06 04:18",
    "weekday": 4,
    "issue_id": 4157,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.04308",
            "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
            "url": "https://huggingface.co/papers/2506.04308",
            "abstract": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.",
            "score": 19,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "ef5abd087929ed17",
            "authors": [
                "Enshen Zhou",
                "Jingkun An",
                "Cheng Chi",
                "Yi Han",
                "Shanyu Rong",
                "Chi Zhang",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Lu Sheng",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Academy of Artificial Intelligence",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04308.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#robotics",
                    "#reasoning",
                    "#dataset",
                    "#3d",
                    "#training",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RoboRefer: Пространственный интеллект для роботов нового поколения",
                    "desc": "RoboRefer - это модель пространственного понимания для роботов, основанная на зрении и языке. Она использует специальный энкодер глубины и обучение с подкреплением для точного понимания 3D-сцен и рассуждений о пространственных отношениях. Авторы также представили большой набор данных RefSpatial и бенчмарк RefSpatial-Bench для обучения и оценки модели. RoboRefer превзошла современные методы, включая Gemini-2.5-Pro, и может применяться для управления различными роботами в реальных условиях."
                },
                "en": {
                    "title": "RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments",
                    "desc": "The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks."
                },
                "zh": {
                    "title": "RoboRefer：提升机器人空间理解与推理能力的创新模型",
                    "desc": "空间指向是具身机器人与三维物理世界互动的基本能力。尽管现有的预训练视觉语言模型（VLMs）很强大，但它们在理解复杂的三维场景和动态推理指示位置方面仍然存在不足。为此，我们提出了RoboRefer，这是一种具有三维感知能力的VLM，通过监督微调（SFT）集成了专门的深度编码器，实现了精确的空间理解。此外，RoboRefer通过强化微调（RFT）推进了多步骤空间推理，采用针对空间指向任务的度量敏感过程奖励函数。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05240",
            "title": "Aligning Latent Spaces with Flow Priors",
            "url": "https://huggingface.co/papers/2506.05240",
            "abstract": "This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.",
            "score": 13,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "c776325b1f9f6966",
            "authors": [
                "Yizhuo Li",
                "Yuying Ge",
                "Yixiao Ge",
                "Ying Shan",
                "Ping Luo"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05240.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#cv",
                    "#math",
                    "#diffusion"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Выравнивание латентных пространств с помощью потоковых моделей",
                    "desc": "Статья представляет новый подход к выравниванию обучаемых латентных пространств с произвольными целевыми распределениями, используя генеративные модели на основе потоков в качестве априорных. Метод сначала предобучает модель потока на целевых признаках, а затем использует ее для регуляризации латентного пространства через функцию потерь выравнивания. Авторы доказывают, что минимизация этой функции потерь является вычислительно эффективным суррогатом для максимизации вариационной нижней границы правдоподобия латентов. Эффективность подхода подтверждается экспериментами по генерации изображений на ImageNet с различными целевыми распределениями."
                },
                "en": {
                    "title": "Aligning Latent Spaces with Flow-Based Models",
                    "desc": "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."
                },
                "zh": {
                    "title": "潜在空间对齐的新方法",
                    "desc": "本文提出了一种新颖的框架，通过利用基于流的生成模型作为先验，将可学习的潜在空间对齐到任意目标分布。我们的方法首先在目标特征上预训练流模型，以捕捉潜在分布。然后，这个固定的流模型通过对齐损失来规范化潜在空间，重新构造流匹配目标，将潜在变量视为优化目标。我们正式证明，最小化这个对齐损失建立了一个计算上可处理的替代目标，用于最大化潜在变量在目标分布下的变分下界的对数似然。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23656",
            "title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models",
            "url": "https://huggingface.co/papers/2505.23656",
            "abstract": "Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.",
            "score": 10,
            "issue_id": 4155,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "aa7bbc7378df2b3e",
            "authors": [
                "Xiangdong Zhang",
                "Jiaqi Liao",
                "Shaofeng Zhang",
                "Fanqing Meng",
                "Xiangpeng Wan",
                "Junchi Yan",
                "Yu Cheng"
            ],
            "affiliations": [
                "Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University",
                "NetMind.AI",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23656.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Физически достоверное видео из текста: VideoREPA улучшает понимание физики в моделях T2V",
                    "desc": "В статье представлен новый метод VideoREPA для улучшения физической достоверности видео, генерируемых моделями text-to-video (T2V). Авторы предлагают дистиллировать понимание физики из моделей видеопонимания в модели T2V путем выравнивания отношений на уровне токенов. Ключевым элементом метода является функция потерь Token Relation Distillation (TRD), которая обеспечивает мягкое руководство для дообучения предобученных моделей T2V. Эмпирические оценки показывают, что VideoREPA значительно улучшает физический здравый смысл базового метода CogVideoX."
                },
                "en": {
                    "title": "Bridging the Physics Gap in Text-to-Video Generation",
                    "desc": "This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation."
                },
                "zh": {
                    "title": "VideoREPA：提升文本到视频模型的物理理解能力",
                    "desc": "最近，文本到视频（T2V）扩散模型的进展使得高保真和真实的视频合成成为可能。然而，当前的T2V模型在生成物理上合理的内容方面常常面临挑战，因为它们对物理的理解能力有限。我们提出了一种新框架，称为VideoREPA，通过对齐令牌级关系，将视频理解基础模型中的物理理解能力提炼到T2V模型中，从而缩小物理理解的差距。我们的实验表明，VideoREPA显著增强了基线方法CogVideoX的物理常识，能够生成与直观物理一致的视频。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05176",
            "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
            "url": "https://huggingface.co/papers/2506.05176",
            "abstract": "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.",
            "score": 8,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "90ebf52dd91334c2",
            "authors": [
                "Yanzhao Zhang",
                "Mingxin Li",
                "Dingkun Long",
                "Xin Zhang",
                "Huan Lin",
                "Baosong Yang",
                "Pengjun Xie",
                "An Yang",
                "Dayiheng Liu",
                "Junyang Lin",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Group",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05176.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#low_resource"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Qwen3 Embedding: Новый стандарт многоязычных текстовых эмбеддингов",
                    "desc": "В работе представлена серия моделей Qwen3 Embedding, улучшающая возможности текстовых эмбеддингов и ранжирования по сравнению с предшественником GTE-Qwen. Модели основаны на фундаментальных моделях Qwen3 и используют многоэтапный процесс обучения, включающий масштабную предварительную подготовку и тонкую настройку на качественных датасетах. Qwen3 Embedding предлагает модели различных размеров (0.6B, 4B, 8B) для задач эмбеддинга и ранжирования, что позволяет оптимизировать их под разные сценарии применения. Эмпирические оценки показывают, что серия Qwen3 Embedding достигает передовых результатов в различных бенчмарках, особенно в многоязычной оценке MTEB и задачах поиска."
                },
                "en": {
                    "title": "Empowering Multilingual Text Understanding with Qwen3 Embeddings",
                    "desc": "The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research."
                },
                "zh": {
                    "title": "Qwen3嵌入系列：多语言文本处理的新突破",
                    "desc": "本文介绍了Qwen3嵌入系列，这是在文本嵌入和重排序能力上相较于GTE-Qwen系列的重大进展。该系列基于Qwen3基础模型，利用其强大的多语言文本理解和生成能力，采用多阶段训练流程，结合大规模无监督预训练和高质量数据集的监督微调。通过有效的模型合并策略，Qwen3嵌入系列确保了模型的鲁棒性和适应性，提供了多种模型规模以满足不同的部署场景。实证评估表明，Qwen3嵌入系列在多项基准测试中取得了最先进的结果，尤其在多语言评估基准MTEB上表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05344",
            "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
            "url": "https://huggingface.co/papers/2506.05344",
            "abstract": "Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.",
            "score": 6,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "0175b3788ebacf29",
            "authors": [
                "Jiahui Wang",
                "Zuyan Liu",
                "Yongming Rao",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Tencent Hunyuan Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05344.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективное зрение: оптимизация визуального восприятия в мультимодальных ИИ",
                    "desc": "Исследование показывает, что мультимодальные большие языковые модели (MLLM) используют лишь небольшую часть (менее 5%) механизмов внимания для обработки визуальной информации. Авторы разработали метод SparseMM для оптимизации KV-кэша, который распределяет вычислительные ресурсы асимметрично, основываясь на визуальной релевантности головок внимания. Этот подход позволяет ускорить вывод MLLM в 1,38 раза и сократить использование памяти на 52% при сохранении производительности. Метод SparseMM показывает лучшее соотношение точности и эффективности по сравнению с существующими методами оптимизации KV-кэша."
                },
                "en": {
                    "title": "Optimizing Visual Understanding in MLLMs with Sparse Attention",
                    "desc": "This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks."
                },
                "zh": {
                    "title": "利用稀疏视觉头加速多模态模型推理",
                    "desc": "多模态大型语言模型（MLLMs）通过扩展预训练的大型语言模型（LLMs）来增加视觉能力。我们研究了MLLMs如何处理视觉输入，分析了它们的注意力机制。我们发现了一个惊人的稀疏现象：在LLMs中，只有少量（大约5%以下）的注意力头积极参与视觉理解，这些被称为视觉头。基于这一发现，我们提出了SparseMM，一种KV-Cache优化策略，根据视觉得分为LLMs中的头分配不对称的计算预算，从而加速MLLMs的推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05287",
            "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?",
            "url": "https://huggingface.co/papers/2506.05287",
            "abstract": "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.",
            "score": 6,
            "issue_id": 4156,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "8b357b07e4b449cd",
            "authors": [
                "Yuqian Yuan",
                "Ronghao Dang",
                "Long Li",
                "Wentong Li",
                "Dian Jiao",
                "Xin Li",
                "Deli Zhao",
                "Fan Wang",
                "Wenqiao Zhang",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05287.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "EOC-Bench: Новый стандарт для оценки когнитивных способностей ИИ в эгоцентрическом зрении",
                    "desc": "Статья представляет новый бенчмарк EOC-Bench для оценки понимания объектов в динамических эгоцентрических сценариях мультимодальными языковыми моделями. EOC-Bench включает 3277 аннотированных пар вопрос-ответ, охватывающих три временные категории и 11 измерений оценки. Авторы разработали систему аннотаций с участием человека и новую метрику для оценки временной точности. EOC-Bench позволяет комплексно оценивать возможности мультимодальных языковых моделей в понимании объектов в воплощенных системах."
                },
                "en": {
                    "title": "EOC-Bench: Advancing Object Cognition in Dynamic Environments",
                    "desc": "This paper introduces EOC-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) in dynamic environments where users interact with objects. Unlike previous benchmarks that focus on static scenes, EOC-Bench assesses how well models understand objects in changing contexts by using 3,277 annotated question-answer pairs across different time frames. The benchmark includes a unique human-in-the-loop annotation framework and a multi-scale temporal accuracy metric to evaluate models' performance in real-time interactions. By providing a comprehensive evaluation tool, EOC-Bench aims to enhance the cognitive abilities of MLLMs in embodied applications."
                },
                "zh": {
                    "title": "动态场景中的物体认知评估新基准",
                    "desc": "多模态大型语言模型（MLLMs）的出现推动了以自我为中心的视觉应用的突破。这些应用需要对物体进行持续的、上下文感知的理解，因为用户在动态和杂乱的环境中与工具互动。然而，现有的体现基准主要集中在静态场景探索上，强调物体的外观和空间属性，而忽视了用户互动所带来的动态变化评估。为了解决这个问题，我们引入了EOC-Bench，这是一个创新的基准，旨在系统地评估动态自我中心场景中的以物体为中心的体现认知。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04209",
            "title": "Language-Image Alignment with Fixed Text Encoders",
            "url": "https://huggingface.co/papers/2506.04209",
            "abstract": "Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.",
            "score": 6,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "921137445b3be92e",
            "authors": [
                "Jingfeng Yang",
                "Ziyang Wu",
                "Yue Zhao",
                "Yi Ma"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04209.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#alignment",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Фиксированный языковой энкодер улучшает визуальное обучение",
                    "desc": "Исследование LIFT предлагает новый подход к обучению визуальных представлений с использованием предобученных языковых моделей (LLM) в качестве фиксированного текстового энкодера. Этот метод превосходит совместное обучение текстовых и визуальных энкодеров, как в CLIP, особенно в задачах композиционного понимания и работы с длинными подписями. LIFT демонстрирует высокую эффективность и вычислительную эффективность, обучая только визуальный энкодер. Результаты исследования открывают новые перспективы использования текстовых эмбеддингов из LLM для улучшения визуального обучения."
                },
                "en": {
                    "title": "LIFT: Efficient Language-Image Alignment with Fixed Text Encoders",
                    "desc": "This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment."
                },
                "zh": {
                    "title": "简化训练，提升视觉理解的LIFT方法",
                    "desc": "本文提出了一种新的方法，称为LIFT（使用固定文本编码器的语言-图像对齐），旨在通过预训练的大型语言模型来指导视觉表示学习。与传统的联合训练方法（如CLIP）相比，LIFT只训练图像编码器，而使用固定的文本编码器，从而简化了训练过程。研究表明，LIFT在处理组合理解和长文本描述时，表现优于CLIP，并且在计算效率上也有显著提升。该研究为如何利用大型语言模型的文本嵌入来指导视觉学习提供了新的思路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05010",
            "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
            "url": "https://huggingface.co/papers/2506.05010",
            "abstract": "ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.",
            "score": 5,
            "issue_id": 4157,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "5d1aa2eb9189bc56",
            "authors": [
                "Zhenran Xu",
                "Xue Yang",
                "Yiyu Wang",
                "Qingli Hu",
                "Zijiao Wu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce",
                "Harbin Institute of Technology (Shenzhen)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05010.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ИИ-ассистент для творчества: ComfyUI-Copilot упрощает создание цифрового искусства",
                    "desc": "ComfyUI-Copilot - это плагин на основе большой языковой модели, разработанный для улучшения удобства использования и эффективности платформы ComfyUI для создания искусства с помощью ИИ. Система использует иерархическую мультиагентную структуру с центральным агентом-ассистентом и специализированными рабочими агентами. ComfyUI-Copilot предлагает интеллектуальные рекомендации по узлам и моделям, а также автоматизированное построение рабочего процесса в один клик. Эффективность системы подтверждена как офлайн-оценками, так и отзывами пользователей, показывающими, что она точно рекомендует узлы и ускоряет разработку рабочих процессов."
                },
                "en": {
                    "title": "Empowering Art Creation with Intelligent Assistance",
                    "desc": "ComfyUI-Copilot is a plugin that leverages a large language model and a multi-agent system to improve the ComfyUI platform for AI art creation. It addresses common challenges faced by users, such as limited documentation and complex workflows, by providing intelligent recommendations and automating workflow construction. The system features a hierarchical structure with a central assistant agent that delegates tasks to specialized worker agents, enhancing usability and efficiency. Validation through user feedback and quantitative evaluations demonstrates that ComfyUI-Copilot effectively lowers barriers for beginners while streamlining processes for experienced users."
                },
                "zh": {
                    "title": "智能助手，轻松创作艺术",
                    "desc": "ComfyUI-Copilot 是一个基于大型语言模型的插件，旨在提升 ComfyUI 这一开源 AI 艺术创作平台的可用性和效率。该系统通过提供智能节点和模型推荐，以及一键式自动化工作流构建，解决了新手用户在使用 ComfyUI 时面临的挑战。它采用了分层的多代理框架，中央助手代理负责任务分配，而专门的工作代理则处理不同的使用场景。通过离线定量评估和在线用户反馈，我们验证了 ComfyUI-Copilot 的有效性，显示其能够准确推荐节点并加速工作流开发。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02620",
            "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
            "url": "https://huggingface.co/papers/2506.02620",
            "abstract": "FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  \t\t\t\t\tAI-generated summary \t\t\t\t Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.",
            "score": 5,
            "issue_id": 4157,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "296d6abe1e32cfa9",
            "authors": [
                "Dongyu Yan",
                "Leyi Wu",
                "Jiantao Lin",
                "Luozhou Wang",
                "Tianshuo Xu",
                "Zhifei Chen",
                "Zhen Yang",
                "Lie Xu",
                "Shunsi Zhang",
                "Yingcong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "Quwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02620.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "FlexPainter: гибкая и согласованная генерация текстур с мультимодальным управлением",
                    "desc": "FlexPainter - это новый конвейер генерации текстур, использующий общее условное пространство вложений для гибкого мультимодального управления. Он обеспечивает высококачественную и согласованную генерацию карт текстур с помощью диффузионных моделей изображений и 3D-ориентированной модели. FlexPainter решает проблемы ограниченной гибкости управления и несогласованности между сгенерированными многоракурсными изображениями. Система включает метод декомпозиции структурной и стилевой информации, а также модуль синхронизации ракурсов для обеспечения локальной согласованности."
                },
                "en": {
                    "title": "FlexPainter: Revolutionizing Texture Generation with Multi-Modal Guidance",
                    "desc": "FlexPainter is a new pipeline designed for generating high-quality texture maps in 3D modeling. It utilizes a shared conditional embedding space to allow for flexible multi-modal guidance, which helps in producing consistent textures from various input types. By employing an image diffusion prior and a 3D-aware model, it generates multi-view images that maintain local consistency and enhance overall quality. The framework has been shown to outperform existing methods in terms of both flexibility and the quality of the generated textures."
                },
                "zh": {
                    "title": "FlexPainter：灵活高效的纹理生成新方法",
                    "desc": "FlexPainter是一种新型的纹理生成管道，利用共享的条件嵌入空间实现灵活的多模态引导，从而确保高质量和一致性的纹理图生成。该方法结合了图像扩散先验和3D感知模型，解决了传统方法在控制灵活性和提示模态方面的限制。通过构建共享的条件嵌入空间，FlexPainter能够在不同输入模态之间进行灵活聚合，提升生成效果。实验结果表明，FlexPainter在灵活性和生成质量上显著优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05331",
            "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2506.05331",
            "abstract": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT",
            "score": 4,
            "issue_id": 4156,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "b3e1648a048ac6b7",
            "authors": [
                "Xinyan Chen",
                "Renrui Zhang",
                "Dongzhi Jiang",
                "Aojun Zhou",
                "Shilin Yan",
                "Weifeng Lin",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05331.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#math",
                    "#games"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Визуальное рассуждение в математике: новый уровень с MINT-CoT",
                    "desc": "Статья представляет MINT-CoT - новый подход к визуальному рассуждению в математических задачах с использованием цепочки размышлений (Chain-of-Thought). Метод адаптивно внедряет релевантные визуальные токены в текстовые шаги рассуждения, динамически выбирая визуальные области любой формы в математических фигурах. Авторы создали датасет MINT-CoT, содержащий 54 тысячи математических задач, где каждый шаг рассуждения согласован с визуальными областями на уровне токенов. Разработанная модель MINT-CoT-7B значительно превосходит базовые модели на нескольких бенчмарках математического визуального рассуждения."
                },
                "en": {
                    "title": "Enhancing Math Reasoning with Visual Interleaving in LLMs",
                    "desc": "This paper introduces MINT-CoT, a novel approach to enhance mathematical reasoning in Large Language Models (LLMs) by integrating visual information more effectively. The method uses Mathematical Interleaved Tokens to dynamically select and incorporate relevant visual regions into the reasoning process, overcoming limitations of previous models that relied on fixed image regions. The authors created a dataset with 54,000 math problems that align visual tokens with reasoning steps, enabling better training of the model. Experimental results show that MINT-CoT-7B significantly outperforms existing models in various math problem-solving tasks, demonstrating its effectiveness in multimodal reasoning."
                },
                "zh": {
                    "title": "MINT-CoT：数学推理的新突破",
                    "desc": "本论文提出了一种新的方法MINT-CoT，用于在多模态领域中增强大型语言模型的数学推理能力。MINT-CoT通过引入数学交错标记，将相关的视觉信息动态地融入文本推理步骤中，从而解决了传统方法在数学问题解决中的局限性。我们构建了一个包含54K数学问题的数据集，确保每个推理步骤与视觉区域在标记级别上对齐。实验结果表明，MINT-CoT-7B模型在多个数学任务上显著优于基线模型，展示了其在视觉交错推理中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05328",
            "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs",
            "url": "https://huggingface.co/papers/2506.05328",
            "abstract": "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.",
            "score": 4,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "774ccf3fd01aa4ef",
            "authors": [
                "Lidong Lu",
                "Guo Chen",
                "Zhiqi Li",
                "Yicheng Liu",
                "Tong Lu"
            ],
            "affiliations": [
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05328.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#long_context",
                    "#training",
                    "#video",
                    "#rl"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Продвинутый подсчет объектов в видео с помощью мультимодального ИИ",
                    "desc": "Статья представляет новый бенчмарк CG-AV-Counting для задач подсчета объектов в длинных видео с использованием мультимодальных вопросов и аннотированных подсказок. Авторы предлагают модель AV-Reasoner, обученную с помощью обучения с подкреплением и куррикулярного обучения для улучшения способности подсчета. Модель достигает лучших результатов на нескольких бенчмарках, демонстрируя эффективность обучения с подкреплением. Однако эксперименты показывают, что рассуждения в языковом пространстве не приносят улучшений на бенчмарках вне домена обучения."
                },
                "en": {
                    "title": "Enhancing Video Counting with CG-AV-Counting and AV-Reasoner",
                    "desc": "This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts."
                },
                "zh": {
                    "title": "提升视频计数能力的新基准与模型",
                    "desc": "尽管视频理解取得了进展，但当前的多模态学习模型在计数任务上仍然存在困难。现有的基准测试受限于短视频、封闭式查询、缺乏线索注释和多模态覆盖不足。本文介绍了CG-AV-Counting，这是一个手动注释的线索基础计数基准，包含1,027个多模态问题和5,845个注释线索，覆盖497个长视频。我们提出的AV-Reasoner模型通过GRPO和课程学习进行训练，能够从相关任务中推广计数能力，并在多个基准测试中取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05327",
            "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2506.05327",
            "abstract": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss",
            "score": 4,
            "issue_id": 4156,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "374ddd58ce0653c6",
            "authors": [
                "Duochao Shi",
                "Weijie Wang",
                "Donny Y. Chen",
                "Zeyu Zhang",
                "Jia-Wang Bian",
                "Bohan Zhuang",
                "Chunhua Shen"
            ],
            "affiliations": [
                "GigaAI",
                "MBZUAI",
                "Monash University, Australia",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05327.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение 3D-реконструкции с помощью умной регуляризации границ объектов",
                    "desc": "Статья представляет новый метод регуляризации для улучшения качества 3D-реконструкции в системах 3D Gaussian Splatting. Авторы предлагают PM-Loss - функцию потерь, основанную на предсказании карты точек с помощью предобученного трансформера. Этот подход позволяет сгладить геометрические разрывы на границах объектов, которые часто возникают при использовании карт глубины. Применение PM-Loss значительно улучшает качество рендеринга для различных архитектур и сцен."
                },
                "en": {
                    "title": "Enhancing 3D Rendering with PM-Loss for Smooth Depth Representation",
                    "desc": "This paper presents PM-Loss, a new regularization technique designed to enhance the quality of 3D point clouds generated from depth maps in 3D Gaussian Splatting (3DGS) systems. The authors address the common issue of depth discontinuities at object boundaries, which can lead to poor rendering quality due to fragmented point clouds. By utilizing a pointmap predicted by a pre-trained transformer, PM-Loss promotes geometric smoothness, improving the overall accuracy of the depth representation. The results demonstrate that incorporating PM-Loss leads to superior rendering outcomes across different architectures and scenes."
                },
                "zh": {
                    "title": "提升3D渲染质量的新方法",
                    "desc": "本文提出了一种新的正则化损失函数PM-Loss，用于改善基于深度图的3D高斯点云渲染。传统方法在物体边界处的深度不连续性会导致点云稀疏，从而影响渲染质量。PM-Loss利用预训练的变换器预测的点图，尽管其准确性不如深度图，但能有效增强几何平滑性。通过改进深度图，我们的方法在不同架构和场景中显著提升了3D高斯点云的渲染效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03077",
            "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
            "url": "https://huggingface.co/papers/2506.03077",
            "abstract": "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  \t\t\t\t\tAI-generated summary \t\t\t\t Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
            "score": 4,
            "issue_id": 4156,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "14c578e79a6d095c",
            "authors": [
                "Qijun Luo",
                "Mengqi Li",
                "Lei Zhao",
                "Xiao Li"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03077.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "StreamBP: Революция в обучении языковых моделей",
                    "desc": "StreamBP - это эффективный метод обратного распространения ошибки для обучения языковых моделей. Он разлагает правило цепочки вдоль последовательности, что значительно снижает затраты памяти на хранение активаций и логитов. StreamBP позволяет обрабатывать более длинные последовательности и ускоряет обучение по сравнению с методом контрольных точек градиента. Метод применим к различным целевым функциям и эффективно масштабируется на несколько GPU."
                },
                "en": {
                    "title": "StreamBP: Efficient Backpropagation for Long Sequences in Language Models",
                    "desc": "StreamBP is a novel backpropagation method designed to efficiently handle long sequence lengths in language models. By decomposing the chain rule in a layer-wise manner, it significantly reduces the memory required for storing activation values during training. This method not only speeds up the backpropagation process but also allows for longer sequences compared to traditional gradient checkpointing techniques. Additionally, StreamBP can be easily integrated into existing transformer training pipelines and supports multi-GPU setups for enhanced performance."
                },
                "zh": {
                    "title": "StreamBP：高效反向传播，助力长序列训练",
                    "desc": "StreamBP是一种高效的反向传播方法，通过对链式法则进行线性分解，显著降低了内存消耗。这使得在训练语言模型时，可以处理更长的序列并加快训练速度。与传统的梯度检查点技术相比，StreamBP能够将反向传播的最大序列长度提高2.8到5.5倍，同时保持相似或更少的反向传播时间。此外，StreamBP还支持多GPU训练，增强了其在实际应用中的灵活性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04633",
            "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations",
            "url": "https://huggingface.co/papers/2506.04633",
            "abstract": "Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.",
            "score": 3,
            "issue_id": 4156,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "05d0ae7d805419c1",
            "authors": [
                "Linjie Li",
                "Mahtab Bigverdi",
                "Jiawei Gu",
                "Zixian Ma",
                "Yinuo Yang",
                "Ziang Li",
                "Yejin Choi",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Stanford University",
                "Sun Yat-sen University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04633.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "STARE: Новый рубеж в оценке пространственного интеллекта ИИ",
                    "desc": "Статья представляет новый бенчмарк STARE для оценки мультимодальных языковых моделей в задачах пространственного мышления и визуального моделирования. Бенчмарк включает 4000 заданий на геометрические преобразования, пространственное мышление и реальные пространственные задачи. Результаты показывают, что модели хорошо справляются с простыми 2D-преобразованиями, но близки к случайному угадыванию в сложных задачах, требующих многошаговых визуальных симуляций. Люди демонстрируют почти идеальную точность, но тратят значительное время на сложные задачи, существенно ускоряясь при использовании промежуточных визуальных симуляций."
                },
                "en": {
                    "title": "STARE: Bridging the Gap in Spatial Reasoning for AI",
                    "desc": "This paper presents STARE, a new benchmark for evaluating multimodal large language models on spatial reasoning tasks that require visual simulations. Unlike existing benchmarks that focus on verbal reasoning, STARE includes 4,000 tasks involving geometric transformations and real-world spatial challenges. The results show that while models perform well on simple 2D tasks, they struggle with complex 3D tasks that require multi-step reasoning. This indicates that current models have difficulty effectively using visual information to enhance their reasoning capabilities."
                },
                "zh": {
                    "title": "STARE：评估空间推理的新基准",
                    "desc": "空间认知对人类智能至关重要，它使我们能够通过视觉模拟解决问题，而不仅仅依赖语言推理。现有的人工智能基准主要评估语言推理，忽视了非语言多步骤视觉模拟的复杂性。我们提出了STARE（空间变换与推理评估），这是一个旨在严格评估多模态大型语言模型在多步骤视觉模拟任务上的基准。评估结果显示，模型在简单的2D变换上表现良好，但在更复杂的任务上，如3D立方体展开和拼图，表现接近随机，表明模型可能无法有效利用中间视觉信息。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04405",
            "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale",
            "url": "https://huggingface.co/papers/2506.04405",
            "abstract": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.",
            "score": 3,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "2cf822e179634776",
            "authors": [
                "Ran Xu",
                "Yuchen Zhuang",
                "Yishan Zhong",
                "Yue Yu",
                "Xiangru Tang",
                "Hang Wu",
                "May D. Wang",
                "Peifeng Ruan",
                "Donghan Yang",
                "Tao Wang",
                "Guanghua Xiao",
                "Carl Yang",
                "Yang Xie",
                "Wenqi Shi"
            ],
            "affiliations": [
                "Emory University",
                "Georgia Tech",
                "UT Southwestern Medical Center",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04405.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#training",
                    "#rl"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "MedAgentGYM: Революция в обучении ИИ для медицинского кодирования",
                    "desc": "MedAgentGYM - это новая среда обучения для улучшения навыков медицинского рассуждения у агентов на основе больших языковых моделей (LLM). Она включает более 72 тысяч задач из 129 категорий, основанных на реальных биомедицинских сценариях. Задачи представлены в виде исполняемых кодовых сред с подробными описаниями, интерактивной обратной связью и верифицируемыми аннотациями. Используя MedAgentGYM, модель Med-Copilot-7B достигла значительного улучшения производительности через тонкую настройку и обучение с подкреплением."
                },
                "en": {
                    "title": "Empowering Medical Reasoning in LLMs with MedAgentGYM",
                    "desc": "MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o."
                },
                "zh": {
                    "title": "MedAgentGYM：提升医学推理能力的创新平台",
                    "desc": "我们介绍了MedAgentGYM，这是第一个公开可用的训练环境，旨在增强大型语言模型（LLM）代理的基于编码的医学推理能力。MedAgentGYM包含72,413个任务实例，涵盖129个类别，来源于真实的生物医学场景。每个任务都在可执行的编码环境中封装，提供详细的任务描述、互动反馈机制、可验证的真实注释和可扩展的训练轨迹生成。通过对30多种LLM的广泛基准测试，发现商业API模型与开源模型之间存在显著的性能差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00830",
            "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers",
            "url": "https://huggingface.co/papers/2506.00830",
            "abstract": "SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.",
            "score": 2,
            "issue_id": 4157,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 июня",
                "en": "June 1",
                "zh": "6月1日"
            },
            "hash": "7a04cf6593bac4b3",
            "authors": [
                "Zhengcong Fei",
                "Hao Jiang",
                "Di Qiu",
                "Baoxuan Gu",
                "Youqiang Zhang",
                "Jiahua Wang",
                "Jialin Bai",
                "Debang Li",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00830.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#audio",
                    "#diffusion",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Революция в синтезе говорящих портретов: от аудио к реалистичному видео",
                    "desc": "SkyReels-Audio - это унифицированная система для создания высококачественных видео с говорящими портретами на основе аудио. Она использует предобученные видео-диффузионные трансформеры и поддерживает генерацию видео бесконечной длины с разнообразными условиями. В системе применяется гибридная стратегия куррикулярного обучения для постепенного согласования аудио и движений лица. Для улучшения локальной согласованности лица вводятся специальные функции потерь и механизм аудио-управляемого безклассового наведения."
                },
                "en": {
                    "title": "Revolutionizing Talking Portraits with SkyReels-Audio",
                    "desc": "SkyReels-Audio is a novel framework that generates high-quality talking portrait videos by using pretrained video diffusion transformers. It allows for the creation and editing of videos based on audio, text, and images, making it versatile for various multimodal inputs. The framework employs a hybrid curriculum learning strategy to ensure that the audio aligns well with facial movements, enhancing the control over video sequences. Additionally, it introduces advanced loss mechanisms to improve facial coherence and uses a sliding-window denoising technique to maintain visual quality and consistency over time."
                },
                "zh": {
                    "title": "SkyReels-Audio：音频驱动的高保真说话肖像生成",
                    "desc": "SkyReels-Audio 是一个统一框架，利用预训练的视频扩散变换器生成高保真且连贯的音频条件下的说话肖像视频。该框架支持无限长度的生成和编辑，并通过多模态输入实现多样化和可控的条件设置。我们采用混合课程学习策略，逐步对齐音频与面部运动，从而实现对长视频序列的精细控制。通过引入面部掩膜损失和音频引导的无分类器指导机制，SkyReels-Audio 在复杂条件下展现出卓越的唇同步精度和身份一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04245",
            "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.04245",
            "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.",
            "score": 2,
            "issue_id": 4155,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "8716d3ca53b1d58f",
            "authors": [
                "Guangchen Lan",
                "Huseyin A. Inan",
                "Sahar Abdelnabi",
                "Janardhan Kulkarni",
                "Lukas Wutschitz",
                "Reza Shokri",
                "Christopher G. Brinton",
                "Robert Sim"
            ],
            "affiliations": [
                "Microsoft",
                "National University of Singapore",
                "Purdue University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04245.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#transfer_learning",
                    "#rl",
                    "#leakage"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Разумное раскрытие информации: обучение ИИ-агентов контекстной целостности",
                    "desc": "Статья посвящена проблеме контекстной целостности (CI) в эпоху автономных агентов, принимающих решения за пользователей. Авторы предлагают метод, использующий языковые модели (LLM) и обучение с подкреплением (RL) для обучения агентов рассуждать о контексте и принимать решения о раскрытии информации. Эксперименты на синтетическом наборе данных показали значительное снижение неуместного раскрытия информации при сохранении производительности задач. Улучшения переносятся на реальные бенчмарки CI, такие как PrivacyLens."
                },
                "en": {
                    "title": "Enhancing Contextual Integrity in Autonomous Agents",
                    "desc": "This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks."
                },
                "zh": {
                    "title": "确保上下文完整性，提升自主代理决策能力",
                    "desc": "在自主代理为用户做决策的时代，确保上下文完整性（CI）成为一个重要问题。本文提出，CI需要代理在执行任务时对其操作的上下文进行推理。我们首先让大型语言模型（LLMs）明确推理CI，以决定披露哪些信息。接着，我们开发了一个强化学习（RL）框架，进一步增强模型进行CI所需的推理能力，实验结果表明，该方法显著减少了不当信息披露，同时保持了任务性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20914",
            "title": "Geometry-Editable and Appearance-Preserving Object Compositon",
            "url": "https://huggingface.co/papers/2505.20914",
            "abstract": "General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.",
            "score": 2,
            "issue_id": 4156,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "f95a4c2b427959d4",
            "authors": [
                "Jianman Lin",
                "Haojie Li",
                "Chunmei Qing",
                "Zhijing Yang",
                "Liang Lin",
                "Tianshui Chen"
            ],
            "affiliations": [
                "Guangdong University of Technology",
                "South China University of Technology",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20914.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точное редактирование геометрии и сохранение деталей в композиции объектов",
                    "desc": "Статья представляет новую модель DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) для композиции объектов в сцене. DGAD использует семантические эмбеддинги для управления геометрией объекта и механизм кросс-внимания для сохранения деталей внешнего вида. Модель интегрирует семантические эмбеддинги в предобученные диффузионные модели для гибкого манипулирования геометрией объекта. DGAD применяет плотный механизм кросс-внимания для извлечения и пространственного выравнивания признаков внешнего вида с соответствующими регионами."
                },
                "en": {
                    "title": "Seamless Object Integration with Geometry and Appearance Preservation",
                    "desc": "The paper presents a new model called Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) for integrating objects into backgrounds while maintaining their detailed appearance. DGAD uses semantic embeddings to understand the desired geometric changes and a cross-attention mechanism to align fine-grained appearance features with these changes. This approach allows for precise editing of object geometry without losing the intricate details of the object's appearance. The model builds on existing diffusion techniques and shows improved performance in object composition tasks through extensive experiments."
                },
                "zh": {
                    "title": "解耦几何与外观保留的物体组合新方法",
                    "desc": "一般物体组合（GOC）旨在将目标物体无缝地融入背景场景中，同时保持其细致的外观细节。现有方法通过语义嵌入与先进的扩散模型结合，实现几何可编辑的生成。然而，这些紧凑的嵌入仅编码高层语义信息，难以保留细致的外观细节。我们提出了一种解耦几何可编辑和外观保留的扩散模型（DGAD），通过语义嵌入捕捉几何变换，并利用交叉注意力机制对齐外观特征，从而实现精确的几何编辑和真实的外观保留。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05282",
            "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
            "url": "https://huggingface.co/papers/2506.05282",
            "abstract": "We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.",
            "score": 1,
            "issue_id": 4156,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "c7d3c7ca688358d9",
            "authors": [
                "Tao Sun",
                "Liyuan Zhu",
                "Shengyu Huang",
                "Shuran Song",
                "Iro Armeni"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05282.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Единый подход к регистрации облаков точек и сборке форм",
                    "desc": "Представлен метод Rectified Point Flow, который объединяет регистрацию облаков точек и сборку многокомпонентных форм в единую условную генеративную задачу. Метод обучает непрерывное поточечное поле скоростей, которое перемещает зашумленные точки к целевым позициям. В отличие от предыдущих подходов, данный метод изначально учитывает симметрии при сборке без явной разметки. Вместе с самоконтролируемым энкодером, фокусирующимся на перекрывающихся точках, метод достигает нового уровня производительности на шести эталонных наборах данных."
                },
                "en": {
                    "title": "Unified Learning for Point Cloud Registration and Shape Assembly",
                    "desc": "This paper presents Rectified Point Flow, a novel approach that combines point cloud registration and multi-part shape assembly into a single generative framework. The method learns a continuous velocity field that aligns noisy point clouds to their target configurations, allowing for the recovery of part poses. Unlike previous methods that require manual symmetry handling, this approach automatically learns assembly symmetries without needing explicit labels. By utilizing a self-supervised encoder for overlapping points, the method achieves state-of-the-art results across multiple benchmarks, enhancing accuracy through joint training on varied datasets."
                },
                "zh": {
                    "title": "统一点云配准与形状组装的创新方法",
                    "desc": "我们提出了修正点流（Rectified Point Flow），这是一种统一的参数化方法，将成对点云配准和多部件形状组装视为一个单一的条件生成问题。该方法在没有姿态信息的情况下，学习一个连续的点位速度场，将噪声点移动到目标位置，并从中恢复部件姿态。与之前的工作不同，我们的方法能够在没有对称标签的情况下，自然地学习组装对称性。通过专注于重叠点的自监督编码器，我们的方法在六个基准测试中实现了新的最先进性能，促进了共享几何先验的学习，从而提高了准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04734",
            "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design",
            "url": "https://huggingface.co/papers/2506.04734",
            "abstract": "Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.",
            "score": 1,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "ce810e0cc38b26e4",
            "authors": [
                "Lin Sun",
                "Weihong Lin",
                "Jinzhu Wu",
                "Yongfu Zhu",
                "Xiaoqi Jian",
                "Guangxiang Zhao",
                "Change Jia",
                "Linglin Zhang",
                "Sai-er Hu",
                "Yuhan Wu",
                "Xiangzheng Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.04734.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#math"
                ],
                "emoji": "🎢",
                "ru": {
                    "title": "Нестабильность оценок: вызов для бенчмаркинга языковых моделей",
                    "desc": "Исследование показывает, что результаты оценки моделей серии Deepseek-R1-Distill подвержены значительным колебаниям из-за различных факторов. Небольшие изменения в условиях оценки могут привести к существенным различиям в результатах. Аналогичные явления наблюдаются и в других моделях, основанных на Deepseek-R1-Distill, а также в модели QwQ-32B. Авторы призывают к созданию более строгой парадигмы оценки производительности моделей машинного обучения."
                },
                "en": {
                    "title": "Ensuring Reliable Evaluations for Deep Learning Models",
                    "desc": "The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results."
                },
                "zh": {
                    "title": "建立更严格的模型评估标准",
                    "desc": "Deepseek-R1-Distill系列模型在数学、科学和编程等领域表现出色，受到开源社区的广泛采用。然而，我们的研究发现，这些模型的基准评估结果受到多种因素的显著波动影响。评估条件的细微差异可能导致结果的重大变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型中，因此我们呼吁建立更严格的模型性能评估范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03238",
            "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach",
            "url": "https://huggingface.co/papers/2506.03238",
            "abstract": "OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.",
            "score": 1,
            "issue_id": 4155,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "f234199601bef528",
            "authors": [
                "Ziheng Zhao",
                "Lisong Dai",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Department of Radiology, Renmin Hospital of Wuhan University",
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03238.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#healthcare",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "ИИ-революция в интерпретации КТ-снимков",
                    "desc": "Модель OminiAbnorm-CT предназначена для автоматизированной интерпретации КТ-изображений. Она превосходит существующие методы в локализации и описании аномалий в различных областях тела с использованием текстовых запросов и визуальных подсказок. Модель основана на всеобъемлющей иерархической системе классификации, разработанной совместно с опытными радиологами. OminiAbnorm-CT обучена на большом наборе данных КТ-изображений с тщательно размеченными аномалиями."
                },
                "en": {
                    "title": "Revolutionizing CT Image Analysis with OminiAbnorm-CT",
                    "desc": "OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology."
                },
                "zh": {
                    "title": "OminiAbnorm-CT：CT图像异常自动解读的新突破",
                    "desc": "OminiAbnorm-CT是一种用于自动解读CT图像的模型，能够在不同身体部位中定位和描述异常情况。该研究通过与资深放射科医生合作，提出了一个包含404种异常发现的层次分类系统。我们还贡献了一个包含超过14.5K CT图像的数据集，并为超过19K异常提供了详细的注释。通过大量实验，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-06-05.html",
    "link_next": "2025-06-09.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "05.06",
        "en": "06/05",
        "zh": "6月5日"
    },
    "short_date_next": {
        "ru": "09.06",
        "en": "06/09",
        "zh": "6月9日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 2,
        "#benchmark": 15,
        "#agents": 2,
        "#cv": 6,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 5,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 10,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "我们开源了两个强大的视觉-语言模型，MiMo-VL-7B-SFT和MiMo-VL-7B-RL，它们在视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在35个任务中超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数量高达78B的模型。在GUI应用中，它在OSWorld-G上得分56.1，甚至超过了专门模型UI-TARS。我们的训练结合了四阶段预训练和混合在策略强化学习。我们还提供了一个包含50多个任务的全面评估套件，以促进可重复性和推动领域发展。模型检查点和完整评估套件可在https://github.com/XiaomiMiMo/MiMo-VL获取。",
        "title": "MiMo-VL Technical Report",
        "pinyin": "Wǒmen kāiyuánle liǎng gè qiángdà de shìjué-yǔyán móxíng, MiMo-VL-7B-SFT hé MiMo-VL-7B-RL, tāmen zài shìjué lǐjiě hé duō móshì tuīlǐ fāngmiàn biǎoxiàn chūsè. MiMo-VL-7B-RL zài 35 gè rènwù zhōng chāoyuèle Qwen2.5-VL-7B, bìng zài OlympiadBench shàng défēn 59.4, chāoguòle cānshù liàng gāodá 78B de móxíng. Zài GUI yìngyòng zhōng, tā zài OSWorld-G shàng défēn 56.1, shènzhì chāoguòle zhuānmén móxíng UI-TARS. Wǒmen de xùnliàn jiéhéle sì jiēduàn yùxùnliàn hé hùnhé zài cèlüè qiángxuéxué. Wǒmen hái tígōngle yīgè bāohán 50 duō gè rènwù de quánmiàn pínggǔ tàojiàn, yǐ cùjìn kě chóngfùxìng hé tuīdòng lǐngyù fāzhǎn. Móxíng jiǎnchá diǎn hé wánzhěng pínggǔ tàojiàn kě zài https://github.com/XiaomiMiMo/MiMo-VL huòqǔ.",
        "vocab": "[\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open source\"},\n    {\"word\": \"视觉-语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"表现出色\", \"pinyin\": \"biǎo xiàn chū sè\", \"trans\": \"perform excellently\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"参数量\", \"pinyin\": \"cān shǔ liàng\", \"trans\": \"parameter quantity\"},\n    {\"word\": \"GUI\", \"pinyin\": \"GUI\", \"trans\": \"Graphical User Interface\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"专门\", \"pinyin\": \"zhuān mén\", \"trans\": \"specialized\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xué xí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"套件\", \"pinyin\": \"tào jiàn\", \"trans\": \"suite\"},\n    {\"word\": \"可重复性\", \"pinyin\": \"kě chóng fù xìng\", \"trans\": \"reproducibility\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"检查点\", \"pinyin\": \"jiǎn chá diǎn\", \"trans\": \"checkpoint\"}\n]",
        "trans": "We have open-sourced two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 tasks and scores 59.4 on the OlympiadBench, surpassing models with up to 78B parameters. In GUI applications, it scores 56.1 on OSWorld-G, even outperforming the specialized model UI-TARS. Our training combines four-stage pretraining and hybrid on-policy reinforcement learning. We also provide a comprehensive evaluation suite with over 50 tasks to promote reproducibility and advance the field. Model checkpoints and the complete evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
        "update_ts": "2025-06-05 09:13"
    }
}