{
    "date": {
        "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 20",
        "zh": "3æœˆ20æ—¥"
    },
    "time_utc": "2025-03-20 06:15",
    "weekday": 3,
    "issue_id": 2803,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.15485",
            "title": "TULIP: Towards Unified Language-Image Pretraining",
            "url": "https://huggingface.co/papers/2503.15485",
            "abstract": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io",
            "score": 14,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "d4b870742a020d5a",
            "authors": [
                "Zineng Tang",
                "Long Lian",
                "Seun Eisape",
                "XuDong Wang",
                "Roei Herzig",
                "Adam Yala",
                "Alane Suhr",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15485.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#architecture",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸŒ·",
                "ru": {
                    "title": "TULIP: Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TULIP - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. TULIP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. TULIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "TULIP: Bridging Vision and Language for Enhanced Image Understanding",
                    "desc": "This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks."
                },
                "zh": {
                    "title": "TULIPï¼šæå‡å›¾åƒç†è§£çš„æ–°æ–¹æ³•",
                    "desc": "å°½ç®¡åƒCLIPå’ŒSigLIPè¿™æ ·çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¯”æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨éœ€è¦é«˜ä¿çœŸå›¾åƒç†è§£çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†TULIPï¼Œè¿™æ˜¯ä¸€ç§å¼€æºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ•°æ®å¢å¼ºå’Œå¯¹æ¯”å­¦ä¹ æ¥æé«˜å›¾åƒç†è§£èƒ½åŠ›ã€‚TULIPèƒ½å¤Ÿå­¦ä¹ ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒå…¨å±€è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTULIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é›¶-shotæ€§èƒ½å’Œå°‘-shotåˆ†ç±»çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15475",
            "title": "Cube: A Roblox View of 3D Intelligence",
            "url": "https://huggingface.co/papers/2503.15475",
            "abstract": "Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.",
            "score": 13,
            "issue_id": 2800,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "89037dc780448ff8",
            "authors": [
                "Foundation AI Team",
                "Kiran Bhat",
                "Nishchaie Khanna",
                "Karun Channa",
                "Tinghui Zhou",
                "Yiheng Zhu",
                "Xiaoxia Sun",
                "Charles Shang",
                "Anirudh Sudarshan",
                "Maurice Chu",
                "Daiqing Li",
                "Kangle Deng",
                "Jean-Philippe Fauconnier",
                "Tijmen Verhulsdonck",
                "Maneesh Agrawala",
                "Kayvon Fatahalian",
                "Alexander Weiss",
                "Christian Reiser",
                "Ravi Kiran Chirravuri",
                "Ravali Kandur",
                "Alejandro Pelaez",
                "Akash Garg",
                "Michael Palleschi",
                "Jessica Wang",
                "Skylar Litz",
                "Leon Liu",
                "Anying Li",
                "David Harmon",
                "Derek Liu",
                "Liangjun Feng",
                "Denis Goupil",
                "Lukas Kuczynski",
                "Jihyun Yoon",
                "Naveen Marri",
                "Peiye Zhuang",
                "Yinan Zhang",
                "Brian Yin",
                "Haomiao Jiang",
                "Marcel van Workum",
                "Thomas Lane",
                "Bryce Erickson",
                "Salil Pathare",
                "Kyle Price",
                "Anupam Singh",
                "David Baszucki"
            ],
            "affiliations": [
                "Foundation AI team, Roblox",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15475.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ 3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Roblox. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Building the Future of 3D Intelligence in Roblox",
                    "desc": "This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation."
                },
                "zh": {
                    "title": "æ„å»º3Dæ™ºèƒ½çš„åŸºç¡€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä¸º3Dæ™ºèƒ½æ„å»ºåŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ”¯æŒå¼€å‘è€…åœ¨Robloxå¹³å°ä¸Šç”Ÿæˆ3Då¯¹è±¡ã€åœºæ™¯å’ŒåŠ¨ç”»è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®è®¾è®¡è¦æ±‚ï¼Œå¹¶ä»‹ç»äº†æ„å»º3Då½¢çŠ¶æ ‡è®°å™¨çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ ‡è®°åŒ–æ–¹æ¡ˆå¯ä»¥åº”ç”¨äºæ–‡æœ¬åˆ°å½¢çŠ¶ç”Ÿæˆã€å½¢çŠ¶åˆ°æ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬åˆ°åœºæ™¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä¸ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åä½œï¼Œä»¥å®ç°åœºæ™¯åˆ†æå’Œæ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15417",
            "title": "Temporal Regularization Makes Your Video Generator Stronger",
            "url": "https://huggingface.co/papers/2503.15417",
            "abstract": "Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.",
            "score": 12,
            "issue_id": 2801,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "8eb262eda880d162",
            "authors": [
                "Harold Haodong Chen",
                "Haojian Huang",
                "Xianfeng Wu",
                "Yexin Liu",
                "Yajing Bai",
                "Wen-Jie Shu",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "HKU",
                "HKUST",
                "UCF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15417.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "FluxFlow: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FluxFlow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FluxFlow Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing Video Generation with Temporal Augmentation",
                    "desc": "This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´è´¨é‡é—®é¢˜ï¼Œå¼ºè°ƒäº†åœ¨å¸§ä¹‹é—´ä¿æŒä¸€è‡´è¿åŠ¨å’ŒçœŸå®åŠ¨æ€çš„é‡è¦æ€§ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†æ—¶é—´å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†FluxFlowç­–ç•¥ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡ã€‚FluxFlowåœ¨æ•°æ®å±‚é¢è¿›è¡Œæ“ä½œï¼Œé€šè¿‡æ§åˆ¶æ—¶é—´æ‰°åŠ¨æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFluxFlowåœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸Šæ˜¾è‘—æ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç©ºé—´ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14868",
            "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
            "url": "https://huggingface.co/papers/2503.14868",
            "abstract": "Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to 8.2times.",
            "score": 12,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "8555fb94242b412c",
            "authors": [
                "Hoigi Seo",
                "Wongi Jeong",
                "Kyungryeol Lee",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, INMC & IPAI Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14868.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºÑ€Ğ°ĞµĞ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Subspace Gradient', Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 'Partial Uniform Timestep Sampling' Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Personalization of Diffusion Models with Subspace Gradient",
                    "desc": "This paper presents a method to improve the efficiency of training diffusion models for image synthesis, particularly for personalization on edge devices. It introduces a quantization technique that allows for fine-tuning without the need for dequantization, thus saving memory during gradient computation. The authors propose a novel approach called Subspace Gradient to reduce noise in gradient estimation by utilizing historical data from personalization tokens. Additionally, they explore the impact of text embeddings on image generation, leading to a new sampling method that optimizes diffusion timesteps, achieving significant memory savings while maintaining performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒå’Œå¾®è°ƒéœ€è¦å¤§é‡è®¡ç®—å’Œå†…å­˜èµ„æºã€‚å°½ç®¡å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯å¯ä»¥å‡å°‘æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨ï¼Œä½†è®­ç»ƒè¿™äº›é‡åŒ–æ¨¡å‹ä»ç„¶éœ€è¦å¤§é‡å†…å­˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ–‡æœ¬åæ¼”å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–åœ¨ä¸å»é‡åŒ–çš„æƒ…å†µä¸‹è¿›è¡Œä¸ªæ€§åŒ–å¾®è°ƒï¼Œä»è€Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†ä¸Šä¸ä¹‹å‰çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å°†è®­ç»ƒå†…å­˜éœ€æ±‚é™ä½äº†å¤šè¾¾8.2å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15265",
            "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2503.15265",
            "abstract": "Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/",
            "score": 9,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "37236f5315cc8aef",
            "authors": [
                "Ruowen Zhao",
                "Junliang Ye",
                "Zhengyi Wang",
                "Guangce Liu",
                "Yiwen Chen",
                "Yikai Wang",
                "Jun Zhu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "ShengShu",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15265.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#alignment",
                    "#rlhf",
                    "#rl",
                    "#3d"
                ],
                "emoji": "ğŸ”·",
                "ru": {
                    "title": "DeepMesh: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DeepMesh - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DeepMesh Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ÑĞµÑ‚Ğ¾Ğº, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Direct Preference Optimization. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµÑ‚ĞºĞ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ."
                },
                "en": {
                    "title": "DeepMesh: Elevating 3D Mesh Generation with Human-Centric Learning",
                    "desc": "This paper introduces DeepMesh, a new framework for generating 3D triangle meshes that enhances both quality and precision. It utilizes a unique pre-training strategy with an innovative tokenization method, improving how data is curated and processed. Additionally, it incorporates Reinforcement Learning to align mesh generation with human preferences through Direct Preference Optimization. By conditioning on point clouds and images, DeepMesh produces detailed and accurately structured meshes, surpassing existing methods in performance."
                },
                "zh": {
                    "title": "DeepMeshï¼šä¼˜åŒ–3Dç½‘æ ¼ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "ä¸‰è§’ç½‘æ ¼åœ¨3Dåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œæ“ä½œå’Œæ¸²æŸ“ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•é€šè¿‡é¢„æµ‹ç¦»æ•£çš„é¡¶ç‚¹æ ‡è®°ç”Ÿæˆç»“æ„åŒ–ç½‘æ ¼ï¼Œä½†å¸¸å¸¸å—åˆ°é¢æ•°é™åˆ¶å’Œç½‘æ ¼ä¸å®Œæ•´æ€§çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepMeshæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°æ¥ä¼˜åŒ–ç½‘æ ¼ç”Ÿæˆï¼šä¸€ç§é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥å’Œå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥3Dç½‘æ ¼ç”Ÿæˆã€‚DeepMeshèƒ½å¤Ÿç”Ÿæˆç»†èŠ‚ä¸°å¯Œã€æ‹“æ‰‘ç²¾ç¡®çš„ç½‘æ ¼ï¼Œä¸”åœ¨ç²¾åº¦å’Œè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12532",
            "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
            "url": "https://huggingface.co/papers/2503.12532",
            "abstract": "Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.",
            "score": 6,
            "issue_id": 2800,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "185728a70d3b80d0",
            "authors": [
                "Fanbin Lu",
                "Zhisheng Zhong",
                "Ziqin Wei",
                "Shu Liu",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12532.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#agents",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "STEVE: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ STEVE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞšĞ°Ğ½ĞµĞ¼Ğ°Ğ½Ğ°-Ğ¢Ğ²ĞµÑ€ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°."
                },
                "en": {
                    "title": "STEVE: Optimizing AI Agents for GUI Manipulation Efficiently",
                    "desc": "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä»£ç†è®­ç»ƒçš„æ–°çªç ´ï¼šSTEVE",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSTEVEçš„æ­¥éª¤éªŒè¯ç®¡é“ï¼Œç”¨äºè®­ç»ƒè®¡ç®—æœºä½¿ç”¨ä»£ç†ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¤§å‹æŒ‡ä»¤é›†ï¼Œå¹¶æ”¶é›†äº†ä¸€äº›æ¬¡ä¼˜ä»£ç†çš„è½¨è¿¹æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨GPT-4oéªŒè¯æ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶ä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…äºŒå…ƒæ ‡ç­¾ï¼Œæœ€åé‡‡ç”¨å¡å°¼æ›¼å’Œç‰¹æ²ƒæ–¯ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–ä»£ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤æ‚çš„æ¡Œé¢ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ•ˆç‡é«˜ã€æˆæœ¬ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12769",
            "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
            "url": "https://huggingface.co/papers/2503.12769",
            "abstract": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.",
            "score": 4,
            "issue_id": 2800,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "0913c8e386f3aae5",
            "authors": [
                "Shenghao Fu",
                "Qize Yang",
                "Yuan-Ming Li",
                "Yi-Xing Peng",
                "Kun-Yu Lin",
                "Xihan Wei",
                "Jian-Fang Hu",
                "Xiaohua Xie",
                "Wei-Shi Zheng"
            ],
            "affiliations": [
                "Guangdong Province Key Laboratory of Information Security Technology, China",
                "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
                "Pazhou Laboratory (Huangpu), China",
                "Peng Cheng Laboratory, China",
                "School of Computer Science and Engineering, Sun Yat-sen University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12769.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#agents"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ViSpeak: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ' Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViSpeak, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸Ñ… Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViSpeak-Instruct Ğ¸ ViSpeak-Bench. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ViSpeak Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ GPT-4 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing User-Agent Interaction through Visual Instruction Feedback",
                    "desc": "This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions."
                },
                "zh": {
                    "title": "æå‡ç”¨æˆ·ä¸ä»£ç†äº’åŠ¨çš„è§†è§‰æŒ‡ä»¤åé¦ˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæµåª’ä½“è§†é¢‘ç†è§£ç”±äºå…¶æ—¶é—´æ•æ„Ÿæ€§ã€å…¨æ¨¡æ€å’Œäº¤äº’ç‰¹æ€§ï¼Œå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†è§‰æŒ‡ä»¤åé¦ˆï¼Œæ¨¡å‹éœ€è¦ç†è§£è§†è§‰å†…å®¹å¹¶ä»ä¸­æå–æŒ‡ä»¤ï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä¸ä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸ƒä¸ªä¸è§†è§‰æ¨¡æ€é«˜åº¦ç›¸å…³çš„å­ä»»åŠ¡ï¼Œå¹¶æ”¶é›†äº†ViSpeak-Instructæ•°æ®é›†ç”¨äºè®­ç»ƒï¼ŒViSpeak-Benchç”¨äºè¯„ä¼°ï¼ŒåŒæ—¶æå‡ºäº†ViSpeakæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æµåª’ä½“è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14505",
            "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
            "url": "https://huggingface.co/papers/2503.14505",
            "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.",
            "score": 2,
            "issue_id": 2801,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "61ef56ac402491c0",
            "authors": [
                "Susung Hong",
                "Ira Kemelmacher-Shlizerman",
                "Brian Curless",
                "Steven M. Seitz"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.14505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ’ƒ",
                "ru": {
                    "title": "Ğ¢Ğ°Ğ½Ñ†ÑƒĞ¹ Ğ¿Ğ¾Ğ´ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ: Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¸Ñ‚Ğ¼Ğµ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸",
                    "desc": "MusicInfuser - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¾Ğ¹. ĞĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, MusicInfuser Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ½Ñ†ĞµĞ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM."
                },
                "en": {
                    "title": "Syncing Dance with Music: Introducing MusicInfuser",
                    "desc": "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."
                },
                "zh": {
                    "title": "éŸ³ä¹ä¸èˆè¹ˆçš„å®Œç¾èåˆ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MusicInfuserï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸æŒ‡å®šçš„éŸ³ä¹è½¨é“åŒæ­¥ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥è½»é‡çº§çš„éŸ³ä¹-è§†é¢‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œä½ç§©é€‚é…å™¨ï¼Œå±•ç¤ºäº†å¦‚ä½•è°ƒæ•´ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”éŸ³ä¹è¾“å…¥ã€‚ä¸ä¹‹å‰éœ€è¦è¿åŠ¨æ•æ‰æ•°æ®çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨èˆè¹ˆè§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒã€‚MusicInfuseråœ¨ä¿æŒåº•å±‚æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15055",
            "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
            "url": "https://huggingface.co/papers/2503.15055",
            "abstract": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework for generating high-quality synthetic training data in specialized domains. While Large Language Models (LLMs) have shown impressive general capabilities, their performance in specialized domains like cybersecurity remains limited by the scarcity of domain-specific training data. ELTEX addresses this challenge by systematically integrating explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge throughout the generation process. We demonstrate ELTEX's effectiveness in the context of blockchain-related cyberattack detection, where we fine-tune Gemma-2B using various combinations of real and ELTEX-generated data. Our results show that the ELTEX-enhanced model achieves performance competitive with GPT-4 across both standard classification metrics and uncertainty calibration, while requiring significantly fewer computational resources. We release a curated synthetic dataset of social media texts for cyberattack detection in blockchain. Our work demonstrates that domain-driven synthetic data generation can effectively bridge the performance gap between resource-efficient models and larger architectures in specialized domains.",
            "score": 0,
            "issue_id": 2802,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "785ffad4856eb286",
            "authors": [
                "Arina Razmyslovich",
                "Kseniia Murasheva",
                "Sofia Sedlova",
                "Julien Capitaine",
                "Eugene Dmitriev"
            ],
            "affiliations": [
                "Distributed Networks Institute (DNI)",
                "Technologies MÃ©sozoÃ¯ques"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15055.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ELTEX: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¯Ğœ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…",
                    "desc": "ELTEX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ELTEX Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ°Ğº Ğ² Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma-2B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ GPT-4. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Data Gap in Cybersecurity with ELTEX",
                    "desc": "The paper introduces ELTEX, a framework designed to create high-quality synthetic training data specifically for specialized fields like cybersecurity. It tackles the issue of limited domain-specific data that affects the performance of Large Language Models (LLMs) in these areas. By combining domain indicator extraction with dynamic prompting, ELTEX ensures that essential domain knowledge is maintained during data generation. The framework is validated through its application in blockchain cyberattack detection, showing that it can enhance model performance while being more resource-efficient than larger models like GPT-4."
                },
                "zh": {
                    "title": "é¢†åŸŸé©±åŠ¨çš„åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "ELTEXï¼ˆé«˜æ•ˆLLMä»¤ç‰Œæå–ï¼‰æ˜¯ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šé¢†åŸŸç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®çš„æ¡†æ¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç½‘ç»œå®‰å…¨ç­‰ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°å—åˆ°é¢†åŸŸç‰¹å®šè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ELTEXé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆæ˜¾å¼é¢†åŸŸæŒ‡ç¤ºç¬¦æå–å’ŒåŠ¨æ€æç¤ºï¼Œç¡®ä¿åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™å…³é”®çš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒELTEXå¢å¼ºçš„æ¨¡å‹åœ¨åŒºå—é“¾ç›¸å…³çš„ç½‘ç»œæ”»å‡»æ£€æµ‹ä¸­ï¼Œæ€§èƒ½ä¸GPT-4ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-19.html",
    "link_next": "2025-03-21.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.03",
        "en": "03/21",
        "zh": "3æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† RWKV-7 \"Goose\"ï¼Œä¸€ç§æ–°çš„åºåˆ—å»ºæ¨¡æ¶æ„ã€‚å®ƒåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œå°½ç®¡è®­ç»ƒçš„ä»¤ç‰Œæ•°é‡å°‘ã€‚RWKV-7 æ¨¡å‹æ¯ä¸ªä»¤ç‰Œçš„å†…å­˜å’Œæ¨ç†æ—¶é—´æ˜¯å¸¸æ•°ã€‚å®ƒå¼•å…¥äº†æ–°çš„ delta è§„åˆ™å’Œæ”¾æ¾çš„å€¼æ›¿æ¢è§„åˆ™ã€‚æˆ‘ä»¬å±•ç¤ºäº† RWKV-7 çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶å‘å¸ƒäº†æ¨¡å‹å’Œæ•°æ®é›†ã€‚",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "pinyin": "WÇ’men jiÃ¨shÃ o le RWKV-7 \"Goose\", yÄ« zhÇ’ng xÄ«n de xÃ¹liÃ¨ jiÃ nmÃ³ jiÃ gÃ²u. TÄ zÃ i duÅyÇ”yÃ¡n rÃ¨nwÃ¹ shÃ ng dÃ¡dÃ o le xÄ«n de zuÃ¬jiÄ xÃ¬ngnÃ©ng, jÇnguÇn xÃ¹nliÃ n de lÃ¬ngpÃ¡i shÃ¹liÃ ng shÇo. RWKV-7 mÃ³xÃ­ng mÄ›i ge lÃ¬ngpÃ¡i de nÃ¨icÃºn hÃ© tuÄ«lÇ shÃ­jiÄn shÃ¬ chÃ¡ngshÃ¹. TÄ yÇnrÃ¹ le xÄ«n de delta guÄ«zÃ© hÃ© fÃ ngsÅng de zhÃ­ tÃ¬huÃ n guÄ«zÃ©. WÇ’men zhÇnshÃ¬ le RWKV-7 de yÇ”yÃ¡n jiÃ nmÃ³ nÃ©nglÃ¬, bÃ¬ng fÄbÃ¹ le mÃ³xÃ­ng hÃ© shÃ¹jÃ¹jÃ­.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ n mÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å¤šè¯­è¨€\", \"pinyin\": \"duÅ yÇ” yÃ¡n\", \"trans\": \"multilingual\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"è¾¾åˆ°\", \"pinyin\": \"dÃ¡ dÃ o\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€ä½³\", \"pinyin\": \"zuÃ¬ jiÄ\", \"trans\": \"best\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å°½ç®¡\", \"pinyin\": \"jÃ¬n guÇn\", \"trans\": \"although\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"ä»¤ç‰Œ\", \"pinyin\": \"lÃ¬ng pÃ i\", \"trans\": \"token\"},\n    {\"word\": \"æ•°é‡\", \"pinyin\": \"shÃ¹ liÃ ng\", \"trans\": \"quantity\"},\n    {\"word\": \"å°‘\", \"pinyin\": \"shÇo\", \"trans\": \"few\"},\n    {\"word\": \"å†…å­˜\", \"pinyin\": \"nÃ¨i cÃºn\", \"trans\": \"memory\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"æ—¶é—´\", \"pinyin\": \"shÃ­ jiÄn\", \"trans\": \"time\"},\n    {\"word\": \"å¸¸æ•°\", \"pinyin\": \"chÃ¡ng shÃ¹\", \"trans\": \"constant\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è§„åˆ™\", \"pinyin\": \"guÄ« zÃ©\", \"trans\": \"rule\"},\n    {\"word\": \"æ”¾æ¾\", \"pinyin\": \"fÃ ng sÅng\", \"trans\": \"relax\"},\n    {\"word\": \"å€¼\", \"pinyin\": \"zhÃ­\", \"trans\": \"value\"},\n    {\"word\": \"æ›¿æ¢\", \"pinyin\": \"tÃ¬ huÃ n\", \"trans\": \"replace\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"}\n]",
        "trans": "We introduced RWKV-7 \"Goose,\" a new sequence modeling architecture. It achieved new best performance on multilingual tasks despite being trained with fewer tokens. The RWKV-7 model has constant memory and inference time per token. It introduces new delta rules and relaxed value substitution rules. We demonstrated the language modeling capabilities of RWKV-7 and released the model and dataset.",
        "update_ts": "2025-03-19 09:12"
    }
}