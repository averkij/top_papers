{
    "date": {
        "ru": "10 апреля",
        "en": "April 10",
        "zh": "4月10日"
    },
    "time_utc": "2025-04-10 08:14",
    "weekday": 3,
    "issue_id": 3165,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05741",
            "title": "DDT: Decoupled Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.05741",
            "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
            "score": 29,
            "issue_id": 3159,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "2f4cd9583b2418f3",
            "authors": [
                "Shuai Wang",
                "Zhi Tian",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed Vision",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DDT: Разделяй и властвуй в мире диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга."
                },
                "en": {
                    "title": "Decoupling for Faster and Better Image Generation",
                    "desc": "This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance."
                },
                "zh": {
                    "title": "解耦扩散变换器：提升生成质量与推理速度的创新方案",
                    "desc": "扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07083",
            "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
            "url": "https://huggingface.co/papers/2504.07083",
            "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
            "score": 17,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "67e58f651d865bad",
            "authors": [
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Ziwei Liu",
                "Gordon Wetzstein",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07083.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "ИИ-оператор: новый стандарт компьютерной кинематографии",
                    "desc": "Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы создали большой мультимодальный датасет DataDoP, содержащий 29 тысяч реальных кадров с траекториями камеры, картами глубины и подробными описаниями. На основе этих данных была обучена авторегрессионная модель GenDoP на базе архитектуры Transformer для генерации художественных и выразительных движений камеры. Эксперименты показали, что GenDoP превосходит существующие методы по управляемости, точности настройки траекторий и стабильности движения."
                },
                "en": {
                    "title": "Revolutionizing Camera Movement with GenDoP",
                    "desc": "This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography."
                },
                "zh": {
                    "title": "创新相机轨迹生成，提升视觉叙事效果",
                    "desc": "本论文介绍了一种新的相机轨迹生成方法，旨在提升视频制作中的视觉叙事效果。我们提出了一个名为GenDoP的自回归模型，利用大型多模态数据集DataDoP，包含29000个真实镜头及其相机轨迹、深度图和详细描述。通过结合文本指导和RGBD输入，GenDoP能够生成高质量、上下文感知的相机运动。实验结果表明，GenDoP在可控性、轨迹调整精细度和运动稳定性方面优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07096",
            "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
            "url": "https://huggingface.co/papers/2504.07096",
            "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
            "score": 9,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "68eafe783b5816f6",
            "authors": [
                "Jiacheng Liu",
                "Taylor Blanton",
                "Yanai Elazar",
                "Sewon Min",
                "YenSung Chen",
                "Arnavi Chheda-Kothary",
                "Huy Tran",
                "Byron Bischoff",
                "Eric Marsh",
                "Michael Schmitz",
                "Cassidy Trier",
                "Aaron Sarnat",
                "Jenna James",
                "Jon Borchardt",
                "Bailey Kuehl",
                "Evie Cheng",
                "Karen Farley",
                "Sruthi Sreeram",
                "Taira Anderson",
                "David Albright",
                "Carissa Schoenick",
                "Luca Soldaini",
                "Dirk Groeneveld",
                "Rock Yuren Pang",
                "Pang Wei Koh",
                "Noah A. Smith",
                "Sophie Lebrecht",
                "Yejin Choi",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Jesse Dodge"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Stanford University",
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07096.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#inference",
                    "#open_source",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Заглянуть в память языковой модели",
                    "desc": "OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделей и их многотриллионным обучающим датасетом. Система находит дословные совпадения между сегментами вывода модели и документами в обучающем корпусе текстов. OLMoTrace использует расширенную версию алгоритма infini-gram и выдает результаты трассировки за несколько секунд. Эта система помогает пользователям понять поведение языковых моделей через призму их обучающих данных, что полезно для проверки фактов, выявления галлюцинаций и исследования креативности моделей."
                },
                "en": {
                    "title": "Trace the Truth: Unveiling Language Model Outputs with OLMoTrace",
                    "desc": "OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output."
                },
                "zh": {
                    "title": "实时追踪语言模型输出的革命性工具",
                    "desc": "OLMoTrace是第一个能够实时追踪语言模型输出与其训练数据之间关系的系统。它可以找到语言模型输出片段与训练文本库中文档的逐字匹配。该系统基于扩展版的infini-gram技术，能够在几秒钟内返回追踪结果。OLMoTrace帮助用户通过训练数据理解语言模型的行为，适用于事实检查、幻觉和语言模型的创造力探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06514",
            "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
            "url": "https://huggingface.co/papers/2504.06514",
            "abstract": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.",
            "score": 8,
            "issue_id": 3160,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "6ca6e88a5650dba9",
            "authors": [
                "Chenrui Fan",
                "Ming Li",
                "Lichao Sun",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06514.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Осторожно: языковые модели склонны к избыточным рассуждениям",
                    "desc": "В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные вопросы с отсутствующими предпосылками. Авторы обнаружили, что модели, обученные рассуждать, генерируют избыточно длинные и неэффективные ответы в таких ситуациях. Интересно, что модели без специального обучения рассуждениям показывают лучшие результаты, быстро идентифицируя некорректность вопроса. Исследование выявляет недостатки в текущих методах обучения моделей рассуждениям и предлагает новые подходы к решению проблемы чрезмерного мышления."
                },
                "en": {
                    "title": "Tackling MiP-Overthinking in Reasoning LLMs",
                    "desc": "This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue."
                },
                "zh": {
                    "title": "揭示推理模型的过度思考问题",
                    "desc": "我们发现，推理大型语言模型（LLMs）在面对缺乏前提的模糊问题时，响应长度显著增加，导致冗余和无效的思考。这种新引入的场景加剧了普遍的过度思考问题，我们称之为MiP-过度思考。研究表明，当前的推理LLMs训练方法未能有效鼓励高效思考，导致思维模式的滥用。相反，未专门训练用于推理的LLMs在处理模糊问题时表现更好，能够快速识别问题并给出更简短的回答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04842",
            "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
            "url": "https://huggingface.co/papers/2504.04842",
            "abstract": "Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.",
            "score": 5,
            "issue_id": 3160,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "5b592626aeda4ec8",
            "authors": [
                "Mengchao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Yaqi Fan",
                "Yunpeng Zhang",
                "Yonggang Qi",
                "Kun Zhao",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04842.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Оживление статичных портретов: новый уровень реализма и контроля",
                    "desc": "Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-диффузионной трансформерной модели. Авторы предлагают двухэтапную стратегию аудиовизуального выравнивания для генерации реалистичных говорящих портретов с контролируемой динамикой движений. Метод включает модуль кросс-внимания для сохранения идентичности лица и модуль модуляции интенсивности движения для управления выразительностью. Экспериментальные результаты показывают улучшение качества, реалистичности и сохранения идентичности по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Realistic Talking Avatars: Synchronizing Motion and Expression",
                    "desc": "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."
                },
                "zh": {
                    "title": "生成可控动画头像的新方法",
                    "desc": "本论文提出了一种新颖的框架，用于从单一静态肖像生成逼真的可动画头像。我们采用了预训练的视频扩散变换器模型，能够生成高保真、连贯的对话肖像，并具备可控的运动动态。核心是双阶段的音视频对齐策略，第一阶段通过音频驱动的动态对齐全场景，第二阶段则使用唇部追踪掩模精细调整唇部动作。实验结果表明，我们的方法在真实感、一致性、运动强度和身份保留方面均优于现有技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07086",
            "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
            "url": "https://huggingface.co/papers/2504.07086",
            "abstract": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.",
            "score": 4,
            "issue_id": 3163,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "b06c700fb29c005d",
            "authors": [
                "Andreas Hochlehnert",
                "Hardik Bhatnagar",
                "Vishaal Udandarao",
                "Samuel Albanie",
                "Ameya Prabhu",
                "Matthias Bethge"
            ],
            "affiliations": [
                "Tübingen AI Center, University of Tübingen",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07086.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Стандартизация оценки математических рассуждений языковых моделей",
                    "desc": "Статья посвящена проблемам оценки способностей языковых моделей к математическим рассуждениям. Авторы обнаружили, что существующие бенчмарки очень чувствительны к тонким деталям реализации, таким как параметры декодирования и форматирование промптов. Предложена стандартизированная система оценки с четко определенными лучшими практиками. Переоценка недавних методов показала, что обучение с подкреплением дает лишь скромные улучшения, в то время как supervised fine-tuning демонстрирует более стабильную генерализацию."
                },
                "en": {
                    "title": "Standardizing Reasoning Evaluations for Language Models",
                    "desc": "This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks."
                },
                "zh": {
                    "title": "推理能力评估的新标准",
                    "desc": "本文探讨了语言模型在推理能力方面的进展，指出当前的数学推理基准测试对实现细节非常敏感。研究发现，许多评估方法缺乏透明性和统计基础，导致性能提升的比较不够清晰。我们提出了一个标准化的评估框架，明确了最佳实践和报告标准，并重新评估了现有方法。结果显示，强化学习方法的改进有限，而监督微调方法在泛化能力上表现更强。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07089",
            "title": "OmniCaptioner: One Captioner to Rule Them All",
            "url": "https://huggingface.co/papers/2504.07089",
            "abstract": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.",
            "score": 3,
            "issue_id": 3164,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "9b09a9ebdec71131",
            "authors": [
                "Yiting Lu",
                "Jiakang Yuan",
                "Zhen Li",
                "Shitian Zhao",
                "Qi Qin",
                "Xinyue Li",
                "Le Zhuo",
                "Licheng Wen",
                "Dongyang Liu",
                "Yuewen Cao",
                "Xiangchao Yan",
                "Xin Li",
                "Botian Shi",
                "Tao Chen",
                "Zhibo Chen",
                "Lei Bai",
                "Bo Zhang",
                "Peng Gao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07089.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#long_context",
                    "#cv",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальное описание изображений для улучшения мультимодального ИИ",
                    "desc": "OmniCaptioner - это универсальная система для создания подробных текстовых описаний различных визуальных данных. Она может работать с естественными изображениями, визуальным текстом и структурированными визуальными данными, преобразуя пиксельную информацию в семантически богатые текстовые представления. Система улучшает визуальное рассуждение с помощью языковых моделей, повышает качество генерации изображений и обеспечивает эффективное обучение с учителем. OmniCaptioner предлагает новый подход к преодолению разрыва между языковыми и визуальными модальностями."
                },
                "en": {
                    "title": "Bridging Visuals and Text with OmniCaptioner",
                    "desc": "OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data."
                },
                "zh": {
                    "title": "OmniCaptioner：视觉与文本的桥梁",
                    "desc": "我们提出了OmniCaptioner，这是一个多功能的视觉描述框架，能够在多种视觉领域生成细致的文本描述。与之前仅限于特定图像类型的方法不同，我们的框架提供了一个统一的解决方案，可以对自然图像、视觉文本（如海报、用户界面、教科书）和结构化视觉（如文档、表格、图表）进行描述。通过将低级像素信息转换为语义丰富的文本表示，我们的框架弥合了视觉和文本模态之间的差距。我们的研究结果显示，OmniCaptioner在视觉推理、图像生成和高效的监督微调方面具有显著优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04010",
            "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
            "url": "https://huggingface.co/papers/2504.04010",
            "abstract": "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.",
            "score": 3,
            "issue_id": 3164,
            "pub_date": "2025-04-05",
            "pub_date_card": {
                "ru": "5 апреля",
                "en": "April 5",
                "zh": "4月5日"
            },
            "hash": "d006058dfff067dc",
            "authors": [
                "Maksim Siniukov",
                "Di Chang",
                "Minh Tran",
                "Hongkun Gong",
                "Ashutosh Chaubey",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "University of Southern California Los Angeles, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04010.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DiTaiListener: революция в генерации реалистичных реакций слушателя",
                    "desc": "Статья представляет DiTaiListener - новый метод генерации естественных движений слушателя в длительных диалогах с использованием видео-диффузионной модели. DiTaiListener состоит из двух компонентов: DiTaiListener-Gen для создания коротких сегментов реакций слушателя и DiTaiListener-Edit для плавного объединения этих сегментов. Модель использует адаптер CTM для обработки аудио- и визуальных сигналов говорящего, обеспечивая согласованность во времени. Количественные и качественные оценки показывают превосходство DiTaiListener над существующими методами в фотореалистичности и выразительности движений."
                },
                "en": {
                    "title": "DiTaiListener: Realistic Listener Motions for Engaging Interactions",
                    "desc": "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."
                },
                "zh": {
                    "title": "自然互动中的听众动作生成新突破",
                    "desc": "本文介绍了一种名为DiTaiListener的模型，旨在生成自然且细腻的听众动作，以改善长时间互动中的表现。该模型利用视频扩散模型和多模态条件，首先生成基于说话者语音和面部动作的短段听众反应。接着，通过DiTaiListener-Edit对过渡帧进行精细化处理，以确保视频的平滑过渡。实验结果表明，DiTaiListener在视觉真实感和动作表现上均达到了最先进的性能，用户研究也显示其在反馈、多样性和流畅性方面明显优于其他竞争模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05287",
            "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
            "url": "https://huggingface.co/papers/2504.05287",
            "abstract": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/",
            "score": 0,
            "issue_id": 3165,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "fab0fa176a952987",
            "authors": [
                "Hui Zhang",
                "Zijian Wu",
                "Linyi Huang",
                "Sammy Christen",
                "Jie Song"
            ],
            "affiliations": [
                "ETH Zurich, Switzerland",
                "HKUST (Guangzhou), China",
                "HKUST, Hong Kong (China)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05287.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#games",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Адаптивный захват объектов роботом по одному изображению",
                    "desc": "Статья представляет метод обучения с подкреплением для захвата разнообразных объектов роботизированной рукой на основе одного изображения. Авторы используют особое представление объекта, ориентированное на взаимодействие с рукой, что повышает устойчивость к вариациям формы. Предложена стратегия смешанного обучения, сочетающая имитационное обучение и обучение с подкреплением для адаптации к внешним помехам. Эксперименты показывают высокую обобщающую способность метода при захвате новых объектов в симуляции и реальности."
                },
                "en": {
                    "title": "Dynamic Grasping: Robots That Adapt and Overcome!",
                    "desc": "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."
                },
                "zh": {
                    "title": "实现灵巧抓取的零-shot学习新方法",
                    "desc": "本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。"
                }
            }
        }
    ],
    "link_prev": "2025-04-09.html",
    "link_next": "2025-04-11.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "short_date_next": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4月11日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。\n\nZhè piān wénzhāng jièshào le Skywork R1V, yīzhǒng duō móshuài tuīlǐ móxíng. Tā tōngguò gāoxiào de duō móshuài zhuǎnwéi fāngfǎ, jiāng R1-series dà yǔyán móxíng kuòzhǎn dào shìjué móshuài. Skywork R1V shǐyòng qīngliàngjí de shìjué tóujīngqì, shíxiàn wúxū chóngxīn xùnliàn yǔyán móxíng huò shìjué biānmǎqì de duō móshuài shìyìng. Wénzhāng hái tíchū le yīzhǒng hùnhé yōuhuà cèlüè hé zì shìyìng chángdù de sīwéi liàn tíxiàng fāngfǎ, tígāo tuīlǐ xiàoyìng. Shíyàn jiéguǒ xiǎnshì, Skywork R1V zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìngqiě móxíng quánzhòng yǐ gōngkāi.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"转移\", \"pinyin\": \"zhuǎn yí\", \"trans\": \"transfer\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"extend\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"投影器\", \"pinyin\": \"tóu yǐng qì\", \"trans\": \"projector\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóng xīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"自适应\", \"pinyin\": \"zì shì yìng\", \"trans\": \"adaptive\"},\n    {\"word\": \"长度\", \"pinyin\": \"cháng dù\", \"trans\": \"length\"},\n    {\"word\": \"思维链\", \"pinyin\": \"sī wéi lián\", \"trans\": \"chain of thought\"},\n    {\"word\": \"提炼\", \"pinyin\": \"tí liàn\", \"trans\": \"extract\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"权重\", \"pinyin\": \"quán zhòng\", \"trans\": \"weights\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}