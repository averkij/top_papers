{
    "date": {
        "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 18",
        "zh": "3æœˆ18æ—¥"
    },
    "time_utc": "2025-03-18 13:21",
    "weekday": 1,
    "issue_id": 2764,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.06053",
            "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
            "url": "https://huggingface.co/papers/2503.06053",
            "abstract": "Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.",
            "score": 69,
            "issue_id": 2758,
            "pub_date": "2025-03-08",
            "pub_date_card": {
                "ru": "8 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 8",
                "zh": "3æœˆ8æ—¥"
            },
            "hash": "c6a544d3dc36bfbf",
            "authors": [
                "Runze Zhang",
                "Guoguang Du",
                "Xiaochuan Li",
                "Qi Jia",
                "Liang Jin",
                "Lu Liu",
                "Jingjing Wang",
                "Cong Xu",
                "Zhenhua Guo",
                "Yaqian Zhao",
                "Xiaoli Gong",
                "Rengang Li",
                "Baoyu Fan"
            ],
            "affiliations": [
                "IEIT System Co., Ltd.",
                "Nankai University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06053.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ ÑÑĞ¶ĞµÑ‚Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DropletVideo-10M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DropletVideo, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Achieving Seamless Video Generation with Spatio-Temporal Consistency",
                    "desc": "This paper addresses the challenge of spatio-temporal consistency in video generation, which is essential for creating coherent and visually consistent narratives. It highlights the limitations of previous research that often focuses on either temporal or spatial aspects without integrating them effectively. The authors introduce a new dataset, DropletVideo-10M, containing 10 million videos with dynamic camera movements and detailed annotations, which aids in training their model. The proposed DropletVideo model demonstrates improved performance in maintaining coherence across both plot progression and camera techniques, ensuring a more seamless video generation experience."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜ã€‚ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µéœ€è¦åœ¨æƒ…èŠ‚åˆç†æ€§å’Œè§†è§‰ä¸€è‡´æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒè§†è§’ä¸‹çš„ç‰©ä½“å’Œåœºæ™¯ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ—¶é—´æˆ–ç©ºé—´ä¸€è‡´æ€§ï¼Œç¼ºä¹å¯¹ä¸¤è€…çš„ç»¼åˆè€ƒè™‘ã€‚æˆ‘ä»¬æå‡ºäº†æ•´ä½“æ—¶ç©ºä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œæ„å»ºäº†åŒ…å«1000ä¸‡æ®µåŠ¨æ€é•œå¤´å’Œç‰©ä½“åŠ¨ä½œçš„è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†DropletVideoæ¨¡å‹ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ—¶ç©ºè¿è´¯æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12533",
            "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
            "url": "https://huggingface.co/papers/2503.12533",
            "abstract": "Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.",
            "score": 39,
            "issue_id": 2755,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "18b42d1274f6b260",
            "authors": [
                "Haoqi Yuan",
                "Yu Bai",
                "Yuhui Fu",
                "Bohan Zhou",
                "Yicheng Feng",
                "Xinrun Xu",
                "Yi Zhan",
                "BÃ¶rje F. Karlsson",
                "Zongqing Lu"
            ],
            "affiliations": [
                "BAAI",
                "BeingBeyond",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12533.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#agi",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Being-0: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ AI Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Being-0 - Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Ğ¤Ğœ) Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¾Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Connector Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¤Ğœ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Being-0 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼ Ğ±Ğ¾Ñ€Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Being-0 Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Bridging High-Level Cognition and Low-Level Skills in Humanoid Robots",
                    "desc": "This paper presents Being-0, a hierarchical framework designed to enhance humanoid robots' performance in complex tasks. It combines a Foundation Model (FM) for high-level cognitive functions with a modular skill library for low-level control, addressing issues of robustness and efficiency. A novel Connector module, utilizing a lightweight vision-language model, translates language-based plans into actionable commands, improving coordination between locomotion and manipulation. The system demonstrates effective real-time performance on humanoid robots in challenging environments, showcasing its ability to tackle long-horizon tasks successfully."
                },
                "zh": {
                    "title": "æå‡ç±»äººæœºå™¨äººæ™ºèƒ½çš„å±‚æ¬¡åŒ–æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBeing-0çš„å±‚æ¬¡åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç±»äººæœºå™¨äººåœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰ä¸æ¨¡å—åŒ–æŠ€èƒ½åº“ç›¸ç»“åˆï¼ŒFMè´Ÿè´£é«˜å±‚æ¬¡çš„è®¤çŸ¥ä»»åŠ¡ï¼Œå¦‚æŒ‡ä»¤ç†è§£å’Œä»»åŠ¡è§„åˆ’ï¼Œè€ŒæŠ€èƒ½åº“åˆ™æä¾›ç¨³å®šçš„è¿åŠ¨å’Œçµå·§æ“ä½œã€‚ä¸ºäº†è¿æ¥è¿™ä¸¤ä¸ªå±‚æ¬¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿æ¥æ¨¡å—ï¼Œåˆ©ç”¨è½»é‡çº§çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†è¯­è¨€è®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æŠ€èƒ½å‘½ä»¤ã€‚é€šè¿‡åœ¨ä½æˆæœ¬çš„è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å¤§éƒ¨åˆ†ç»„ä»¶ï¼ŒBeing-0åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­å±•ç°å‡ºé«˜æ•ˆçš„å®æ—¶æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12885",
            "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
            "url": "https://huggingface.co/papers/2503.12885",
            "abstract": "Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.",
            "score": 33,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "e6332652493dc1ab",
            "authors": [
                "Dewei Zhou",
                "Mingwei Li",
                "Zongxin Yang",
                "Yi Yang"
            ],
            "affiliations": [
                "DBMI, HMS, Harvard University",
                "RELER, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12885.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DreamRenderer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "DreamRenderer: Precise Control in Image Generation",
                    "desc": "This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content."
                },
                "zh": {
                    "title": "DreamRendererï¼šç²¾ç¡®æ§åˆ¶å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamRendererçš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤šå®ä¾‹å†…å®¹æ§åˆ¶æ–¹é¢çš„ä¸è¶³ã€‚DreamRendereråŸºäºFLUXæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ¡†æˆ–æ©ç ç²¾ç¡®æ§åˆ¶æ¯ä¸ªå®ä¾‹çš„å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“è§†è§‰å’Œè°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½¿ç”¨æ¡¥æ¥å›¾åƒæ ‡è®°æ¥ç¡®ä¿æ–‡æœ¬å±æ€§çš„å‡†ç¡®ç»‘å®šï¼ŒäºŒæ˜¯åœ¨å…³é”®å±‚ä¸­åº”ç”¨ç¡¬å›¾åƒå±æ€§ç»‘å®šï¼Œä»¥æé«˜å®ä¾‹å±æ€§æ¸²æŸ“çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamRendereråœ¨å›¾åƒæˆåŠŸç‡ä¸Šæ¯”FLUXæé«˜äº†17.7%ï¼Œå¹¶ä¸”åœ¨å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ä¸Šæå‡äº†å¤šè¾¾26.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13327",
            "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
            "url": "https://huggingface.co/papers/2503.13327",
            "abstract": "We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.",
            "score": 21,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "24a5e5d1d86949d5",
            "authors": [
                "Lan Chen",
                "Qi Mao",
                "Yuchao Gu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "MIPG, Communication University of China",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13327.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Edit Transfer'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ĞµĞ³Ğ¾ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ DiT. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ 42 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Edit Transfer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ TIE Ğ¸ RIE Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ½ĞµĞ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Images with Just One Example!",
                    "desc": "This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations."
                },
                "zh": {
                    "title": "ç¼–è¾‘è½¬ç§»ï¼šå°‘æ ·æœ¬å­¦ä¹ çš„çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®¾ç½®ï¼Œç§°ä¸ºç¼–è¾‘è½¬ç§»ï¼ˆEdit Transferï¼‰ï¼Œæ¨¡å‹é€šè¿‡å•ä¸€çš„æº-ç›®æ ‡ç¤ºä¾‹å­¦ä¹ å˜æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚ä¸æ–‡æœ¬æ–¹æ³•åœ¨è¯­ä¹‰æ“ä½œä¸Šè¡¨ç°ä¼˜å¼‚ä½†åœ¨å‡ ä½•ç»†èŠ‚ä¸Šå­˜åœ¨å›°éš¾ä¸åŒï¼Œç¼–è¾‘è½¬ç§»é€šè¿‡æ˜ç¡®å­¦ä¹ æº-ç›®æ ‡å¯¹çš„ç¼–è¾‘å˜æ¢ï¼Œå…‹æœäº†æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒçš„å±€é™æ€§ã€‚æˆ‘ä»¬å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ï¼Œå¹¶åœ¨DiTåŸºç¡€çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šè¿›è¡Œæ„å»ºã€‚å°½ç®¡ä»…ä½¿ç”¨42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç¼–è¾‘è½¬ç§»åœ¨å¤šæ ·çš„éåˆšæ€§åœºæ™¯ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å°‘æ ·æœ¬è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12590",
            "title": "Personalize Anything for Free with Diffusion Transformer",
            "url": "https://huggingface.co/papers/2503.12590",
            "abstract": "Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.",
            "score": 19,
            "issue_id": 2754,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "25e3ee07c4a11ed2",
            "authors": [
                "Haoran Feng",
                "Zehuan Huang",
                "Lin Li",
                "Hairong Lv",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12590.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Personalize Anything, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ğ¼ĞµĞ½Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Personalize Anything: Efficient Image Generation with Diffusion Transformers",
                    "desc": "This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°è§†è§’",
                    "desc": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ¦‚å¿µç”Ÿæˆå›¾åƒï¼Œå¹¶å…è®¸çµæ´»ç¼–è¾‘ã€‚æœ€è¿‘çš„æ— è®­ç»ƒæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºåŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œä½†åœ¨èº«ä»½ä¿æŒã€é€‚ç”¨æ€§å’Œä¸æ‰©æ•£å˜æ¢å™¨çš„å…¼å®¹æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æ­ç¤ºäº†æ‰©æ•£å˜æ¢å™¨çš„æ½œåŠ›ï¼Œé€šè¿‡ç®€å•åœ°ç”¨å‚è€ƒå¯¹è±¡çš„å»å™ªä»¤ç‰Œæ›¿æ¢å®ç°é›¶-shotçš„å¯¹è±¡é‡å»ºã€‚æˆ‘ä»¬æå‡ºçš„â€œä¸ªæ€§åŒ–ä»»ä½•äº‹ç‰©â€æ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´æ­¥è‡ªé€‚åº”ä»¤ç‰Œæ›¿æ¢å’Œè¡¥ä¸æ‰°åŠ¨ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13434",
            "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
            "url": "https://huggingface.co/papers/2503.13434",
            "abstract": "Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/",
            "score": 15,
            "issue_id": 2758,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "c8ae2d6baee8bf12",
            "authors": [
                "Yaowei Li",
                "Lingen Li",
                "Zhaoyang Zhang",
                "Xiaoyu Li",
                "Guangzhi Wang",
                "Hongxiang Li",
                "Xiaodong Cun",
                "Ying Shan",
                "Yuexian Zou"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13434.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ²",
                    "desc": "BlobCtrl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 'Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ²', Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. BlobCtrl Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Precision and Flexibility in Visual Manipulation with BlobCtrl",
                    "desc": "BlobCtrl is a new framework designed for element-level visual manipulation in digital content creation, addressing the limitations of current diffusion-based methods. It uses a probabilistic blob-based representation to separate and manage spatial location, semantic content, and identity, allowing for precise editing of visual elements. The framework features a dual-branch diffusion architecture that integrates foreground and background seamlessly, along with a self-supervised training approach that enhances model performance through tailored data augmentation. Additionally, BlobCtrl introduces BlobData for extensive training and BlobBench for evaluation, demonstrating superior efficiency and effectiveness in various manipulation tasks."
                },
                "zh": {
                    "title": "BlobCtrlï¼šç²¾ç¡®çµæ´»çš„è§†è§‰å†…å®¹åˆ›ä½œæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlobCtrlçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•°å­—å†…å®¹åˆ›ä½œä¸­å…ƒç´ çº§è§†è§‰æ“ä½œçš„ç²¾ç¡®æ€§å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ¦‚ç‡çš„blobè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£è€¦ç©ºé—´ä½ç½®ã€è¯­ä¹‰å†…å®¹å’Œèº«ä»½ä¿¡æ¯ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å…ƒç´ çº§æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†åŒåˆ†æ”¯æ‰©æ•£æ¶æ„å’Œè‡ªç›‘ç£è®­ç»ƒèŒƒå¼ï¼Œä»¥å¢å¼ºå‰æ™¯å’ŒèƒŒæ™¯çš„æ— ç¼é›†æˆï¼Œå¹¶å¼•å…¥å¯æ§çš„dropoutç­–ç•¥æ¥å¹³è¡¡ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlobCtrlåœ¨å¤šç§å…ƒç´ çº§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºç²¾ç¡®å’Œçµæ´»çš„è§†è§‰å†…å®¹åˆ›ä½œæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13435",
            "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
            "url": "https://huggingface.co/papers/2503.13435",
            "abstract": "With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D",
            "score": 14,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "c17d4be710ed24e0",
            "authors": [
                "Ling Yang",
                "Kaixin Zhu",
                "Juanxi Tian",
                "Bohan Zeng",
                "Mingbao Lin",
                "Hongjuan Pei",
                "Wentao Zhang",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13435.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸŒ€",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WideRange4D Ğ´Ğ»Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Progress4D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 4D-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Progress4D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° WideRange4D. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Advancing 4D Reconstruction with Wide Spatial Movements",
                    "desc": "This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments."
                },
                "zh": {
                    "title": "çªç ´ç©ºé—´é™åˆ¶ï¼Œå®ç°é«˜è´¨é‡4Dé‡å»º",
                    "desc": "éšç€3Dé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œ4Dé‡å»ºç ”ç©¶ä¹Ÿåœ¨ä¸æ–­è¿›æ­¥ã€‚ç°æœ‰çš„4Dé‡å»ºæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ï¼Œä½†åœ¨è·å–å¤šè§†è§’è§†é¢‘æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰åŸºå‡†ä¸»è¦å±•ç¤ºæœ‰é™åœºæ™¯ä¸­çš„åŠ¨ä½œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„4Dé‡å»ºåŸºå‡†WideRange4Dï¼ŒåŒ…å«ä¸°å¯Œçš„4Dåœºæ™¯æ•°æ®ï¼Œå…è®¸å¯¹4Dç”Ÿæˆæ–¹æ³•çš„èƒ½åŠ›è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„4Dé‡å»ºæ–¹æ³•Progress4Dï¼Œåœ¨å„ç§å¤æ‚çš„4Dåœºæ™¯é‡å»ºä»»åŠ¡ä¸­ç”Ÿæˆç¨³å®šä¸”é«˜è´¨é‡çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11751",
            "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
            "url": "https://huggingface.co/papers/2503.11751",
            "abstract": "Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.",
            "score": 12,
            "issue_id": 2755,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "0adbcb6b7ba858f8",
            "authors": [
                "Zhaofeng Wu",
                "Michihiro Yasunaga",
                "Andrew Cohen",
                "Yoon Kim",
                "Asli Celikyilmaz",
                "Marjan Ghazvininejad"
            ],
            "affiliations": [
                "FAIR at Meta",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11751.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#training",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² NLP",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (reward models) Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… reWordBench Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ²Ğ¸Ğ´Ğ°Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Robustness in Reward Models for NLP",
                    "desc": "This paper investigates the reliability of reward models in natural language processing (NLP), which are crucial for evaluating text and enhancing model alignment. The authors introduce **reWordBench**, a tool that modifies inputs to test the robustness of these models against overfitting. Their findings reveal that many state-of-the-art reward models perform poorly when faced with slight input changes, indicating a lack of robustness. To address this issue, they propose training reward models to provide consistent scores for paraphrased inputs, resulting in improved performance and higher-quality outputs during alignment tasks."
                },
                "zh": {
                    "title": "æå‡å¥–åŠ±æ¨¡å‹é²æ£’æ€§ï¼Œå‡å°‘è¿‡æ‹Ÿåˆå½±å“",
                    "desc": "å¥–åŠ±æ¨¡å‹åœ¨ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæ—¢æ˜¯å¯æ‰©å±•çš„æ–‡æœ¬è¯„ä¼°å·¥å…·ï¼Œä¹Ÿæ˜¯è®¸å¤šå¯¹é½ç®—æ³•å’Œæ¨ç†æ—¶ç®—æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡æœ€è¿‘çš„å¥–åŠ±æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™å¯èƒ½éƒ¨åˆ†æ˜¯ç”±äºè¿‡æ‹Ÿåˆç°è±¡ï¼Œå½±å“äº†å¯¹å…¶çœŸå®èƒ½åŠ›çš„ç†è§£ã€‚æˆ‘ä»¬æ„å»ºäº†reWordBenchï¼Œç³»ç»Ÿåœ°å¯¹å¥–åŠ±æ¨¡å‹è¾“å…¥è¿›è¡Œæ„ä¹‰æˆ–æ’åä¿æŒçš„è½¬æ¢ï¼Œå‘ç°å³ä½¿æ˜¯å¾®å°çš„è¾“å…¥å˜åŒ–ï¼Œæœ€å…ˆè¿›çš„å¥–åŠ±æ¨¡å‹ä¹Ÿä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå…¶è„†å¼±æ€§ã€‚ä¸ºæé«˜å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬æå‡ºæ˜¾å¼è®­ç»ƒæ¨¡å‹å¯¹åŒä¹‰å¥èµ‹äºˆç›¸ä¼¼åˆ†æ•°ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿæ”¹å–„äº†æ¨¡å‹å¯¹å…¶ä»–ä¸åŒç±»å‹è½¬æ¢çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13399",
            "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
            "url": "https://huggingface.co/papers/2503.13399",
            "abstract": "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.",
            "score": 11,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "50d4f1f510eff333",
            "authors": [
                "James Burgess",
                "Jeffrey J Nirschl",
                "Laura Bravo-SÃ¡nchez",
                "Alejandro Lozano",
                "Sanket Rajan Gupte",
                "Jesus G. Galaz-Montoya",
                "Yuhui Zhang",
                "Yuchang Su",
                "Disha Bhowmik",
                "Zachary Coman",
                "Sarina M. Hasan",
                "Alexandra Johannesson",
                "William D. Leineweber",
                "Malvika G Nair",
                "Ridhi Yarlagadda",
                "Connor Zuraski",
                "Wah Chiu",
                "Sarah Cohen",
                "Jan N. Hansen",
                "Manuel D Leonetti",
                "Chad Liu",
                "Emma Lundberg",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Chan Zuckerberg Biohub Network",
                "KTH Royal Institute of Technology",
                "Princeton University",
                "Stanford University",
                "Tsinghua University",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13399.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "MicroVQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MicroVQA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1042 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 53%, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery",
                    "desc": "This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research."
                },
                "zh": {
                    "title": "MicroVQAï¼šæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†MicroVQAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©å­¦ç ”ç©¶çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç§‘å­¦ç ”ç©¶ä¸­æ‰€éœ€çš„ä¸‰ç§æ¨ç†èƒ½åŠ›ï¼šä¸“å®¶å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒææ¡ˆã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ—¶å­˜åœ¨ä¸è¶³ï¼ŒMicroVQAé€šè¿‡1,042ä¸ªç”±ç”Ÿç‰©å­¦ä¸“å®¶ç­–åˆ’çš„å¤šé¡¹é€‰æ‹©é¢˜æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚ç ”ç©¶å‘ç°ï¼Œæ ‡å‡†çš„å¤šé¡¹é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•å®¹æ˜“äº§ç”Ÿè¯­è¨€æ·å¾„ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæµç¨‹æ¥ä¼˜åŒ–é—®é¢˜å’Œç­”æ¡ˆçš„ç»“æ„ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å°å‹LLMsçš„è¡¨ç°ç•¥é€Šäºé¡¶çº§æ¨¡å‹ï¼Œä½†è¯­è¨€æ¨ç†çš„éš¾åº¦ä½äºå¤šæ¨¡æ€æ¨ç†ï¼Œè¿™çªæ˜¾äº†åœ¨ç§‘å­¦æ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12605",
            "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2503.12605",
            "abstract": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving extensive success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus to ensure consistent thriving in this field, where, unfortunately, an up-to-date review of this domain is lacking. To bridge this gap, we present the first systematic survey of MCoT reasoning, elucidating the relevant foundational concepts and definitions. We offer a comprehensive taxonomy and an in-depth analysis of current methodologies from diverse perspectives across various application scenarios. Furthermore, we provide insights into existing challenges and future research directions, aiming to foster innovation toward multimodal AGI.",
            "score": 11,
            "issue_id": 2762,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "4b0cc0276cb6afed",
            "authors": [
                "Yaoting Wang",
                "Shengqiong Wu",
                "Yuecheng Zhang",
                "William Wang",
                "Ziwei Liu",
                "Jiebo Luo",
                "Hao Fei"
            ],
            "affiliations": [
                "CUHK",
                "NTU",
                "NUS",
                "UCSB",
                "UR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12605.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#robotics",
                    "#healthcare",
                    "#3d",
                    "#video",
                    "#agi",
                    "#reasoning",
                    "#survey",
                    "#audio"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ÑˆĞ°Ğ³ Ğº AGI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (MCoT) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ MCoT, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning for Future AI Innovations",
                    "desc": "This paper introduces multimodal chain-of-thought (MCoT) reasoning, which enhances human-like reasoning processes across different types of data such as images, videos, and audio. It reviews various methodologies and innovative reasoning paradigms that have been developed to tackle the unique challenges posed by these multimodal contexts. The authors present a systematic survey that includes foundational concepts, a comprehensive taxonomy, and an analysis of current approaches in MCoT applications. Additionally, the paper highlights existing challenges and suggests future research directions to advance the field towards multimodal artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨ç†çš„æœªæ¥ä¹‹è·¯",
                    "desc": "å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆMCoTï¼‰æ¨ç†å°†äººç±»çš„é€æ­¥æ¨ç†ä¼˜åŠ¿æ‰©å±•åˆ°å¤šç§æ•°æ®ç±»å‹ï¼Œè¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»“åˆæ–¹é¢ã€‚ç°æœ‰çš„MCoTç ”ç©¶è®¾è®¡äº†å¤šç§æ–¹æ³•å’Œåˆ›æ–°çš„æ¨ç†èŒƒå¼ï¼Œä»¥åº”å¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ã€éŸ³é¢‘ã€3Då’Œç»“æ„åŒ–æ•°æ®ç­‰ä¸åŒæ¨¡æ€çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¹¶åœ¨æœºå™¨äººæŠ€æœ¯ã€åŒ»ç–—ä¿å¥ã€è‡ªåŠ¨é©¾é©¶å’Œå¤šæ¨¡æ€ç”Ÿæˆç­‰åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡å¦‚æ­¤ï¼ŒMCoTä»é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œéœ€è¦è¿›ä¸€æ­¥å…³æ³¨ï¼Œä»¥ç¡®ä¿è¯¥é¢†åŸŸçš„æŒç»­å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æä¾›äº†MCoTæ¨ç†çš„é¦–æ¬¡ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œé˜æ˜ç›¸å…³çš„åŸºç¡€æ¦‚å¿µå’Œå®šä¹‰ï¼Œå¹¶å¯¹å½“å‰æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»å’Œæ·±å…¥åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13082",
            "title": "Free-form language-based robotic reasoning and grasping",
            "url": "https://huggingface.co/papers/2503.13082",
            "abstract": "Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.",
            "score": 8,
            "issue_id": 2762,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "c29bd4c3e364d62c",
            "authors": [
                "Runyu Jiao",
                "Alice Fasoli",
                "Francesco Giuliari",
                "Matteo Bortolon",
                "Sergio Povoli",
                "Guofeng Mei",
                "Yiming Wang",
                "Fabio Poiesi"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler",
                "Istituto Italiano di Tecnologia",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13082.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#robotics",
                    "#agents",
                    "#reasoning",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ…Ğ²Ğ°Ñ‚Ğ°Ñ‚ÑŒ Ğ¿Ğ¾-Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FreeGrasp Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. FreeGrasp Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FreeGraspData Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering Robots with Language: Grasping Made Easy!",
                    "desc": "This paper investigates the use of Vision-Language Models (VLMs) for robotic grasping tasks based on human instructions, particularly in cluttered environments. The authors introduce a method called FreeGrasp, which utilizes pre-trained VLMs to enhance spatial reasoning by detecting objects as keypoints and annotating them on images. They also create a new synthetic dataset, FreeGraspData, to support their research, as no existing dataset fits their needs. The results show that FreeGrasp achieves state-of-the-art performance in understanding and executing grasping tasks, demonstrating the potential of VLMs in robotics."
                },
                "zh": {
                    "title": "åŸºäºäººç±»æŒ‡ä»¤çš„æ™ºèƒ½æŠ“å–æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºäººç±»æŒ‡ä»¤çš„æœºå™¨äººæŠ“å–ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­è¿›è¡ŒæŠ“å–çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•FreeGraspï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥ç†è§£äººç±»æŒ‡ä»¤å’Œç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ‰€æœ‰ç‰©ä½“æ£€æµ‹ä¸ºå…³é”®ç‚¹ï¼Œå¹¶åœ¨å›¾åƒä¸Šæ ‡æ³¨è¿™äº›å…³é”®ç‚¹ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†FreeGraspDataï¼Œä»¥æ”¯æŒè¿™ä¸€ä»»åŠ¡çš„ç ”ç©¶ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12937",
            "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
            "url": "https://huggingface.co/papers/2503.12937",
            "abstract": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.",
            "score": 8,
            "issue_id": 2755,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "dbdf6970963a4489",
            "authors": [
                "Jingyi Zhang",
                "Jiaxing Huang",
                "Huanjin Yao",
                "Shunyu Liu",
                "Xikun Zhang",
                "Shijian Lu",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12937.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Step-wise Group Relative Policy Optimization (StepGRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. StepGRPO Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»: Step-wise Reasoning Accuracy Reward (StepRAR) Ğ¸ Step-wise Reasoning Validity Reward (StepRVR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 8 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering MLLMs with Step-wise Reasoning Rewards",
                    "desc": "This paper presents a new approach to improve the reasoning abilities of machine learning language models (MLLMs) by using reinforcement learning. The proposed method, called Step-wise Group Relative Policy Optimization (StepGRPO), focuses on rewarding MLLMs for their reasoning steps rather than just imitating correct paths. It introduces two innovative rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), which encourage models to follow logical reasoning processes. The results show that MLLMs trained with StepGRPO, referred to as R1-VL, perform significantly better in step-by-step reasoning tasks across multiple benchmarks."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶é€šå¸¸é€šè¿‡åœ¨é«˜è´¨é‡çš„æ¨ç†æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒæ¥å¢å¼ºå¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¾€å¾€å¯¼è‡´æ¨¡å‹ä»…ä»…æ¨¡ä»¿æˆåŠŸçš„æ¨ç†è·¯å¾„ï¼Œè€Œä¸ç†è§£é”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨è¶…è¶Šè¢«åŠ¨æ¨¡ä»¿ç§¯ææ¨ç†è·¯å¾„ï¼Œæå‡MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”é€æ­¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStepGRPOï¼‰ï¼Œä½¿MLLMsèƒ½å¤Ÿé€šè¿‡ç®€å•ã€æœ‰æ•ˆå’Œå¯†é›†çš„é€æ­¥å¥–åŠ±è‡ªæˆ‘æå‡æ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒStepGRPOå¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„åŸºäºè§„åˆ™çš„æ¨ç†å¥–åŠ±ï¼šé€æ­¥æ¨ç†å‡†ç¡®æ€§å¥–åŠ±ï¼ˆStepRARï¼‰å’Œé€æ­¥æ¨ç†æœ‰æ•ˆæ€§å¥–åŠ±ï¼ˆStepRVRï¼‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11495",
            "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
            "url": "https://huggingface.co/papers/2503.11495",
            "abstract": "Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (\"when\") and then analyse the spatial relationships (\"where\") between key objects, and finally leverage these relationships to draw inferences (\"what\"). However, can Video Large Language Models (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained \"memory\" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",
            "score": 8,
            "issue_id": 2754,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "93b4a63d45a11f3d",
            "authors": [
                "Zixu Cheng",
                "Jian Hu",
                "Ziquan Liu",
                "Chenyang Si",
                "Wei Li",
                "Shaogang Gong"
            ],
            "affiliations": [
                "Nanjing University",
                "Nanyang Technological University",
                "Queen Mary University of London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11495.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº V-STaR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RSTR), Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Spatio-Temporal Reasoning",
                    "desc": "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç§°ä¸ºè§†é¢‘æ—¶ç©ºæ¨ç†ï¼ˆV-STaRï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è¯†åˆ«å¯¹è±¡ã€äº‹ä»¶å‘ç”Ÿæ—¶é—´å’Œç©ºé—´ä½ç½®æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ç»†è‡´æ¨ç†é“¾çš„é—®é¢˜æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„Video-LLMsåœ¨æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13444",
            "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
            "url": "https://huggingface.co/papers/2503.13444",
            "abstract": "Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.",
            "score": 6,
            "issue_id": 2755,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "4845903cb3dc6761",
            "authors": [
                "Ye Liu",
                "Kevin Qinghong Lin",
                "Chang Wen Chen",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13444.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VideoMind: Ğ¢ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoMind - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‡Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Chain-of-LoRA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VideoMind Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "VideoMind: Advancing Video Understanding with Temporal Reasoning",
                    "desc": "This paper presents VideoMind, a new video-language agent that enhances understanding of videos by linking answers to visual evidence. It introduces a role-based workflow that includes a planner, grounder, verifier, and answerer to facilitate effective temporal reasoning in videos. The authors also propose a Chain-of-LoRA strategy, which allows for efficient role-switching without the need for multiple models, thus improving both efficiency and flexibility. The results show that VideoMind outperforms existing methods on various benchmarks, highlighting its capability in grounded video question-answering and temporal reasoning tasks."
                },
                "zh": {
                    "title": "VideoMindï¼šè§†é¢‘ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è¯­è¨€æ™ºèƒ½ä½“ï¼Œåä¸ºVideoMindï¼Œæ—¨åœ¨å®ç°è§†é¢‘çš„æ—¶é—´åŸºç¡€ç†è§£ã€‚è¯¥æ™ºèƒ½ä½“é€šè¿‡è§’è‰²é©±åŠ¨çš„å·¥ä½œæµç¨‹ï¼Œæ•´åˆäº†è§„åˆ’è€…ã€å®šä½è€…ã€éªŒè¯è€…å’Œå›ç­”è€…ç­‰å…³é”®è§’è‰²ï¼Œä»¥æé«˜è§†é¢‘çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†é«˜æ•ˆæ•´åˆè¿™äº›è§’è‰²ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„Chain-of-LoRAç­–ç•¥ï¼Œå…è®¸é€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å®ç°è§’è‰²ä¹‹é—´çš„æ— ç¼åˆ‡æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoMindåœ¨å¤šé¡¹è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†è§†é¢‘æ™ºèƒ½ä½“å’Œé•¿æ—¶åºæ¨ç†çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11412",
            "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
            "url": "https://huggingface.co/papers/2503.11412",
            "abstract": "Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.",
            "score": 6,
            "issue_id": 2756,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "df14c3a2c0dfe3fd",
            "authors": [
                "Shiyuan Yang",
                "Zheng Gu",
                "Liang Hou",
                "Xin Tao",
                "Pengfei Wan",
                "Xiaodong Chen",
                "Jing Liao"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Kuaishou Technology",
                "Shenzhen University",
                "Tianjin University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11412.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³: Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ´Ğ¾ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "MTV-Inpaint - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ñ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. MTV-Inpaint Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Unified Video Inpainting: Complete and Insert with Control!",
                    "desc": "This paper presents MTV-Inpaint, a novel framework for video inpainting that integrates scene completion and object insertion tasks. It utilizes a dual-branch spatial attention mechanism within a text-to-video diffusion U-Net to achieve seamless control over both tasks. The framework also introduces a two-stage pipeline that effectively manages long videos by combining keyframe inpainting with in-between frame propagation. Extensive experiments show that MTV-Inpaint outperforms existing methods and offers versatility for various applications, including multi-modal inpainting and object editing."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†é¢‘ä¿®å¤ï¼Œåœºæ™¯è¡¥å…¨ä¸ç‰©ä½“æ’å…¥çš„å®Œç¾ç»“åˆ",
                    "desc": "è§†é¢‘ä¿®å¤æ˜¯æŒ‡åœ¨è§†é¢‘ä¸­ä¿®æ”¹å±€éƒ¨åŒºåŸŸï¼Œä»¥ç¡®ä¿ç©ºé—´å’Œæ—¶é—´çš„ä¸€è‡´æ€§ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åœºæ™¯è¡¥å…¨ä¸Šï¼Œç¼ºä¹å¯æ§åœ°æ’å…¥æ–°ç‰©ä½“çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MTV-Inpaintï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡è§†é¢‘ä¿®å¤æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¼ ç»Ÿçš„åœºæ™¯è¡¥å…¨å’Œæ–°ç‰©ä½“æ’å…¥ä»»åŠ¡ã€‚é€šè¿‡è®¾è®¡åŒåˆ†æ”¯ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒMTV-Inpaintå®ç°äº†åœºæ™¯è¡¥å…¨å’Œç‰©ä½“æ’å…¥çš„æ— ç¼é›†æˆï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€æ§åˆ¶ï¼Œé€‚ç”¨äºé•¿è§†é¢‘çš„å¤„ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13070",
            "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
            "url": "https://huggingface.co/papers/2503.13070",
            "abstract": "Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0.",
            "score": 5,
            "issue_id": 2756,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "3d6b6c6e117e1abd",
            "authors": [
                "Yihong Luo",
                "Tianyang Hu",
                "Weijian Luo",
                "Kenji Kawaguchi",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST (GZ)",
                "NUS",
                "Xiaohongshu Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13070.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#diffusion",
                    "#alignment",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "R0: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ R0. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, R0 Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ ÑĞ¸Ğ»Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Image Generation: Prioritizing Rewards Over Diffusion",
                    "desc": "This paper addresses the challenge of aligning generated images with complex text prompts and human preferences in AI-generated content. It introduces R0, a new approach that emphasizes reward maximization over traditional diffusion distillation methods, which are seen as inefficient. The authors argue that as conditions for image generation become more specific, the influence of reward signals becomes more significant than diffusion losses. Their findings suggest a shift in focus towards reward-centric generation strategies, which could enhance the effectiveness of text-to-image models."
                },
                "zh": {
                    "title": "å¥–åŠ±é©±åŠ¨çš„å›¾åƒç”Ÿæˆæ–°è§†è§’",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­ï¼Œå°†ç”Ÿæˆå›¾åƒä¸å¤æ‚æ–‡æœ¬æç¤ºå’Œäººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•R0ï¼Œé€šè¿‡æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–æ¥ä¼˜åŒ–å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ¡ä»¶å˜å¾—æ›´åŠ å…·ä½“ä¸”å¥–åŠ±ä¿¡å·æ›´å¼ºæ—¶ï¼Œå¥–åŠ±åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èµ·ç€ä¸»å¯¼ä½œç”¨ï¼Œè€Œæ‰©æ•£æŸå¤±åˆ™æˆä¸ºä¸€ç§è¿‡äºæ˜‚è´µçš„æ­£åˆ™åŒ–å½¢å¼ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ‰©æ•£åè®­ç»ƒå’Œæ¡ä»¶ç”Ÿæˆçš„è§‚å¿µï¼Œå¼ºè°ƒäº†åœ¨å¤æ‚æ¡ä»¶ä¸‹å¥–åŠ±çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10719",
            "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2503.10719",
            "abstract": "Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene/script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: https://lvas-agent.github.io",
            "score": 4,
            "issue_id": 2760,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "121476792dd15d11",
            "authors": [
                "Yehang Zhang",
                "Xinli Xu",
                "Xiaojie Xu",
                "Li Liu",
                "Yingcong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10719.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#agents",
                    "#benchmark",
                    "#video",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ÑƒĞ±Ğ»ÑĞ¶ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "LVAS-Agent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ÑƒĞ±Ğ»ÑĞ¶Ğ°. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½/ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ñ†Ğ¸ĞºĞ» Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LVAS-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 207 Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Long-Form Video Dubbing with LVAS-Agent",
                    "desc": "This paper presents LVAS-Agent, a new framework for generating audio that matches long videos, like movies, to improve viewer experience. The framework breaks down the audio synthesis process into four key steps: segmenting scenes, generating scripts, designing sounds, and synthesizing audio. It introduces innovative techniques such as a discussion-correction mechanism to refine scripts and a generation-retrieval loop to ensure that audio aligns well with the video over time. Additionally, the authors provide LVAS-Bench, a benchmark dataset of 207 long videos to evaluate the effectiveness of their approach against existing methods."
                },
                "zh": {
                    "title": "é•¿è§†é¢‘é…éŸ³çš„æ–°çªç ´ï¼šLVAS-Agent",
                    "desc": "è§†é¢‘åˆ°éŸ³é¢‘åˆæˆæ˜¯ä¸ºè§†è§‰å†…å®¹ç”ŸæˆåŒæ­¥éŸ³é¢‘çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è§‚ä¼—çš„æ²‰æµ¸æ„Ÿå’Œå™äº‹è¿è´¯æ€§ã€‚ç„¶è€Œï¼Œå¯¹äºé•¿ç¯‡å†…å®¹çš„è§†é¢‘åˆ°éŸ³é¢‘é…éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºåŠ¨æ€è¯­ä¹‰å˜åŒ–ã€æ—¶é—´é”™ä½å’Œç¼ºä¹ä¸“é—¨çš„æ•°æ®é›†ã€‚ç°æœ‰æ–¹æ³•åœ¨çŸ­è§†é¢‘ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿è§†é¢‘ï¼ˆå¦‚ç”µå½±ï¼‰ä¸­ç”±äºåˆæˆç¢ç‰‡åŒ–å’Œè·¨åœºæ™¯ä¸€è‡´æ€§ä¸è¶³è€Œè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†LVAS-Agentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡åä½œè§’è‰²ä¸“ä¸šåŒ–æ¨¡æ‹Ÿä¸“ä¸šé…éŸ³å·¥ä½œæµç¨‹ï¼Œåˆ†è§£é•¿è§†é¢‘åˆæˆä¸ºåœºæ™¯åˆ†å‰²ã€è„šæœ¬ç”Ÿæˆã€å£°éŸ³è®¾è®¡å’ŒéŸ³é¢‘åˆæˆå››ä¸ªæ­¥éª¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10704",
            "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
            "url": "https://huggingface.co/papers/2503.10704",
            "abstract": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.",
            "score": 4,
            "issue_id": 2756,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "e5f88bd433320c2f",
            "authors": [
                "Jing Wang",
                "Fengzhuo Zhang",
                "Xiaoli Li",
                "Vincent Y. F. Tan",
                "Tianyu Pang",
                "Chao Du",
                "Aixin Sun",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "A*STAR",
                "Nanyang Technological University",
                "National University of Singapore",
                "Sea AI Lab",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10704.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#inference",
                    "#architecture",
                    "#math",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ (ARVDM) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Meta-ARVDM - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ARVDM. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ARVDM: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. Ğ”Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Generation with Meta-ARVDM: Tackling Memory Bottlenecks!",
                    "desc": "This paper focuses on improving Auto-Regressive Video Diffusion Models (ARVDM) for generating realistic long videos. The authors introduce Meta-ARVDM, a comprehensive framework that encompasses existing ARVDM methods and provides a theoretical analysis of their performance. They identify two key issues: error accumulation and memory bottleneck, the latter of which is shown to be unavoidable through an information-theoretic approach. To address these challenges, the authors propose new network architectures that utilize more past frames and optimize the balance between memory usage and inference efficiency, demonstrating their findings through experiments on DMLab and Minecraft."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ï¼Œç ´è§£å†…å­˜ç“¶é¢ˆ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆARVDMï¼‰çš„ç†è®ºåŸºç¡€ï¼Œå¹¶æå‡ºäº†Meta-ARVDMè¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–äº†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ã€‚é€šè¿‡åˆ†æMeta-ARVDMç”Ÿæˆçš„è§†é¢‘ä¸çœŸå®è§†é¢‘ä¹‹é—´çš„KLæ•£åº¦ï¼Œæ­ç¤ºäº†ARVDMå›ºæœ‰çš„ä¸¤ä¸ªé‡è¦ç°è±¡ï¼šè¯¯å·®ç´¯ç§¯å’Œå†…å­˜ç“¶é¢ˆã€‚æˆ‘ä»¬é€šè¿‡ä¿¡æ¯è®ºçš„ä¸å¯èƒ½æ€§ç»“æœè¯æ˜äº†å†…å­˜ç“¶é¢ˆç°è±¡æ˜¯æ— æ³•é¿å…çš„ã€‚ä¸ºäº†è§£å†³å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†å¤šç§ç½‘ç»œç»“æ„ï¼Œä»¥æ˜¾å¼åˆ©ç”¨æ›´å¤šçš„è¿‡å»å¸§ï¼Œå¹¶é€šè¿‡å‹ç¼©å¸§å®ç°äº†å†…å­˜ç“¶é¢ˆç¼“è§£ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„æ˜¾è‘—æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13369",
            "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
            "url": "https://huggingface.co/papers/2503.13369",
            "abstract": "Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.",
            "score": 2,
            "issue_id": 2760,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "30b312815a3aaed9",
            "authors": [
                "Wan Ju Kang",
                "Eunki Kim",
                "Na Min An",
                "Sangryul Kim",
                "Haemin Choi",
                "Ki Hoon Kwak",
                "James Thorne"
            ],
            "affiliations": [
                "KAIST AI",
                "Sungkyunkwan University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13369.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#data",
                    "#low_resource",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ½ĞµĞ·Ñ€ÑÑ‡Ğ¸Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ ÑĞ»ĞµĞ¿Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ²Ğ¸Ğ´ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ñ€ÑÑ‡Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sightation, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 5000 Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering BLV Education with Vision-Language Models",
                    "desc": "This paper addresses the challenge of creating effective diagram descriptions for blind and low-vision (BLV) users, highlighting the differences in needs between annotators and end users. It proposes a novel approach where sighted annotators assess diagram descriptions generated by vision-language models (VLM) instead of creating them directly, reducing bias and improving quality. The study introduces 'Sightation', a comprehensive dataset containing 5,000 diagrams and 137,000 samples designed for various machine learning tasks such as preference and reasoning. The findings suggest that using sighted assessments can enhance the educational resources available for BLV learners, making them more accessible and relevant."
                },
                "zh": {
                    "title": "ä¸ºè§†è§‰éšœç¢ç”¨æˆ·ç”Ÿæˆç²¾å‡†å›¾è¡¨æè¿°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰éšœç¢ç”¨æˆ·ï¼ˆBLVï¼‰å¯¹å›¾è¡¨æè¿°çš„éœ€æ±‚ä¸è§†è§‰æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå›¾è¡¨æè¿°ï¼Œå¹¶è®©è§†è§‰æ­£å¸¸çš„è¯„ä¼°è€…å¯¹è¿™äº›æè¿°è¿›è¡Œè¯„ä¼°ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§è¯„ä¼°æ–¹å¼å¯¹ä¸“ä¸šçš„è§†è§‰éšœç¢æ•™è‚²è€…éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤Ÿå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°æœåŠ¡äºè§†è§‰éšœç¢å­¦ä¹ è€…ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†åä¸ºSightationçš„æ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªå›¾è¡¨å’Œ137000ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12530",
            "title": "Basic Category Usage in Vision Language Models",
            "url": "https://huggingface.co/papers/2503.12530",
            "abstract": "The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well established expert basic level shift, further suggesting that VLMs acquire cognitive categorization behaviors from the human data on which they are trained.",
            "score": 1,
            "issue_id": 2755,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "eca67c7a2633554a",
            "authors": [
                "Hunter Sawyer",
                "Jesse Roberts",
                "Kyle Moore"
            ],
            "affiliations": [
                "Tennessee Tech University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12530.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 Vision Instruct Ğ¸ Molmo 7B-D Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ´Ğ²Ğ¸Ğ³ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ VLM Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ."
                },
                "en": {
                    "title": "Bridging Human and Machine: Basic Level Categorization in Vision-Language Models",
                    "desc": "This paper explores how vision-language models (VLMs) categorize visual stimuli at a basic level, similar to human categorization as identified by Rosch. The study shows that Llama 3.2 Vision Instruct and Molmo 7B-D models exhibit a preference for basic level categorization, which aligns with human behavior. Additionally, the models reflect complex human categorization nuances, such as distinguishing between biological and non-biological entities. This suggests that VLMs learn cognitive categorization patterns from the human data they are trained on, highlighting the influence of human cognitive processes on machine learning models."
                },
                "zh": {
                    "title": "æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åŸºæœ¬åˆ†ç±»è¡Œä¸º",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„åŸºæœ¬åˆ†ç±»æ°´å¹³ï¼Œè¿™ä¸€æ¦‚å¿µæœ€æ—©ç”±Roschåœ¨1976å¹´æå‡ºã€‚ç ”ç©¶å‘ç°ï¼ŒLlama 3.2 Vision Instructå’ŒMolmo 7B-Dè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨åˆ†ç±»æ—¶æ›´å€¾å‘äºä½¿ç”¨ä¸äººç±»è¡Œä¸ºä¸€è‡´çš„åŸºæœ¬åˆ†ç±»æ°´å¹³ã€‚æ¨¡å‹çš„åå¥½è¿˜ä¸äººç±»çš„ç»†å¾®è¡Œä¸ºç›¸ç¬¦ï¼Œä¾‹å¦‚ç”Ÿç‰©ä¸éç”Ÿç‰©çš„åŸºæœ¬åˆ†ç±»æ•ˆåº”ï¼Œä»¥åŠä¸“å®¶çš„åŸºæœ¬åˆ†ç±»è½¬å˜ã€‚è¿™è¡¨æ˜ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»äººç±»æ•°æ®ä¸­å­¦ä¹ äº†è®¤çŸ¥åˆ†ç±»è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12528",
            "title": "Investigating Human-Aligned Large Language Model Uncertainty",
            "url": "https://huggingface.co/papers/2503.12528",
            "abstract": "Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.",
            "score": 1,
            "issue_id": 2755,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "7c140046351fe245",
            "authors": [
                "Kyle Moore",
                "Jesse Roberts",
                "Daryl Watson",
                "Pamela Wisniewski"
            ],
            "affiliations": [
                "Tennessee Tech University",
                "Vanderbilt University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12528.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#hallucinations",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM: ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ñ†ĞµĞ»ÑŒÑ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€ (ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ top-k) Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑ€ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Aligning Model Uncertainty with Human Perception",
                    "desc": "This paper explores how to measure uncertainty in large language models to improve their control and enhance user trust. It examines various uncertainty measures, particularly focusing on Bayesian methods and a new approach called top-k entropy, to see how well they align with human perceptions of uncertainty. The study reveals that while some measures lose their effectiveness as model size increases, combining different measures can maintain a strong correlation with human behavior regardless of model size. Ultimately, the findings suggest that a multi-measure approach can lead to better alignment with human understanding of uncertainty in language models."
                },
                "zh": {
                    "title": "é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶æ¨¡å‹å¹¶å¢å¼ºç”¨æˆ·ä¿¡ä»»ã€‚æˆ‘ä»¬åˆ†æäº†å¤šç§ä¸ç¡®å®šæ€§åº¦é‡ï¼Œæ—¨åœ¨æ‰¾å‡ºä¸äººç±»ç¾¤ä½“ä¸ç¡®å®šæ€§ç›¸å…³çš„åº¦é‡ã€‚ç ”ç©¶å‘ç°ï¼Œè´å¶æ–¯åº¦é‡å’Œä¸€ç§å˜ä½“çš„ç†µåº¦é‡ï¼ˆtop-kç†µï¼‰åœ¨æ¨¡å‹è§„æ¨¡å˜åŒ–æ—¶ä¸äººç±»è¡Œä¸ºä¸€è‡´ã€‚é€šè¿‡å¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å‘ç°ç»“åˆå¤šç§ä¸ç¡®å®šæ€§åº¦é‡å¯ä»¥åœ¨å‡å°‘è§„æ¨¡ä¾èµ–æ€§çš„åŒæ—¶ï¼Œä¿æŒä¸äººç±»è¡Œä¸ºçš„ç›¸ä¼¼æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-17.html",
    "link_next": "2025-03-19.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "17.03",
        "en": "03/17",
        "zh": "3æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 7,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 3,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚å®ƒæŒ‡å‡ºï¼Œåˆæ ¼çš„ç”Ÿæˆè§†é¢‘å¿…é¡»ç¡®ä¿æƒ…èŠ‚åˆç†æ€§å’Œè¿è´¯æ€§ï¼ŒåŒæ—¶ä¿æŒç‰©ä½“å’Œåœºæ™¯åœ¨ä¸åŒè§†è§’ä¸‹çš„è§†è§‰ä¸€è‡´æ€§ã€‚è¿‡å»çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ—¶é—´æˆ–ç©ºé—´ä¸€è‡´æ€§ä¸Šï¼Œæˆ–è€…å®ƒä»¬çš„åŸºæœ¬ç»“åˆã€‚ç„¶è€Œï¼Œæ‘„åƒæœºç§»åŠ¨å¯èƒ½å¼•å…¥æ–°ç‰©ä½“æˆ–æ¶ˆé™¤ç°æœ‰ç‰©ä½“ï¼Œä»è€Œå½±å“å‰é¢çš„å™è¿°ã€‚æœ¬æ–‡ä»‹ç»å¹¶ç ”ç©¶äº†æ•´ä½“æ—¶ç©ºä¸€è‡´æ€§ï¼Œè€ƒè™‘äº†æƒ…èŠ‚è¿›å±•å’Œæ‘„åƒæŠ€æœ¯ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»¥åŠå…ˆå‰å†…å®¹å¯¹åç»­ç”Ÿæˆçš„é•¿æœŸå½±å“ã€‚\n\nç ”ç©¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºå’Œæ¨¡å‹å¼€å‘ã€‚é¦–å…ˆï¼Œæ„å»ºäº†åŒ…å«1000ä¸‡ä¸ªè§†é¢‘çš„DropletVideo-10Mæ•°æ®é›†ï¼Œæ¯ä¸ªè§†é¢‘å¹³å‡æœ‰206ä¸ªå­—çš„æè¿°ã€‚ç„¶åï¼Œå¼€å‘å¹¶è®­ç»ƒäº†DropletVideoæ¨¡å‹ï¼Œæ“…é•¿åœ¨è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒæ—¶ç©ºè¿è´¯æ€§ã€‚æ•°æ®é›†å’Œæ¨¡å‹å¯ä»¥åœ¨https://dropletx.github.ioè®¿é—®ã€‚",
        "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le shÃ¬pÇn shÄ“ngchÃ©ng zhÅng de shÃ­kÅng yÄ«zhÃ¬xÃ¬ng. TÄ zhÇchÅ«, hÃ©gÃ© de shÄ“ngchÃ©ng shÃ¬pÇn bÃ¬xÅ« quÃ¨bÇo qÃ­ngjiÃ¨ hÃ©lÇxÃ¬ng hÃ© liÃ¡nhÃ©ngxÃ¬ng, tÃ³ngshÃ­ bÇochÃ­ wÃ¹tÇ hÃ© chÇngjÇng zÃ i bÃ¹tÃ³ng shÃ¬jiÇo xiÃ  de shÃ¬juÃ© yÄ«zhÃ¬xÃ¬ng. GuÃ²qÃ¹ de yÃ¡njiÅ« zhÇ”yÃ o jÃ­zhÃ²ng zÃ i shÃ­jiÄn huÃ² kÅngjiÄn yÄ«zhÃ¬xÃ¬ng shÃ ng, huÃ²zhÄ› tÄmen de jÄ«bÄ›n jiÃ©hÃ©. RÃ¡n'Ã©r, shÃ¨xiÃ ngjÄ« yÃ­dÃ²ng kÄ›nÃ©ng yÇnrÃ¹ xÄ«n wÃ¹tÇ huÃ² xiÄochÃº xiÃ nyÇ’u wÃ¹tÇ, zÃ²ng'Ã©r yÇngxiÇng qiÃ¡nmiÃ n de xÃ¹shÃ¹. BÄ›nwÃ©n jiÃ¨shÃ o bÃ¬ng yÃ¡njiÅ« le zhÄ›ngtÇ shÃ­kÅng yÄ«zhÃ¬xÃ¬ng, kÇolÇœ le qÃ­ngjiÃ¨ jÃ¬nzhÇn hÃ© shÃ¨xiÃ ng jÃ¬shÃ¹ zhÄ«jiÄn de xiÃ©tÃ³ng zuÃ²yÃ²ng, yÇjiÇn qiÃ¡nqiÃ¡n nÃ¨irÃ³ng duÃ¬ hÃ²uxÃ¹ shÄ“ngchÃ©ng de chÃ¡ngqÄ« yÇngxiÇng.\n\nYÃ¡njiÅ« bÄokuÃ² shÃ¹jÃ¹jÃ­ gÃ²uchÃ©ng hÃ© mÃ³xÃ­ng kÄifÄ. ShÇ’uxiÄn, gÃ²uchÃ©ng le bÄohÃ¡n 1000 wÃ n gÃ¨ shÃ¬pÇn de DropletVideo-10M shÃ¹jÃ¹jÃ­, mÄ›i gÃ¨ shÃ¬pÇn pÃ­ngjÅ«n yÇ’u 206 gÃ¨ zÃ¬ de miÃ¡oshÃ¹. RÃ¡nhÃ²u, kÄifÄ hÃ© xÃ¹nliÃ n le DropletVideo mÃ³xÃ­ng, shÃ nchÃ¡ng zÃ i shÃ¬pÇn shÄ“ngchÃ©ng guÃ²chÃ©ng zhÅng bÇochÃ­ shÃ­kÅng liÃ¡nhÃ©ngxÃ¬ng. ShÃ¹jÃ¹jÃ­ hÃ© mÃ³xÃ­ng kÄ›yÇ zÃ i https://dropletx.github.io fÇngwÃ¨n.",
        "vocab": "[{'word': 'è§†é¢‘ç”Ÿæˆ', 'pinyin': 'shÃ¬pÃ­n shÄ“ngchÃ©ng', 'trans': 'video generation'},\n{'word': 'æ—¶ç©ºä¸€è‡´æ€§', 'pinyin': 'shÃ­kÅng yÄ«zhÃ¬xÃ¬ng', 'trans': 'spatiotemporal consistency'},\n{'word': 'åˆæ ¼', 'pinyin': 'hÃ©gÃ©', 'trans': 'qualified'},\n{'word': 'æƒ…èŠ‚åˆç†æ€§', 'pinyin': 'qÃ­ngjiÃ© hÃ©lÇxÃ¬ng', 'trans': 'plot rationality'},\n{'word': 'è¿è´¯æ€§', 'pinyin': 'liÃ¡nhÃ©ngxÃ¬ng', 'trans': 'coherence'},\n{'word': 'è§†è§‰ä¸€è‡´æ€§', 'pinyin': 'shÃ¬juÃ© yÄ«zhÃ¬xÃ¬ng', 'trans': 'visual consistency'},\n{'word': 'æ‘„åƒæœº', 'pinyin': 'shÃ¨xiÃ ngjÄ«', 'trans': 'camera'},\n{'word': 'ååŒä½œç”¨', 'pinyin': 'xiÃ©tÃ³ng zuÃ²yÃ²ng', 'trans': 'synergistic effect'},\n{'word': 'é•¿æœŸå½±å“', 'pinyin': 'chÃ¡ngqÄ« yÇngxiÇng', 'trans': 'long-term impact'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹jÃ­', 'trans': 'dataset'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'},\n{'word': 'æ„å»º', 'pinyin': 'gÃ²ujiÃ n', 'trans': 'construct'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'train'},\n{'word': 'è®¿é—®', 'pinyin': 'fÇngwÃ¨n', 'trans': 'access'}]",
        "trans": "This article discusses the spatiotemporal consistency in video generation. It points out that a qualified generated video must ensure the reasonableness and coherence of the plot while maintaining the visual consistency of objects and scenes from different angles. Previous research has mainly focused on temporal or spatial consistency, or their basic combination. However, camera movements may introduce new objects or remove existing ones, affecting the previous narrative. This paper introduces and studies overall spatiotemporal consistency, considering the synergy between plot development and cinematographic techniques, as well as the long-term impact of previous content on subsequent generation.\n\nThe research includes the construction of a dataset and model development. First, the DropletVideo-10M dataset was constructed, containing 10 million videos, each with an average description of 206 words. Then, the DropletVideo model was developed and trained, excelling in maintaining spatiotemporal coherence during the video generation process. The dataset and model can be accessed at https://dropletx.github.io.",
        "update_ts": "2025-03-18 09:12"
    }
}