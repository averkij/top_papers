{
    "date": {
        "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 14",
        "zh": "10æœˆ14æ—¥"
    },
    "time_utc": "2025-10-14 17:11",
    "weekday": 1,
    "issue_id": 6413,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.11696",
            "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
            "url": "https://huggingface.co/papers/2510.11696",
            "abstract": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.",
            "score": 94,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "dd78eef9fba0abb4",
            "authors": [
                "Wei Huang",
                "Yi Ge",
                "Shuai Yang",
                "Yicheng Xiao",
                "Huizi Mao",
                "Yujun Lin",
                "Hanrong Ye",
                "Sifei Liu",
                "Ka Chun Cheung",
                "Hongxu Yin",
                "Yao Lu",
                "Xiaojuan Qi",
                "Song Han",
                "Yukang Chen"
            ],
            "affiliations": [
                "HKU",
                "MIT",
                "NVIDIA",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11696.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#optimization",
                    "#inference",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ² Ğ¿Ğ¾Ğ»Ñ‚Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ QeRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ NVFP4 ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Low-Rank Adaptation (LoRA), Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Adaptive Quantization Noise Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑˆÑƒĞ¼Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ 32B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ GPU H100 80GB Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 90.8% Ğ½Ğ° GSM8K Ğ¸ 77.4% Ğ½Ğ° MATH 500 Ğ´Ğ»Ñ 7B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Accelerating RL Training for LLMs with QeRL",
                    "desc": "QeRL is a novel framework that enhances reinforcement learning (RL) for large language models (LLMs) by integrating NVFP4 quantization with Low-Rank Adaptation (LoRA) and an Adaptive Quantization Noise mechanism. This combination significantly accelerates the RL training process, reducing memory usage and rollout times while improving overall performance. The introduction of quantization noise helps increase policy entropy, which promotes better exploration of strategies during training. Experimental results show that QeRL achieves over 1.5 times speedup in the rollout phase and matches the performance of full-parameter fine-tuning on key mathematical benchmarks."
                },
                "zh": {
                    "title": "QeRLï¼šåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
                    "desc": "QeRLæ˜¯ä¸€ä¸ªå¢å¼ºé‡åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡ç»“åˆNVFP4é‡åŒ–å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼ŒåŠ é€Ÿäº†å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜å¼€é”€ã€‚QeRLè¿˜å¼•å…¥äº†è‡ªé€‚åº”é‡åŒ–å™ªå£°æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒä¸­çš„å™ªå£°ï¼Œä»è€Œæé«˜ç­–ç•¥çš„æ¢ç´¢æ€§ï¼Œå‘ç°æ›´å¥½çš„ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒQeRLåœ¨å¼ºåŒ–å­¦ä¹ çš„å›åˆé˜¶æ®µå®ç°äº†è¶…è¿‡1.5å€çš„åŠ é€Ÿï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11690",
            "title": "Diffusion Transformers with Representation Autoencoders",
            "url": "https://huggingface.co/papers/2510.11690",
            "abstract": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
            "score": 77,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "477819306d0110e0",
            "authors": [
                "Boyang Zheng",
                "Nanye Ma",
                "Shengbang Tong",
                "Saining Xie"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11690.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "RAE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ VAE-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ² Diffusion Transformers Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (DINO, SigLIP, MAE) Ğ² Ğ¿Ğ°Ñ€Ğµ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Representation Autoencoders (RAE). Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ImageNet Ñ FID 1.51 Ğ±ĞµĞ· guidance Ğ¸ 1.13 Ñ guidance."
                },
                "en": {
                    "title": "Upgrade Diffusion Transformers with Representation Autoencoders!",
                    "desc": "This paper discusses improving Diffusion Transformers (DiTs) by replacing the traditional Variational Autoencoder (VAE) with pretrained representation encoders, creating what are called Representation Autoencoders (RAEs). The authors highlight that using VAEs limits the generative quality due to outdated architectures and low-dimensional latent spaces. By employing advanced encoders like DINO and MAE, RAEs achieve better reconstructions and richer latent representations, which enhance the performance of DiTs. The study demonstrates that this approach leads to faster convergence and superior image generation results, suggesting that RAEs should become the standard for training diffusion transformers."
                },
                "zh": {
                    "title": "ç”¨RAEæå‡æ‰©æ•£å˜æ¢å™¨çš„ç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹æ–¹æ³•ï¼Œé€šè¿‡ç”¨é¢„è®­ç»ƒçš„è¡¨ç¤ºç¼–ç å™¨æ›¿æ¢ä¼ ç»Ÿçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œæ¥æå‡æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformersï¼‰çš„ç”Ÿæˆè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡¨ç¤ºè‡ªç¼–ç å™¨ï¼ˆRAEï¼‰ï¼Œè¿™ç§æ¨¡å‹ç»“åˆäº†é«˜è´¨é‡çš„é‡å»ºå’Œä¸°å¯Œçš„è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼Œå…‹æœäº†VAEçš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRAEåœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­æœ‰æ•ˆè¿è¡Œï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰è¾…åŠ©æŸå¤±çš„æƒ…å†µä¸‹å®ç°äº†æ›´å¿«çš„æ”¶æ•›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAEåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæˆä¸ºæ‰©æ•£å˜æ¢å™¨è®­ç»ƒçš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10689",
            "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
            "url": "https://huggingface.co/papers/2510.10689",
            "abstract": "OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.",
            "score": 37,
            "issue_id": 6400,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "abfeb09f1c3ed33a",
            "authors": [
                "Caorui Li",
                "Yu Chen",
                "Yiyan Ji",
                "Jin Xu",
                "Zhenyu Cui",
                "Shihao Li",
                "Yuanxing Zhang",
                "Jiafu Tang",
                "Zhenghao Song",
                "Dingling Zhang",
                "Ying He",
                "Haoxiang Liu",
                "Yuxuan Wang",
                "Qiufeng Wang",
                "Zhenhe Wu",
                "Jiehui Luo",
                "Zhiyu Pan",
                "Weihao Xie",
                "Chenchen Zhang",
                "Zhaohui Wang",
                "Jiayi Tian",
                "Yanghai Wang",
                "Zhe Cao",
                "Minxin Dai",
                "Ke Wang",
                "Runzhe Wen",
                "Yinghao Ma",
                "Yaning Pan",
                "Sungkyun Chang",
                "Termeh Taheri",
                "Haiwen Xia",
                "Christos Plachouras",
                "Emmanouil Benetos",
                "Yizhi Li",
                "Ge Zhang",
                "Jian Yang",
                "Tianhao Peng",
                "Zili Wang",
                "Minghao Liu",
                "Junran Peng",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "NJU-LINK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10689.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² AI",
                    "desc": "OmniVideoBench â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ reasoning Ğ² multimodal LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ reasoning traces, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 628 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞºÑƒĞ½Ğ´ Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 13 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ temporal reasoning, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ reasoning, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ open-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ closed-source."
                },
                "en": {
                    "title": "Bridging the Gap in Audio-Visual Reasoning with OmniVideoBench",
                    "desc": "OmniVideoBench is a new benchmark designed to evaluate how well multimodal large language models (MLLMs) understand and reason about videos by integrating both audio and visual information. It addresses the shortcomings of existing benchmarks that often overlook the synergy between these modalities or present them in a logically inconsistent way. The benchmark includes 1000 question-answer pairs derived from a diverse set of 628 videos, focusing on various reasoning tasks such as temporal reasoning and causal inference. By highlighting the performance gap between human reasoning and MLLMs, OmniVideoBench aims to encourage the development of models that can better handle complex audio-visual reasoning tasks."
                },
                "zh": {
                    "title": "OmniVideoBenchï¼šéŸ³è§†é¢‘æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "OmniVideoBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘-è§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºæ¨¡æ€äº’è¡¥æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰åŸºå‡†æœªèƒ½å…¨é¢è¯„ä¼°éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ååŒæ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚OmniVideoBenchåŒ…å«1000å¯¹é«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œæ¶µç›–äº†å¤šç§é—®é¢˜ç±»å‹ï¼Œå¦‚æ—¶é—´æ¨ç†ã€ç©ºé—´å®šä½å’Œå› æœæ¨ç†ç­‰ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹å¤šç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹æ€§èƒ½ä¸äººç±»æ¨ç†ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†çœŸå®éŸ³é¢‘-è§†è§‰æ¨ç†çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11052",
            "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
            "url": "https://huggingface.co/papers/2510.11052",
            "abstract": "Latent Refinement Decoding (LRD) improves parallel sequence generation by maintaining global consistency and iterative refinement, enhancing accuracy and reducing latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.",
            "score": 34,
            "issue_id": 6407,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "a47e611e6a53b4b6",
            "authors": [
                "Qinglin Zhu",
                "Yizhen Yao",
                "Runcong Zhao",
                "Yanzheng Xiang",
                "Amrutha Saseendran",
                "Chen Jin",
                "Philip Alexander Teare",
                "Bin Liang",
                "Yulan He",
                "Lin Gui"
            ],
            "affiliations": [
                "Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, UK",
                "Kings College London, UK",
                "The Alan Turing Institute, UK",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11052.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Latent Refinement Decoding (LRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², LRD Ğ½Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…, Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ ĞºĞ°Ğº ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 10.6 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Parallel Sequence Generation with Latent Refinement Decoding",
                    "desc": "Latent Refinement Decoding (LRD) is a new method for generating sequences in parallel while ensuring accuracy and reducing delays. It consists of two main stages: the first stage refines predictions by keeping uncertain tokens in the mix, which helps maintain a consistent understanding of the overall context. The second stage focuses on finalizing confident predictions while still allowing for adjustments based on feedback from the earlier stage. This approach not only enhances the quality of generated sequences but also significantly speeds up the generation process, making it a powerful alternative to traditional autoregressive models."
                },
                "zh": {
                    "title": "æ½œåœ¨ç²¾ç‚¼è§£ç ï¼šæå‡å¹¶è¡Œç”Ÿæˆçš„å‡†ç¡®æ€§ä¸é€Ÿåº¦",
                    "desc": "æ½œåœ¨ç²¾ç‚¼è§£ç ï¼ˆLRDï¼‰é€šè¿‡ä¿æŒå…¨å±€ä¸€è‡´æ€§å’Œè¿­ä»£ç²¾ç‚¼ï¼Œæ”¹å–„äº†å¹¶è¡Œåºåˆ—ç”Ÿæˆçš„æ•ˆæœï¼Œæå‡äº†å‡†ç¡®æ€§å¹¶å‡å°‘äº†å»¶è¿Ÿã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­ä»ç„¶æ˜¯æ ‡å‡†ï¼Œä½†ç”±äºä¸¥æ ¼çš„é¡ºåºè§£ç ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿã€‚LRDå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¿æŒæ©ç ä½ç½®çš„åˆ†å¸ƒæ··åˆï¼Œå»ºç«‹æ›´å…¨å±€ä¸€è‡´çš„ä¿¡å¿µï¼›ç¬¬äºŒé˜¶æ®µåˆ™åœ¨ä¿ç•™ä¸ç¡®å®šçš„æ ‡è®°çš„åŒæ—¶ï¼Œé€æ­¥ç¡®å®šè‡ªä¿¡çš„æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLRDåœ¨ç¼–ç å’Œæ¨ç†ä»»åŠ¡ä¸­å‡æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶å®ç°äº†é«˜è¾¾10.6å€çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10201",
            "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
            "url": "https://huggingface.co/papers/2510.10201",
            "abstract": "RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.",
            "score": 31,
            "issue_id": 6400,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "052965c5a0959004",
            "authors": [
                "Jinghao Zhang",
                "Naishan Zheng",
                "Ruilin Li",
                "Dongzhou Cheng",
                "Zheming Liang",
                "Feng Zhao",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Shanghai Innovation Institute",
                "Southeast University",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10201.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#rlhf",
                    "#optimization",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· flow-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RLFR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ reasoning-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² reinforcement learning. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ flow-Ğ¿Ğ¾Ğ»Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ÑÑ‚ÑÑ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑÑ… ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… language Ğ¸ multimodal reasoning Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº reward shaping."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Flow Rewards from Latent Space",
                    "desc": "The paper introduces RLFR, a method that enhances reinforcement learning by using flow rewards from latent space to create verifiable rewards. This approach addresses the limitations of traditional binary verification in exploring reasoning paths effectively. By constructing flow fields from both high-quality off-policy data and on-policy rejection sampling, RLFR quantifies policy latents' velocity deviations to generate reward signals. Experiments show that RLFR improves context comprehension and reliability in reward shaping, suggesting a new direction for using auxiliary signals in reinforcement learning."
                },
                "zh": {
                    "title": "åˆ©ç”¨æµå¥–åŠ±æå‡å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºRLFRï¼Œåˆ©ç”¨æ¥è‡ªæ½œåœ¨ç©ºé—´çš„æµå¥–åŠ±æ¥æ”¹å–„å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚RLFRé€šè¿‡æ„å»ºæ¨¡å‹æ½œåœ¨çš„æµåœºï¼Œç»“åˆé«˜è´¨é‡çš„ç¦»çº¿æ•°æ®å’Œåœ¨çº¿æ‹’ç»é‡‡æ ·æ•°æ®ï¼Œé‡åŒ–ç­–ç•¥æ½œåœ¨çš„é€Ÿåº¦åå·®ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæµå¥–åŠ±åœ¨è¯­è¨€å’Œå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¯é æ€§ï¼Œå±•ç¤ºäº†æ½œåœ¨ç©ºé—´çš„è¡¨è¾¾èƒ½åŠ›å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚è¯¥æ–¹æ³•ä¸ºä½¿ç”¨è¾…åŠ©ä¿¡å·è¿›è¡Œå¥–åŠ±å¡‘é€ æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09285",
            "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.09285",
            "abstract": "VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs.",
            "score": 30,
            "issue_id": 6402,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "00aba49403e001f1",
            "authors": [
                "Siyuan Huang",
                "Xiaoye Qu",
                "Yafu Li",
                "Yun Luo",
                "Zefeng He",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09285.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ VPPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLMs) Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ¼ĞµĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. VPPO ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ¾, Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¸ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Token Perception in RLVR",
                    "desc": "This paper introduces VPPO, a new policy gradient algorithm designed to improve multimodal Reinforcement Learning with Verifiable Rewards (RLVR) by focusing on token perception. It highlights the importance of visual perception in optimizing learning signals for Large Vision-Language Models (LVLMs). The authors analyze how visual dependency varies across generated tokens and trajectories, revealing that only a few tokens are crucial for effective reasoning. VPPO enhances learning by reweighting advantages based on visual dependency and concentrating updates on key perceptual tokens, leading to significant performance improvements in reasoning tasks."
                },
                "zh": {
                    "title": "VPPOï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°ç­–ç•¥",
                    "desc": "VPPOæ˜¯ä¸€ç§æ–°é¢–çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä»¤ç‰Œæ„ŸçŸ¥æ¥ä¼˜åŒ–å¤šæ¨¡æ€å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œä»è€Œæé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å›æ»šè½¨è¿¹ä¸­ï¼Œåªæœ‰å°‘æ•°ä»¤ç‰Œä¸è§†è§‰æ¨ç†æœ‰è¾ƒé«˜çš„ä¾èµ–æ€§ï¼Œè€Œä¸åŒè½¨è¿¹çš„è§†è§‰ä¾èµ–æ€§å·®å¼‚æ˜¾è‘—ã€‚VPPOé€šè¿‡é‡æ–°åŠ æƒè½¨è¿¹çš„ä¼˜åŠ¿å’Œä¸“æ³¨äºæ„ŸçŸ¥å…³é”®ä»¤ç‰Œæ¥ç²¾ç‚¼å­¦ä¹ ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVPPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºRLè°ƒä¼˜æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10395",
            "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
            "url": "https://huggingface.co/papers/2510.10395",
            "abstract": "AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.",
            "score": 25,
            "issue_id": 6399,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "11416ff379dbecb2",
            "authors": [
                "Xinlong Chen",
                "Yue Ding",
                "Weihong Lin",
                "Jingyun Hua",
                "Linli Yao",
                "Yang Shi",
                "Bozhou Li",
                "Yuanxing Zhang",
                "Qiang Liu",
                "Pengfei Wan",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Nanjing University",
                "New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
                "Peking University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10395.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "AVoCaDO â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 107 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· GRPO Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². AVoCaDO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ open-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "AVoCaDO: Enhancing Video Captions with Audio-Visual Harmony",
                    "desc": "AVoCaDO is an advanced audiovisual video captioner that improves the quality of video descriptions by ensuring that the timing of audio and visual elements aligns well. It uses a two-stage post-training process, starting with fine-tuning on a large dataset of high-quality captions to enhance its understanding of audiovisual content. The second stage employs specialized reward functions to boost the accuracy of dialogue and maintain coherence in the captions while controlling their length. Overall, AVoCaDO shows superior performance compared to existing models in various benchmarks, making it a significant advancement in the field of video captioning."
                },
                "zh": {
                    "title": "AVoCaDOï¼šæå‡éŸ³è§†é¢‘å­—å¹•ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§",
                    "desc": "AVoCaDOæ˜¯ä¸€ç§éŸ³è§†é¢‘å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ—¶é—´ä¸€è‡´æ€§å’Œå¯¹è¯å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„åè®­ç»ƒæµç¨‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œé¦–å…ˆå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶ååˆ©ç”¨å®šåˆ¶çš„å¥–åŠ±å‡½æ•°è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAVoCaDOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨éŸ³è§†é¢‘å­—å¹•ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨ä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—ç«äº‰åŠ›çš„æˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11712",
            "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
            "url": "https://huggingface.co/papers/2510.11712",
            "abstract": "DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.",
            "score": 23,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "23c04d717e11fab7",
            "authors": [
                "Haoran Feng",
                "Dizhe Zhang",
                "Xiangtai Li",
                "Bo Du",
                "Lu Qi"
            ],
            "affiliations": [
                "Insta360 Research",
                "Nanyang Technological University",
                "Tsinghua University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11712.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#optimization",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DiT360 â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ guidance Ğ¾Ñ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ padding, yaw loss Ğ´Ğ»Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ cube loss Ğ´Ğ»Ñ ÑƒÑ‡ĞµÑ‚Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text-to-panorama, inpainting Ğ¸ outpainting Ğ¿Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Panoramic Image Generation with DiT360",
                    "desc": "The DiT360 framework improves the generation of panoramic images by using a combination of perspective and panoramic data for training. It addresses challenges in maintaining geometric accuracy and realistic image quality, which are often hindered by the scarcity of high-quality panoramic datasets. DiT360 employs key techniques such as inter-domain transformation and intra-domain augmentation to enhance image quality and consistency. Through extensive testing, the framework shows superior performance in boundary consistency and image fidelity across various tasks, including text-to-panorama and inpainting."
                },
                "zh": {
                    "title": "DiT360ï¼šæå‡å…¨æ™¯å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DiT360æ¡†æ¶é€šè¿‡åœ¨é€è§†å’Œå…¨æ™¯æ•°æ®ä¸Šè¿›è¡Œæ··åˆè®­ç»ƒï¼Œå¢å¼ºäº†å…¨æ™¯å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç”Ÿæˆè´¨é‡ä¸­çš„å‡ ä½•ä¿çœŸåº¦å’Œç…§ç‰‡çœŸå®æ„Ÿé—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„çœŸå®å…¨æ™¯æ•°æ®ã€‚DiT360åŒ…å«å¤šä¸ªå…³é”®æ¨¡å—ï¼Œç”¨äºè·¨åŸŸè½¬æ¢å’ŒåŸŸå†…å¢å¼ºï¼Œæå‡äº†æ„ŸçŸ¥è´¨é‡å¹¶è§„èŒƒäº†å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿã€‚é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¾¹ç•Œä¸€è‡´æ€§å’Œå›¾åƒä¿çœŸåº¦æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04617",
            "title": "Making Mathematical Reasoning Adaptive",
            "url": "https://huggingface.co/papers/2510.04617",
            "abstract": "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
            "score": 22,
            "issue_id": 6398,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "82cf47c00d882ce9",
            "authors": [
                "Zhejian Lai",
                "Xiang Geng",
                "Zhijun Wang",
                "Yang Bai",
                "Jiahuan Li",
                "Rongxiang Weng",
                "Jingang Wang",
                "Xuezhi Cao",
                "Xunliang Cai",
                "Shujian Huang"
            ],
            "affiliations": [
                "Meituan Inc., China",
                "Nanjing University, Nanjing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04617.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#math"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ reasoning Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AdaR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ reasoning Ğ² LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ (spurious reasoning), ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ RLVR (reinforcement learning) Ğ´Ğ»Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ reasoning. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AdaR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs' Reasoning with AdaR Framework",
                    "desc": "The AdaR framework aims to improve the robustness and generalization of large language models (LLMs) in mathematical reasoning tasks. It addresses the issue of spurious reasoning by synthesizing logically equivalent queries and employing Reinforcement Learning with Value Regularization (RLVR) to penalize incorrect logic. By extracting problem-solving logic and generating answers through code execution, AdaR enhances data quality and encourages models to rely on sound reasoning. Experimental results show that AdaR significantly boosts performance in mathematical reasoning while ensuring efficient use of data."
                },
                "zh": {
                    "title": "AdaRæ¡†æ¶ï¼šæå‡LLMsæ•°å­¦æ¨ç†çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†AdaRæ¡†æ¶ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„LLMså¸¸å¸¸å› è¡¨é¢ç‰¹å¾å¯¼è‡´é”™è¯¯æ¨ç†ï¼Œç¼ºä¹æ·±å±‚æ¬¡çš„é€»è¾‘æ€è€ƒã€‚AdaRé€šè¿‡åˆæˆé€»è¾‘ç­‰ä»·çš„æŸ¥è¯¢å¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å˜ä½“ï¼ˆRLVRï¼‰æ¥æƒ©ç½šè™šå‡é€»è¾‘ï¼Œä»è€Œä¿ƒè¿›æ¨¡å‹çš„è‡ªé€‚åº”æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaRæ˜¾è‘—æé«˜äº†æ•°å­¦æ¨ç†çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•°æ®æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11701",
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "url": "https://huggingface.co/papers/2510.11701",
            "abstract": "Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL",
            "score": 21,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "d0b700b02dfd5da7",
            "authors": [
                "Zhaochen Yu",
                "Ling Yang",
                "Jiaru Zou",
                "Shuicheng Yan",
                "Mengdi Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Princeton University",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11701.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº AIME2024/2025 Ğ¸ GPQA-Diamond. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ supervised fine-tuning Ğ¸ reinforcement learning, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ RL."
                },
                "en": {
                    "title": "Boosting LLM Reasoning with Agentic RL Techniques",
                    "desc": "This paper explores how agentic reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) using real datasets and effective exploration techniques. The authors identify key practices such as using real tool-use trajectories instead of synthetic ones, which significantly enhances model performance. They also emphasize the importance of exploration-friendly techniques and a deliberative strategy that minimizes tool calls to boost training efficiency and accuracy. Overall, the findings provide a practical framework for enhancing agentic reasoning in smaller models, achieving results comparable to larger models on challenging benchmarks."
                },
                "zh": {
                    "title": "ä»£ç†å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆagentic RLï¼‰å¦‚ä½•é€šè¿‡çœŸå®æ•°æ®é›†ã€æ¢ç´¢æŠ€æœ¯å’Œæ·±æ€ç†Ÿè™‘çš„ç­–ç•¥æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨çœŸå®çš„å·¥å…·ä½¿ç”¨è½¨è¿¹æ›¿ä»£åˆæˆè½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„åˆå§‹åŒ–æ•ˆæœï¼Œå¹¶ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†èƒ½å¤Ÿæ”¯æŒæ¢ç´¢ï¼Œæå‡å¼ºåŒ–å­¦ä¹ çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å‹å¥½çš„æ¢ç´¢æŠ€æœ¯å’Œå‡å°‘å·¥å…·è°ƒç”¨çš„æ·±æ€ç­–ç•¥èƒ½å¤Ÿæé«˜è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›ç®€å•çš„å®è·µï¼Œç ”ç©¶å±•ç¤ºäº†åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°å‹æ¨¡å‹ä¹Ÿèƒ½å–å¾—ä¼˜å¼‚çš„ä»£ç†æ¨ç†è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11341",
            "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.11341",
            "abstract": "A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.",
            "score": 20,
            "issue_id": 6405,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "cf816d298d9db724",
            "authors": [
                "Haomin Wang",
                "Jinhui Yin",
                "Qi Wei",
                "Wenguang Zeng",
                "Lixin Gu",
                "Shenglong Ye",
                "Zhangwei Gao",
                "Yaohui Wang",
                "Yanting Zhang",
                "Yuanqi Li",
                "Yanwen Guo",
                "Wenhai Wang",
                "Kai Chen",
                "Yu Qiao",
                "Hongjie Zhang"
            ],
            "affiliations": [
                "Donghua University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11341.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SVG Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ InternSVG â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹ SVG. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ SAgoge, ÑĞ°Ğ¼Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ SVG-Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸ĞºĞ¾Ğ½ĞºĞ¸, Ğ¸Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ SVG, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼. InternSVG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SArena Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing SVG with Unified Multimodal Learning",
                    "desc": "This paper introduces a unified multimodal large language model (MLLM) called InternSVG, designed specifically for understanding, editing, and generating Scalable Vector Graphics (SVG). It addresses challenges in SVG modeling by utilizing the SAgoge dataset, which is the largest collection of SVG-related data, including static and dynamic graphics. The model employs a two-stage training strategy that enhances its ability to handle both simple and complex SVG tasks. Experimental results demonstrate that InternSVG significantly outperforms existing models, showcasing its effectiveness in various SVG applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡SVGå¤„ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºSVGçš„ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œæ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è®ºæ–‡ä»‹ç»äº†InternSVGç³»åˆ—ï¼Œæ ¸å¿ƒæ˜¯SAgogeæ•°æ®é›†ï¼Œæ¶µç›–é™æ€å›¾å½¢å’ŒåŠ¨æ€åŠ¨ç”»ï¼Œæ”¯æŒå¤šç§éš¾åº¦çš„ä»»åŠ¡ã€‚åŸºäºè¿™äº›èµ„æºï¼Œæå‡ºäº†SArenaåŸºå‡†ï¼Œç¡®ä¿ä»»åŠ¡å®šä¹‰å’Œè¯„ä¼°æ ‡å‡†åŒ–ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09781",
            "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
            "url": "https://huggingface.co/papers/2510.09781",
            "abstract": "AuraGen and Safiron address pre-execution safety gaps in LLM agents by synthesizing benign trajectories, injecting risks, and using a cross-planner adapter for robust risk detection and explanation.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.",
            "score": 20,
            "issue_id": 6400,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "dac793cd7df1f38a",
            "authors": [
                "Yue Huang",
                "Hang Hua",
                "Yujun Zhou",
                "Pengcheng Jing",
                "Manish Nagireddy",
                "Inkit Padhi",
                "Greta Dolcetti",
                "Zhangchen Xu",
                "Subhajit Chaudhury",
                "Ambrish Rawat",
                "Liubov Nedoshivina",
                "Pin-Yu Chen",
                "Prasanna Sattigeri",
                "Xiangliang Zhang"
            ],
            "affiliations": [
                "Ca Foscari University of Venice",
                "IBM Research",
                "MIT-IBM Watson AI Lab",
                "University of Notre Dame",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09781.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#interpretability",
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#security",
                    "#agents"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑƒĞ³Ñ€Ğ¾Ğ·Ñƒ Ğ´Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AuraGen Ğ¸ Safiron â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. AuraGen Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Safiron â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ğ¸Ğº Ñ cross-planner Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ¸ÑĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Pre-Exec Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Pre-Execution Safety for LLM Agents: A New Frontier",
                    "desc": "The paper introduces AuraGen and Safiron, two innovative solutions aimed at enhancing the safety of large language model (LLM) agents before they execute actions. AuraGen synthesizes safe trajectories and injects labeled risks to create a comprehensive dataset for pre-execution safety assessments. Safiron serves as a foundational guardrail that utilizes a cross-planner adapter to identify and categorize risks while providing explanations for its decisions. The authors also present Pre-Exec Bench, a benchmark for evaluating the effectiveness of these safety measures, demonstrating significant improvements over existing methods."
                },
                "zh": {
                    "title": "æå‡LLMä»£ç†çš„æ‰§è¡Œå‰å®‰å…¨æ€§",
                    "desc": "AuraGenå’ŒSafironæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æ‰§è¡Œå‰çš„å®‰å…¨æ¼æ´ã€‚AuraGené€šè¿‡åˆæˆè‰¯æ€§è½¨è¿¹ã€æ³¨å…¥é£é™©å¹¶ä½¿ç”¨è‡ªåŠ¨åŒ–å¥–åŠ±æ¨¡å‹ï¼Œç”Ÿæˆå¯é çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ‰§è¡Œå‰çš„å®‰å…¨æ€§ã€‚Safironåˆ™ç»“åˆäº†è·¨è§„åˆ’é€‚é…å™¨å’Œç´§å‡‘çš„å®ˆæŠ¤æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯†åˆ«é£é™©å¹¶ç”Ÿæˆåˆç†è§£é‡Šï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†Pre-Exec BenchåŸºå‡†ï¼Œè¯„ä¼°ä¸åŒå·¥å…·å’Œåˆ†æ”¯è½¨è¿¹ä¸‹çš„é£é™©æ£€æµ‹å’Œåˆ†ç±»èƒ½åŠ›ï¼Œç¡®ä¿ä»£ç†ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11652",
            "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
            "url": "https://huggingface.co/papers/2510.11652",
            "abstract": "The Acadreason benchmark evaluates LLMs and agents on high-level academic reasoning across multiple domains, revealing significant capability gaps.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.",
            "score": 19,
            "issue_id": 6403,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "953813557c0c5560",
            "authors": [
                "Xin Gui",
                "King Zhu",
                "JinCheng Ren",
                "Qianben Chen",
                "Zekun Moore Wang",
                "Yizhi LI",
                "Xinpeng Liu",
                "Xiaowan Li",
                "Wenli Ren",
                "Linyu Miao",
                "Tianrui Qin",
                "Ziqi Shu",
                "He Zhu",
                "Xiangru Tang",
                "Dingfeng Shi",
                "Jiaheng Liu",
                "Yuchen Eleanor Jiang",
                "Minghao Liu",
                "Ge Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO AI Agent Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11652.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ½ĞµĞ¿Ğ¾ĞºĞ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Acadreason Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 50 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ»ĞµÑ‚ Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞµ, Ğ¿Ñ€Ğ°Ğ²Ñƒ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ 10 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 20 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ², Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ GPT-5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 16 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ»Ğ¸ 40 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ˜Ğ˜ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº ÑÑƒĞ¿ĞµÑ€Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Bridging the Gap in Academic Reasoning for AI",
                    "desc": "The Acadreason benchmark is a new tool designed to assess the reasoning abilities of large language models (LLMs) and agents in academic contexts. It includes 50 carefully crafted problems from five domains: computer science, economics, law, mathematics, and philosophy, all sourced from high-quality publications. The benchmark aims to address the lack of rigorous evaluations for high-level reasoning tasks in existing models. Results indicate that current LLMs, including the advanced GPT-5, struggle with these complex academic challenges, revealing significant gaps in their reasoning capabilities."
                },
                "zh": {
                    "title": "Acadreasonï¼šæ­ç¤ºå­¦æœ¯æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
                    "desc": "AcadreasonåŸºå‡†æµ‹è¯•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ™ºèƒ½ä½“åœ¨å¤šä¸ªé¢†åŸŸçš„é«˜æ°´å¹³å­¦æœ¯æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›å·®è·ã€‚è¯¥åŸºå‡†åŒ…å«50ä¸ªä¸“å®¶æ³¨é‡Šçš„å­¦æœ¯é—®é¢˜ï¼Œæ¶µç›–è®¡ç®—æœºç§‘å­¦ã€ç»æµå­¦ã€æ³•å¾‹ã€æ•°å­¦å’Œå“²å­¦ç­‰äº”ä¸ªé«˜æ¨ç†é¢†åŸŸã€‚æ‰€æœ‰é—®é¢˜å‡æ¥è‡ªè¿‘å¹´æ¥çš„é¡¶çº§å‡ºç‰ˆç‰©ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„æ³¨é‡Šå’Œè´¨é‡æ§åˆ¶ï¼Œä»¥ç¡®ä¿å…¶å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¯å›ç­”æ€§ã€‚ç³»ç»Ÿè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°LLMså¾—åˆ†ä½äº20åˆ†ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„GPT-5ä¹Ÿä»…è·å¾—16åˆ†ï¼Œè¡¨æ˜åœ¨è¶…æ™ºèƒ½å­¦æœ¯ç ”ç©¶ä»»åŠ¡ä¸­ï¼ŒLLMså’Œæ™ºèƒ½ä½“ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„èƒ½åŠ›å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08886",
            "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
            "url": "https://huggingface.co/papers/2510.08886",
            "abstract": "FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.  \t\t\t\t\tAI-generated summary \t\t\t\t The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.",
            "score": 18,
            "issue_id": 6398,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "af8af45f11c7cfc5",
            "authors": [
                "Yan Wang",
                "Keyi Wang",
                "Shanshan Yang",
                "Jaisal Patel",
                "Jeff Zhao",
                "Fengran Mo",
                "Xueqing Peng",
                "Lingfei Qian",
                "Jimin Huang",
                "Guojun Xiong",
                "Xiao-Yang Liu",
                "Jian-Yun Nie"
            ],
            "affiliations": [
                "Columbia University USA",
                "The Fin AI USA",
                "University of Montreal Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08886.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ¸ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½ Ğ¿Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ°ÑƒĞ´Ğ¸Ñ‚Ñƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FinAuditing Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ XBRL. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ US-GAAP. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 13 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°Ğ»Ğ° Ğ½Ğ° 60-90% Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ AI Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ°."
                },
                "en": {
                    "title": "FinAuditing: Bridging the Gap in Financial Reasoning for LLMs",
                    "desc": "FinAuditing is a new benchmark designed to assess large language models (LLMs) on structured financial auditing tasks, particularly focusing on the challenges posed by hierarchical financial documents. It highlights the difficulties LLMs face in reasoning over complex, taxonomy-driven structures like those found in Generally Accepted Accounting Principles (GAAP) and eXtensible Business Reporting Language (XBRL) filings. The benchmark includes three specific subtasks: FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each addressing different aspects of financial auditing. Results from testing 13 advanced LLMs show significant performance drops, revealing their limitations in handling structured financial reasoning, thus paving the way for improved financial intelligence systems."
                },
                "zh": {
                    "title": "FinAuditingï¼šæ­ç¤ºLLMsåœ¨è´¢åŠ¡å®¡è®¡ä¸­çš„å±€é™æ€§",
                    "desc": "FinAuditingæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–è´¢åŠ¡å®¡è®¡ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†LLMsåœ¨å¤„ç†åŸºäºåˆ†ç±»æ³•çš„å±‚æ¬¡è´¢åŠ¡æ–‡æ¡£æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡å®šä¹‰ä¸‰ä¸ªäº’è¡¥çš„å­ä»»åŠ¡ï¼ŒFinSMã€FinREå’ŒFinMRï¼ŒFinAuditingä¸“æ³¨äºè¯­ä¹‰ä¸€è‡´æ€§ã€å…³ç³»ä¸€è‡´æ€§å’Œæ•°å€¼ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†å±‚æ¬¡å¤šæ–‡æ¡£ç»“æ„æ—¶ï¼Œå‡†ç¡®ç‡ä¸‹é™é«˜è¾¾60-90%ï¼Œæ˜¾ç¤ºå‡ºç°ä»£LLMsåœ¨è´¢åŠ¡æ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11391",
            "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
            "url": "https://huggingface.co/papers/2510.11391",
            "abstract": "DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.",
            "score": 17,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "3b81d49ce99c4b06",
            "authors": [
                "Junpeng Liu",
                "Yuzhong Zhao",
                "Bowen Cao",
                "Jiayu Ding",
                "Yilin Jia",
                "Tengchao Lv",
                "Yupan Huang",
                "Shaohan Huang",
                "Nan Yang",
                "Li Dong",
                "Lei Cui",
                "Tao Ge",
                "Xun Wang",
                "Huitian Jiao",
                "Sun Mao",
                "FNU Kartik",
                "Si-Qing Chen",
                "Wai Lam",
                "Furu Wei"
            ],
            "affiliations": [
                "CUHK",
                "Microsoft",
                "UCAS",
                "UMich",
                "XJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11391.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#data",
                    "#agents",
                    "#alignment"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ñ‚ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾",
                    "desc": "DocReward â€” ÑÑ‚Ğ¾ reward model Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ³Ğ»ÑĞ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ DocPair Ğ¸Ğ· 117 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ. DocReward Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ¸ GPT-5 Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 30.6 Ğ¸ 19.4 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 60.8% Ğ¿Ğ¾Ğ±ĞµĞ´ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 37.7% Ñƒ GPT-5."
                },
                "en": {
                    "title": "Enhancing Document Quality with DocReward",
                    "desc": "DocReward is a novel document reward model designed to assess and improve the structural and stylistic quality of generated documents. Unlike previous models that focus solely on textual content, DocReward evaluates documents based on their visual structure and style, which are essential for enhancing readability and user engagement. It utilizes a large dataset called DocPair, consisting of 117,000 paired documents across various domains, to train its scoring system using the Bradley-Terry loss function. The results show that DocReward significantly outperforms existing models like GPT-4o and GPT-5 in both accuracy and user preference, making it a valuable tool for generating high-quality documents."
                },
                "zh": {
                    "title": "æå‡æ–‡æ¡£è´¨é‡çš„æ™ºèƒ½è¯„ä¼°å·¥å…·",
                    "desc": "DocRewardæ˜¯ä¸€ç§æ–‡æ¡£å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡ç”Ÿæˆæ–‡æ¡£çš„ç»“æ„å’Œé£æ ¼è´¨é‡ã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«117Kå¯¹æ–‡æ¡£çš„å¤šé¢†åŸŸæ•°æ®é›†ï¼Œå…¨é¢è¯„ä¼°æ–‡æ¡£çš„ä¸“ä¸šæ€§ã€‚DocRewardä½¿ç”¨Bradley-TerryæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹æ–‡æ¡£è¿›è¡Œè¯„åˆ†ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†GPT-4oå’ŒGPT-5ã€‚é€šè¿‡å¤–éƒ¨è¯„ä¼°ï¼ŒDocRewardåœ¨ç”Ÿæˆæ–‡æ¡£æ—¶æ˜¾ç¤ºå‡ºæ›´é«˜çš„èƒœç‡ï¼Œè¯æ˜äº†å…¶åœ¨æŒ‡å¯¼ç”Ÿæˆä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10666",
            "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
            "url": "https://huggingface.co/papers/2510.10666",
            "abstract": "BrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.",
            "score": 17,
            "issue_id": 6401,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "9bc4755021fb3942",
            "authors": [
                "Zhengbo Zhang",
                "Zhiheng Lyu",
                "Junhao Gong",
                "Hongzhu Yi",
                "Xinming Wang",
                "Yuxuan Zhou",
                "Jiabing Yang",
                "Ping Nie",
                "Yan Huang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Chinese Academy of Sciences",
                "Independent Researcher",
                "Peking University",
                "Tsinghua University",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10666.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ¼ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº",
                    "desc": "BrowserAgent â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ¼ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ñ€Ğ¾Ğ´Ğµ ÑĞºÑ€Ğ¾Ğ»Ğ»Ğ¸Ğ½Ğ³Ğ°, ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Playwright. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° (Supervised Fine-Tuning Ğ¸ Rejection Fine-Tuning), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Open-QA Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. BrowserAgent-7B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Search-R1 Ğ½Ğ° multi-hop QA Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° HotpotQA."
                },
                "en": {
                    "title": "BrowserAgent: Human-like Browsing for Enhanced QA Performance",
                    "desc": "BrowserAgent is an innovative interactive web agent designed to perform complex tasks by mimicking human-like browsing actions. It utilizes a two-stage training process, consisting of Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT), to enhance its ability to generalize from limited training data. By directly interacting with raw web pages through predefined actions, BrowserAgent demonstrates improved reasoning capabilities, particularly in multi-hop question answering tasks. The introduction of an explicit memory mechanism allows it to retain key information, leading to significant performance gains over existing models like Search-R1."
                },
                "zh": {
                    "title": "BrowserAgentï¼šæ›´æ™ºèƒ½çš„äº¤äº’å¼ç½‘ç»œä»£ç†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBrowserAgentçš„äº¤äº’å¼ç½‘ç»œä»£ç†ï¼Œèƒ½å¤Ÿé€šè¿‡æ¨¡æ‹Ÿäººç±»çš„æµè§ˆè¡Œä¸ºæ¥è§£å†³å¤æ‚çš„å¼€æ”¾å¼é—®ç­”ä»»åŠ¡ã€‚è¯¥ä»£ç†é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œæ‹’ç»å¾®è°ƒï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é‡è¿œå°‘äºç°æœ‰çš„Search-R1ï¼ŒBrowserAgentåœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šä»ç„¶å–å¾—äº†æ›´å…·ç«äº‰åŠ›çš„ç»“æœã€‚é€šè¿‡å¼•å…¥æ˜¾å¼è®°å¿†æœºåˆ¶ï¼ŒBrowserAgentè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10197",
            "title": "Don't Just Fine-tune the Agent, Tune the Environment",
            "url": "https://huggingface.co/papers/2510.10197",
            "abstract": "Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce Environment Tuning, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. Environment Tuning orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.",
            "score": 17,
            "issue_id": 6402,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "9c877038a77b147c",
            "authors": [
                "Siyuan Lu",
                "Zechuan Wang",
                "Hongxuan Zhang",
                "Qintong Wu",
                "Leilei Gan",
                "Chenyi Zhuang",
                "Jinjie Gu",
                "Tao Lin"
            ],
            "affiliations": [
                "AWorld Team, Inclusion AI",
                "Nanjing University",
                "Shanghai Innovation Institute",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10197.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#agents",
                    "#transfer_learning",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Environment Tuning â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ supervised fine-tuning Ğ½Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ curriculum, Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹ Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ reward-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 400 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ SFT Ğ¸ cold-start Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ reinforcement learning, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¸ data-efficient Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Empowering LLMs with Dynamic Environment Tuning for Robust Learning",
                    "desc": "This paper introduces Environment Tuning, a new training method for Large Language Model (LLM) agents that helps them learn complex behaviors from specific problem instances. It addresses the limitations of traditional supervised fine-tuning and reinforcement learning by using a structured curriculum and environment augmentation to provide real-time feedback. The approach allows agents to explore and learn without needing extensive pre-collected expert data, leading to better performance on both familiar and unfamiliar tasks. The results show that this method not only competes well with existing techniques but also enhances the agents' ability to generalize to new situations."
                },
                "zh": {
                    "title": "ç¯å¢ƒè°ƒä¼˜ï¼šä»å®ä¾‹å­¦ä¹ å¤æ‚è¡Œä¸ºçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ç¯å¢ƒè°ƒä¼˜æ˜¯ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†èƒ½å¤Ÿç›´æ¥ä»é—®é¢˜å®ä¾‹ä¸­å­¦ä¹ å¤æ‚è¡Œä¸ºï¼Œè€Œæ— éœ€ä¾èµ–é¢„å…ˆæ”¶é›†çš„ä¸“å®¶è½¨è¿¹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–è¯¾ç¨‹ã€å¯æ“ä½œçš„ç¯å¢ƒå¢å¼ºå’Œç»†ç²’åº¦çš„è¿›å±•å¥–åŠ±æ¥ç»„ç»‡å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œç¡®ä¿ç¨³å®šå’Œé«˜æ•ˆçš„æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œç¯å¢ƒè°ƒä¼˜åœ¨ä»…ä½¿ç”¨400ä¸ªé—®é¢˜å®ä¾‹çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨åˆ†å¸ƒå†…è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè®­ç»ƒæ›´å¼ºå¤§å’Œæ•°æ®é«˜æ•ˆçš„ä»£ç†é“ºå¹³äº†é“è·¯ï¼Œæ ‡å¿—ç€ä»é™æ€è½¨è¿¹çš„ç›‘ç£å¾®è°ƒå‘åŠ¨æ€ç¯å¢ƒæ¢ç´¢çš„èŒƒå¼è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11026",
            "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
            "url": "https://huggingface.co/papers/2510.11026",
            "abstract": "GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce GIR-Bench, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at https://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
            "score": 16,
            "issue_id": 6400,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "c4d705283eb33f75",
            "authors": [
                "Hongxiang Li",
                "Yaowei Li",
                "Bin Lin",
                "Yuwei Niu",
                "Yuhang Yang",
                "Xiaoshuang Huang",
                "Jiayin Cai",
                "Xiaolong Jiang",
                "Yao Hu",
                "Long Chen"
            ],
            "affiliations": [
                "Peking University",
                "The Hong Kong University of Science and Technology",
                "University of Science and Technology of China",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11026.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GIR-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ reasoning-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ reasoning Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ MLLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Multimodal Model Reasoning and Consistency",
                    "desc": "GIR-Bench is a new benchmark designed to evaluate unified multimodal models that combine language understanding and image generation. It focuses on three key areas: the consistency between understanding and generation, the ability to generate images based on reasoning, and the capacity for multi-step reasoning in editing tasks. The benchmark aims to identify gaps in the models' capabilities, particularly in how well they align understanding with generation. By providing tailored evaluation pipelines, GIR-Bench offers a more nuanced assessment of these models' performance in complex visual tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "GIR-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æ–°åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨ç†è§£ä¸ç”Ÿæˆçš„ä¸€è‡´æ€§ã€åŸºäºæ¨ç†çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»¥åŠç¼–è¾‘ä¸­çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸‰ä¸ªä¸åŒçš„è§†è§’æ¥è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œç¡®ä¿å¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œç»†è‡´çš„è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç»Ÿä¸€æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œä½†ç†è§£ä¸ç”Ÿæˆä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚GIR-Benchçš„è®¾è®¡æ—¨åœ¨æä¾›ç³»ç»ŸåŒ–çš„è¯„ä¼°ï¼Œå¸®åŠ©ç ”ç©¶è€…è¯†åˆ«æ¨¡å‹çš„ä¸è¶³ä¹‹å¤„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10670",
            "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
            "url": "https://huggingface.co/papers/2510.10670",
            "abstract": "A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.",
            "score": 15,
            "issue_id": 6398,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "e9218d263c01e3bc",
            "authors": [
                "Yu Li",
                "Menghan Xia",
                "Gongye Liu",
                "Jianhong Bai",
                "Xintao Wang",
                "Conglang Zhang",
                "Yuxuan Lin",
                "Ruihang Chu",
                "Pengfei Wan",
                "Yujiu Yang"
            ],
            "affiliations": [
                "HKUST",
                "HUST",
                "Kling Team, Kuaishou Technology",
                "Tsinghua University",
                "Wuhan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10670.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Text-to-Video Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ…. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ 4D ÑÑ†ĞµĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² T2V Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ‚ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… world models Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ğ² 4D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Harnessing Video Generation for 4D Viewpoint Prediction",
                    "desc": "This paper presents a two-stage approach to adapt pre-trained Text-to-Video (T2V) models for predicting viewpoints in 4D scenes. The first stage involves integrating a 4D scene representation into the T2V model using an adaptive learning branch, allowing the model to generate videos that visually represent different viewpoints. The second stage formulates viewpoint extraction as a denoising process, utilizing a camera extrinsic diffusion branch that processes both the generated video and the 4D scene. The results demonstrate that this method outperforms existing techniques, highlighting the potential of T2V models for real-world 4D interactions."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿›è¡Œ4Dè§†è§’é¢„æµ‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡é€‚åº”æ€§å­¦ä¹ åˆ†æ”¯å’Œç›¸æœºå¤–éƒ¨æ‰©æ•£åˆ†æ”¯ï¼Œå°†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼ˆT2Vï¼‰åº”ç”¨äº4Dåœºæ™¯çš„è§†è§’é¢„æµ‹ã€‚é¦–å…ˆï¼Œé€šè¿‡é€‚åº”æ€§å­¦ä¹ åˆ†æ”¯å°†4Dåœºæ™¯è¡¨ç¤ºæ³¨å…¥åˆ°é¢„è®­ç»ƒçš„T2Væ¨¡å‹ä¸­ï¼Œä½¿å¾—ç”Ÿæˆçš„è§†é¢‘èƒ½å¤Ÿè‡ªç„¶åœ°åµŒå…¥è§†è§’ä¿¡æ¯ã€‚æ¥ç€ï¼Œå°†è§†è§’æå–è¿‡ç¨‹è§†ä¸ºä¸€ç§æ··åˆæ¡ä»¶å¼•å¯¼çš„ç›¸æœºå¤–éƒ¨å»å™ªè¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥å¼•å…¥ç›¸æœºå¤–éƒ¨æ‰©æ•£åˆ†æ”¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰ç«äº‰è€…ï¼ŒéªŒè¯äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç°å®ä¸–ç•Œ4Däº¤äº’ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11027",
            "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
            "url": "https://huggingface.co/papers/2510.11027",
            "abstract": "Vlaser, a Vision-Language-Action Model, integrates high-level reasoning with low-level control for embodied agents, achieving state-of-the-art performance in embodied reasoning tasks and competitive results in robot benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
            "score": 14,
            "issue_id": 6400,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "7d5c7211a6478915",
            "authors": [
                "Ganlin Yang",
                "Tianyi Zhang",
                "Haoran Hao",
                "Weiyun Wang",
                "Yibin Liu",
                "Dehui Wang",
                "Guanzhou Chen",
                "Zijian Cai",
                "Junting Chen",
                "Weijie Su",
                "Wengang Zhou",
                "Yu Qiao",
                "Jifeng Dai",
                "Jiangmiao Pang",
                "Gen Luo",
                "Wenhai Wang",
                "Yao Mu",
                "Zhi Hou"
            ],
            "affiliations": [
                "Fudan University",
                "NUS",
                "Nanjing University",
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shenzhen University",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11027.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Vlaser â€” ÑÑ‚Ğ¾ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ embodied Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Vlaser-6M Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, grounding, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸, ĞºĞ°Ğº Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. Vlaser Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ WidowX Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Google Robot Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ."
                },
                "en": {
                    "title": "Bridging Reasoning and Action in Robotics with Vlaser",
                    "desc": "Vlaser is a Vision-Language-Action Model that combines high-level reasoning with low-level control for robots, enhancing their ability to understand and act in complex environments. This model addresses the gap between reasoning using Vision-Language Models (VLMs) and the practical application of these insights in robot control. By utilizing the Vlaser-6M dataset, it demonstrates superior performance in various embodied reasoning tasks such as spatial reasoning and task planning. Additionally, the study explores how different initializations of VLMs can improve the fine-tuning process for VLA policies, leading to better results in real-world robot benchmarks."
                },
                "zh": {
                    "title": "Vlaserï¼šè¿æ¥æ¨ç†ä¸è¡ŒåŠ¨çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "Vlaseræ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œæ—¨åœ¨å°†é«˜å±‚æ¬¡æ¨ç†ä¸ä½å±‚æ¬¡æ§åˆ¶ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜å…·èº«æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªå…·èº«æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ç©ºé—´æ¨ç†å’Œä»»åŠ¡è§„åˆ’ã€‚VlaseråŸºäºé«˜è´¨é‡çš„Vlaser-6Mæ•°æ®é›†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹ä¸è¡ŒåŠ¨ç­–ç•¥å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡ç³»ç»Ÿç ”ç©¶ä¸åŒçš„è§†è§‰-è¯­è¨€æ¨¡å‹åˆå§‹åŒ–å¯¹ç›‘ç£å­¦ä¹ çš„å½±å“ï¼ŒVlaseråœ¨WidowXåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09541",
            "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
            "url": "https://huggingface.co/papers/2510.09541",
            "abstract": "The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.",
            "score": 14,
            "issue_id": 6399,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "637441c2a3e6a38a",
            "authors": [
                "Chenyu Wang",
                "Paria Rashidinejad",
                "DiJia Su",
                "Song Jiang",
                "Sid Wang",
                "Siyan Zhao",
                "Cai Zhou",
                "Shannon Zejiang Shen",
                "Feiyu Chen",
                "Tommi Jaakkola",
                "Yuandong Tian",
                "Bo Liu"
            ],
            "affiliations": [
                "MIT",
                "Meta Superintelligence Labs",
                "UCLA",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09541.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#diffusion",
                    "#rl",
                    "#reinforcement_learning"
                ],
                "emoji": "ğŸ¥ª",
                "ru": {
                    "title": "Ğ¡ÑĞ½Ğ´Ğ²Ğ¸Ñ‡ Ğ¸Ğ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (dLLM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ log-likelihood. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ¸Ğ¶Ğ½ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ (ELBO), Ñ‡Ñ‚Ğ¾ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sandwiched Policy Gradient Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²ĞµÑ€Ñ…Ğ½ÑÑ Ğ¸ Ğ½Ğ¸Ğ¶Ğ½ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ log-likelihood Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ° 3.6% Ğ² GSM8K, 2.6% Ğ² MATH500, 18.4% Ğ² Countdown Ğ¸ 27.0% Ğ² Sudoku Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Reinforcing dLLMs with Balanced Policy Gradients",
                    "desc": "The Sandwiched Policy Gradient (SPG) method enhances reinforcement learning for diffusion large language models (dLLMs) by utilizing both upper and lower bounds of log-likelihood. This approach addresses the limitations of traditional policy gradient methods, which struggle with the intractable log-likelihood of dLLMs. By avoiding the biases introduced by one-sided approximations like the evidence lower bound (ELBO), SPG provides a more accurate estimation of policy gradients. Experimental results demonstrate that SPG outperforms existing methods, achieving significant improvements in various benchmark tasks."
                },
                "zh": {
                    "title": "å¤¹å¿ƒç­–ç•¥æ¢¯åº¦ï¼šæå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºå¤¹å¿ƒç­–ç•¥æ¢¯åº¦ï¼ˆSPGï¼‰ï¼Œç”¨äºæ”¹è¿›æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ã€‚ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç”±äºæ— æ³•ç›´æ¥å¤„ç†å¤æ‚çš„å¯¹æ•°ä¼¼ç„¶ï¼Œéš¾ä»¥ä¸äººç±»åå¥½æˆ–ç‰¹å®šä»»åŠ¡å¥–åŠ±å¯¹é½ã€‚SPGæ–¹æ³•åˆ©ç”¨äº†çœŸå®å¯¹æ•°ä¼¼ç„¶çš„ä¸Šä¸‹ç•Œï¼Œä»è€Œå…‹æœäº†ä»¥å¾€æ–¹æ³•ä¸­å¼•å…¥çš„åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºäºè¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰çš„æ–¹æ³•ï¼Œæå‡äº†dLLMsçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11718",
            "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
            "url": "https://huggingface.co/papers/2510.11718",
            "abstract": "CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
            "score": 12,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "3599d56e3f7eba18",
            "authors": [
                "Chengqi Duan",
                "Kaiyue Sun",
                "Rongyao Fang",
                "Manyuan Zhang",
                "Yan Feng",
                "Ying Luo",
                "Yufang Liu",
                "Ke Wang",
                "Peng Pei",
                "Xunliang Cai",
                "Hongsheng Li",
                "Yi Ma",
                "Xihui Liu"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11718.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ CodePlot-CoT â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Math-VR â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 178 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚ÑŒ Chain-of-Thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 21% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ reasoning."
                },
                "en": {
                    "title": "Empowering Math with Code and Visual Thinking",
                    "desc": "CodePlot-CoT is a novel model that enhances mathematical reasoning by integrating text and executable plotting code. It addresses the limitations of existing models that struggle with visual tasks, such as drawing or plotting, by generating both reasoning and visual outputs. The model is trained on a new dataset called Math-VR, which includes a large number of math problems requiring visual reasoning. Experimental results demonstrate that CodePlot-CoT significantly improves performance on these tasks, marking a breakthrough in multimodal mathematical reasoning."
                },
                "zh": {
                    "title": "ä»£ç é©±åŠ¨çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–°æ–¹å‘",
                    "desc": "CodePlot-CoTæ˜¯ä¸€ç§åŸºäºä»£ç çš„æ€ç»´é“¾æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡ç”Ÿæˆæ–‡æœ¬å’Œå¯æ‰§è¡Œçš„ç»˜å›¾ä»£ç ï¼Œå¸®åŠ©è§£å†³éœ€è¦è§†è§‰è¾…åŠ©çš„æ•°å­¦é—®é¢˜ã€‚æˆ‘ä»¬æ„å»ºäº†Math-VRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­æ•°å­¦é—®é¢˜æ•°æ®é›†ï¼ŒåŒ…å«178Kæ ·æœ¬ï¼Œä»¥æ”¯æŒè§†è§‰æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodePlot-CoTæ¨¡å‹åœ¨æ–°åŸºå‡†ä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†21%çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„åŸºäºä»£ç çš„æ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09008",
            "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2510.09008",
            "abstract": "A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. Based on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.",
            "score": 12,
            "issue_id": 6400,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "fdcb0591cc75977a",
            "authors": [
                "Hoigi Seo",
                "Dong Un Kang",
                "Hyunjin Cho",
                "Joohoon Lee",
                "Se Young Chun"
            ],
            "affiliations": [
                "Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09008.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLM), ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² vision encoder ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ adversarial perturbations Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ self-attention Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ vision encoder Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ÑÑ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Masking Uncertainty to Combat Object Hallucination in LVLMs",
                    "desc": "This paper addresses the problem of object hallucination in large vision-language models (LVLMs), where the model generates descriptions of objects not present in the input image. The authors identify uncertain visual tokens in the vision encoder (VE) as a major contributor to this issue. They conduct a statistical analysis revealing a correlation between high epistemic uncertainty in visual tokens and the occurrence of hallucinations. To combat this, they propose a method that masks these uncertain tokens during the self-attention process, effectively reducing hallucinations while maintaining the model's performance."
                },
                "zh": {
                    "title": "å‡å°‘è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡å°‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç‰©ä½“å¹»è§‰çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰ç¼–ç å™¨ä¸­çš„ä¸ç¡®å®šè§†è§‰æ ‡è®°æ˜¯å¯¼è‡´ç‰©ä½“å¹»è§‰çš„å…³é”®å› ç´ ã€‚é€šè¿‡ç»Ÿè®¡åˆ†æï¼Œæˆ‘ä»¬å‘ç°é«˜ä¸ç¡®å®šæ€§çš„è§†è§‰æ ‡è®°ä¸å¹»è§‰çš„å‘ç”Ÿå­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è¯†åˆ«å’Œå±è”½è¿™äº›ä¸ç¡®å®šçš„è§†è§‰æ ‡è®°ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†ç‰©ä½“å¹»è§‰çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10637",
            "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
            "url": "https://huggingface.co/papers/2510.10637",
            "abstract": "RoboSimGS, a Real2Sim2Real framework, uses 3D Gaussian Splatting and mesh primitives to create scalable, high-fidelity, and physically interactive simulation environments, enabling successful zero-shot sim-to-real transfer for robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
            "score": 9,
            "issue_id": 6401,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "363fbdb2dee9a5da",
            "authors": [
                "Haoyu Zhao",
                "Cheng Zeng",
                "Linghao Zhuang",
                "Yaxi Zhao",
                "Shengke Xue",
                "Hao Wang",
                "Xingyue Zhao",
                "Zhongyu Li",
                "Kehan Li",
                "Siteng Huang",
                "Mingxiu Chen",
                "Xin Li",
                "Deli Zhao",
                "Hua Zou"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Huazhong University of Science and Technology",
                "Hupan Lab",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "Wuhan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10637.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹",
                    "desc": "RoboSimGS - ÑÑ‚Ğ¾ framework Real2Sim2Real, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Gaussian Splatting Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ mesh-Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ğ°ĞºĞ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (zero-shot transfer)."
                },
                "en": {
                    "title": "Bridging the Gap: Realistic Simulations for Robotic Learning",
                    "desc": "RoboSimGS is a framework designed to improve robotic learning by creating realistic simulation environments from real-world images. It uses 3D Gaussian Splatting to achieve high-quality visuals and mesh primitives to ensure accurate physical interactions. This approach allows robots to learn from simulated data and successfully apply that knowledge to real-world tasks without needing additional training. By leveraging a Multi-modal Large Language Model, RoboSimGS automates the generation of realistic object properties and structures, enhancing the overall effectiveness of robotic manipulation."
                },
                "zh": {
                    "title": "RoboSimGSï¼šå®ç°æ¨¡æ‹Ÿåˆ°çœŸå®çš„æ— ç¼è½¬ç§»",
                    "desc": "RoboSimGSæ˜¯ä¸€ä¸ªæ–°çš„Real2Sim2Realæ¡†æ¶ï¼Œåˆ©ç”¨3Dé«˜æ–¯ç‚¹äº‘å’Œç½‘æ ¼åŸä»¶åˆ›å»ºå¯æ‰©å±•çš„é«˜ä¿çœŸç‰©ç†äº¤äº’æ¨¡æ‹Ÿç¯å¢ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå°†å¤šè§†è§’çš„çœŸå®ä¸–ç•Œå›¾åƒè½¬æ¢ä¸ºé€‚åˆæœºå™¨äººæ“ä½œçš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œä»è€Œå®ç°é›¶-shotçš„æ¨¡æ‹Ÿåˆ°çœŸå®è½¬ç§»ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼ŒRoboSimGSèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„ç‰©ä½“èµ„äº§ï¼Œå¹¶æ¨æ–­ç‰©ä½“çš„ç‰©ç†å±æ€§å’Œå¤æ‚çš„è¿åŠ¨ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨RoboSimGSç”Ÿæˆçš„æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜ç°æœ‰æ–¹æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10023",
            "title": "Skill-Targeted Adaptive Training",
            "url": "https://huggingface.co/papers/2510.10023",
            "abstract": "A new fine-tuning strategy, STAT, uses a teacher model's metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.",
            "score": 9,
            "issue_id": 6399,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "31df1a52ae903fc1",
            "authors": [
                "Yinghui He",
                "Abhishek Panigrahi",
                "Yong Lin",
                "Sanjeev Arora"
            ],
            "affiliations": [
                "Princeton Language and Intelligence, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10023.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° STAT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. Ğ£Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ¸Ğ±Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ (STAT-Sel), Ğ»Ğ¸Ğ±Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² (STAT-Syn). STAT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 7.5% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MATH Ğ¸ 4.6% Ğ½Ğ° out-of-distribution Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ reinforcement learning Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging Skill Gaps with STAT: A New Era in Fine-Tuning",
                    "desc": "The paper introduces a new fine-tuning strategy called STAT, which leverages the metacognitive abilities of a teacher model to enhance the performance of a student model. By identifying skill gaps through a Missing-Skill-Profile, the teacher model can adaptively reweight existing training examples or synthesize new ones to address these gaps. This approach leads to significant improvements in performance on both in-distribution and out-of-distribution benchmarks, outperforming traditional supervised fine-tuning methods. The findings suggest that integrating skill-targeted adaptive training can enhance existing machine learning training pipelines."
                },
                "zh": {
                    "title": "åˆ©ç”¨å…ƒè®¤çŸ¥æå‡æ¨¡å‹æŠ€èƒ½çš„å¾®è°ƒç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥STATï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›æ¥è¯†åˆ«å’Œè§£å†³å­¦ç”Ÿæ¨¡å‹çš„æŠ€èƒ½å·®è·ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚æ•™å¸ˆæ¨¡å‹é€šè¿‡ä»»åŠ¡æ•°æ®é›†åˆ›å»ºæ‰€éœ€æŠ€èƒ½åˆ—è¡¨ï¼Œå¹¶æ ¹æ®å­¦ç”Ÿçš„å›ç­”ç›‘æ§å…¶æŠ€èƒ½åº”ç”¨æƒ…å†µï¼Œå½¢æˆç¼ºå¤±æŠ€èƒ½æ¡£æ¡ˆã€‚STATç­–ç•¥åŒ…æ‹¬ä¸¤ç§æ–¹æ³•ï¼šSTAT-Selé€šè¿‡è°ƒæ•´ç°æœ‰è®­ç»ƒæ ·æœ¬çš„æƒé‡æ¥é€‚åº”ç¼ºå¤±æŠ€èƒ½ï¼Œè€ŒSTAT-Synåˆ™åˆæˆæ¶‰åŠç¼ºå¤±æŠ€èƒ½çš„é¢å¤–ç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTATåœ¨MATHä»»åŠ¡ä¸Šæé«˜äº†æœ€å¤š7.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–åŸºå‡†ä¸Šå¹³å‡æå‡äº†4.6%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11498",
            "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
            "url": "https://huggingface.co/papers/2510.11498",
            "abstract": "ReLook, a vision-grounded reinforcement learning framework, enhances front-end code generation by integrating a multimodal LLM for visual feedback and forced optimization, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.",
            "score": 8,
            "issue_id": 6400,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "3bef69cc56c29324",
            "authors": [
                "Yuhang Li",
                "Chenchen Zhang",
                "Ruilin Lv",
                "Ao Liu",
                "Ken Deng",
                "Yuanxing Zhang",
                "Jiaheng Liu",
                "Wiggin Zhou",
                "Bo Zhou"
            ],
            "affiliations": [
                "Independent Researcher",
                "LLM Department, Tencent",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11498.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#rl",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´-ĞºĞ¾Ğ´Ğ°",
                    "desc": "ReLook â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ„Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´-ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ: multimodal LLM Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ´Ğ°Ñ‘Ñ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Forced Optimization Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹."
                },
                "en": {
                    "title": "ReLook: Enhancing Front-End Code Generation with Vision-Grounded Learning",
                    "desc": "ReLook is a new framework that uses reinforcement learning to improve front-end code generation by incorporating visual feedback from a multimodal large language model (MLLM). It creates a loop where the agent generates code, checks it against visual outputs, and refines it based on feedback, ensuring that only valid and improving code revisions are accepted. This approach prevents issues like reward hacking by enforcing a zero-reward rule for incorrect renders and introduces Forced Optimization to maintain progress. In tests, ReLook outperformed existing methods, demonstrating the effectiveness of combining visual perception with reinforcement learning in code generation tasks."
                },
                "zh": {
                    "title": "ReLookï¼šè§†è§‰é©±åŠ¨çš„å‰ç«¯ä»£ç ç”Ÿæˆæ–°çªç ´",
                    "desc": "ReLookæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å‰ç«¯ä»£ç ç”Ÿæˆçš„æ•ˆæœã€‚å®ƒé€šè¿‡é›†æˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æä¾›è§†è§‰åé¦ˆå’Œå¼ºåˆ¶ä¼˜åŒ–ï¼Œä»è€Œè¶…è¶Šç°æœ‰çš„æ–¹æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç†ä½¿ç”¨MLLMä½œä¸ºè§†è§‰è¯„ä¼°å·¥å…·ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç åœ¨è§†è§‰ä¸Šæ˜¯æœ‰æ•ˆçš„ã€‚é€šè¿‡å¼•å…¥å¼ºåˆ¶ä¼˜åŒ–æœºåˆ¶ï¼ŒReLookèƒ½å¤ŸæŒç»­æ”¹è¿›ç”Ÿæˆçš„ä»£ç ï¼Œæœ€ç»ˆåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08026",
            "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2510.08026",
            "abstract": "A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
            "score": 7,
            "issue_id": 6398,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "f4c7a863b396ac9a",
            "authors": [
                "Chen Huang",
                "Wei Lu",
                "Wenxuan Zhang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08026.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ·Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ² Ñ„Ğ°Ğ·Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, Ğ½Ğ¸Ğ·ĞºĞ°Ñ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ â€” Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ PEAR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ² Ñ„Ğ°Ğ·Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Large Reasoning Models Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PEAR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Conciseness and Accuracy with PEAR",
                    "desc": "The paper introduces a new reward mechanism called Phase Entropy Aware Reward (PEAR) that helps large reasoning models (LRMs) generate concise yet accurate responses. It identifies a relationship between model entropy and response length, where higher entropy during the thinking phase leads to longer, more exploratory responses, while lower entropy in the final answer phase results in more deterministic outputs. PEAR adjusts the reward based on the entropy at different reasoning stages, penalizing excessive exploration in the thinking phase while allowing some flexibility in the final answer phase. This approach effectively reduces response length without compromising accuracy, demonstrating improved performance across various benchmarks and robustness to out-of-distribution scenarios."
                },
                "zh": {
                    "title": "æ§åˆ¶æ¨ç†é•¿åº¦ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„PEARæœºåˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé˜¶æ®µç†µæ„ŸçŸ¥å¥–åŠ±ï¼ˆPEARï¼‰çš„å¥–åŠ±æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†é•¿åº¦ã€‚é€šè¿‡è°ƒæ•´ä¸åŒé˜¶æ®µçš„ç†µï¼ŒPEARåœ¨ç®€æ´æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„ç†µä¸å“åº”é•¿åº¦ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ï¼Œæ€è€ƒé˜¶æ®µçš„ç†µè¾ƒé«˜ï¼Œè€Œæœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ç†µè¾ƒä½ã€‚PEARé€šè¿‡åœ¨æ€è€ƒé˜¶æ®µæƒ©ç½šè¿‡é«˜çš„ç†µï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„çµæ´»æ€§ä»¥æ­£ç¡®è§£å†³ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07841",
            "title": "Self-Improving LLM Agents at Test-Time",
            "url": "https://huggingface.co/papers/2510.07841",
            "abstract": "A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.  \t\t\t\t\tAI-generated summary \t\t\t\t One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
            "score": 7,
            "issue_id": 6398,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "60bb755e99449195",
            "authors": [
                "Emre Can Acikgoz",
                "Cheng Qian",
                "Heng Ji",
                "Dilek Hakkani-TÃ¼r",
                "Gokhan Tur"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07841.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TT-SI). ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ»Ğ¾Ñ…Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ† Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 5.48% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 68 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Language Models Through Self-Improvement at Test-Time",
                    "desc": "This paper introduces a novel method called Test-Time Self-Improvement (TT-SI) for enhancing language models by generating additional training examples from uncertain cases. The approach involves three key steps: identifying challenging samples, creating similar examples from these samples, and fine-tuning the model using the newly generated data. By focusing on self-awareness and self-data augmentation, TT-SI allows models to improve their performance significantly while using far fewer training samples. Empirical results show that this method leads to an average accuracy gain of 5.48% across various benchmarks, demonstrating its effectiveness compared to traditional learning techniques."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›ï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›çš„æ–¹æ³•ï¼Œé€šè¿‡ä»ä¸ç¡®å®šçš„æ¡ˆä¾‹ä¸­ç”Ÿæˆé¢å¤–çš„è®­ç»ƒæ ·æœ¬ï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆè¯†åˆ«æ¨¡å‹éš¾ä»¥å¤„ç†çš„æ ·æœ¬ï¼Œå…¶æ¬¡ä»è¿™äº›ä¸ç¡®å®šæ ·æœ¬ä¸­ç”Ÿæˆç›¸ä¼¼çš„ä¾‹å­ï¼Œæœ€ååœ¨æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†5.48%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬å‡å°‘äº†68å€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›ç®—æ³•ä¸ºæ„å»ºæ›´å¼ºå¤§çš„æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10868",
            "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
            "url": "https://huggingface.co/papers/2510.10868",
            "abstract": "Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline.",
            "score": 5,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "4abc0308cd558402",
            "authors": [
                "Soroush Mehraban",
                "Andrea Iaboni",
                "Babak Taati"
            ],
            "affiliations": [
                "KITE Research Institute, UHN",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10868.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸƒ",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-ÑĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ÑĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (Human Mesh Recovery). ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ², Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ â€” ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ½Ğµ Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ”Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ diffusion-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ prior-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğ°Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 2.3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient 3D Human Mesh Recovery with Smart Merging and Decoding",
                    "desc": "This paper presents two innovative merging strategies to enhance 3D Human Mesh Recovery (HMR) while reducing computational costs. The Error-Constrained Layer Merging (ECLM) technique optimally merges transformer layers with minimal impact on accuracy, specifically the Mean Per Joint Position Error (MPJPE). Additionally, the Mask-guided Token Merging (Mask-ToMe) method targets the reduction of background tokens that do not significantly affect predictions. To maintain performance despite these reductions, a diffusion-based decoder is introduced, which utilizes temporal context and pose priors from extensive motion capture data, resulting in improved efficiency and performance."
                },
                "zh": {
                    "title": "æå‡3Däººç±»ç½‘æ ¼æ¢å¤çš„é€Ÿåº¦ä¸æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸¤ç§é’ˆå¯¹3Däººç±»ç½‘æ ¼æ¢å¤çš„åˆå¹¶ç­–ç•¥ï¼Œåˆ†åˆ«æ˜¯è¯¯å·®çº¦æŸå±‚åˆå¹¶ï¼ˆECLMï¼‰å’ŒåŸºäºæ©ç çš„æ ‡è®°åˆå¹¶ï¼ˆMask-ToMeï¼‰ï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜æ€§èƒ½ã€‚ECLMé€‰æ‹©æ€§åœ°åˆå¹¶å¯¹æ¯ä¸ªå…³èŠ‚ä½ç½®è¯¯å·®ï¼ˆMPJPEï¼‰å½±å“æœ€å°çš„å˜æ¢å™¨å±‚ï¼Œè€ŒMask-ToMeåˆ™ä¸“æ³¨äºåˆå¹¶å¯¹æœ€ç»ˆé¢„æµ‹è´¡çŒ®è¾ƒå°çš„èƒŒæ™¯æ ‡è®°ã€‚ä¸ºäº†åº”å¯¹åˆå¹¶å¯èƒ½å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„è§£ç å™¨ï¼Œåˆ©ç”¨æ—¶é—´ä¸Šä¸‹æ–‡å’Œä»å¤§è§„æ¨¡åŠ¨ä½œæ•æ‰æ•°æ®é›†ä¸­å­¦ä¹ çš„å§¿æ€å…ˆéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€é«˜2.3å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šç•¥æœ‰æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09905",
            "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
            "url": "https://huggingface.co/papers/2510.09905",
            "abstract": "LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.  \t\t\t\t\tAI-generated summary \t\t\t\t When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities.",
            "score": 5,
            "issue_id": 6399,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "8ab2b377c7e94c73",
            "authors": [
                "Xi Fang",
                "Weijie Xu",
                "Yuchong Zhang",
                "Stephanie Eckman",
                "Scott Nickleach",
                "Chandan K. Reddy"
            ],
            "affiliations": [
                "Amazon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09905.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#multimodal",
                    "#healthcare"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ LLM ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾. ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ AI-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Bias in AI: Emotional Interpretation Reflects Social Hierarchies",
                    "desc": "This paper explores how large language models (LLMs) interpret emotions based on user profiles, revealing systematic biases. It shows that LLMs provide different emotional responses to the same situation depending on whether the user is perceived as advantaged or disadvantaged. The study evaluates 15 models using human-validated emotional intelligence tests, finding that models often favor profiles with higher social status. This highlights a significant issue in AI personalization, where memory-enhanced systems may unintentionally perpetuate social hierarchies in emotional reasoning."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–AIå¯èƒ½åŠ å‰§ç¤¾ä¼šä¸å¹³ç­‰",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æƒ…æ„Ÿç†è§£å’Œæ”¯æŒæ–¹é¢çš„ç³»ç»Ÿæ€§åè§ï¼Œå°¤å…¶æ˜¯å¦‚ä½•å—åˆ°ç”¨æˆ·æ¡£æ¡ˆçš„å½±å“ã€‚æˆ‘ä»¬è¯„ä¼°äº†15ä¸ªæ¨¡å‹åœ¨ç»è¿‡äººç±»éªŒè¯çš„æƒ…æ„Ÿæ™ºåŠ›æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå‘ç°ç›¸åŒæƒ…å¢ƒä¸‹ä¸åŒç”¨æˆ·æ¡£æ¡ˆä¼šå¯¼è‡´æƒ…æ„Ÿè§£è¯»çš„æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰ä¼˜åŠ¿èƒŒæ™¯çš„ç”¨æˆ·æ›´å®¹æ˜“è·å¾—å‡†ç¡®çš„æƒ…æ„Ÿè§£è¯»ï¼Œä¸”åœ¨æƒ…æ„Ÿç†è§£å’Œæ”¯æŒå»ºè®®ä»»åŠ¡ä¸­ï¼ŒLLMsåœ¨ä¸åŒäººå£ç»Ÿè®¡å› ç´ ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å¢å¼ºè®°å¿†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¯èƒ½æ— æ„ä¸­åŠ å‰§ç¤¾ä¼šä¸å¹³ç­‰çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09212",
            "title": "Stable Video Infinity: Infinite-Length Video Generation with Error\n  Recycling",
            "url": "https://huggingface.co/papers/2510.09212",
            "abstract": "Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.",
            "score": 5,
            "issue_id": 6405,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "318f363d980e473c",
            "authors": [
                "Wuyang Li",
                "Wentao Pan",
                "Po-Chien Luan",
                "Yang Gao",
                "Alexandre Alahi"
            ],
            "affiliations": [
                "VITA@EPFL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09212.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#story_generation",
                    "#video",
                    "#training",
                    "#diffusion"
                ],
                "emoji": "â™¾ï¸",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº",
                    "desc": "Stable Video Infinity (SVI) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ÑÑĞ¶ĞµÑ‚Ğ¾Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Error-Recycling Fine-Tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Diffusion Transformer Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾regÑ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ² replay memory Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SVI Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ â€” Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Generate Infinite Videos with Consistency and Control!",
                    "desc": "Stable Video Infinity (SVI) is a novel approach for generating infinite-length videos that maintain high temporal consistency and allow for controllable storylines. It addresses the limitations of existing long-video methods by introducing Error-Recycling Fine-Tuning, which helps the Diffusion Transformer (DiT) learn from its own errors during video generation. This method involves recycling self-generated errors as supervisory prompts, enabling the model to correct its mistakes and improve the quality of the output. SVI can produce videos of any length without increasing inference costs and works well with various input conditions like audio and text."
                },
                "zh": {
                    "title": "æ— é™è§†é¢‘ç”Ÿæˆçš„ç¨³å®šæ€§ä¸å¯æ§æ€§",
                    "desc": "Stable Video Infinityï¼ˆSVIï¼‰æ˜¯ä¸€ç§ç”Ÿæˆæ— é™é•¿åº¦è§†é¢‘çš„æ–°æ–¹æ³•ï¼Œå…·æœ‰é«˜æ—¶é—´ä¸€è‡´æ€§å’Œå¯æ§çš„æ•…äº‹æƒ…èŠ‚ã€‚è¯¥æ–¹æ³•é€šè¿‡é”™è¯¯å›æ”¶å¾®è°ƒæŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰è‡ªç”Ÿæˆçš„é”™è¯¯æ¥æ”¹è¿›è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSVIèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œçº æ­£è‡ªèº«é”™è¯¯ï¼Œä»è€Œé¿å…äº†é‡å¤åŠ¨ä½œå’ŒåŒè´¨åœºæ™¯çš„é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSVIå¯ä»¥åœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆä»å‡ ç§’åˆ°æ— é™æ—¶é•¿çš„è§†é¢‘ï¼Œå¹¶å…¼å®¹å¤šç§è¾“å…¥æ¡ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11512",
            "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
            "url": "https://huggingface.co/papers/2510.11512",
            "abstract": "LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
            "score": 4,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "d8a8f971250d7cbb",
            "authors": [
                "Jianhao Yuan",
                "Fabio Pizzati",
                "Francesco Pinto",
                "Lars Kunze",
                "Ivan Laptev",
                "Paul Newman",
                "Philip Torr",
                "Daniele De Martini"
            ],
            "affiliations": [
                "MBZUAI",
                "UWE Bristol",
                "University of Chicago",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11512.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#inference",
                    "#diffusion",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "ğŸ±",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LikePhys â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ denoising objective ĞºĞ°Ğº ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. ĞĞ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Plausibility Preference Error Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°."
                },
                "en": {
                    "title": "Evaluating Intuitive Physics in Video Models with LikePhys",
                    "desc": "LikePhys is a novel method for evaluating how well video diffusion models understand intuitive physics. It uses a denoising objective to differentiate between physically valid and impossible video sequences, providing a more accurate assessment than previous methods. The evaluation metric, called Plausibility Preference Error (PPE), aligns closely with human preferences and outperforms existing evaluation baselines. The study also reveals that while current models face challenges with complex dynamics, they show improvement in physics understanding as their capacity and inference settings are enhanced."
                },
                "zh": {
                    "title": "è¯„ä¼°è§†é¢‘æ¨¡å‹çš„ç›´è§‚ç‰©ç†ç†è§£",
                    "desc": "LikePhys æ˜¯ä¸€ç§è¯„ä¼°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ç›´è§‚ç‰©ç†ç†è§£çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä½¿ç”¨å»å™ªç›®æ ‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤ŸåŒºåˆ†ç‰©ç†æœ‰æ•ˆå’Œä¸å¯èƒ½çš„è§†é¢‘ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«åäºŒç§åœºæ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼ŒLikePhys çš„è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œä¼˜äºç°æœ‰çš„è¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶è¿˜åˆ†æäº†æ¨¡å‹è®¾è®¡å’Œæ¨ç†è®¾ç½®å¯¹ç›´è§‚ç‰©ç†ç†è§£çš„å½±å“ï¼Œæ˜¾ç¤ºå‡ºéšç€æ¨¡å‹èƒ½åŠ›çš„æå‡ï¼Œç‰©ç†ç†è§£æœ‰æ˜æ˜¾æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10062",
            "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
            "url": "https://huggingface.co/papers/2510.10062",
            "abstract": "HUME provides human performance baselines for text embedding tasks, enhancing the interpretability of model evaluations and revealing dataset and language-specific challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
            "score": 4,
            "issue_id": 6402,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "bc0c26a6373016ef",
            "authors": [
                "Adnan El Assadi",
                "Isaac Chung",
                "Roman Solomatin",
                "Niklas Muennighoff",
                "Kenneth Enevoldsen"
            ],
            "affiliations": [
                "Aarhus University",
                "Carleton University",
                "SberAI",
                "Stanford University",
                "Zendesk"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10062.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#low_resource"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ HUME â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 16 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸Ğ· MTEB, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ embedding-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 80.1% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 77.6% Ñƒ Ğ»ÑĞ´ĞµĞ¹, Ğ½Ğ¾ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "HUME: Bridging Human and Model Performance in Text Embeddings",
                    "desc": "HUME is a framework designed to establish human performance baselines for text embedding tasks, which helps in evaluating and interpreting model performance. By comparing how humans and models perform on various tasks, it highlights the strengths and weaknesses of embedding models, especially in capturing meaning and nuance. The framework assesses human performance across multiple datasets, revealing significant variations in model effectiveness depending on the dataset and language resources. This approach not only provides valuable insights into task difficulty but also aids in the development of better models and benchmarks for text embeddings."
                },
                "zh": {
                    "title": "HUMEï¼šæå‡æ–‡æœ¬åµŒå…¥ä»»åŠ¡çš„å¯è§£é‡Šæ€§",
                    "desc": "HUMEæ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åµŒå…¥ä»»åŠ¡çš„äººç±»æ€§èƒ½åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹è¯„ä¼°çš„å¯è§£é‡Šæ€§ï¼Œå¹¶æ­ç¤ºæ•°æ®é›†å’Œè¯­è¨€ç‰¹å®šçš„æŒ‘æˆ˜ã€‚é€šè¿‡æ¯”è¾ƒäººç±»å’Œæ¨¡å‹çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£åµŒå…¥æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œäº†è§£å®ƒä»¬åœ¨æ•æ‰æ„ä¹‰å’Œç»†å¾®å·®åˆ«æ–¹é¢çš„æˆåŠŸä¸å¤±è´¥ã€‚HUMEæµ‹é‡äº†16ä¸ªMTEBæ•°æ®é›†ä¸Šçš„äººç±»è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºäººç±»çš„å¹³å‡è¡¨ç°ä¸º77.6%ï¼Œè€Œæœ€ä½³åµŒå…¥æ¨¡å‹ä¸º80.1%ã€‚è¯¥æ¡†æ¶æä¾›äº†äººç±»æ€§èƒ½åŸºå‡†ã€ä»»åŠ¡éš¾åº¦æ¨¡å¼çš„æ´å¯Ÿï¼Œå¹¶ä¸ºæ¨¡å‹å’ŒåŸºå‡†çš„å¼€å‘æä¾›äº†æ›´æœ‰æ„ä¹‰çš„è§£é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07624",
            "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum\n  Likelihood Estimation",
            "url": "https://huggingface.co/papers/2510.07624",
            "abstract": "A bilevel optimization framework is used to align generative models with high-quality datasets in the absence of explicit reward signals, with applications in classification and model-based reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po .",
            "score": 4,
            "issue_id": 6406,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "7bec8447126da923",
            "authors": [
                "Abdelhakim Benechehab",
                "Gabriel Singer",
                "Corentin LÃ©ger",
                "Youssef Attia El Hili",
                "Giuseppe Paolo",
                "Albert Thomas",
                "Maurizio Filippone",
                "BalÃ¡zs KÃ©gl"
            ],
            "affiliations": [
                "Cognizant AI Lab, Paris",
                "Department of Data Science, EURECOM",
                "Huawei Noahs Ark Lab, Paris",
                "Statistics Program, KAUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07624.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ framework Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Maximum Likelihood Estimation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³Ğ´Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ğ° policy gradient â€” Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğº Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ model-based reinforcement learning. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° RL-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°."
                },
                "en": {
                    "title": "Aligning Generative Models Without Explicit Rewards",
                    "desc": "This paper introduces a bilevel optimization framework designed to improve the alignment of generative models with high-quality datasets, even when explicit reward signals are not available. It contrasts traditional Maximum Likelihood Estimation with Reinforcement Learning methods, highlighting the latter's dependency on reward signals, which can be scarce in real-world scenarios. The proposed framework treats the reward function as a variable in an outer optimization problem, while using a policy gradient objective for the inner problem. The authors provide theoretical insights and demonstrate the framework's applicability in areas like classification and model-based reinforcement learning."
                },
                "zh": {
                    "title": "åŒå±‚ä¼˜åŒ–ï¼šæ— å¥–åŠ±ä¿¡å·ä¸‹çš„ç”Ÿæˆæ¨¡å‹å¯¹é½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºåœ¨ç¼ºä¹æ˜ç¡®å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œå°†ç”Ÿæˆæ¨¡å‹ä¸é«˜è´¨é‡æ•°æ®é›†å¯¹é½ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºåˆ†ç±»å’ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡å°†å¥–åŠ±å‡½æ•°è§†ä¸ºå¤–å±‚é—®é¢˜çš„ä¼˜åŒ–å˜é‡ï¼Œå†…å±‚åˆ™å®šä¹‰ä¸ºç­–ç•¥æ¢¯åº¦ç›®æ ‡ï¼Œè§£å†³äº†ç”Ÿæˆæ¨¡å‹å¯¹é½çš„åŸºæœ¬é—®é¢˜ã€‚æˆ‘ä»¬å¯¹è¯¥ä¼˜åŒ–é—®é¢˜è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è¡¨æ ¼åˆ†ç±»å’ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11650",
            "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
            "url": "https://huggingface.co/papers/2510.11650",
            "abstract": "InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
            "score": 3,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "86292c83b31b7099",
            "authors": [
                "Yuxuan Xue",
                "Xianghui Xie",
                "Margaret Kostyrko",
                "Gerard Pons-Moll"
            ],
            "affiliations": [
                "University of TÃ¼bingen, Germany",
                "University of TÃ¼bingen, TÃ¼bingen AI Center, Germany",
                "University of TÃ¼bingen, TÃ¼bingen AI Center, MPI for Informatics, SIC, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11650.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ 3D-Ğ»ÑĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "InfiniHuman â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… foundation-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ InfiniHumanData Ğ¸Ğ· 111 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ (Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚, ÑÑ‚Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´ĞµĞ¶Ğ´Ğ°) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ InfiniHumanGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‚ĞµĞ»Ğ° SMPL Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unbounded 3D Human Avatar Generation with InfiniHuman",
                    "desc": "The InfiniHuman framework addresses the challenge of generating realistic 3D human avatars by distilling existing models to create large-scale, richly annotated datasets. It utilizes a diffusion-based generative pipeline to produce high-quality avatars that are controllable and diverse in attributes like ethnicity and clothing. The framework includes InfiniHumanData, a dataset with 111K unique identities, each annotated with detailed descriptions and images. This approach significantly enhances visual quality and generation speed compared to current methods, making it a practical solution for scalable avatar generation."
                },
                "zh": {
                    "title": "æ— é™å¯èƒ½çš„3Däººç±»æ•°æ®ç”Ÿæˆ",
                    "desc": "InfiniHumanæ¡†æ¶é€šè¿‡æ‰©æ•£ç”Ÿæˆç®¡é“æç‚¼ç°æœ‰æ¨¡å‹ï¼Œç”Ÿæˆå¤§è§„æ¨¡ã€ä¸°å¯Œæ³¨é‡Šçš„3Däººç±»æ•°æ®ï¼Œå…·æœ‰é«˜è§†è§‰è´¨é‡ã€å¿«é€Ÿæ€§å’Œå¯æ§æ€§ã€‚ç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„3Däººç±»å¤´åƒä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¶µç›–å¤šæ ·å±æ€§æ—¶ã€‚æˆ‘ä»¬æå‡ºçš„InfiniHumanDataæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€å’Œå›¾åƒç”Ÿæˆæ¨¡å‹åˆ›å»ºå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼ŒInfiniHumanGenå®ç°äº†å¿«é€Ÿã€çœŸå®ä¸”å¯ç²¾ç¡®æ§åˆ¶çš„å¤´åƒç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10047",
            "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.10047",
            "abstract": "SwarmSys, a distributed multi-agent framework inspired by swarm intelligence, enhances scalability and adaptability in long-horizon reasoning through specialized roles and self-organizing mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence.",
            "score": 3,
            "issue_id": 6411,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "76689d6960a0d8cf",
            "authors": [
                "Ruohao Li",
                "Hongjun Liu",
                "Leyi Zhao",
                "Zisu Li",
                "Jiawei Li",
                "Jiajun Jiang",
                "Linning Xu",
                "Chen Zhao",
                "Mingming Fan",
                "Chen Liang"
            ],
            "affiliations": [
                "Indiana University",
                "NYU Shanghai",
                "New York University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10047.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Ğ Ğ¾ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SwarmSys â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸, Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ñ‹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹, Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ñ„ĞµÑ€Ğ¾Ğ¼Ğ¾Ğ½Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. SwarmSys Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° LLM."
                },
                "en": {
                    "title": "SwarmSys: Harnessing Swarm Intelligence for Scalable Multi-Agent Reasoning",
                    "desc": "SwarmSys is a distributed multi-agent framework that enhances long-horizon reasoning by using principles from swarm intelligence. It features three specialized rolesâ€”Explorers, Workers, and Validatorsâ€”that work together through self-organizing mechanisms to improve scalability and adaptability. The framework employs adaptive agent profiles and a pheromone-inspired reinforcement mechanism to dynamically allocate tasks without needing centralized control. SwarmSys has shown superior performance in various reasoning tasks, indicating that swarm-inspired coordination can significantly boost the effectiveness of multi-agent systems."
                },
                "zh": {
                    "title": "ç¾¤ä½“æ™ºèƒ½é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¨ç†æ–°èŒƒå¼",
                    "desc": "SwarmSysæ˜¯ä¸€ä¸ªå—ç¾¤ä½“æ™ºèƒ½å¯å‘çš„åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶é—´æ¨ç†çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ç§ä¸“ä¸šè§’è‰²ï¼ˆæ¢ç´¢è€…ã€å·¥ä½œè€…å’ŒéªŒè¯è€…ï¼‰ä¹‹é—´çš„è¿­ä»£äº’åŠ¨å®ç°åè°ƒï¼Œæ”¯æŒåŠ¨æ€ä»»åŠ¡åˆ†é…å’Œè‡ªç»„ç»‡æ”¶æ•›ã€‚SwarmSysç»“åˆäº†è‡ªé€‚åº”ä»£ç†å’Œäº‹ä»¶é…ç½®æ–‡ä»¶ã€åŸºäºåµŒå…¥çš„æ¦‚ç‡åŒ¹é…ä»¥åŠç±»ä¼¼ä¿¡æ¯ç´ çš„å¼ºåŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å…¨å±€ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆåä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSwarmSysåœ¨ç¬¦å·æ¨ç†ã€ç ”ç©¶ç»¼åˆå’Œç§‘å­¦ç¼–ç¨‹ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºçº¿ï¼Œæå‡äº†å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09189",
            "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
            "url": "https://huggingface.co/papers/2510.09189",
            "abstract": "A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available.",
            "score": 3,
            "issue_id": 6399,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "ef9184902ab3f7b8",
            "authors": [
                "Changjiang Gao",
                "Zixian Huang",
                "Jingyang Gong",
                "Shujian Huang",
                "Lei Li",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligent Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09189.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#reasoning",
                    "#training",
                    "#machine_translation",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ instruct-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-XPlus Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑƒĞ°Ñ…Ğ¸Ğ»Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Qwen3, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Translation with Layer-Selective Tuning",
                    "desc": "This paper presents a new method for improving translation in both high- and low-resource languages using a technique called layer-selective tuning on parallel data. The authors introduce the Qwen3-XPlus models, which show notable gains in translation quality, measured by metrics like spBLEU and xComet, especially in low-resource languages such as Swahili. By leveraging small parallel datasets, these models achieve better performance on multilingual tasks while retaining strong reasoning capabilities similar to the original Qwen3 instruct model. This approach simplifies the process of enhancing multilingual translation, making it more accessible for various languages."
                },
                "zh": {
                    "title": "ç¿»è¯‘å¢å¼ºï¼Œæ¨ç†èƒ½åŠ›åŒæå‡ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¿»è¯‘å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡å¯¹å¹³è¡Œæ•°æ®è¿›è¡Œå±‚é€‰æ‹©æ€§è°ƒä¼˜ï¼Œæå‡äº†é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„ç¿»è¯‘æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†Qwen3-XPlusæ¨¡å‹ï¼Œåœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ–¯ç“¦å¸Œé‡Œè¯­ï¼‰ä¸­å®ç°äº†æ˜¾è‘—çš„ç¿»è¯‘æ€§èƒ½æå‡ï¼ŒspBLEUå’ŒxCometæŒ‡æ ‡å‡è¶…è¿‡äº†15å’Œ40ã€‚å³ä½¿ä»…ä½¿ç”¨å°è§„æ¨¡çš„å¹³è¡Œæ•°æ®é›†ï¼ŒQwen3-XPlusåœ¨ä¸ƒä¸ªå¤šè¯­è¨€ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†1åˆ†ï¼ŒåŒæ—¶åœ¨15ä¸ªæµè¡Œçš„æ¨ç†æ•°æ®é›†ä¸Šä¿æŒäº†ä¸Qwen3æŒ‡ä»¤æ¨¡å‹ç›¸å½“çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¤šè¯­è¨€å¢å¼ºæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å¤æ‚æ€§ï¼Œå¹¶æé«˜äº†å¯¹æ›´å¹¿æ³›è¯­è¨€çš„å¯åŠæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04201",
            "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven\n  World Knowledge",
            "url": "https://huggingface.co/papers/2510.04201",
            "abstract": "World-To-Image enhances text-to-image generation by integrating web-based knowledge retrieval and multimodal prompt optimization, improving semantic accuracy and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available herehttps://github.com/mhson-kyle/World-To-Image.",
            "score": 3,
            "issue_id": 6410,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "40d451801cd647f2",
            "authors": [
                "Moo Hyun Son",
                "Jintaek Oh",
                "Sun Bin Mun",
                "Jaechul Roh",
                "Sehyun Choi"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "The Hong Kong University of Science and Technology",
                "TwelveLabs",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04201.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#benchmark",
                    "#rag",
                    "#diffusion"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ World-To-Image â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ text-to-image Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°. ĞšĞ¾Ğ³Ğ´Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑÑ…, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ñ‰ĞµÑ‚ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 8.1% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ñ‚Ñ€Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging Knowledge Gaps in Text-to-Image Generation",
                    "desc": "World-To-Image is a new framework that enhances text-to-image (T2I) generation by integrating real-time web knowledge retrieval. It addresses the challenge of generating images for unfamiliar or out-of-distribution concepts by using an agent that searches the internet for relevant images. This information is then utilized to optimize prompts, guiding the generative model to produce more accurate and visually appealing images. The framework demonstrates significant improvements in semantic alignment and visual quality, outperforming existing methods and achieving results efficiently in just a few iterations."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "World-To-Image æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚å®ƒé€šè¿‡åŠ¨æ€æœç´¢ç½‘ç»œè·å–æœªçŸ¥æ¦‚å¿µçš„å›¾åƒï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„çŸ¥è¯†åŸºç¡€ã€‚è¯¥æ¡†æ¶è¿˜è¿›è¡Œå¤šæ¨¡æ€æç¤ºä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒåœ¨è¯­ä¹‰å’Œè§†è§‰è´¨é‡ä¸Šéƒ½æ›´ä¸ºå‡†ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWorld-To-Image åœ¨è¯­ä¹‰å¯¹é½å’Œè§†è§‰ç¾å­¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10681",
            "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
            "url": "https://huggingface.co/papers/2510.10681",
            "abstract": "RePro, a reinforcement learning-based method, generates high-quality rephrasings of pretraining data to enhance the efficiency and accuracy of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at https://github.com/cxcscmu/RePro.",
            "score": 2,
            "issue_id": 6400,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "5b9d6a188a6e99ef",
            "authors": [
                "Zichun Yu",
                "Chenyan Xiong"
            ],
            "affiliations": [
                "Language Technologies Institute, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10681.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#optimization",
                    "#open_source",
                    "#transfer_learning",
                    "#rl"
                ],
                "emoji": "â™»ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RePro â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¼Ñ‹ÑĞ»Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RePro, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.7-14% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 2-3 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "RePro: Recycling Data for Smarter Language Models",
                    "desc": "RePro is a reinforcement learning method designed to improve the quality of pretraining data for large language models (LLMs). It generates high-quality rephrasings of existing data while preserving the original meaning and structure. By using a combination of quality and faithfulness rewards, RePro trains a smaller language model to effectively recycle data, leading to significant accuracy improvements in downstream tasks. The results show that RePro enhances data efficiency and outperforms existing methods, making it a valuable tool for optimizing LLM pretraining."
                },
                "zh": {
                    "title": "ReProï¼šé«˜æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ•°æ®çš„é‡è¿°æ–¹æ³•",
                    "desc": "ReProæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é‡è¿°ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç”Ÿæˆæœ‰æ•ˆä¸”å¿ å®çš„é‡è¿°ï¼Œä¿æŒåŸå§‹æ•°æ®çš„æ ¸å¿ƒè¯­ä¹‰å’Œç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReProåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ç›¸è¾ƒäºä»…ä½¿ç”¨åŸå§‹æ•°æ®çš„åŸºçº¿æ¨¡å‹ï¼Œæå‡äº†4.7%åˆ°14.0%çš„ç›¸å¯¹å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒReProåœ¨ä¿¡æ¯ä¿ç•™å’Œå¯¹åŸå§‹æ•°æ®ç‰¹å¾çš„å¿ å®åæ˜ æ–¹é¢ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08744",
            "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
            "url": "https://huggingface.co/papers/2510.08744",
            "abstract": "DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5times fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000times larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.",
            "score": 2,
            "issue_id": 6399,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "4fd43962056ad19e",
            "authors": [
                "Gang Liu",
                "Jie Chen",
                "Yihan Zhu",
                "Michael Sun",
                "Tengfei Luo",
                "Nitesh V Chawla",
                "Meng Jiang"
            ],
            "affiliations": [
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab, IBM Research",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08744.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ’Š",
                "ru": {
                    "title": "ĞœĞ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼: DemoDiff ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "DemoDiff â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹ Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸, Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Node Pair Encoding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² 5.5 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 0.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ°Ğº Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ DemoDiff Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ² 100-1000 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Molecular Design with DemoDiff",
                    "desc": "DemoDiff is a novel diffusion model designed for molecular design that leverages demonstration-conditioned learning. It utilizes a small set of molecule-score examples to guide a denoising Transformer, allowing it to generate molecules that meet specific target properties. This approach overcomes the limitations of existing large language models and domain-specific methods by using a new molecular tokenizer that operates at the motif level, significantly reducing the complexity of the input data. The model has been pretrained on a vast dataset and has shown superior performance across multiple design tasks, establishing itself as a powerful tool for in-context molecular design."
                },
                "zh": {
                    "title": "DemoDiffï¼šåˆ†å­è®¾è®¡çš„æ–°åŸºç¡€æ¨¡å‹",
                    "desc": "DemoDiffæ˜¯ä¸€ç§åŸºäºç¤ºä¾‹çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆ†å­è®¾è®¡ã€‚å®ƒé€šè¿‡å°‘é‡çš„åˆ†å­è¯„åˆ†ç¤ºä¾‹æ¥æŒ‡å¯¼å»å™ªTransformerï¼Œä»è€Œç”Ÿæˆç¬¦åˆç›®æ ‡å±æ€§çš„åˆ†å­ã€‚ä¸ä¼ ç»Ÿçš„æ–‡æœ¬æè¿°æ–¹æ³•ç›¸æ¯”ï¼ŒDemoDiffåœ¨å¤šä¸ªè®¾è®¡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œé¢†åŸŸç‰¹å®šçš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹çš„é¢„è®­ç»ƒä½¿ç”¨äº†ä¸€ç§æ–°çš„åˆ†å­æ ‡è®°å™¨ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºåˆ†å­åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05213",
            "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
            "url": "https://huggingface.co/papers/2510.05213",
            "abstract": "VER, a Vision Expert Transformer, dynamically selects task-relevant experts from a pretrained vision expert library, achieving state-of-the-art performance across diverse robotic tasks with parameter-efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
            "score": 2,
            "issue_id": 6402,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "48b97783c123b14c",
            "authors": [
                "Yixiao Wang",
                "Mingxiao Huo",
                "Zhixuan Liang",
                "Yushi Du",
                "Lingfeng Sun",
                "Haotian Lin",
                "Jinghuan Shang",
                "Chensheng Peng",
                "Mohit Bansal",
                "Mingyu Ding",
                "Masayoshi Tomizuka"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Peking University",
                "Stony Brook University",
                "UC Berkeley",
                "UNC-Chapel Hill",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05213.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#training",
                    "#agents",
                    "#robotics",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "VER â€” ÑÑ‚Ğ¾ Vision Expert Transformer Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… vision foundation models. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºÑƒÑ routing-ÑĞµÑ‚ÑŒ (Ğ¼ĞµĞ½ĞµĞµ 0.4% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Patchwise Expert Routing Ñ Curriculum Top-K Annealing Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. VER Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 17 Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆÑƒĞ¼."
                },
                "en": {
                    "title": "Dynamic Expert Selection for Efficient Robotic Learning",
                    "desc": "The paper introduces VER, a Vision Expert Transformer designed to enhance robotic learning by dynamically selecting relevant experts from a library of pretrained vision models. This approach allows for efficient fine-tuning, requiring only a small fraction of parameters to adapt to various tasks without the need for extensive retraining. By implementing Patchwise Expert Routing with Curriculum Top-K Annealing, VER improves the selection process, ensuring that the model focuses on critical visual features while ignoring irrelevant background information. The results demonstrate that VER achieves state-of-the-art performance across 17 diverse robotic tasks, showcasing its flexibility and efficiency in integrating domain knowledge."
                },
                "zh": {
                    "title": "åŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œæå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡",
                    "desc": "VERæ˜¯ä¸€ç§è§†è§‰ä¸“å®¶å˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»é¢„è®­ç»ƒçš„è§†è§‰ä¸“å®¶åº“ä¸­åŠ¨æ€é€‰æ‹©ä¸ä»»åŠ¡ç›¸å…³çš„ä¸“å®¶ï¼Œä»è€Œåœ¨å¤šç§æœºå™¨äººä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä»»åŠ¡çš„é€šç”¨æ€§ä¸Šå­˜åœ¨é™åˆ¶ã€‚VERé€šè¿‡ç²¾ç®€çš„è·¯ç”±ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œä»…éœ€ä¸åˆ°0.4%çš„å‚æ•°ï¼Œå°±èƒ½çµæ´»é€‰æ‹©åˆé€‚çš„ä¸“å®¶ï¼Œé¿å…äº†ç¹é‡çš„å…¨é‡é‡è®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨17ä¸ªä¸åŒçš„æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèšç„¦äºä»»åŠ¡å…³é”®åŒºåŸŸï¼Œå‡å°‘æ— å…³åŒºåŸŸçš„å¹²æ‰°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01427",
            "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge\n  Mining",
            "url": "https://huggingface.co/papers/2510.01427",
            "abstract": "Falconer combines large language models with lightweight proxy models to achieve scalable and efficient knowledge mining, reducing inference costs and accelerating large-scale operations.  \t\t\t\t\tAI-generated summary \t\t\t\t At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.",
            "score": 2,
            "issue_id": 6413,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "ddb76c7cbc2c5fa2",
            "authors": [
                "Sipeng Zhang",
                "Longfei Yun",
                "Zilong Wang",
                "Jingbo Shang",
                "Letian Peng"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01427.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#inference",
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: LLM Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Falconer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²: Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ²Ğµ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Falconer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ½Ğ° 90% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 20 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Falconer: Efficient Knowledge Mining with LLMs and Proxies",
                    "desc": "Falconer is a framework that enhances knowledge mining by integrating large language models (LLMs) with lightweight proxy models. It addresses the high costs of deploying LLMs at scale by using them to plan and annotate tasks, while smaller models handle execution efficiently. This approach simplifies the process by combining classification and extraction into two main operations, allowing for a single model to manage multiple tasks. Experimental results demonstrate that Falconer achieves comparable accuracy to state-of-the-art LLMs while significantly reducing inference costs and improving processing speed."
                },
                "zh": {
                    "title": "Falconerï¼šé«˜æ•ˆçŸ¥è¯†æŒ–æ˜çš„æ–°æ¡†æ¶",
                    "desc": "Falconer æ˜¯ä¸€ä¸ªç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§ä»£ç†æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯æ‰©å±•å’Œé«˜æ•ˆçš„çŸ¥è¯†æŒ–æ˜ã€‚å®ƒé€šè¿‡å°†ç”¨æˆ·æŒ‡ä»¤åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„ç®¡é“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè§„åˆ’å’Œæ³¨é‡Šï¼Œä»è€Œè®­ç»ƒå°å‹ä»£ç†æ¨¡å‹ã€‚è¯¥æ¡†æ¶å°†åˆ†ç±»å’Œæå–ç»Ÿä¸€ä¸ºä¸¤ä¸ªåŸºæœ¬æ“ä½œï¼Œç®€åŒ–äº†ä»»åŠ¡å¤„ç†æµç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒFalconer åœ¨æŒ‡ä»¤è·Ÿéšå‡†ç¡®æ€§ä¸Šä¸æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½äº†90%ï¼Œå¹¶åŠ é€Ÿäº†çŸ¥è¯†æŒ–æ˜çš„é€Ÿåº¦è¶…è¿‡20å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11713",
            "title": "Are Large Reasoning Models Interruptible?",
            "url": "https://huggingface.co/papers/2510.11713",
            "abstract": "Large Reasoning Models evaluated in dynamic scenarios with interruptions and changing context show significant performance drops compared to static evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.",
            "score": 1,
            "issue_id": 6400,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "db4c17199de43bcd",
            "authors": [
                "Tsung-Han Wu",
                "Mihran Miroyan",
                "David M. Chan",
                "Trevor Darrell",
                "Narges Norouzi",
                "Joseph E. Gonzalez"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11713.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ´ÑƒĞ¼Ğ°ĞµÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Â«Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€Â», Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğµ Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ½Ğ¸Ğ¶Ğ°Ğ»Ğ°ÑÑŒ Ğ´Ğ¾ 60%, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: ÑƒÑ‚ĞµÑ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¿Ğ°Ğ½Ğ¸ĞºĞ° Ñ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ¼ Ğ¾Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Challenging the Frozen World: Evaluating LRMs in Dynamic Contexts",
                    "desc": "This paper investigates the performance of Large Reasoning Models (LRMs) in dynamic environments, where interruptions and changing contexts can significantly impact their effectiveness. Traditionally, LRMs are evaluated in static scenarios, which do not reflect real-world applications where tasks may evolve over time. The authors demonstrate that static evaluations can overestimate the robustness of these models, revealing performance drops of up to 60% in realistic settings. They identify new failure modes such as reasoning leakage, panic, and self-doubt, which highlight the challenges LRMs face when adapting to interruptions and context changes."
                },
                "zh": {
                    "title": "æŒ‘æˆ˜é™æ€è¯„ä¼°ï¼Œæå‡åŠ¨æ€æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­æ–­å’Œå˜åŒ–çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™äº›æ¨¡å‹åœ¨é™æ€ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œä½†åœ¨ç°ä»£æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¿™ç§å‡è®¾ä¸å†é€‚ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨é¢å¯¹ä¸­æ–­æˆ–ä¸Šä¸‹æ–‡å˜åŒ–æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™é«˜è¾¾60%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯èƒ½å‡ºç°æ–°çš„å¤±è´¥æ¨¡å¼ï¼Œå¦‚æ¨ç†æ³„æ¼ã€ææ…Œå’Œè‡ªæˆ‘æ€€ç–‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11647",
            "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
            "url": "https://huggingface.co/papers/2510.11647",
            "abstract": "IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.",
            "score": 1,
            "issue_id": 6399,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "d6b492d4f314d156",
            "authors": [
                "Yinan Chen",
                "Jiangning Zhang",
                "Teng Hu",
                "Yuxiang Zeng",
                "Zhucun Xue",
                "Qingdong He",
                "Chengjie Wang",
                "Yong Liu",
                "Xiaobin Hu",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tencent Youtu Lab",
                "University of Auckland",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11647.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° AI-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "IVEBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 600 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ 32 Ğ´Ğ¾ 1024 ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 35 Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ°Ñ‘Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‰Ğ¸ĞµÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "IVEBench: A Comprehensive Benchmark for Instruction-Guided Video Editing",
                    "desc": "IVEBench is a new benchmark suite designed for evaluating instruction-guided video editing, addressing the shortcomings of existing benchmarks. It features a diverse collection of 600 high-quality videos and covers a wide range of editing tasks, ensuring comprehensive assessment. The evaluation protocol is multi-dimensional, focusing on video quality, adherence to instructions, and overall fidelity, using both traditional metrics and advanced assessments from large language models. Extensive testing shows that IVEBench effectively benchmarks the latest methods in instruction-guided video editing, providing results that align well with human judgment."
                },
                "zh": {
                    "title": "IVEBenchï¼šæŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘çš„å…¨æ–°è¯„ä¼°æ ‡å‡†",
                    "desc": "IVEBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹æŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¤šæ ·åŒ–çš„è§†é¢‘æ¥æºã€å…¨é¢çš„ä»»åŠ¡è¦†ç›–å’Œå¤šç»´åº¦çš„è¯„ä¼°åè®®ï¼Œæä¾›äº†æ›´ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚IVEBenchåŒ…å«600ä¸ªé«˜è´¨é‡æºè§†é¢‘ï¼Œæ¶µç›–ä¸ƒä¸ªè¯­ä¹‰ç»´åº¦ï¼Œå¹¶è®¾æœ‰8ç±»ç¼–è¾‘ä»»åŠ¡åŠ35ä¸ªå­ç±»åˆ«ã€‚è¯¥åŸºå‡†è¿˜å»ºç«‹äº†ä¸€ä¸ªä¸‰ç»´è¯„ä¼°åè®®ï¼Œç»“åˆäº†ä¼ ç»ŸæŒ‡æ ‡å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æœ€å…ˆè¿›çš„æŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11496",
            "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
            "url": "https://huggingface.co/papers/2510.11496",
            "abstract": "AndesVL, a suite of mobile-side MLLMs with reduced parameters, achieves top-tier performance across various benchmarks compared to similar-scale models.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoR",
            "score": 1,
            "issue_id": 6412,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "26e04dbc57f4bf84",
            "authors": [
                "Zhiwei Jin",
                "Xiaohui Song",
                "Nan Wang",
                "Yafei Liu",
                "Chao Li",
                "Xin Li",
                "Ruichen Wang",
                "Zhihao Li",
                "Qi Qi",
                "Long Cheng",
                "Dongze Hao",
                "Quanlong Zheng",
                "Yanhao Zhang",
                "Haobo Ji",
                "Jian Ma",
                "Zhitong Zheng",
                "Zhenyi Lin",
                "Haolin Deng",
                "Xin Zou",
                "Xiaojie Yin",
                "Ruilin Wang",
                "Liankai Cai",
                "Haijing Liu",
                "Yuqing Qiu",
                "Ke Chen",
                "Zixian Li",
                "Chi Xie",
                "Huafei Li",
                "Chenxing Li",
                "Chuangchuang Wang",
                "Kai Tang",
                "Zhiguang Zhu",
                "Kai Tang",
                "Wenmei Gao",
                "Rui Wang",
                "Jun Wu",
                "Chao Liu",
                "Qin Xie",
                "Chen Chen",
                "Haonan Lu"
            ],
            "affiliations": [
                "OPPO AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11496.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#long_context",
                    "#multilingual",
                    "#benchmark",
                    "#optimization",
                    "#small_models",
                    "#architecture",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞœĞ¾Ñ‰Ğ½Ñ‹Ğµ multimodal Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AndesVL â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… multimodal LLM Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 0.6B Ğ´Ğ¾ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ‚Ğ½ÑĞ¼Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², AndesVL ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ LLM Qwen3 Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ¿-ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². AndesVL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, multilingual Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ GUI-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile Devices: AndesVL's Breakthrough",
                    "desc": "AndesVL is a collection of mobile-friendly machine learning language models (MLLMs) designed to operate efficiently on devices with limited resources. Unlike larger cloud-based models that require significant memory and power, AndesVL maintains high performance with a smaller parameter range of 0.6B to 4B. The paper details the architecture, training methods, and datasets used for AndesVL, demonstrating its effectiveness in various tasks such as image understanding and multilingual processing. Overall, AndesVL showcases that it is possible to achieve competitive results in machine learning while optimizing for mobile device constraints."
                },
                "zh": {
                    "title": "ç§»åŠ¨è®¾å¤‡çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "AndesVLæ˜¯ä¸€å¥—é’ˆå¯¹ç§»åŠ¨è®¾å¤‡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…¶å‚æ•°é‡ä»6äº¿åˆ°40äº¿ä¸ç­‰ã€‚ä¸å…¶ä»–å¤§å‹äº‘ç«¯æ¨¡å‹ç›¸æ¯”ï¼ŒAndesVLåœ¨å†…å­˜ã€åŠŸè€—å’Œè®¡ç®—èƒ½åŠ›æ–¹é¢æ›´é€‚åˆè¾¹ç¼˜è®¾å¤‡ã€‚è¯¥è®ºæ–‡è¯¦ç»†ä»‹ç»äº†AndesVLçš„æ¨¡å‹æ¶æ„ã€è®­ç»ƒæµç¨‹å’Œè®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨å¤šä¸ªå¼€æºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒåœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç†è§£ã€æ¨ç†ä¸æ•°å­¦ã€å¤šå›¾åƒç†è§£ç­‰é¢†åŸŸè¾¾åˆ°äº†é¡¶å°–æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10606",
            "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large\n  Vision-and-Language Models",
            "url": "https://huggingface.co/papers/2510.10606",
            "abstract": "ViSurf combines Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards to enhance Large Vision-and-Language Models, outperforming individual methods and two-stage approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical post-training paradigms for Large Vision-and-Language Models (LVLMs) include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). SFT leverages external guidance to inject new knowledge, whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities and overall performance. However, our analysis reveals that SFT often leads to sub-optimal performance, while RLVR struggles with tasks that exceed the model's internal knowledge base. To address these limitations, we propose ViSurf (Visual Supervised-and-Reinforcement Fine-Tuning), a unified post-training paradigm that integrates the strengths of both SFT and RLVR within a single stage. We analyze the derivation of the SFT and RLVR objectives to establish the ViSurf objective, providing a unified perspective on these two paradigms. The core of ViSurf involves injecting ground-truth labels into the RLVR rollouts, thereby providing simultaneous external supervision and internal reinforcement. Furthermore, we introduce three novel reward control strategies to stabilize and optimize the training process. Extensive experiments across several diverse benchmarks demonstrate the effectiveness of ViSurf, outperforming both individual SFT, RLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates these findings, validating the derivation and design principles of ViSurf.",
            "score": 1,
            "issue_id": 6410,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "c6ae5b2b34b2d519",
            "authors": [
                "Yuqi Liu",
                "Liangyu Chen",
                "Jiazhen Liu",
                "Mingkang Zhu",
                "Zhisheng Zhong",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "RUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10606.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ViSurf: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· supervision Ğ¸ reinforcement Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViSurf â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ğ¸ reinforcement learning Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SFT Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°Ñ‘Ñ‚ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ° RLVR Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ViSurf Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ (ground-truth labels) Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ RLVR, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞµ supervision Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ reinforcement Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ViSurf Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ SFT Ğ¸ RLVR, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ViSurf: Uniting Supervised and Reinforcement Learning for Superior Model Performance",
                    "desc": "ViSurf is a novel approach that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve the performance of Large Vision-and-Language Models (LVLMs). By integrating these two methods, ViSurf addresses the limitations of SFT, which can lead to sub-optimal results, and RLVR, which may struggle with tasks beyond the model's knowledge. The method injects ground-truth labels into RLVR rollouts, allowing for both external supervision and internal reinforcement during training. Extensive experiments show that ViSurf outperforms traditional SFT, RLVR, and two-stage approaches, confirming its effectiveness and innovative design."
                },
                "zh": {
                    "title": "ViSurfï¼šèåˆç›‘ç£ä¸å¼ºåŒ–å­¦ä¹ çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ViSurfæ˜¯ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è§†è§‰ä¸è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¾èµ–å¤–éƒ¨æŒ‡å¯¼ï¼Œè€Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åˆ™åˆ©ç”¨å†…éƒ¨å¼ºåŒ–æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSFTå¸¸å¸¸å¯¼è‡´æ¬¡ä¼˜è¡¨ç°ï¼Œè€ŒRLVRåœ¨è¶…å‡ºæ¨¡å‹çŸ¥è¯†èŒƒå›´çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ViSurfé€šè¿‡å°†çœŸå®æ ‡ç­¾æ³¨å…¥RLVRçš„å›åˆä¸­ï¼Œå®ç°äº†å¤–éƒ¨ç›‘ç£ä¸å†…éƒ¨å¼ºåŒ–çš„ç»“åˆï¼Œä»è€Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å•ç‹¬çš„SFTå’ŒRLVRæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10493",
            "title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable\n  High-Accuracy Authorship Attribution",
            "url": "https://huggingface.co/papers/2510.10493",
            "abstract": "A study on authorship attribution of JavaScript code generated by large language models using a custom dataset and advanced machine learning classifiers demonstrates high accuracy even after code transformations.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing a critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as a single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000 Node.js back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and a modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As a result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: https://github.com/LLM-NodeJS-dataset.",
            "score": 1,
            "issue_id": 6401,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "470da7606b9a3faa",
            "authors": [
                "Norbert Tihanyi",
                "Bilel Cherif",
                "Richard A. Dubniczky",
                "Mohamed Amine Ferrag",
                "TamÃ¡s Bisztray"
            ],
            "affiliations": [
                "EÃ¶tvÃ¶s LorÃ¡nd University, Budapest, Hungary",
                "Technology Innovation Institute, Abu Dhabi, United Arab Emirates",
                "United Arab Emirates University, Al Ain, United Arab Emirates",
                "University of Oslo, Oslo, Norway"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10493.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#security",
                    "#architecture",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ°Ğ¶Ğ´Ğ°Ñ LLM Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‡ĞµÑ€Ğº Ğ² JavaScript-ĞºĞ¾Ğ´Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LLM-NodeJS Ğ¸Ğ· 50,000 Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ½Ğ° JavaScript, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 20 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ°Ñ LLM Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ¸ Ğ² ĞºĞ¾Ğ´Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° CodeT5-JSA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 95.8% Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ 88.5% Ğ¿Ğ¾ Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ BERT Ğ¸ CodeBERT. ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ„ÑƒÑĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ°, Ñ‚Ğ°Ğº ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸."
                },
                "en": {
                    "title": "Unmasking AI: Identifying Code Authors with Precision",
                    "desc": "This paper investigates how to identify which large language model (LLM) generated specific JavaScript code, a process known as authorship attribution. The authors created a unique dataset called LLM-NodeJS, consisting of 50,000 JavaScript programs from 20 different LLMs, with multiple transformed versions to enhance the analysis. They developed a custom machine learning model, CodeT5-JSA, which achieved high accuracy in classifying the authorship of the code, even after significant modifications. The study highlights that different LLMs produce distinct stylistic signatures, allowing for effective attribution and accountability in AI-generated code."
                },
                "zh": {
                    "title": "æ­ç¤ºAIç”Ÿæˆä»£ç çš„ä½œè€…èº«ä»½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„JavaScriptä»£ç çš„ä½œè€…å½’å±é—®é¢˜ï¼Œä½¿ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†å’Œå…ˆè¿›çš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨ä»£ç ç»è¿‡å˜æ¢åï¼Œæ¨¡å‹ä»èƒ½é«˜æ•ˆåœ°è¯†åˆ«å‡ºç”Ÿæˆä»£ç çš„å…·ä½“æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†LLM-NodeJSæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª20ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„50,000ä¸ªNode.jsåç«¯ç¨‹åºï¼Œæä¾›äº†å¤šç§å˜ä½“ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚é€šè¿‡å¯¹æ¯”ä¼ ç»Ÿåˆ†ç±»å™¨å’Œæ”¹è¿›çš„Transformerç¼–ç å™¨ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç±»å½’å±ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡é«˜è¾¾95.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09871",
            "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed\n  Societal Biases in LLMs",
            "url": "https://huggingface.co/papers/2510.09871",
            "abstract": "CoBia, a suite of adversarial attacks, reveals that LLMs often fail to reject biased follow-up questions, highlighting embedded biases in conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia.",
            "score": 1,
            "issue_id": 6407,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "5aa7c7b68471d29a",
            "authors": [
                "Nafiseh Nikeghbal",
                "Amir Hossein Kargaran",
                "Jana Diesner"
            ],
            "affiliations": [
                "LMU Munich & Munich Center for Machine Learning",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09871.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#alignment",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#ethics"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ CoBia â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ°Ğ´Ğ²ĞµÑ€ÑĞ°Ñ€Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞºĞ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, ÑĞ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ğ¾Ğ½Ğ° Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 11 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğµ ÑÑ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unmasking Bias: CoBia's Challenge to LLMs",
                    "desc": "The paper introduces CoBia, a set of adversarial attacks designed to expose biases in large language models (LLMs) during conversations. It systematically tests how LLMs respond to biased statements and whether they can reject biased follow-up questions. The study evaluates multiple LLMs across various socio-demographic categories to assess their alignment with ethical standards. Findings indicate that LLMs frequently fail to address and reject biased inquiries, revealing significant embedded biases in their conversational behavior."
                },
                "zh": {
                    "title": "æ­ç¤ºLLMsä¸­çš„åè§ï¼šCoBiaçš„æŒ‘æˆ˜",
                    "desc": "CoBiaæ˜¯ä¸€å¥—å¯¹æŠ—æ€§æ”»å‡»å·¥å…·ï¼Œæ—¨åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä¸­å¯¹åè§åç»­é—®é¢˜çš„æ‹’ç»èƒ½åŠ›ä¸è¶³ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨å®‰å…¨æ£€æŸ¥ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬ä»å¯èƒ½åœ¨å¯¹è¯ä¸­è¡¨ç°å‡ºæœ‰å®³è¡Œä¸ºï¼Œå¦‚ç§æ—æ­§è§†è§‚ç‚¹ã€‚é€šè¿‡æ„å»ºç‰¹å®šçš„å¯¹è¯åœºæ™¯ï¼ŒCoBiaè¯„ä¼°æ¨¡å‹åœ¨é¢å¯¹è™šæ„çš„åè§å£°æ˜æ—¶çš„ååº”èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¯¹è¯ä¸­ç»å¸¸æœªèƒ½æ‹’ç»åè§çš„åç»­é—®é¢˜ï¼Œæ­ç¤ºäº†æ·±å±‚æ¬¡çš„åµŒå…¥åè§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09474",
            "title": "Multimodal Policy Internalization for Conversational Agents",
            "url": "https://huggingface.co/papers/2510.09474",
            "abstract": "Multimodal Policy Internalization (MPI) internalizes complex multimodal policies into model parameters, enhancing policy adherence and performance in conversational agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.",
            "score": 1,
            "issue_id": 6413,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "075badde1981865c",
            "authors": [
                "Zhenhailong Wang",
                "Jiateng Liu",
                "Amin Fazel",
                "Ritesh Sarkhel",
                "Xing Fan",
                "Xiang Li",
                "Chenlei Guo",
                "Heng Ji",
                "Ruhi Sarikaya"
            ],
            "affiliations": [
                "Amazon",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09474.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#dataset",
                    "#games",
                    "#synthetic",
                    "#agents",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multimodal Policy Internalization (MPI) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº (Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ) Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ TriMPI â€” Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ continual pretraining Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ñ…, supervised finetuning Ğ¸ PolicyRollout â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Internalizing Multimodal Policies for Smarter Conversational Agents",
                    "desc": "Multimodal Policy Internalization (MPI) is a method that integrates complex multimodal policies directly into the parameters of conversational agents, improving their ability to follow these policies and perform better. Traditional systems often struggle with lengthy and intricate policies, which can lead to high computational costs and reduced adherence. MPI addresses this by internalizing reasoning-intensive policies, allowing agents to operate without needing to reference the policy during their responses. The proposed TriMPI framework enhances this process through a three-stage training approach, resulting in improved accuracy, generalization, and resilience against forgetting previous knowledge."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ”¿ç­–å†…åŒ–ï¼šæå‡å¯¹è¯ä»£ç†æ€§èƒ½çš„å…³é”®",
                    "desc": "å¤šæ¨¡æ€æ”¿ç­–å†…åŒ–ï¼ˆMPIï¼‰æ˜¯ä¸€ç§å°†å¤æ‚çš„å¤šæ¨¡æ€æ”¿ç­–å†…åŒ–ä¸ºæ¨¡å‹å‚æ•°çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¯¹è¯ä»£ç†çš„æ”¿ç­–éµå¾ªæ€§å’Œæ€§èƒ½ã€‚ç°ä»£å¯¹è¯ä»£ç†å¦‚ChatGPTå’ŒAlexa+ä¾èµ–äºé¢„å®šä¹‰çš„æ”¿ç­–ï¼Œè¿™äº›æ”¿ç­–è§„å®šäº†å…ƒæ•°æ®ã€å“åº”é£æ ¼å’Œå·¥å…·ä½¿ç”¨è§„åˆ™ã€‚éšç€å¤šæ¨¡æ€ä»£ç†çš„å…´èµ·ï¼Œç®¡ç†è§†è§‰å’Œå¤šæ¨¡æ€è¡Œä¸ºçš„æ”¿ç­–å˜å¾—è‡³å…³é‡è¦ï¼Œä½†ç›¸å…³ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºçš„TriMPIæ¡†æ¶é€šè¿‡æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œæ”¿ç­–å›æ»šç­‰æ­¥éª¤ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¯¹é—å¿˜çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09023",
            "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses\n  Against Llm Jailbreaks and Prompt Injections",
            "url": "https://huggingface.co/papers/2510.09023",
            "abstract": "Defenses against jailbreaks and prompt injections in language models should be evaluated against adaptive attackers using advanced optimization techniques to ensure robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed.   Instead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.",
            "score": 1,
            "issue_id": 6413,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "826b1769a9c577bd",
            "authors": [
                "Milad Nasr",
                "Nicholas Carlini",
                "Chawin Sitawarin",
                "Sander V. Schulhoff",
                "Jamie Hayes",
                "Michael Ilie",
                "Juliette Pluto",
                "Shuang Song",
                "Harsh Chaudhari",
                "Ilia Shumailov",
                "Abhradeep Thakurta",
                "Kai Yuanqing Xiao",
                "Andreas Terzis",
                "Florian TramÃ¨r"
            ],
            "affiliations": [
                "AI Sequrity Company",
                "Anthropic",
                "ETH ZÃ¼rich",
                "Google DeepMind",
                "HackAPrompt",
                "MATS",
                "Northeastern University",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#rl",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ»Ğ¾Ğ¼Ğ°ÑÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ jailbreak-Ğ°Ñ‚Ğ°Ğº Ğ¸ prompt injection Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ»Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ gradient descent, reinforcement learning, random search Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñƒ, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ 12 Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… Ğ·Ğ°Ñ‰Ğ¸Ñ‚ Ñ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 90%, Ñ…Ğ¾Ñ‚Ñ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ ÑÑ‚Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ñ… Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Evaluating Defenses Against Smart Attackers",
                    "desc": "This paper discusses the need for better evaluation methods for defenses against jailbreaks and prompt injections in language models. Current methods often use static attack strings or weak optimization techniques, which do not accurately reflect real-world threats. The authors propose that defenses should be tested against adaptive attackers who can modify their strategies and optimize their attacks. By using advanced optimization techniques, they demonstrate that many existing defenses can be easily bypassed, highlighting the importance of robust evaluation in developing effective defenses."
                },
                "zh": {
                    "title": "è¯„ä¼°è¯­è¨€æ¨¡å‹é˜²å¾¡çš„é²æ£’æ€§",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å¦‚ä½•è¯„ä¼°è¯­è¨€æ¨¡å‹é˜²å¾¡æœºåˆ¶çš„é²æ£’æ€§ã€‚å½“å‰çš„é˜²å¾¡æªæ–½é€šå¸¸åªé’ˆå¯¹é™æ€çš„æ”»å‡»å­—ç¬¦ä¸²æˆ–è®¡ç®—èƒ½åŠ›è¾ƒå¼±çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ç¼ºé™·ã€‚æˆ‘ä»¬å»ºè®®åº”å¯¹é€‚åº”æ€§æ”»å‡»è€…è¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›æ”»å‡»è€…ä¼šæ ¹æ®é˜²å¾¡æœºåˆ¶çš„è®¾è®¡è°ƒæ•´æ”»å‡»ç­–ç•¥ï¼Œå¹¶ä¼˜åŒ–å…¶ç›®æ ‡ã€‚é€šè¿‡ç³»ç»Ÿåœ°è°ƒæ•´å’Œæ‰©å±•ä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬æˆåŠŸç»•è¿‡äº†12ç§æœ€è¿‘çš„é˜²å¾¡æªæ–½ï¼Œæ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06582",
            "title": "Through the Perspective of LiDAR: A Feature-Enriched and\n  Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud\n  Segmentation",
            "url": "https://huggingface.co/papers/2510.06582",
            "abstract": "A semi-automated pipeline using spherical projection, feature enrichment, and ensemble learning reduces manual annotation effort for TLS point cloud segmentation while maintaining high accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.   Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.",
            "score": 1,
            "issue_id": 6409,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "b18368b686c920b1",
            "authors": [
                "Fei Zhang",
                "Rob Chancia",
                "Josie Clapp",
                "Amirhossein Hassanzadeh",
                "Dimah Dera",
                "Richard MacKenzie",
                "Jan van Aardt"
            ],
            "affiliations": [
                "Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA",
                "U.S. Forest Service, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06582.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#data",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ»Ğ°Ğ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ñ‡ĞµÑ€ĞµĞ· ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ pipeline Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ»Ğ°Ğ·ĞµÑ€Ğ½Ñ‹Ğ¼ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ 3D Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµÑ‚ĞºÑƒ, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ensemble learning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ pseudo-labels Ñ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Mangrove3D Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ»ĞµÑĞ¾Ğ², Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ ~12 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºĞ°Ğ½Ğ¾Ğ² Ñ mIoU Ğ¾ĞºĞ¾Ğ»Ğ¾ 0.76. Pipeline Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Streamlining TLS Point Cloud Segmentation with Smart Automation",
                    "desc": "This paper presents a semi-automated pipeline designed to enhance the segmentation of terrestrial laser scanning (TLS) point clouds while minimizing the need for manual annotation. The method employs spherical projection to convert 3D data into a 2D format, enriches the data with various features, and utilizes ensemble learning to generate pseudo-labels and uncertainty maps. These uncertainty maps help identify areas that require further annotation, ensuring that the segmentation remains accurate. The authors also introduce the Mangrove3D dataset and provide insights into data efficiency and feature importance, demonstrating that high-quality segmentation can be achieved with fewer annotated scans."
                },
                "zh": {
                    "title": "åŠè‡ªåŠ¨åŒ–ç®¡é“ï¼Œæå‡TLSç‚¹äº‘åˆ†å‰²æ•ˆç‡ä¸ç²¾åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œåˆ©ç”¨çƒé¢æŠ•å½±ã€ç‰¹å¾å¢å¼ºå’Œé›†æˆå­¦ä¹ æ¥å‡å°‘å¯¹åœ°é¢æ¿€å…‰æ‰«æï¼ˆTLSï¼‰ç‚¹äº‘åˆ†å‰²çš„äººå·¥æ ‡æ³¨å·¥ä½œï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚è¯¥æ–¹æ³•å°†3Dç‚¹æŠ•å½±åˆ°2Dçƒé¢ç½‘æ ¼ä¸­ï¼Œåˆ©ç”¨å¤šæºç‰¹å¾ä¸°å¯Œåƒç´ ï¼Œå¹¶è®­ç»ƒå¤šä¸ªåˆ†å‰²ç½‘ç»œç”Ÿæˆä¼ªæ ‡ç­¾å’Œä¸ç¡®å®šæ€§å›¾ï¼Œåè€…ç”¨äºæŒ‡å¯¼æ¨¡ç³ŠåŒºåŸŸçš„æ ‡æ³¨ã€‚é€šè¿‡è¿™ç§ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†Mangrove3Dæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†æ•°æ®æ•ˆç‡å’Œç‰¹å¾é‡è¦æ€§ï¼Œå‘ç°å¤§çº¦12ä¸ªæ ‡æ³¨æ‰«æåæ€§èƒ½è¶‹äºé¥±å’Œï¼Œå‡ ä½•ç‰¹å¾è´¡çŒ®æœ€å¤§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡è·¨æ•°æ®é›†æµ‹è¯•éªŒè¯äº†ç‰¹å¾å¢å¼ºç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11218",
            "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and\n  Long-Form Answers",
            "url": "https://huggingface.co/papers/2510.11218",
            "abstract": "LLMs exhibit inconsistent factual knowledge retrieval between simple and complex queries, highlighting a reliability gap that undermines trustworthiness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
            "score": 0,
            "issue_id": 6409,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "c2e5d9c6b926219e",
            "authors": [
                "Saad Obaid ul Islam",
                "Anne Lauscher",
                "Goran GlavaÅ¡"
            ],
            "affiliations": [
                "Data Science Group, University of Hamburg",
                "WÃ¼NLP, CAIDAS, University of WÃ¼rzburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11218.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#data",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ½Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ´Ğ°Ñ‚Ğµ Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ­Ğ¹Ğ½ÑˆÑ‚ĞµĞ¹Ğ½Ğ°, Ğ½Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ„Ğ°ĞºÑ‚Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SLAQ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ Ğ¸ Ğ² ÑĞ¾ÑÑ‚Ğ°Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· 16 LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚ momentum, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğº AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼."
                },
                "en": {
                    "title": "Bridging the Reliability Gap in LLMs: Simple vs. Complex Queries",
                    "desc": "This paper investigates the inconsistency in factual knowledge retrieval by large language models (LLMs) when faced with simple versus complex queries. It introduces a framework called Short-Long Form Alignment for Factual Question Answering (SLAQ) to evaluate how LLMs respond to factual questions in isolation compared to when they are embedded in more complex contexts. The study reveals a systematic misalignment in answers, indicating that LLMs struggle with maintaining factual accuracy as query complexity increases. Additionally, it highlights the importance of understanding this reliability gap to improve the trustworthiness of LLMs in real-world applications."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ç®€å•å’Œå¤æ‚æŸ¥è¯¢æ—¶è¡¨ç°å‡ºä¸ä¸€è‡´çš„äº‹å®çŸ¥è¯†æ£€ç´¢ï¼Œæ˜¾ç¤ºå‡ºä¸€ç§å¯é æ€§å·®è·ï¼Œè¿™å‰Šå¼±äº†å®ƒä»¬çš„å¯ä¿¡åº¦ã€‚å°½ç®¡æ¨¡å‹åœ¨ç®€å•çš„äº‹å®é—®ç­”åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æŸ¥è¯¢ä¸­å´å¸¸å¸¸å‡ºç°é”™è¯¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºçŸ­é•¿å½¢å¼å¯¹é½çš„æ¡†æ¶ï¼ˆSLAQï¼‰ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä¸åŒæŸ¥è¯¢å¤æ‚åº¦ä¸‹çš„å›ç­”ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨çŸ­æŸ¥è¯¢å’Œé•¿æŸ¥è¯¢ä¸­çš„å›ç­”å­˜åœ¨ç³»ç»Ÿæ€§ä¸å¯¹é½ï¼Œä¸”è¿™ç§ä¸å¯¹é½ä¸æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„ç›¸äº’ä½œç”¨å¯†åˆ‡ç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08561",
            "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
            "url": "https://huggingface.co/papers/2510.08561",
            "abstract": "MultiCOIN, a video inbetweening framework using the Diffusion Transformer, enables multi-modal controls for precise and flexible video interpolation.  \t\t\t\t\tAI-generated summary \t\t\t\t Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce MultiCOIN, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.",
            "score": 0,
            "issue_id": 6413,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "ec9cec0306b9b05b",
            "authors": [
                "Maham Tanveer",
                "Yang Zhou",
                "Simon Niklaus",
                "Ali Mahdavi Amiri",
                "Hao Zhang",
                "Krishna Kumar Singh",
                "Nanxuan Zhao"
            ],
            "affiliations": [
                "Adobe Research",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08561.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#games",
                    "#training",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "MultiCOIN â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾: ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "MultiCOIN: Precision Video Interpolation with Multi-Modal Control",
                    "desc": "MultiCOIN is a video inbetweening framework that enhances video interpolation by using the Diffusion Transformer architecture. It allows users to control various aspects of video generation, such as depth transitions, motion trajectories, and specific target regions, providing a high level of flexibility and precision. The framework separates content and motion controls into distinct branches, ensuring that the model can effectively learn and apply these multi-modal inputs during the denoising process. Through extensive testing, MultiCOIN demonstrates improved customization and accuracy in creating smooth transitions between video frames, addressing limitations found in previous methods."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ§åˆ¶ï¼Œç²¾ç¡®è§†é¢‘æ’å€¼çš„æœªæ¥",
                    "desc": "MultiCOINæ˜¯ä¸€ä¸ªè§†é¢‘æ’å€¼æ¡†æ¶ï¼Œä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformerï¼‰æ¥å®ç°å¤šæ¨¡æ€æ§åˆ¶ï¼Œèƒ½å¤Ÿç²¾ç¡®ä¸”çµæ´»åœ°è¿›è¡Œè§†é¢‘æ’å€¼ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨ç”Ÿæˆå¤æ‚è¿åŠ¨å’Œç”¨æˆ·æ„å›¾å¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæä¾›äº†å¯¹ä¸­é—´å¸§ç»†èŠ‚çš„ç²¾ç»†æ§åˆ¶ã€‚é€šè¿‡å°†è¿åŠ¨æ§åˆ¶æ˜ å°„åˆ°ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç‚¹çŠ¶è¡¨ç¤ºï¼ŒMultiCOINå®ç°äº†å†…å®¹æ§åˆ¶å’Œè¿åŠ¨æ§åˆ¶çš„åˆ†ç¦»ç¼–ç ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ§åˆ¶ä½¿å¾—è§†é¢‘å™äº‹æ›´åŠ åŠ¨æ€ã€å¯å®šåˆ¶å’Œä¸Šä¸‹æ–‡å‡†ç¡®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04587",
            "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
            "url": "https://huggingface.co/papers/2510.04587",
            "abstract": "A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.  \t\t\t\t\tAI-generated summary \t\t\t\t Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired \"where to look\" and \"why it matters\" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.",
            "score": 0,
            "issue_id": 6398,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "89586b8e177d522a",
            "authors": [
                "Sheng Wang",
                "Ruiming Wu",
                "Charles Herndon",
                "Yihang Liu",
                "Shunsuke Koga",
                "Jeanne Shen",
                "Zhi Huang"
            ],
            "affiliations": [
                "Department of Biostatistics, Epidemiology & Informatics, University of Pennsylvania",
                "Department of Electrical and System Engineering, University of Pennsylvania",
                "Department of Pathology and Laboratory Medicine, University of Pennsylvania",
                "Department of Pathology, Stanford University",
                "Department of Pathology, University of California at San Francisco"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04587.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#reasoning",
                    "#healthcare",
                    "#interpretability",
                    "#science",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ AI Session Recorder â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğµ Ğ³Ğ¸ÑÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ°Ğ³ĞµĞ½Ñ‚ Pathologist-o3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 100% Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ 84.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ°ÑÑ‚Ğ°Ğ·Ğ¾Ğ² Ğ² Ğ»Ğ¸Ğ¼Ñ„Ğ¾ÑƒĞ·Ğ»Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ OpenAI o3. Ğ­Ñ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Transforming Expert Navigation into Smart Pathology Diagnosis",
                    "desc": "This paper presents a new framework that captures expert navigation behavior in whole-slide imaging to enhance pathology diagnosis. It introduces the AI Session Recorder, which records how pathologists interact with images and converts this data into actionable commands for an AI system. The resulting model, Pathologist-o3, uses this behavioral data to identify areas of interest and provide reasoning for its decisions, achieving impressive metrics in detecting metastasis. This approach not only improves diagnostic accuracy but also paves the way for more practical and explainable AI systems in clinical settings."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç—…ç†è¯Šæ–­çš„æ–°è·¯å¾„",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡è®°å½•å’Œåˆ©ç”¨ä¸“å®¶åœ¨å…¨åˆ‡ç‰‡æˆåƒä¸­çš„å¯¼èˆªè¡Œä¸ºï¼Œæ„å»ºäº†ä¸€ç§ç”¨äºç—…ç†è¯Šæ–­çš„æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨è½¬ç§»æ€§è‚¿ç˜¤æ£€æµ‹ä¸­å®ç°äº†é«˜ç²¾åº¦å’Œé«˜å¬å›ç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†AIä¼šè¯è®°å½•å™¨ï¼Œèƒ½å¤Ÿæ— ç¼è®°å½•ä¸“å®¶çš„å¸¸è§„å¯¼èˆªï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„è¡Œä¸ºå‘½ä»¤å’Œè¾¹ç•Œæ¡†ã€‚æœ€ç»ˆï¼ŒåŸºäºè¿™äº›è¡Œä¸ºæ•°æ®ï¼Œæˆ‘ä»¬æ„å»ºäº†Pathologist-o3ï¼Œä¸€ä¸ªèƒ½å¤Ÿæå‡ºæ„Ÿå…´è¶£åŒºåŸŸå¹¶è¿›è¡Œè¡Œä¸ºå¼•å¯¼æ¨ç†çš„åŒé˜¶æ®µæ™ºèƒ½ä½“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-13.html",
    "link_next": "2025-10-15.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "13.10",
        "en": "10/13",
        "zh": "10æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10æœˆ15æ—¥"
    },
    "categories": {
        "#dataset": 19,
        "#data": 10,
        "#benchmark": 28,
        "#agents": 13,
        "#cv": 5,
        "#rl": 14,
        "#rlhf": 4,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 4,
        "#audio": 0,
        "#video": 7,
        "#multimodal": 20,
        "#math": 4,
        "#multilingual": 3,
        "#architecture": 8,
        "#healthcare": 2,
        "#training": 28,
        "#robotics": 2,
        "#agi": 3,
        "#games": 5,
        "#interpretability": 4,
        "#reasoning": 20,
        "#transfer_learning": 7,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 4,
        "#optimization": 28,
        "#survey": 2,
        "#diffusion": 10,
        "#alignment": 6,
        "#story_generation": 1,
        "#hallucinations": 4,
        "#long_context": 1,
        "#synthetic": 5,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 12,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 2,
        "#reinforcement_learning": 1
    }
}