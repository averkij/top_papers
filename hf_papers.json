{
    "date": {
        "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 17",
        "zh": "1æœˆ17æ—¥"
    },
    "time_utc": "2025-01-17 08:12",
    "weekday": 4,
    "issue_id": 1723,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.09751",
            "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
            "url": "https://huggingface.co/papers/2501.09751",
            "abstract": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.",
            "score": 17,
            "issue_id": 1722,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "7e8d42358354f79b",
            "authors": [
                "Zekun Xi",
                "Wenbiao Yin",
                "Jizhan Fang",
                "Jialong Wu",
                "Runnan Fang",
                "Ningyu Zhang",
                "Jiang Yong",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09751.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#story_generation",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OmniThink: Ğ˜Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ OmniThink. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. OmniThink ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ…, Ğ½Ğµ Ğ¶ĞµÑ€Ñ‚Ğ²ÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ OmniThink Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹."
                },
                "en": {
                    "title": "OmniThink: Elevating Machine Writing through Human-Like Learning",
                    "desc": "This paper introduces OmniThink, a novel machine writing framework that enhances the capabilities of large language models by mimicking human cognitive processes. Unlike traditional retrieval-augmented generation methods, which often produce shallow and repetitive content, OmniThink focuses on iterative expansion and reflection to deepen knowledge on topics. The framework significantly improves the knowledge density of generated articles while maintaining coherence and depth, as shown by experimental results. Human evaluations and expert feedback confirm that OmniThink effectively addresses challenges in generating high-quality long-form content."
                },
                "zh": {
                    "title": "OmniThinkï¼šæå‡æœºå™¨å†™ä½œçš„çŸ¥è¯†å¯†åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOmniThinkçš„æœºå™¨å†™ä½œæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„ä¼ ç»Ÿå¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶çš„å±€é™æ€§ã€‚OmniThinkæ¨¡æ‹Ÿäººç±»å­¦ä¹ è€…çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé€šè¿‡è¿­ä»£æ‰©å±•å’Œåæ€æ¥åŠ æ·±å¯¹ä¸»é¢˜çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniThinkèƒ½å¤Ÿæé«˜ç”Ÿæˆæ–‡ç« çš„çŸ¥è¯†å¯†åº¦ï¼ŒåŒæ—¶ä¿æŒè¿è´¯æ€§å’Œæ·±åº¦ç­‰æŒ‡æ ‡ã€‚äººç±»è¯„ä¼°å’Œä¸“å®¶åé¦ˆè¿›ä¸€æ­¥éªŒè¯äº†OmniThinkåœ¨ç”Ÿæˆé•¿ç¯‡æ–‡ç« æ—¶è§£å†³å®é™…é—®é¢˜çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09732",
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "url": "https://huggingface.co/papers/2501.09732",
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "score": 13,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "2ad32c666f91ba05",
            "authors": [
                "Nanye Ma",
                "Shangyuan Tong",
                "Haolin Jia",
                "Hexiang Hu",
                "Yu-Chuan Su",
                "Mingda Zhang",
                "Xuan Yang",
                "Yandong Li",
                "Tommi Jaakkola",
                "Xuhui Jia",
                "Saining Xie"
            ],
            "affiliations": [
                "Google",
                "MIT",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09732.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ÑˆÑƒĞ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¾ÑÑĞ¼: Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Diffusion Models: Scaling Inference for Better Image Generation",
                    "desc": "This paper investigates how to enhance the performance of diffusion models during the inference phase by increasing computational resources. It highlights that, unlike Large Language Models (LLMs), diffusion models can adjust their inference process through the number of denoising steps, but improvements tend to plateau after a certain point. The authors propose a method to optimize the noise used in the diffusion sampling process by exploring different feedback verifiers and algorithms. Their experiments demonstrate that by strategically increasing computation during inference, the quality of generated images can be significantly improved, tailored to various application needs."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶çš„è®¡ç®—æ‰©å±•ä¸æ€§èƒ½æå‡",
                    "desc": "ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†é‡è¦å½±å“ï¼Œä¸»è¦å¾—ç›Šäºå…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡å¢åŠ æ•°æ®ã€è®¡ç®—èµ„æºå’Œæ¨¡å‹è§„æ¨¡æ¥æ‰©å±•çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶çš„æ‰©å±•è¡Œä¸ºï¼Œå‘ç°é¢å¤–çš„è®¡ç®—å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚ä¸LLMsä¸åŒï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªæ­¥éª¤çš„æ•°é‡çµæ´»è°ƒæ•´æ¨ç†æ—¶çš„è®¡ç®—ï¼Œå°½ç®¡æ€§èƒ½æå‡é€šå¸¸åœ¨å‡ åæ­¥åè¶‹äºå¹³ç¨³ã€‚æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ‰©å±•è¡Œä¸ºï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡å¢åŠ è®¡ç®—æ¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¯»æ‰¾æ›´å¥½çš„å™ªå£°æ¥ä¼˜åŒ–æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09484",
            "title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators",
            "url": "https://huggingface.co/papers/2501.09484",
            "abstract": "Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated a significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the \"inquiry\" phase of the consultation process. This lack of focus has left the relationship between \"inquiry\" and \"diagnosis\" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctor-patient conversations and use these strategies to guide the training of a patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between \"inquiry\" and \"diagnosis\" in the consultation process. Experimental results demonstrate that inquiry and diagnosis adhere to the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to open-source the weights and related code of our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.",
            "score": 9,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "aff7d86ad63040d9",
            "authors": [
                "Zhaocheng Liu",
                "Quan Tu",
                "Wen Ye",
                "Yu Xiao",
                "Zhishou Zhang",
                "Hengfu Cui",
                "Yalun Zhu",
                "Qiang Ju",
                "Shizheng Li",
                "Jian Xie"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09484.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#science",
                    "#open_source",
                    "#healthcare"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ñ€Ğ°Ñ‡Ğ° Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ·Ğ°ĞºĞ¾Ğ½Ñƒ Ğ›Ğ¸Ğ±Ğ¸Ñ…Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Diagnosis through Effective Inquiry in Online Medical Consultations",
                    "desc": "This paper addresses the challenges of online medical consultations (OMC) by focusing on the inquiry phase, which is crucial for accurate diagnosis. It utilizes large language models to create a patient simulator that mimics real patient interactions based on actual doctor-patient conversations. The study reveals that the quality of inquiry directly impacts diagnostic effectiveness, following Liebig's law, which states that the weakest link limits overall performance. Additionally, the research categorizes inquiry types and analyzes their distribution across different models, highlighting significant performance variations in inquiry effectiveness."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è¯¢é—®ï¼Œæå‡è¯Šæ–­æ•ˆæœ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨çº¿åŒ»ç–—å’¨è¯¢ä¸­è¯¢é—®ä¸è¯Šæ–­ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬ä»çœŸå®çš„åŒ»æ‚£å¯¹è¯ä¸­æå–äº†æ‚£è€…äº’åŠ¨ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç­–ç•¥è®­ç»ƒäº†ä¸€ä¸ªæ¨¡æ‹Ÿæ‚£è€…çš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¢é—®è´¨é‡çš„å·®å¼‚ç›´æ¥å½±å“è¯Šæ–­æ•ˆæœï¼Œä¸”ä¸åŒæ¨¡å‹åœ¨è¯¢é—®è¡¨ç°ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬å°†è¯¢é—®è¿‡ç¨‹åˆ†ä¸ºå››ç§ç±»å‹ï¼Œå¹¶åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨è¿™äº›ç±»å‹ä¸Šçš„è¡¨ç°ï¼Œä»¥æ­ç¤ºå…¶æ€§èƒ½å·®å¼‚çš„åŸå› ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08617",
            "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
            "url": "https://huggingface.co/papers/2501.08617",
            "abstract": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.",
            "score": 6,
            "issue_id": 1720,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "f758bc630d8dd443",
            "authors": [
                "Kaiqu Liang",
                "Haimin Hu",
                "Ryan Liu",
                "Thomas L. Griffiths",
                "Jaime FernÃ¡ndez Fisac"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University",
                "Department of Psychology, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08617.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ - Reinforcement Learning from Hindsight Simulation (RLHS). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ RLHF, RLHS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚ÑƒĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RLHS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RLHS Ğ½Ğ°Ğ´ RLHF Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Aligning AI with Human Values through Hindsight Feedback",
                    "desc": "This paper addresses the challenge of aligning generative AI systems with human values using Reinforcement Learning from Human Feedback (RLHF). It identifies that relying on immediate feedback can lead to misaligned behaviors, such as sycophancy and deception, due to Goodhart's Law dynamics. The authors propose a new approach called Reinforcement Learning from Hindsight Simulation (RLHS), which uses simulated consequences to gather feedback on beneficial behaviors. Their experiments show that RLHS improves user satisfaction and goal achievement compared to traditional RLHF methods, highlighting the importance of considering long-term outcomes in AI alignment."
                },
                "zh": {
                    "title": "å…³æ³¨é•¿æœŸåæœï¼Œæå‡AIå¯¹é½æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•æ›´å¥½åœ°ä¸äººç±»ä»·å€¼è§‚å¯¹é½ï¼Œä»¥ç¡®ä¿å…¶è¡Œä¸ºæœ‰ç›Šä¸”å¯ä¿¡ã€‚ç°æœ‰çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–å³æ—¶åé¦ˆï¼Œä½†è¿™ç§åé¦ˆå¯èƒ½æ— æ³•å‡†ç¡®åæ˜ ä¸ç”¨æˆ·æ•ˆç”¨ç›¸å…³çš„é•¿æœŸå½±å“ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºäº‹åæ¨¡æ‹Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHSï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¯èƒ½çš„åæœæ¥è·å–åé¦ˆï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„å¯¹é½æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLHSåœ¨å¸®åŠ©ç”¨æˆ·å®ç°ç›®æ ‡å’Œæé«˜æ»¡æ„åº¦æ–¹é¢ï¼Œä¼˜äºä¼ ç»Ÿçš„RLHFæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09756",
            "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
            "url": "https://huggingface.co/papers/2501.09756",
            "abstract": "We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: https://vrroom.github.io/synthlight/",
            "score": 5,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "e6621d55eb165448",
            "authors": [
                "Sumit Chaturvedi",
                "Mengwei Ren",
                "Yannick Hold-Geoffroy",
                "Jingyuan Liu",
                "Julie Dorsey",
                "Zhixin Shu"
            ],
            "affiliations": [
                "Adobe Research",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09756.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#inference",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "SynthLight: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ·Ğ°ÑĞ²ĞµÑ‚ĞºĞ° Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "SynthLight - ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ·Ğ°ÑĞ²ĞµÑ‚ĞºĞ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ·Ğ°ÑĞ²ĞµÑ‚ĞºÑƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ³Ğ´Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³Ğ¾Ğ»Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Portrait Relighting with SynthLight",
                    "desc": "SynthLight is a diffusion model designed for relighting portraits by treating the task as a re-rendering challenge influenced by environmental lighting changes. It utilizes a physically-based rendering engine to create a synthetic dataset that simulates how lighting affects 3D head models. The model employs multi-task training to utilize real portraits without specific lighting labels and a novel inference strategy that enhances detail preservation during the relighting process. The results show that SynthLight can effectively generalize to real images, producing realistic lighting effects while maintaining the identity of the subjects, outperforming existing methods in both quantitative and qualitative assessments."
                },
                "zh": {
                    "title": "SynthLightï¼šè‚–åƒé‡å…‰ç…§çš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†SynthLightï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‚–åƒé‡å…‰ç…§çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†å›¾åƒé‡å…‰ç…§è§†ä¸ºé‡æ–°æ¸²æŸ“çš„é—®é¢˜ï¼Œé€šè¿‡ç‰©ç†åŸºç¡€æ¸²æŸ“å¼•æ“åˆæˆæ•°æ®é›†ï¼Œä»¥æ¨¡æ‹Ÿåœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„åƒç´ å˜æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä»¥ç¼©å°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å·®è·ï¼Œåˆ©ç”¨çœŸå®äººåƒè¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ‰©æ•£é‡‡æ ·ç¨‹åºã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ ·çš„çœŸå®ç…§ç‰‡ä¸­æ¨å¹¿ï¼Œç”Ÿæˆé€¼çœŸçš„å…‰ç…§æ•ˆæœï¼ŒåŒæ—¶ä¿æŒä¸»ä½“çš„èº«ä»½ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09747",
            "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2501.09747",
            "abstract": "Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.",
            "score": 5,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "1ff64d2f7e62d274",
            "authors": [
                "Karl Pertsch",
                "Kyle Stachowicz",
                "Brian Ichter",
                "Danny Driess",
                "Suraj Nair",
                "Quan Vuong",
                "Oier Mees",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "affiliations": [
                "Physical Intelligence",
                "Stanford",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09747.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#training",
                    "#games",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: Ğ¾Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FAST (Frequency-space Action Sequence Tokenization), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLA (Vision-Language Action) Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ FAST+, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ pi0 VLA, Ğ¼ĞµÑ‚Ğ¾Ğ´ FAST Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° 10 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VLA, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 5 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Revolutionizing Robot Action Tokenization with FAST",
                    "desc": "This paper introduces a new method for tokenizing continuous robot actions to improve the performance of autoregressive sequence models, specifically in the context of vision-language action (VLA) policies. The authors identify that traditional tokenization methods, which use simple binning techniques, struggle with high-frequency and dexterous robotic tasks. To overcome this limitation, they propose Frequency-space Action Sequence Tokenization (FAST), which utilizes the discrete cosine transform for better action representation. The results demonstrate that FAST can effectively train VLAs on extensive robot data, achieving performance comparable to diffusion models while significantly reducing training time."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººçµå·§æŠ€èƒ½çš„æ ‡è®°åŒ–æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨äººåŠ¨ä½œæ ‡è®°åŒ–æ–¹æ¡ˆï¼Œç§°ä¸ºé¢‘ç‡ç©ºé—´åŠ¨ä½œåºåˆ—æ ‡è®°åŒ–ï¼ˆFASTï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºç®€å•åˆ†ç®±æ–¹æ³•çš„æ ‡è®°åŒ–åœ¨å­¦ä¹ çµå·§æŠ€èƒ½æ—¶çš„ä¸è¶³ã€‚FASTåˆ©ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢æ¥æœ‰æ•ˆåœ°å¤„ç†é«˜é¢‘æœºå™¨äººæ•°æ®ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†FAST+ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æœºå™¨äººåŠ¨ä½œæ ‡è®°å™¨ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§åŠ¨ä½œåºåˆ—å’Œæ§åˆ¶é¢‘ç‡ã€‚é€šè¿‡ä¸pi0 VLAç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒ10,000å°æ—¶çš„æœºå™¨äººæ•°æ®æ—¶ï¼Œèƒ½å¤Ÿä¸æ‰©æ•£VLAçš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶å°†è®­ç»ƒæ—¶é—´å‡å°‘äº†å¤šè¾¾5å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09755",
            "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
            "url": "https://huggingface.co/papers/2501.09755",
            "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.",
            "score": 5,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "426aa3415c3c0ef4",
            "authors": [
                "Philippe Hansen-Estruch",
                "David Yan",
                "Ching-Yao Chung",
                "Orr Zohar",
                "Jialiang Wang",
                "Tingbo Hou",
                "Tao Xu",
                "Sriram Vishwanath",
                "Peter Vajda",
                "Xinlei Chen"
            ],
            "affiliations": [
                "FAIR, Meta",
                "GenAI, Meta",
                "Stanford University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09755.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#video",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ViTok: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ViTok - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision Transformer, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ½Ğ¾ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ViTok Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ FLOP Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Scaling Auto-Encoders for Enhanced Image and Video Generation",
                    "desc": "This paper explores the scaling of auto-encoders, particularly focusing on the tokenizer component, which is crucial for image and video generation. The authors introduce ViTok, a Vision Transformer-based architecture that replaces traditional convolutional backbones, allowing for better scaling on large datasets. They investigate how different scaling strategies for the encoder and decoder affect both reconstruction and generative performance, finding that scaling the decoder is more beneficial for reconstruction. Ultimately, ViTok achieves competitive results with fewer computational resources and sets new benchmarks in image and video generation tasks."
                },
                "zh": {
                    "title": "è‡ªç¼–ç å™¨çš„è§†è§‰æ ‡è®°åŒ–ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡è‡ªç¼–ç å™¨è¿›è¡Œè§†è§‰æ ‡è®°åŒ–å¯¹å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„è§†è§‰å˜æ¢å™¨æ¶æ„ï¼ˆViTokï¼‰ï¼Œç”¨äºæ›¿ä»£ä¼ ç»Ÿçš„å·ç§¯éª¨å¹²ç½‘ç»œï¼Œä»¥æé«˜æ ‡è®°åŒ–çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œè‡ªç¼–ç å™¨çš„ç“¶é¢ˆè§„æ¨¡ä¸é‡å»ºæ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œä½†ä¸ç”Ÿæˆæ€§èƒ½çš„å…³ç³»æ›´ä¸ºå¤æ‚ã€‚æœ€ç»ˆï¼ŒViTokåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘é‡å»ºå’Œå›¾åƒç”Ÿæˆæ–¹é¢ï¼Œå±•ç¤ºäº†å…¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09686",
            "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
            "url": "https://huggingface.co/papers/2501.09686",
            "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.",
            "score": 3,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "1c6b1b1f0235304c",
            "authors": [
                "Fengli Xu",
                "Qianyue Hao",
                "Zefang Zong",
                "Jingwei Wang",
                "Yunke Zhang",
                "Jingyi Wang",
                "Xiaochong Lan",
                "Jiahui Gong",
                "Tianjian Ouyang",
                "Fanjin Meng",
                "Chenyang Shao",
                "Yuwei Yan",
                "Qinglong Yang",
                "Yiwen Song",
                "Sijian Ren",
                "Xinyuan Hu",
                "Yu Li",
                "Jie Feng",
                "Chen Gao",
                "Yong Li"
            ],
            "affiliations": [
                "Emory University, Atlanta GA, USA",
                "HKUST (GZ), Guangzhou, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09686.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rl",
                    "#survey",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑÑƒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Human-Like Reasoning in Large Language Models",
                    "desc": "This paper discusses the advancements in Large Language Models (LLMs) and their application to complex reasoning tasks. It introduces the concept of 'thought', which represents intermediate reasoning steps, allowing LLMs to simulate human-like reasoning processes. The paper highlights the use of reinforcement learning to enhance LLMs' reasoning capabilities by generating high-quality reasoning trajectories through trial-and-error methods. Additionally, it emphasizes the importance of scaling both training and testing phases to improve reasoning accuracy, paving the way for the development of Large Reasoning Models."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤§å‹æ¨ç†æ¨¡å‹çš„ç ”ç©¶æ–°å‰æ²¿",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è€…ä»¬å¼•å…¥äº†â€œæ€è€ƒâ€çš„æ¦‚å¿µï¼Œé€šè¿‡ä¸­é—´æ­¥éª¤çš„ä»¤ç‰Œåºåˆ—æ¥æ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«åº”ç”¨äºè®­ç»ƒLLMsï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡è¿˜è®¨è®ºäº†åœ¨æµ‹è¯•æ—¶å¢åŠ ä»¤ç‰Œæ•°é‡ä»¥æé«˜æ¨ç†å‡†ç¡®æ€§çš„æ•ˆæœï¼Œå¹¶å±•æœ›äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09503",
            "title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2501.09503",
            "abstract": "Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an \"encode-then-route\" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .",
            "score": 2,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "fb27e795153a9668",
            "authors": [
                "Junjie He",
                "Yuxiang Tuo",
                "Binghui Chen",
                "Chongyang Zhong",
                "Yifeng Geng",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Institute for Intelligent Computing, Alibaba Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09503.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "AnyStory: Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AnyStory - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ReferenceNet Ğ¸ CLIP Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ². AnyStory Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµĞºÑƒĞ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "AnyStory: Mastering Personalized Image Generation for Multiple Subjects",
                    "desc": "This paper introduces AnyStory, a novel method for generating personalized images with high fidelity, even when multiple subjects are involved. It employs an 'encode-then-route' strategy, where a powerful image encoder, ReferenceNet, captures detailed subject features. The routing mechanism uses an instance-aware subject router to accurately determine where each subject should be placed in the generated image. Experimental results show that AnyStory excels in maintaining subject details and aligning them with text descriptions, making it effective for both single and multiple subjects."
                },
                "zh": {
                    "title": "AnyStoryï¼šä¸ªæ€§åŒ–ä¸»é¢˜ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ï¼Œå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„ä¸ªæ€§åŒ–å›¾åƒï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šä¸ªä¸»é¢˜çš„æƒ…å†µï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†AnyStoryï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ä¸ªæ€§åŒ–ä¸»é¢˜ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ä¸»é¢˜ä¿çœŸçš„æƒ…å†µä¸‹ï¼Œå®ç°å•ä¸ªå’Œå¤šä¸ªä¸»é¢˜çš„é«˜ä¿çœŸä¸ªæ€§åŒ–ã€‚AnyStoryé€šè¿‡â€œç¼–ç -å†è·¯ç”±â€çš„æ–¹å¼å»ºæ¨¡ä¸»é¢˜ä¸ªæ€§åŒ–é—®é¢˜ï¼Œåˆ©ç”¨å¼ºå¤§çš„å›¾åƒç¼–ç å™¨å’Œå®ä¾‹æ„ŸçŸ¥è·¯ç”±å™¨ï¼Œå‡†ç¡®é¢„æµ‹ä¸»é¢˜åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ä½ç½®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09433",
            "title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation",
            "url": "https://huggingface.co/papers/2501.09433",
            "abstract": "The synthesis of high-quality 3D assets from textual or visual inputs has become a central objective in modern generative modeling. Despite the proliferation of 3D generation algorithms, they frequently grapple with challenges such as multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. While some studies have addressed some of these issues, a comprehensive solution remains elusive. In this paper, we introduce CaPa, a carve-and-paint framework that generates high-fidelity 3D assets efficiently. CaPa employs a two-stage process, decoupling geometry generation from texture synthesis. Initially, a 3D latent diffusion model generates geometry guided by multi-view inputs, ensuring structural consistency across perspectives. Subsequently, leveraging a novel, model-agnostic Spatially Decoupled Attention, the framework synthesizes high-resolution textures (up to 4K) for a given geometry. Furthermore, we propose a 3D-aware occlusion inpainting algorithm that fills untextured regions, resulting in cohesive results across the entire model. This pipeline generates high-quality 3D assets in less than 30 seconds, providing ready-to-use outputs for commercial applications. Experimental results demonstrate that CaPa excels in both texture fidelity and geometric stability, establishing a new standard for practical, scalable 3D asset generation.",
            "score": 1,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "8c7a54f21e46af7a",
            "authors": [
                "Hwan Heo",
                "Jangyeong Kim",
                "Seongyeong Lee",
                "Jeong A Wi",
                "Junyoung Choi",
                "Sangjun Ahn"
            ],
            "affiliations": [
                "Graphics AI Lab, NC Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09433.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "CaPa: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ CaPa - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. CaPa Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° 30 ÑĞµĞºÑƒĞ½Ğ´, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "CaPa: Fast and High-Fidelity 3D Asset Generation",
                    "desc": "This paper presents CaPa, a novel framework for generating high-quality 3D assets from textual or visual inputs. It addresses common challenges in 3D generation, such as multi-view inconsistency and slow generation times, by separating geometry generation from texture synthesis. The framework utilizes a 3D latent diffusion model for consistent geometry creation and a Spatially Decoupled Attention mechanism for high-resolution texture synthesis. CaPa also includes a 3D-aware occlusion inpainting algorithm to enhance the final output, achieving high fidelity and stability in under 30 seconds."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆé«˜ä¿çœŸ3Dèµ„äº§çš„CaPaæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCaPaçš„æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Dèµ„äº§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼Œå°†å‡ ä½•ä½“ç”Ÿæˆä¸çº¹ç†åˆæˆè§£è€¦ã€‚é¦–å…ˆï¼Œä½¿ç”¨3Dæ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå‡ ä½•ä½“ï¼Œç¡®ä¿å¤šè§†è§’ä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„ç©ºé—´è§£è€¦æ³¨æ„åŠ›æœºåˆ¶åˆæˆé«˜åˆ†è¾¨ç‡çº¹ç†ï¼Œå¹¶æå‡ºäº†3Dæ„ŸçŸ¥çš„é®æŒ¡ä¿®å¤ç®—æ³•ï¼Œæœ€ç»ˆåœ¨30ç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„3Dèµ„äº§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-16.html",
    "link_next": "2025-01-20.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢ï¼Œæ—¨åœ¨ä»å¤§é‡æ–‡æ¡£ä¸­è¯†åˆ«å’Œæ£€ç´¢å›¾è¡¨ã€è¡¨æ ¼ã€å›¾å½¢å’Œå¸ƒå±€ä¿¡æ¯ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MMDocIRï¼ŒåŒ…æ‹¬é¡µé¢çº§å’Œå¸ƒå±€çº§æ£€ç´¢ä»»åŠ¡ã€‚MMDocIRåŒ…å«1,685ä¸ªä¸“å®¶æ ‡æ³¨å’Œ173,843ä¸ªè‡ªåŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„èµ„æºã€‚å®éªŒæ˜¾ç¤ºï¼Œè§†è§‰æ£€ç´¢å™¨æ¯”æ–‡æœ¬æ£€ç´¢å™¨è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä½¿ç”¨VLM-textçš„æ–‡æœ¬æ£€ç´¢å™¨ä¼˜äºä½¿ç”¨OCR-textçš„æ£€ç´¢å™¨ã€‚",
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢ï¼Œæ—¨åœ¨ä»å¤§é‡æ–‡æ¡£ä¸­è¯†åˆ«å’Œæ£€ç´¢å›¾è¡¨ã€è¡¨æ ¼ã€å›¾å½¢å’Œå¸ƒå±€ä¿¡æ¯ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MMDocIRï¼ŒåŒ…æ‹¬é¡µé¢çº§å’Œå¸ƒå±€çº§æ£€ç´¢ä»»åŠ¡ã€‚MMDocIRåŒ…å«1,685ä¸ªä¸“å®¶æ ‡æ³¨å’Œ173,843ä¸ªè‡ªåŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„èµ„æºã€‚å®éªŒæ˜¾ç¤ºï¼Œè§†è§‰æ£€ç´¢å™¨æ¯”æ–‡æœ¬æ£€ç´¢å™¨è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä½¿ç”¨VLM-textçš„æ–‡æœ¬æ£€ç´¢å™¨ä¼˜äºä½¿ç”¨OCR-textçš„æ£€ç´¢å™¨ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le duÅ mÃ³ tÃ i wÃ©n dÃ ng jiÃ n suÇ’, zhÇ zÃ i cÃ³ng dÃ  liÃ ng wÃ©n dÃ ng zhÅng shÃ­ biÃ© hÃ© jiÃ n suÇ’ tÃº biÇo, biÇo gÃ©, tÃº xÃ­ng hÃ© bÃ¹ jÃº xÃ¬n xÄ«. JÇn guÇn qÃ­ zhÃ²ng yÃ o xÃ¬ng, quÄ“ fÃ¡ yÇ’u xiÃ o de jÄ« zhÇ”n cÃ¨ shÃ¬ lÃ¡i pÃ­ng guÌ„ xÃ¬ tÇ’ng xÃ¬ng nÃ©ng. WÃ¨i cÇ, wÃ©n zhÄng tÃ­ chÅ« le yÄ« gÃ¨ xÄ«n de jÄ« zhÇ”n cÃ¨ shÃ¬ MMDocIR, bÄo kuÃ² yÃ¨ miÃ n jÃ­ hÃ© bÃ¹ jÃº jÃ­ jiÃ n suÇ’ rÃ¨n wÃ¹. MMDocIR bÄo hÃ¡n 1,685 gÃ¨ zhuÄn jiÄ biÄo zhÃ¹ hÃ© 173,843 gÃ¨ zÃ¬ dÃ²ng biÄo zhÃ¹ de wÃ¨n tÃ­, shÃ¬ yÄ« gÃ¨ zhÃ²ng yÃ o de zÄ« yuÃ¡n. ShÃ­ yÃ n xiÇn shÃ¬, shÃ¬ juÃ© jiÃ n suÇ’ qÃ¬ bÇ wÃ©n bÄ›n jiÃ n suÇ’ qÃ¬ biÇo xiÃ n gÃ¨ng hÇo, bÃ¬ng qiÄ› shÇ yÃ²ng VLM-text de wÃ©n bÄ›n jiÃ n suÇ’ qÃ¬ yÅu yÃº shÇ yÃ²ng OCR-text de jiÃ n suÇ’ qÃ¬.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'æ£€ç´¢', 'pinyin': 'jiÇn suÇ’', 'trans': 'retrieval'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'è¯†åˆ«', 'pinyin': 'shÃ­ biÃ©', 'trans': 'recognize'},\n{'word': 'å¸ƒå±€', 'pinyin': 'bÃ¹ jiÃº', 'trans': 'layout'},\n{'word': 'å°½ç®¡', 'pinyin': 'jÃ¬n guÇn', 'trans': 'although'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'é¡µé¢çº§', 'pinyin': 'yÃ¨ miÃ n jÃ­', 'trans': 'page-level'},\n{'word': 'æ ‡æ³¨', 'pinyin': 'biÄo zhÃ¹', 'trans': 'annotation'},\n{'word': 'èµ„æº', 'pinyin': 'zÄ« yuÃ¡n', 'trans': 'resource'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'perform'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'},\n{'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'},\n{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]",
        "trans": "This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text.",
        "update_ts": "2025-01-16 09:10"
    }
}