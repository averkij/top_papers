{
    "date": {
        "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 6",
        "zh": "12æœˆ6æ—¥"
    },
    "time_utc": "2024-12-06 03:28",
    "weekday": 4,
    "issue_id": 980,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.04454",
            "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
            "url": "https://huggingface.co/papers/2412.04454",
            "abstract": "Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.",
            "score": 7,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "a088657cce2c618c",
            "authors": [
                "Yiheng Xu",
                "Zekun Wang",
                "Junli Wang",
                "Dunjie Lu",
                "Tianbao Xie",
                "Amrita Saha",
                "Doyen Sahoo",
                "Tao Yu",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce Research",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04454.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#games",
                    "#multimodal",
                    "#agents",
                    "#training",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Aguvis: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ“Ğ˜ĞŸ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Aguvis - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ğ“Ğ˜ĞŸ), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Aguvis Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Aguvis Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Aguvis: Revolutionizing Autonomous GUI Interaction with Pure Vision",
                    "desc": "This paper presents Aguvis, a novel framework designed for autonomous GUI agents that operates purely on visual inputs. Unlike previous methods that depend on textual representations, Aguvis utilizes image-based observations and natural language instructions to interact with GUI elements. The framework incorporates explicit planning and reasoning, allowing it to effectively navigate and perform tasks in complex digital environments. Through extensive testing, Aguvis demonstrates superior performance compared to existing methods, marking a significant advancement in the development of fully autonomous GUI agents."
                },
                "zh": {
                    "title": "Aguvisï¼šå®Œå…¨è‡ªä¸»çš„è§†è§‰GUIä»£ç†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAguvisçš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŸºäºçº¯è§†è§‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå¹³å°ä¸Šæ“ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚Aguvisé€šè¿‡å›¾åƒè§‚å¯Ÿå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç»“åˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç‹¬ç«‹å¯¼èˆªå’Œä¸å¤æ‚æ•°å­—ç¯å¢ƒäº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAguvisåœ¨ç¦»çº¿å’Œåœ¨çº¿åœºæ™¯ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæˆä¸ºé¦–ä¸ªå®Œå…¨è‡ªä¸»çš„çº¯è§†è§‰GUIä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01506",
            "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
            "url": "https://huggingface.co/papers/2412.01506",
            "abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.",
            "score": 6,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "c35e5586d464b27c",
            "authors": [
                "Jianfeng Xiang",
                "Zelong Lv",
                "Sicheng Xu",
                "Yu Deng",
                "Ruicheng Wang",
                "Bowen Zhang",
                "Dong Chen",
                "Xin Tong",
                "Jiaolong Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01506.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (SLAT), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹. SLAT Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ 3D-ÑĞµÑ‚ĞºÑƒ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ SLAT, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 500 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation with SLAT!",
                    "desc": "This paper presents a new method for creating high-quality 3D assets using a unified Structured LATent (SLAT) representation. The SLAT allows for flexible decoding into various formats like Radiance Fields, 3D Gaussians, and meshes by combining a sparse 3D grid with detailed visual features from a vision model. The authors utilize rectified flow transformers designed for SLAT, training models with up to 2 billion parameters on a large dataset of 500,000 diverse 3D objects. The results show significant improvements in quality and versatility, enabling local 3D editing and output format selection that previous models could not achieve."
                },
                "zh": {
                    "title": "çµæ´»é«˜æ•ˆçš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºå¤šåŠŸèƒ½å’Œé«˜è´¨é‡çš„3Dèµ„äº§åˆ›å»ºã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç»Ÿä¸€çš„ç»“æ„åŒ–æ½œåœ¨(SLAT)è¡¨ç¤ºï¼Œèƒ½å¤Ÿè§£ç ä¸ºä¸åŒçš„è¾“å‡ºæ ¼å¼ï¼Œå¦‚è¾å°„åœºã€3Dé«˜æ–¯å’Œç½‘æ ¼ã€‚é€šè¿‡å°†ç¨€ç–çš„3Dç½‘æ ¼ä¸ä»å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–çš„å¯†é›†å¤šè§†è§’è§†è§‰ç‰¹å¾ç›¸ç»“åˆï¼Œæˆ‘ä»¬å…¨é¢æ•æ‰äº†ç»“æ„ï¼ˆå‡ ä½•ï¼‰å’Œçº¹ç†ï¼ˆå¤–è§‚ï¼‰ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨è§£ç è¿‡ç¨‹ä¸­ä¿æŒçµæ´»æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨é’ˆå¯¹SLATçš„æ•´æµæµå˜æ¢å™¨è¿›è¡Œ3Dç”Ÿæˆï¼Œè®­ç»ƒäº†é«˜è¾¾20äº¿å‚æ•°çš„æ¨¡å‹ï¼Œç”Ÿæˆçš„ç»“æœåœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹çš„è´¨é‡æ˜¾è‘—è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04467",
            "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
            "url": "https://huggingface.co/papers/2412.04467",
            "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .",
            "score": 4,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "5539efbb0d3e8e80",
            "authors": [
                "Senqiao Yang",
                "Yukang Chen",
                "Zhuotao Tian",
                "Chengyao Wang",
                "Jingyao Li",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HITSZ",
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04467.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#interpretability",
                    "#multimodal",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "VisionZip: Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑƒÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ˜Ğ˜",
                    "desc": "VisionZip - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº CLIP Ğ¸ SigLIP. VisionZip Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Streamlining Visual Tokens for Enhanced Efficiency in Vision-Language Models",
                    "desc": "This paper presents VisionZip, a method designed to reduce redundancy in visual tokens used in vision-language models. By selecting only the most informative tokens, VisionZip improves computational efficiency without sacrificing performance. The method shows significant improvements in both model inference speed and overall accuracy, outperforming previous state-of-the-art techniques by at least 5%. The authors emphasize the importance of optimizing visual feature extraction rather than simply increasing token length."
                },
                "zh": {
                    "title": "VisionZipï¼šé«˜æ•ˆå‡å°‘è§†è§‰æ ‡è®°å†—ä½™çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›å±•é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æ¥æé«˜æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿæ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisionZipï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œå‡å°‘è§†è§‰æ ‡è®°çš„å†—ä½™ï¼Œæé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01820",
            "title": "Towards Universal Soccer Video Understanding",
            "url": "https://huggingface.co/papers/2412.01820",
            "abstract": "As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "fa9f7e4132ee5026",
            "authors": [
                "Jiayuan Rao",
                "Haoning Wu",
                "Hao Jiang",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Alibaba Group, China",
                "CMIC, Shanghai Jiao Tong University, China",
                "School of Artificial Intelligence, Shanghai Jiao Tong University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01820.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "âš½",
                "ru": {
                    "title": "MatchVision: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchVision Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SoccerReplay-1988, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 1988 Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹. MatchVision Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Soccer Video Analysis with MatchVision",
                    "desc": "This paper presents a new framework for understanding soccer videos using machine learning. It introduces SoccerReplay-1988, a large dataset containing videos and annotations from nearly 2,000 soccer matches, which helps in training models. The authors also develop MatchVision, a visual-language model that processes spatiotemporal data from soccer videos and performs well in tasks like event classification and commentary generation. The results show that MatchVision outperforms existing models, setting a new standard for research in sports video analysis."
                },
                "zh": {
                    "title": "è¶³çƒè§†é¢‘ç†è§£çš„æ–°æ ‡å‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºç†è§£è¶³çƒè§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†SoccerReplay-1988ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è¶³çƒé¢†åŸŸçš„é¦–ä¸ªè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionï¼Œèƒ½å¤Ÿåˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒMatchVisionåœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ•°æ®å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04106",
            "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
            "url": "https://huggingface.co/papers/2412.04106",
            "abstract": "Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "7b2720f9a6c27027",
            "authors": [
                "Haoning Wu",
                "Ziheng Zhao",
                "Ya Zhang",
                "Weidi Xie",
                "Yanfeng Wang"
            ],
            "affiliations": [
                "CMIC, Shanghai Jiao Tong University, China",
                "School of Artificial Intelligence, Shanghai Jiao Tong University, China",
                "Shanghai AI Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04106.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#healthcare",
                    "#synthetic",
                    "#cv",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœĞ Ğ¢ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MedGen-1M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MRGen Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞœĞ Ğ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ĞœĞ Ğ¢."
                },
                "en": {
                    "title": "Generating Synthetic Data for Medical Image Segmentation",
                    "desc": "This paper presents a novel approach to medical image segmentation by using generative models to create synthetic data for modalities that lack mask annotations. The authors introduce a large dataset called MedGen-1M, which includes radiology images with modality labels and some organ mask annotations, facilitating research in medical image generation. They propose a diffusion-based data engine named MRGen that generates MR images based on text prompts and available masks, allowing for the training of segmentation models on unannotated data. Extensive experiments demonstrate the effectiveness of MRGen in synthesizing training samples and improving segmentation performance across various MRI modalities."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åˆ›æ–°",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æœªæ ‡æ³¨æ¨¡æ€çš„æ•°æ®åˆæˆã€‚ç ”ç©¶è€…ä»¬æ”¶é›†å¹¶æ•´ç†äº†ä¸€ä¸ªå¤§å‹çš„æ”¾å°„å­¦å›¾åƒ-æ–‡æœ¬æ•°æ®é›†MedGen-1Mï¼ŒåŒ…å«æ¨¡æ€æ ‡ç­¾ã€å±æ€§ã€åŒºåŸŸå’Œå™¨å®˜ä¿¡æ¯ï¼Œä»¥åŠéƒ¨åˆ†å™¨å®˜çš„æ©è†œæ³¨é‡Šã€‚è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ•°æ®å¼•æ“MRGenï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå’Œæ©è†œç”Ÿæˆç¼ºä¹æ©è†œæ³¨é‡Šçš„MRå›¾åƒï¼Œä»è€Œè®­ç»ƒæœªæ ‡æ³¨æ¨¡æ€çš„åˆ†å‰²æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ•°æ®å¼•æ“èƒ½å¤Ÿæœ‰æ•ˆåˆæˆè®­ç»ƒæ ·æœ¬ï¼Œæ¨åŠ¨MRIåˆ†å‰²å‘æœªæ ‡æ³¨æ¨¡æ€çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03679",
            "title": "Evaluating Language Models as Synthetic Data Generators",
            "url": "https://huggingface.co/papers/2412.03679",
            "abstract": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.",
            "score": 2,
            "issue_id": 980,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "2b80634d3f712590",
            "authors": [
                "Seungone Kim",
                "Juyoung Suk",
                "Xiang Yue",
                "Vijay Viswanathan",
                "Seongyun Lee",
                "Yizhong Wang",
                "Kiril Gashteovski",
                "Carolin Lawrence",
                "Sean Welleck",
                "Graham Neubig"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "KAIST AI",
                "NEC Laboratories Europe",
                "Ss. Cyril and Methodius University of Skopje",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03679.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AgoraBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AgoraBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² 1,26 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 6 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² 99 ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¯Ğœ Ğ¸Ğ¼ĞµÑÑ‚ ÑĞ²Ğ¾Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ ÑÑ‚Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking the Power of Language Models in Data Generation",
                    "desc": "This paper introduces AgoraBench, a benchmark designed to evaluate the data generation capabilities of various language models (LMs). It highlights the importance of synthetic data in enhancing the performance of LMs, especially in post-training scenarios. The study synthesizes a large dataset using six different LMs and trains 99 student models to analyze their data generation strengths and weaknesses. Key findings indicate that while some LMs excel in creating new problems, others are better at refining existing ones, and that data generation quality is influenced by several intrinsic features rather than just the LM's problem-solving skills."
                },
                "zh": {
                    "title": "è¯„ä¼°è¯­è¨€æ¨¡å‹æ•°æ®ç”Ÿæˆèƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "éšç€åˆæˆæ•°æ®åœ¨è¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒä¸­çš„ä½¿ç”¨å¢åŠ ï¼Œè¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ•°æ®çš„èƒ½åŠ›å˜å¾—ä¸ç›´æ¥è§£å†³é—®é¢˜çš„èƒ½åŠ›åŒæ ·é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼€å‘æœ‰æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒè¯­è¨€æ¨¡å‹ä½œä¸ºæ•°æ®ç”Ÿæˆå™¨çš„ç³»ç»Ÿæ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AgoraBenchï¼Œä¸€ä¸ªæä¾›æ ‡å‡†åŒ–è®¾ç½®å’ŒæŒ‡æ ‡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ•°æ®ç”Ÿæˆæ–¹é¢å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ•°æ®ç”Ÿæˆèƒ½åŠ›ä¸è§£å†³é—®é¢˜çš„èƒ½åŠ›å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼Œè€Œæ˜¯ä¸æ•°æ®è´¨é‡çš„å¤šä¸ªå†…åœ¨ç‰¹å¾æœ‰å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04280",
            "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing",
            "url": "https://huggingface.co/papers/2412.04280",
            "abstract": "We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution 1024 times 1024 content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.",
            "score": 2,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "4467ff5ceea9cb1d",
            "authors": [
                "Jinbin Bai",
                "Wei Chow",
                "Ling Yang",
                "Xiangtai Li",
                "Juncheng Li",
                "Hanwang Zhang",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "Peking University",
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04280.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "HumanEdit: Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼",
                    "desc": "HumanEdit - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5751 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑˆĞµÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²: Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚, Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ğ»ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². HumanEdit Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "HumanEdit: Elevating Image Editing with Human-Centric Instructions",
                    "desc": "HumanEdit is a newly created dataset aimed at improving instruction-guided image editing by incorporating significant human feedback. Unlike previous datasets that lacked sufficient human input, HumanEdit utilizes human annotators to create data pairs and provide valuable feedback, ensuring alignment with human preferences. The dataset consists of 5,751 high-resolution images and includes six types of editing instructions, allowing for a wide range of image manipulation tasks. By offering detailed instructions and masks, HumanEdit sets a new standard for versatility and accuracy in image editing research."
                },
                "zh": {
                    "title": "HumanEditï¼šç²¾å‡†å¤šæ ·çš„å›¾åƒç¼–è¾‘æ•°æ®é›†",
                    "desc": "HumanEditæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„äººç±»å¥–åŠ±æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºæŒ‡å¯¼å›¾åƒç¼–è¾‘ã€‚ä¸ä»¥å¾€çš„å¤§è§„æ¨¡ç¼–è¾‘æ•°æ®é›†ä¸åŒï¼ŒHumanEdité€šè¿‡äººç±»æ³¨é‡Šè€…æ„å»ºæ•°æ®å¯¹ï¼Œå¹¶ç”±ç®¡ç†å‘˜æä¾›åé¦ˆï¼Œç¡®ä¿æ•°æ®ä¸äººç±»åå¥½çš„å¯¹é½ã€‚è¯¥æ•°æ®é›†åŒ…å«5751å¼ å›¾åƒï¼Œç»è¿‡2500å¤šä¸ªå°æ—¶çš„äººå·¥åŠªåŠ›ï¼Œæ¶µç›–å…­ç§ä¸åŒç±»å‹çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚HumanEditä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ¨åŠ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01169",
            "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
            "url": "https://huggingface.co/papers/2412.01169",
            "abstract": "We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.",
            "score": 1,
            "issue_id": 980,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "7f3df6f7d4733664",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Zichun Liao",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "Salesforce AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01169.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "OmniFlow: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "OmniFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (rectified flow) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. OmniFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MMDiT Ğ¸Ğ· Stable Diffusion 3, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "OmniFlow: Bridging Modalities for Any-to-Any Generation",
                    "desc": "OmniFlow is a new generative model that can create outputs across different types of data, like turning text into images or audio. It builds on the rectified flow (RF) framework, allowing it to understand and generate multiple types of data together. This model not only improves performance on tasks like text-to-image and text-to-audio synthesis but also introduces a way for users to control how different data types relate to each other in the generated results. Additionally, it features a unique architecture that enhances existing models and provides insights into optimizing generative tasks across various modalities."
                },
                "zh": {
                    "title": "OmniFlowï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„çµæ´»è§£å†³æ–¹æ¡ˆ",
                    "desc": "OmniFlowæ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨å¤„ç†ä»»æ„åˆ°ä»»æ„çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°éŸ³é¢‘å’ŒéŸ³é¢‘åˆ°å›¾åƒçš„åˆæˆã€‚å®ƒåœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æ”¹è¿›äº†ä¿®æ­£æµï¼ˆRFï¼‰æ¡†æ¶ï¼Œä»¥å¤„ç†å¤šç§æ¨¡æ€çš„è”åˆåˆ†å¸ƒã€‚OmniFlowåœ¨å¤šç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ä»»æ„åˆ°ä»»æ„æ¨¡å‹ï¼Œæä¾›äº†çµæ´»çš„æ¨¡æ€å¯¹é½æ§åˆ¶æœºåˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ¢è®¨äº†ä¿®æ­£æµå˜æ¢å™¨åœ¨å¤§è§„æ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ç”Ÿæˆä¸­çš„è®¾è®¡é€‰æ‹©ï¼Œä¸ºä¼˜åŒ–ä¸åŒæ¨¡æ€çš„æ€§èƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04431",
            "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
            "url": "https://huggingface.co/papers/2412.04431",
            "abstract": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.",
            "score": 0,
            "issue_id": 980,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "f297f288187d4cc4",
            "authors": [
                "Jian Han",
                "Jinlai Liu",
                "Yi Jiang",
                "Bin Yan",
                "Yuqi Zhang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04431.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Infinity: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Infinity - Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Infinity Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GenEval Ğ¸ ImageReward. Infinity Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ 1024x1024 Ğ·Ğ° 0.8 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ text-to-image Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚."
                },
                "en": {
                    "title": "Infinity: Redefining Text-to-Image Generation with Infinite Vocabulary",
                    "desc": "Infinity is a novel Bitwise Visual AutoRegressive Model designed to create high-resolution, photorealistic images based on language instructions. It introduces an infinite-vocabulary tokenizer and classifier, along with a bitwise self-correction mechanism, which enhances the detail and quality of generated images. By scaling both the tokenizer and transformer sizes, Infinity significantly improves upon traditional visual autoregressive models. It sets new performance records in text-to-image generation, outperforming leading diffusion models and achieving faster generation times without additional optimization."
                },
                "zh": {
                    "title": "Infinityï¼šæ— é™å¯èƒ½çš„è§†è§‰ç”Ÿæˆæ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Infinityï¼Œä¸€ç§åŸºäºä½çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸçš„å›¾åƒã€‚Infinityåœ¨ä½å…ƒä»¤ç‰Œé¢„æµ‹æ¡†æ¶ä¸‹é‡æ–°å®šä¹‰äº†è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œé‡‡ç”¨æ— é™è¯æ±‡é‡çš„ä»¤ç‰Œå™¨å’Œåˆ†ç±»å™¨ï¼Œä»¥åŠä½å…ƒè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèƒ½åŠ›å’Œç»†èŠ‚è¡¨ç°ã€‚é€šè¿‡ç†è®ºä¸Šå°†ä»¤ç‰Œå™¨çš„è¯æ±‡é‡æ‰©å±•åˆ°æ— é™ï¼Œå¹¶åŒæ—¶æ‰©å±•å˜æ¢å™¨çš„è§„æ¨¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„VARæ¨¡å‹é‡Šæ”¾äº†å¼ºå¤§çš„æ‰©å±•èƒ½åŠ›ã€‚Infinityåœ¨è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­åˆ›ä¸‹æ–°çºªå½•ï¼Œè¶…è¶Šäº†é¡¶çº§æ‰©æ•£æ¨¡å‹ï¼Œå¦‚SD3-Mediumå’ŒSDXLã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-05.html",
    "link_next": "2024-12-09.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "09.12",
        "en": "12/09",
        "zh": "12æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ SNOOPIï¼Œæ—¨åœ¨æ”¹è¿›å•æ­¥æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼æœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæ‰©æ•£æ¨¡å‹éª¨æ¶æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œä¸”ä¸æ”¯æŒè´Ÿé¢æç¤ºæŒ‡å¯¼ã€‚SNOOPI é€šè¿‡ PG-SB å’Œ NASA ä¸¤ç§æ–¹æ³•è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSNOOPI æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚",
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ SNOOPIï¼Œæ—¨åœ¨æ”¹è¿›å•æ­¥æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼æœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæ‰©æ•£æ¨¡å‹éª¨æ¶æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œä¸”ä¸æ”¯æŒè´Ÿé¢æç¤ºæŒ‡å¯¼ã€‚SNOOPI é€šè¿‡ PG-SB å’Œ NASA ä¸¤ç§æ–¹æ³•è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSNOOPI æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de kuÃ ngjiÃ  SNOOPI, zhÇ zÃ i gÇijÃ¬n dÄn bÃ¹ kuÃ²sÃ n mÃ³xÃ­ng de zhÇdÇo jÄ«zhÃ¬. XiÃ nyÇ’u fÄngfÇ zÃ i chÇ”lÇ bÃ¹tÃ³ng kuÃ²sÃ n mÃ³xÃ­ng gÇ”jiÃ  shÃ­ biÇoxiÃ n bÃ¹Ã­culai, qiÄ› bÃ¹ zhÄ«chÃ­ fÃ¹miÃ n tÃ­shÃ¬ zhÇdÇo. SNOOPI tÅngguÃ² PG-SB hÃ© NASA liÇng zhÇ’ng fÄngfÇ jiÄ›juÃ© le zhÃ¨xiÄ“ wÃ¨ntÃ­. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, SNOOPI xiÇnzhÃ¹ tÃ­shÄ“ng le jÄ«zhÇ”n mÃ³xÃ­ng de xÃ­ngnÃ©ng, dÃ¡ dÃ o le xÄ«n de zuÃ¬jiÄ shuÇpÃ­ng.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'æ”¹è¿›', 'pinyin': 'gÇijÃ¬n', 'trans': 'improve'},\n{'word': 'å•æ­¥', 'pinyin': 'dÄnbÃ¹', 'trans': 'single-step'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇdÇo', 'trans': 'guidance'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'},\n{'word': 'å¤„ç†', 'pinyin': 'chÇ”lÇ', 'trans': 'handle'},\n{'word': 'ä¸åŒ', 'pinyin': 'bÃ¹tÃ³ng', 'trans': 'different'},\n{'word': 'éª¨æ¶', 'pinyin': 'gÇ”jiÃ ', 'trans': 'skeleton'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'ä¸ç¨³å®š', 'pinyin': 'bÃ¹wÄ›ndÃ¬ng', 'trans': 'unstable'},\n{'word': 'ä¸”', 'pinyin': 'qiÄ›', 'trans': 'and'},\n{'word': 'ä¸æ”¯æŒ', 'pinyin': 'bÃ¹ zhÄ«chÃ­', 'trans': 'not support'},\n{'word': 'è´Ÿé¢', 'pinyin': 'fÃ¹miÃ n', 'trans': 'negative'},\n{'word': 'æç¤º', 'pinyin': 'tÃ­shÃ¬', 'trans': 'prompt'},\n{'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through'},\n{'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'},\n{'word': 'NASA', 'pinyin': '', 'trans': 'NASA'},\n{'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solve'},\n{'word': 'è¿™äº›', 'pinyin': 'zhÃ¨xiÄ“', 'trans': 'these'},\n{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨ntÃ­', 'trans': 'problems'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'æå‡', 'pinyin': 'tÃ­shÄ“ng', 'trans': 'enhance'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ­ngnÃ©ng', 'trans': 'performance'},\n{'word': 'è¾¾åˆ°', 'pinyin': 'dÃ¡dÃ o', 'trans': 'reach'},\n{'word': 'æ–°çš„', 'pinyin': 'xÄ«n de', 'trans': 'new'},\n{'word': 'æœ€ä½³', 'pinyin': 'zuÃ¬jiÄ', 'trans': 'best'},\n{'word': 'æ°´å¹³', 'pinyin': 'shuÇpÃ­ng', 'trans': 'level'}]",
        "trans": "This article introduces a new framework called SNOOPI, aimed at improving the guidance mechanism of single-step diffusion models. Existing methods perform unstably when handling different diffusion model backbones and do not support negative prompt guidance. SNOOPI addresses these issues through the PG-SB and NASA methods. Experimental results demonstrate that SNOOPI significantly enhances the performance of benchmark models, achieving new best-in-class levels.",
        "update_ts": "2024-12-05 09:11"
    }
}