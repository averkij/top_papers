{
    "date": {
        "ru": "18 июля",
        "en": "July 18",
        "zh": "7月18日"
    },
    "time_utc": "2025-07-18 02:57",
    "weekday": 4,
    "issue_id": 4883,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.13334",
            "title": "A Survey of Context Engineering for Large Language Models",
            "url": "https://huggingface.co/papers/2507.13334",
            "abstract": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.",
            "score": 18,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "e0191e89e0360224",
            "authors": [
                "Lingrui Mei",
                "Jiayu Yao",
                "Yuyao Ge",
                "Yiwei Wang",
                "Baolong Bi",
                "Yujun Cai",
                "Jiazhi Liu",
                "Mingyu Li",
                "Zhong-Zhi Li",
                "Duzhen Zhang",
                "Chenlin Zhou",
                "Jiayi Mao",
                "Tianze Xia",
                "Jiafeng Guo",
                "Shenghua Liu"
            ],
            "affiliations": [
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "Peking University",
                "The University of Queensland",
                "Tsinghua University",
                "University of California, Merced",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13334.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#survey",
                    "#multimodal",
                    "#long_context",
                    "#architecture",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Контекстная инженерия: оптимизация входных данных для раскрытия потенциала LLM",
                    "desc": "Эта статья представляет собой обзор области контекстной инженерии для больших языковых моделей (LLM). Авторы систематизируют компоненты и методы оптимизации контекстной информации, подаваемой в LLM во время вывода. Рассматриваются такие темы как извлечение и генерация контекста, обработка контекста, системы памяти и мультиагентные системы. Выявлен существенный разрыв между способностью моделей понимать сложный контекст и генерировать столь же сложные выходные данные."
                },
                "en": {
                    "title": "Optimizing Context for Superior AI Outputs",
                    "desc": "This paper introduces Context Engineering, a new approach to optimize the information provided to Large Language Models (LLMs) for better performance. It breaks down the process into key components like context retrieval, processing, and management, and shows how these can be combined in advanced systems like retrieval-augmented generation and multi-agent systems. The authors analyze over 1300 research papers to highlight a significant gap: while LLMs can understand complex contexts well, they struggle to generate sophisticated long-form content. The paper aims to provide a roadmap for future research to enhance the capabilities of LLMs in generating high-quality outputs."
                },
                "zh": {
                    "title": "优化上下文，提升语言模型能力",
                    "desc": "本文介绍了上下文工程（Context Engineering），这是一个系统优化大型语言模型（LLMs）信息负载的正式学科。研究表明，LLMs的性能主要取决于推理过程中提供的上下文信息。我们对上下文工程进行了全面的分类，分析了其基础组件及其在智能系统中的复杂实现。通过对1300多篇研究论文的系统分析，本文揭示了当前模型在生成复杂长文本输出方面的显著局限性，并强调了未来研究的优先方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13332",
            "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
            "url": "https://huggingface.co/papers/2507.13332",
            "abstract": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.",
            "score": 9,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "d03adfbd7cec2623",
            "authors": [
                "Zhouqi Hua",
                "Wenwei Zhang",
                "Chengqi Lyu",
                "Yuzhe Gu",
                "Songyang Gao",
                "Kuikun Liu",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13332.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Имитация машины Тьюринга для улучшения обобщающей способности языковых моделей",
                    "desc": "Метод TAIL имитирует процессы выполнения машины Тьюринга для улучшения способности больших языковых моделей (LLM) к обобщению на более длинные последовательности. Он синтезирует данные цепочки рассуждений, которые имитируют процесс выполнения машины Тьюринга, линейно расширяя шаги рассуждений до атомарных состояний. TAIL значительно улучшает способность к обобщению по длине и производительность модели Qwen2.5-7B на различных задачах, используя только синтетические данные. Эксперименты показывают, что ключевые концепции машины Тьюринга необходимы для TAIL для обобщения по длине."
                },
                "en": {
                    "title": "Enhancing LLMs with Turing Machine Imitation for Better Generalization",
                    "desc": "The paper introduces TAIL, a novel method that enhances the performance of large language models (LLMs) by mimicking the execution processes of Turing Machines. It addresses the challenge of length generalization, enabling LLMs to solve longer sequences than those seen during training. TAIL synthesizes chain-of-thought data to improve reasoning capabilities and reduce shortcut learning, which often leads to poor generalization. The method demonstrates significant improvements in performance across various tasks using synthetic data, highlighting the importance of Turing Machine concepts in LLM reasoning."
                },
                "zh": {
                    "title": "提升LLM长度泛化能力的图灵机模仿学习",
                    "desc": "本文提出了一种名为TAIL的方法，旨在提高大型语言模型（LLM）的长度泛化能力。TAIL通过模拟图灵机的执行过程，合成链式思维数据，从而减少了快捷学习现象。该方法通过线性扩展推理步骤，改善了动态和长距离数据访问的难度。实验结果表明，TAIL在多个任务上显著提升了模型的性能，展示了图灵机的关键概念在长度泛化中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13347",
            "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
            "url": "https://huggingface.co/papers/2507.13347",
            "abstract": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.",
            "score": 1,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "006f7b52edb3a67a",
            "authors": [
                "Yifan Wang",
                "Jianjun Zhou",
                "Haoyi Zhu",
                "Wenzheng Chang",
                "Yang Zhou",
                "Zizun Li",
                "Junyi Chen",
                "Jiangmiao Pang",
                "Chunhua Shen",
                "Tong He"
            ],
            "affiliations": [
                "SII",
                "Shanghai AI Lab",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13347.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Революция в реконструкции 3D сцен без опорных кадров",
                    "desc": "Статья представляет новую нейронную сеть π^3 для реконструкции визуальной геометрии без фиксированного опорного вида. В отличие от предыдущих методов, π^3 использует полностью перестановочно-эквивариантную архитектуру для предсказания аффинно-инвариантных поз камеры и масштабно-инвариантных локальных карт точек. Такой подход делает модель устойчивой к порядку входных данных и хорошо масштабируемой. π^3 достигает лучших результатов в оценке положения камеры, глубины и реконструкции плотной карты точек."
                },
                "en": {
                    "title": "Revolutionizing Visual Geometry with Permutation-Equivariance",
                    "desc": "The paper presents pi^3, a novel permutation-equivariant neural network designed for visual geometry reconstruction without relying on a fixed reference view. Traditional methods often depend on a specific viewpoint, which can introduce biases and lead to inaccuracies. In contrast, pi^3 utilizes a fully permutation-equivariant architecture to predict camera poses and point maps that are invariant to scale and reference frames. This innovative approach enhances robustness and scalability, allowing pi^3 to achieve state-of-the-art results in tasks like camera pose estimation and depth estimation."
                },
                "zh": {
                    "title": "无参考视角的视觉几何重建新方法",
                    "desc": "本文介绍了一种名为pi^3的神经网络，它在视觉几何重建中不依赖于固定的参考视角。传统方法通常将重建锚定在特定的视点，这种偏置可能导致不稳定和失败。与此不同，pi^3采用完全的置换等变架构，能够在没有参考框架的情况下预测仿射不变的相机姿态和尺度不变的局部点图。该模型的设计使其对输入顺序具有内在的鲁棒性，并且具有很高的可扩展性，从而在相机姿态估计、单目/视频深度估计和密集点图重建等任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12720",
            "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
            "url": "https://huggingface.co/papers/2507.12720",
            "abstract": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens",
            "score": 1,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "195b799c7dd66533",
            "authors": [
                "Abraham Toluase Owodunni",
                "Orevaoghene Ahia",
                "Sachin Kumar"
            ],
            "affiliations": [
                "The Ohio State University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12720.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#optimization",
                    "#multilingual"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Гибкая токенизация для улучшения языковых моделей",
                    "desc": "FLEXITOKENS - это байтовая языковая модель с обучаемым токенизатором, которая снижает чрезмерную фрагментацию токенов и улучшает производительность в многоязычных задачах и задачах с разнообразной морфологией. Модель включает подмодуль, который учится предсказывать границы между байтовыми последовательностями, кодируя их в сегменты переменной длины. В отличие от существующих методов без токенизаторов, FLEXITOKENS использует упрощенную целевую функцию обучения, обеспечивающую большую гибкость при адаптации. Эксперименты показывают, что FLEXITOKENS последовательно снижает чрезмерную фрагментацию токенов и достигает до 10% улучшения производительности в целевых задачах по сравнению с подсловными и другими токенизаторами на основе градиентов."
                },
                "en": {
                    "title": "FLEXITOKENS: Adaptive Tokenization for Enhanced Language Model Performance",
                    "desc": "FLEXITOKENS is a novel byte-level language model that introduces a learnable tokenizer to enhance adaptability in tokenization. Traditional subword tokenizers are rigid and often lead to over-fragmentation, especially in multilingual and morphologically diverse contexts. By allowing the model to learn token boundaries dynamically, FLEXITOKENS reduces inefficiencies and improves performance on various tasks. The results show up to a 10% increase in performance compared to existing tokenization methods, demonstrating its effectiveness in handling diverse data distributions."
                },
                "zh": {
                    "title": "FLEXITOKENS：灵活的字节级语言模型",
                    "desc": "FLEXITOKENS是一种字节级语言模型，具有可学习的分词器，旨在减少分词过度碎片化的问题。传统的子词分词器在适应新数据时往往不够灵活，导致在处理不同语言或脚本时效率低下。通过引入可学习的分词器，FLEXITOKENS能够根据输入字节序列自适应地预测分界，从而生成可变长度的分段。实验结果表明，FLEXITOKENS在多语言基准测试和形态多样性任务中表现优异，性能提升可达10%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12508",
            "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
            "url": "https://huggingface.co/papers/2507.12508",
            "abstract": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.",
            "score": 1,
            "issue_id": 4883,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "1a89f50f8edd267e",
            "authors": [
                "Yuncong Yang",
                "Jiageng Liu",
                "Zheyuan Zhang",
                "Siyuan Zhou",
                "Reuben Tan",
                "Jianwei Yang",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard",
                "JHU",
                "Microsoft Research",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12508.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#benchmark",
                    "#diffusion",
                    "#rl",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MindJourney: 3D-рассуждения для моделей машинного зрения без дообучения",
                    "desc": "MindJourney - это фреймворк, который улучшает способности моделей машинного зрения и обработки естественного языка (VLM) к пространственному рассуждению в 3D-пространстве. Он сочетает VLM с управляемой моделью мира, основанной на видеодиффузии. Этот подход позволяет VLM создавать траекторию камеры, а модель мира генерирует соответствующие виды на каждом шаге. MindJourney достигает улучшения производительности на 8% в задачах пространственного рассуждения без дополнительного обучения."
                },
                "en": {
                    "title": "MindJourney: Enhancing 3D Reasoning in Vision-Language Models",
                    "desc": "MindJourney is a novel framework that enhances vision-language models (VLMs) by integrating them with a video diffusion-based world model, enabling better spatial reasoning in 3D environments. This approach allows VLMs to generate a camera trajectory and synthesize views dynamically, facilitating improved understanding of 3D dynamics without the need for fine-tuning. By leveraging multi-view evidence during interactive exploration, MindJourney achieves an average performance boost of over 8% on spatial reasoning tasks. This method demonstrates the effectiveness of coupling VLMs with world models for robust 3D reasoning, offering a straightforward solution for enhancing model capabilities at test time."
                },
                "zh": {
                    "title": "MindJourney：提升视觉-语言模型的3D推理能力",
                    "desc": "MindJourney 是一种增强视觉-语言模型的框架，通过与基于视频扩散的世界模型结合，实现了3D推理能力的提升。该方法在不进行微调的情况下，显著提高了空间推理任务的表现，尤其是在SAT基准测试中平均提升了8%。传统的视觉-语言模型在处理3D动态时常常表现不佳，而MindJourney通过迭代生成相机轨迹并合成多视图证据，帮助模型更好地理解空间关系。此方法展示了将世界模型与视觉-语言模型结合的潜力，为3D推理提供了一种简单有效的解决方案。"
                }
            }
        }
    ],
    "link_prev": "2025-07-17.html",
    "link_next": "2025-07-21.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "17.07",
        "en": "07/17",
        "zh": "7月17日"
    },
    "short_date_next": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7月21日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}