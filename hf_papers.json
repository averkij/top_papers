{
    "date": {
        "ru": "27 марта",
        "en": "March 27",
        "zh": "3月27日"
    },
    "time_utc": "2025-03-27 02:20",
    "weekday": 3,
    "issue_id": 2919,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.20240",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
            "url": "https://huggingface.co/papers/2503.20240",
            "abstract": "Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.",
            "score": 7,
            "issue_id": 2919,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "4add99d8d7510263",
            "authors": [
                "Prin Phunyaphibarn",
                "Phillip Y. Lee",
                "Jaihoon Kim",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20240.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Улучшение CFG: замена безусловного шума для качественной генерации",
                    "desc": "Статья посвящена улучшению метода Classifier-Free Guidance (CFG) в обучении условных диффузионных моделей. Авторы обнаружили, что совместное обучение условного и безусловного предсказания шума приводит к ухудшению качества генерации. Они предлагают заменять безусловный шум в CFG на предсказания базовой модели, что значительно улучшает условную генерацию. Эксперименты подтверждают эффективность этого подхода для различных моделей генерации изображений и видео."
                },
                "en": {
                    "title": "Enhancing Conditional Generation with Improved Unconditional Noise",
                    "desc": "This paper discusses Classifier-Free Guidance (CFG), a technique used in training conditional diffusion models. The authors identify that using a single network for both conditional and unconditional noise prediction leads to poor performance in generating unconditional noise, which negatively impacts the quality of conditional outputs. They propose a solution where the unconditional noise predictions are replaced with those from a better-performing base model, resulting in improved conditional generation. The findings are validated through experiments on various CFG-based models for generating images and videos, demonstrating the effectiveness of their approach."
                },
                "zh": {
                    "title": "提升条件生成质量的无条件噪声替代",
                    "desc": "无分类器引导（CFG）是一种在训练条件扩散模型中使用的基本技术。传统的CFG训练方法是使用单一网络同时学习条件和无条件噪声预测，但这种联合学习会导致无条件噪声的先验质量较差。研究表明，使用更好的无条件生成模型的噪声替代可以显著提高条件生成的质量。我们通过实验验证了这一点，适用于多种基于CFG的条件模型，包括图像和视频生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20756",
            "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
            "url": "https://huggingface.co/papers/2503.20756",
            "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.",
            "score": 2,
            "issue_id": 2919,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "6b8affbfbdd5a426",
            "authors": [
                "Chenxi Wang",
                "Jizhan Fang",
                "Xiang Chen",
                "Bozhong Tian",
                "Ziwen Xu",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20756.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Редактирование знаний для улучшения автономного вождения",
                    "desc": "Статья описывает применение больших мультимодальных моделей (LMM) в системах автономного вождения (ADS). Авторы предлагают использовать редактирование знаний для улучшения работы моделей без полного переобучения. Они также представляют набор данных ADS-Edit для оценки редактирования знаний в контексте автономного вождения. Проведенные эксперименты показывают перспективность этого подхода для развития автономных транспортных средств."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with Targeted Knowledge Editing",
                    "desc": "This paper discusses the use of Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS) and the challenges they face, such as traffic knowledge misunderstanding and complex road conditions. To overcome these issues, the authors propose a method called Knowledge Editing, which allows for specific adjustments to a model's behavior without needing to retrain it entirely. They also introduce ADS-Edit, a specialized dataset for multimodal knowledge editing in ADS, which includes diverse real-world scenarios and various data types. The paper presents experimental results that highlight the potential of knowledge editing to enhance the performance of autonomous driving systems."
                },
                "zh": {
                    "title": "知识编辑助力自动驾驶系统的进步",
                    "desc": "本文探讨了大型多模态模型（LMMs）在自动驾驶系统（ADS）中的应用潜力。尽管LMMs有前景，但在交通知识理解、复杂路况和车辆多样性等方面面临挑战。为了解决这些问题，我们提出了知识编辑的方法，可以在不完全重训练的情况下，针对性地修改模型的行为。同时，我们还引入了ADS-Edit，这是一个专为自动驾驶设计的多模态知识编辑数据集，包含多种真实场景和数据类型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20220",
            "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
            "url": "https://huggingface.co/papers/2503.20220",
            "abstract": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.",
            "score": 1,
            "issue_id": 2919,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "f65c515bbb6af49b",
            "authors": [
                "Weijie Guo",
                "Guofeng Zhang",
                "Wufei Ma",
                "Alan Yuille"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20220.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#3d",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Точная 3D-поза без 3D-разметки",
                    "desc": "DINeMo - это новая нейронная сетевая модель для оценки 3D/6D позы объектов на уровне категорий без использования 3D-разметки. Модель использует псевдо-соответствия, полученные из крупных визуальных фундаментальных моделей, применяя двунаправленный метод генерации. DINeMo значительно превосходит предыдущие методы оценки 3D-позы с нулевым и малым количеством примеров, сокращая разрыв с полностью контролируемыми методами на 67.3%. Модель эффективно масштабируется при добавлении неразмеченных изображений в процессе обучения."
                },
                "en": {
                    "title": "Revolutionizing 3D Pose Estimation with Unlabeled Data",
                    "desc": "This paper introduces DINeMo, a new neural mesh model designed for category-level 3D/6D pose estimation without relying on 3D annotations. It utilizes pseudo-correspondence generated from large visual foundation models, allowing it to learn from unlabeled data effectively. The model employs a bidirectional approach to generate pseudo correspondences, combining local appearance features with global context. Experimental results show that DINeMo significantly improves performance in zero- and few-shot 3D pose estimation, achieving results close to fully-supervised methods while being more scalable and efficient."
                },
                "zh": {
                    "title": "无标注3D姿态估计的新突破",
                    "desc": "本论文提出了一种新的神经网格模型DINeMo，用于类别级的3D/6D姿态估计，旨在提高3D场景理解的能力。与以往依赖3D标注的学习方法不同，DINeMo通过利用大型视觉基础模型获得的伪对应关系进行训练，从而避免了对3D标注的依赖。我们采用双向伪对应生成方法，结合局部外观特征和全局上下文信息，显著提升了模型的鲁棒性。实验结果表明，DINeMo在车类数据集上的表现超越了之前的零样本和少样本3D姿态估计方法，缩小了与完全监督方法的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20020",
            "title": "Gemini Robotics: Bringing AI into the Physical World",
            "url": "https://huggingface.co/papers/2503.20020",
            "abstract": "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.",
            "score": 1,
            "issue_id": 2919,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "5edeeaed81b90426",
            "authors": [
                "Gemini Robotics Team",
                "Saminda Abeyruwan",
                "Joshua Ainslie",
                "Jean-Baptiste Alayrac",
                "Montserrat Gonzalez Arenas",
                "Travis Armstrong",
                "Ashwin Balakrishna",
                "Robert Baruch",
                "Maria Bauza",
                "Michiel Blokzijl",
                "Steven Bohez",
                "Konstantinos Bousmalis",
                "Anthony Brohan",
                "Thomas Buschmann",
                "Arunkumar Byravan",
                "Serkan Cabi",
                "Ken Caluwaerts",
                "Federico Casarini",
                "Oscar Chang",
                "Jose Enrique Chen",
                "Xi Chen",
                "Hao-Tien Lewis Chiang",
                "Krzysztof Choromanski",
                "David D'Ambrosio",
                "Sudeep Dasari",
                "Todor Davchev",
                "Coline Devin",
                "Norman Di Palo",
                "Tianli Ding",
                "Adil Dostmohamed",
                "Danny Driess",
                "Yilun Du",
                "Debidatta Dwibedi",
                "Michael Elabd",
                "Claudio Fantacci",
                "Cody Fong",
                "Erik Frey",
                "Chuyuan Fu",
                "Marissa Giustina",
                "Keerthana Gopalakrishnan",
                "Laura Graesser",
                "Leonard Hasenclever",
                "Nicolas Heess",
                "Brandon Hernaez",
                "Alexander Herzog",
                "R. Alex Hofer",
                "Jan Humplik",
                "Atil Iscen",
                "Mithun George Jacob",
                "Deepali Jain",
                "Ryan Julian",
                "Dmitry Kalashnikov",
                "M. Emre Karagozler",
                "Stefani Karp",
                "Chase Kew",
                "Jerad Kirkland",
                "Sean Kirmani",
                "Yuheng Kuang",
                "Thomas Lampe",
                "Antoine Laurens",
                "Isabel Leal",
                "Alex X. Lee",
                "Tsang-Wei Edward Lee",
                "Jacky Liang",
                "Yixin Lin",
                "Sharath Maddineni",
                "Anirudha Majumdar",
                "Assaf Hurwitz Michaely",
                "Robert Moreno",
                "Michael Neunert",
                "Francesco Nori",
                "Carolina Parada",
                "Emilio Parisotto",
                "Peter Pastor",
                "Acorn Pooley",
                "Kanishka Rao",
                "Krista Reymann",
                "Dorsa Sadigh",
                "Stefano Saliceti",
                "Pannag Sanketi",
                "Pierre Sermanet",
                "Dhruv Shah",
                "Mohit Sharma",
                "Kathryn Shea",
                "Charles Shu",
                "Vikas Sindhwani",
                "Sumeet Singh",
                "Radu Soricut",
                "Jost Tobias Springenberg",
                "Rachel Sterneck",
                "Razvan Surdulescu",
                "Jie Tan",
                "Jonathan Tompson",
                "Vincent Vanhoucke",
                "Jake Varley",
                "Grace Vesom",
                "Giulia Vezzani",
                "Oriol Vinyals",
                "Ayzaan Wahid",
                "Stefan Welker",
                "Paul Wohlhart",
                "Fei Xia",
                "Ted Xiao",
                "Annie Xie",
                "Jinyu Xie",
                "Peng Xu",
                "Sichun Xu",
                "Ying Xu",
                "Zhuo Xu",
                "Yuxiang Yang",
                "Rui Yao",
                "Sergey Yaroshenko",
                "Wenhao Yu",
                "Wentao Yuan",
                "Jingwei Zhang",
                "Tingnan Zhang",
                "Allan Zhou",
                "Yuxiang Zhou"
            ],
            "affiliations": [
                "Gemini Robotics Team, Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20020.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#ethics",
                    "#games",
                    "#reasoning",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Gemini Robotics: ИИ выходит в реальный мир",
                    "desc": "Статья представляет семейство моделей Gemini Robotics, основанных на Gemini 2.0 и предназначенных для управления роботами. Gemini Robotics - это усовершенствованная мультимодальная модель типа Vision-Language-Action, способная напрямую контролировать роботов и выполнять сложные манипуляции. Модель Gemini Robotics-ER расширяет возможности рассуждений Gemini на физический мир, улучшая пространственное и временное понимание. Исследователи демонстрируют, как эти модели могут быть применены в различных робототехнических задачах, включая обнаружение объектов, прогнозирование траекторий и адаптацию к новым воплощениям роботов."
                },
                "en": {
                    "title": "Empowering Robots with Gemini Robotics: A Leap in Multimodal AI",
                    "desc": "This paper presents Gemini Robotics, a new family of AI models designed specifically for robotic applications, building on the Gemini 2.0 framework. The model integrates Vision-Language-Action (VLA) capabilities, allowing robots to perform complex manipulation tasks with smooth and adaptive movements. It can learn from limited demonstrations and adapt to new tasks and robot designs, showcasing its versatility in various environments. Additionally, the paper introduces Gemini Robotics-ER, which enhances the model's reasoning abilities in physical contexts, addressing safety and practical applications in robotics."
                },
                "zh": {
                    "title": "Gemini Robotics：通用机器人的新纪元",
                    "desc": "最近大型多模态模型的进展使得数字领域的通用能力显著提升，但将其应用于机器人等物理代理仍然面临挑战。本文介绍了一种新型的人工智能模型，专为机器人设计，基于Gemini 2.0构建。Gemini Robotics是一个先进的视觉-语言-动作（VLA）通用模型，能够直接控制机器人，执行复杂的操作任务，并对物体类型和位置的变化具有鲁棒性。通过额外的微调，Gemini Robotics可以专门化为新的能力，包括解决长时间跨度的高灵巧任务，以及从少量示例中学习新任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20201",
            "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
            "url": "https://huggingface.co/papers/2503.20201",
            "abstract": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.",
            "score": 0,
            "issue_id": 2919,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "a9b1bed8d26f5055",
            "authors": [
                "Salaheddin Alzubi",
                "Creston Brooks",
                "Purva Chiniya",
                "Edoardo Contente",
                "Chiara von Gerlach",
                "Lucas Irwin",
                "Yihan Jiang",
                "Arda Kaz",
                "Windsor Nguyen",
                "Sewoong Oh",
                "Himanshu Tyagi",
                "Pramod Viswanath"
            ],
            "affiliations": [
                "Princeton University",
                "Sentient",
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20201.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ODS: открытый ИИ-поиск на уровне проприетарных решений",
                    "desc": "Open Deep Search (ODS) - это новая система, объединяющая возможности открытых языковых моделей с инструментами веб-поиска для ответов на запросы. ODS состоит из двух компонентов: Open Search Tool (инструмент поиска) и Open Reasoning Agent (агент рассуждений), которые работают с базовой языковой моделью по выбору пользователя. Система демонстрирует высокую эффективность на бенчмарках SimpleQA и FRAMES, превосходя существующие решения. ODS позволяет улучшить возможности любых языковых моделей, добавляя им функции поиска и рассуждений."
                },
                "en": {
                    "title": "Empowering Open-Source LLMs with Advanced Search and Reasoning",
                    "desc": "Open Deep Search (ODS) is a new framework designed to enhance the reasoning abilities of open-source large language models (LLMs) by integrating them with advanced web search tools. It features two main components: the Open Search Tool, which performs web searches, and the Open Reasoning Agent, which interprets tasks and coordinates actions, including using the search tool. ODS has shown to significantly improve performance on benchmarks like SimpleQA and FRAMES, even surpassing proprietary models like GPT-4o Search Preview in accuracy. This innovation allows users to leverage powerful open-source LLMs while achieving state-of-the-art results in query answering tasks."
                },
                "zh": {
                    "title": "开放深度搜索：提升开源LLM的推理与搜索能力",
                    "desc": "我们介绍了开放深度搜索（ODS），旨在缩小专有搜索人工智能解决方案与开源解决方案之间的差距。ODS的主要创新是通过推理代理增强最新开源大语言模型（LLM）的推理能力，使其能够明智地使用网络搜索工具来回答查询。ODS由两个组件组成：开放搜索工具和开放推理代理，后者负责解释任务并协调一系列操作，包括调用工具。通过与强大的开源推理LLM（如DeepSeek-R1）结合，ODS在SimpleQA和FRAMES两个基准测试中几乎达到了现有的最先进水平，甚至在FRAMES基准上提高了9.7%的准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19953",
            "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
            "url": "https://huggingface.co/papers/2503.19953",
            "abstract": "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.",
            "score": 0,
            "issue_id": 2919,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "01093e8b98f32607",
            "authors": [
                "Stefan Stojanov",
                "David Wendt",
                "Seungwoo Kim",
                "Rahul Venkatesh",
                "Kevin Feigelis",
                "Jiajun Wu",
                "Daniel LK Yamins"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19953.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Самообучаемая оценка движения в видео без размеченных данных",
                    "desc": "Opt-CWM - это самообучаемый метод для оценки потока и окклюзии на основе предобученной модели предсказания следующего кадра. Он работает путем оптимизации контрфактических проб, извлекающих информацию о движении из базовой видеомодели. Этот подход позволяет избежать использования фиксированных эвристик и обучаться на неограниченных видеовходах. Opt-CWM достигает наилучших результатов в оценке движения на реальных видео без использования размеченных данных."
                },
                "en": {
                    "title": "Revolutionizing Motion Estimation with Self-Supervised Learning",
                    "desc": "This paper presents Opt-CWM, a self-supervised method for estimating motion in videos, which is crucial for applications like video generation and robotics. Unlike traditional approaches that rely on synthetic data or specific heuristics, Opt-CWM utilizes a pre-trained next-frame prediction model to learn motion information directly from real-world video inputs. The technique optimizes counterfactual probes, allowing it to adaptively extract flow and occlusion data without needing fixed rules. As a result, Opt-CWM achieves state-of-the-art performance in motion estimation while eliminating the requirement for labeled datasets."
                },
                "zh": {
                    "title": "自监督运动估计的新突破",
                    "desc": "本论文探讨了视频中的运动估计问题，这是计算机视觉中的一个重要课题，广泛应用于可控视频生成和机器人技术。现有的方法主要依赖合成数据训练或特定情境的启发式调整，这限制了模型在真实场景中的表现。我们提出了一种名为Opt-CWM的自监督技术，通过预训练的下一帧预测模型进行流动和遮挡估计。Opt-CWM通过优化反事实探针来提取运动信息，避免了固定启发式的需求，并在无需标注数据的情况下，在真实视频上实现了最先进的运动估计性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19462",
            "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
            "url": "https://huggingface.co/papers/2503.19462",
            "abstract": "Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challenges present in existing diffusion distillation methods and propose a novel efficient method, namely AccVideo, to reduce the inference steps for accelerating video diffusion models with synthetic dataset. We leverage the pretrained video diffusion model to generate multiple valid denoising trajectories as our synthetic dataset, which eliminates the use of useless data points during distillation. Based on the synthetic dataset, we design a trajectory-based few-step guidance that utilizes key data points from the denoising trajectories to learn the noise-to-video mapping, enabling video generation in fewer steps. Furthermore, since the synthetic dataset captures the data distribution at each diffusion timestep, we introduce an adversarial training strategy to align the output distribution of the student model with that of our synthetic dataset, thereby enhancing the video quality. Extensive experiments demonstrate that our model achieves 8.5x improvements in generation speed compared to the teacher model while maintaining comparable performance. Compared to previous accelerating methods, our approach is capable of generating videos with higher quality and resolution, i.e., 5-seconds, 720x1280, 24fps.",
            "score": 0,
            "issue_id": 2919,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "721d2bb59c963434",
            "authors": [
                "Haiyu Zhang",
                "Xinyuan Chen",
                "Yaohui Wang",
                "Xihui Liu",
                "Yunhong Wang",
                "Yu Qiao"
            ],
            "affiliations": [
                "Beihang University",
                "Shanghai AI Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19462.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#video",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Ускорение генерации видео с помощью синтетических данных и дистилляции диффузионных моделей",
                    "desc": "Статья представляет новый метод AccVideo для ускорения генерации видео с помощью диффузионных моделей. Авторы используют предобученную модель для создания синтетического набора данных с множественными траекториями денойзинга. Предложенный подход включает траекторно-ориентированное руководство по малошаговой генерации и состязательное обучение для улучшения качества видео. Эксперименты показывают 8.5-кратное ускорение генерации по сравнению с исходной моделью при сохранении сопоставимого качества."
                },
                "en": {
                    "title": "Accelerating Video Generation with AccVideo",
                    "desc": "This paper addresses the slow and resource-intensive process of video generation using diffusion models, which require many steps for denoising. The authors introduce AccVideo, a new method that reduces the number of inference steps needed by utilizing a synthetic dataset generated from a pretrained video diffusion model. By focusing on key data points from denoising trajectories, the model learns to map noise to video more efficiently. Additionally, an adversarial training strategy is employed to improve the quality of the generated videos, resulting in significant speed improvements while maintaining high resolution and quality."
                },
                "zh": {
                    "title": "加速视频生成，提升质量与效率",
                    "desc": "扩散模型在视频生成领域取得了显著进展，但其迭代去噪的特性导致生成视频需要大量推理步骤，速度慢且计算成本高。本文分析了现有扩散蒸馏方法中的挑战，并提出了一种新颖的高效方法AccVideo，以减少推理步骤，加速视频扩散模型。我们利用预训练的视频扩散模型生成多个有效的去噪轨迹作为合成数据集，从而在蒸馏过程中消除无用数据点。通过设计基于轨迹的少步引导，我们能够在更少的步骤中实现视频生成，同时引入对抗训练策略以提高视频质量。"
                }
            }
        }
    ],
    "link_prev": "2025-03-26.html",
    "link_next": "2025-03-28.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "26.03",
        "en": "03/26",
        "zh": "3月26日"
    },
    "short_date_next": {
        "ru": "28.03",
        "en": "03/28",
        "zh": "3月28日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 3,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 2,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了长上下文自回归模型在语言生成中的进步，但在视频生成中仍面临挑战。作者提出了Frame AutoRegressive (FAR)，一种用于视频自回归建模的强大基准。FAR通过建模连续帧之间的时间因果依赖性，实现了比Token AR和视频扩散变压器更好的收敛。然而，长上下文视觉建模面临视觉冗余和计算成本高的问题。为解决这些问题，作者提出了FlexRoPE和长短期上下文建模技术，使得在长视频序列上的训练更加高效，并在短长视频生成中取得了最佳性能。",
        "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
        "pinyin": "Zhè piān wénzhāng tǎolùnle cháng shàngxiàwén zìhuíguī móxíng zài yǔyán shēngchéng zhōng de jìnbù, dàn zài shìpǐn shēngchéng zhōng réng miànliào tiǎozhàn. Zuòzhě tíchūle Frame AutoRegressive (FAR), yīzhǒng yòngyú shìpǐn zìhuíguī jiànmó de qiángdà jīzhǔn. FAR tōngguò jiànmó liánxù zhēn jiān de shíjiān yīnguǒ yīlàixìng, shíxiànle bǐ Token AR hé shìpǐn kuòsàn biànshūqì gèng hǎo de shōulǐan. Rán'ér, cháng shàngxiàwén shìjùe jiànmó miànliào shìjùe rǒngyú hé jìsuàn chéngběn gāo de wèntí. Wèi jiějué zhèxiē wèntí, zuòzhě tíchūle FlexRoPE hé chángduǎnqī shàngxiàwén jiànmó jìshù, shǐdé zài cháng shìpǐn xùliè shàng de xùnliàn gèngjiā gāoxiào, bìng zài duǎn cháng shìpǐn shēngchéng zhōng qudéle zuìjiā xìngnéng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"长上下文\", \"pinyin\": \"cháng shàng xià wén\", \"trans\": \"long context\"},\n    {\"word\": \"自回归\", \"pinyin\": \"zì huí guī\", \"trans\": \"autoregressive\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"进步\", \"pinyin\": \"jìn bù\", \"trans\": \"progress\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"Frame\", \"pinyin\": \"Frame\", \"trans\": \"Frame\"},\n    {\"word\": \"AutoRegressive\", \"pinyin\": \"AutoRegressive\", \"trans\": \"AutoRegressive\"},\n    {\"word\": \"FAR\", \"pinyin\": \"FAR\", \"trans\": \"FAR\"},\n    {\"word\": \"用于\", \"pinyin\": \"yòng yú\", \"trans\": \"used for\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"自回归建模\", \"pinyin\": \"zì huí guī jiàn mó\", \"trans\": \"autoregressive modeling\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"建模\", \"pinyin\": \"jiàn mó\", \"trans\": \"modeling\"},\n    {\"word\": \"连续\", \"pinyin\": \"lián xù\", \"trans\": \"continuous\"},\n    {\"word\": \"帧\", \"pinyin\": \"zhèn\", \"trans\": \"frame\"},\n    {\"word\": \"之间\", \"pinyin\": \"zhī jiān\", \"trans\": \"between\"},\n    {\"word\": \"时间\", \"pinyin\": \"shí jiān\", \"trans\": \"time\"},\n    {\"word\": \"因果\", \"pinyin\": \"yīn guǒ\", \"trans\": \"causal\"},\n    {\"word\": \"依赖性\", \"pinyin\": \"yī lài xìng\", \"trans\": \"dependency\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"收敛\", \"pinyin\": \"shōu liǎn\", \"trans\": \"convergence\"},\n    {\"word\": \"Token\", \"pinyin\": \"Token\", \"trans\": \"Token\"},\n    {\"word\": \"AR\", \"pinyin\": \"AR\", \"trans\": \"AR\"},\n    {\"word\": \"视频扩散变压器\", \"pinyin\": \"shì pín kuò sàn biàn yā qì\", \"trans\": \"video diffusion transformer\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"冗余\", \"pinyin\": \"róng yú\", \"trans\": \"redundancy\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"computation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"FlexRoPE\", \"pinyin\": \"FlexRoPE\", \"trans\": \"FlexRoPE\"},\n    {\"word\": \"长短期\", \"pinyin\": \"cháng duǎn qī\", \"trans\": \"long short-term\"},\n    {\"word\": \"上下文建模技术\", \"pinyin\": \"shàng xià wén jiàn mó jì shù\", \"trans\": \"context modeling techniques\"},\n    {\"word\": \"使得\", \"pinyin\": \"shǐ dé\", \"trans\": \"make\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"序列\", \"pinyin\": \"xù liè\", \"trans\": \"sequence\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔ dé\", \"trans\": \"achieve\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"optimal\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"}\n]",
        "trans": "This article discusses the advancements of long-context autoregressive models in language generation but highlights the challenges they still face in video generation. The authors introduce Frame AutoRegressive (FAR), a powerful benchmark for autoregressive modeling in video. FAR achieves better convergence than Token AR and Video Diffusion Transformers by modeling the temporal causal dependencies between consecutive frames. However, long-context visual modeling faces issues of visual redundancy and high computational costs. To address these problems, the authors propose FlexRoPE and long-short term context modeling techniques, making training on long video sequences more efficient and achieving state-of-the-art performance in both short and long video generation.",
        "update_ts": "2025-03-26 09:12"
    }
}