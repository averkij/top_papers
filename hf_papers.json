{
    "date": {
        "ru": "27 Ğ¸ÑĞ½Ñ",
        "en": "June 27",
        "zh": "6æœˆ27æ—¥"
    },
    "time_utc": "2025-06-29 18:32",
    "weekday": 4,
    "issue_id": 4545,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.20670",
            "title": "MMSearch-R1: Incentivizing LMMs to Search",
            "url": "https://huggingface.co/papers/2506.20670",
            "abstract": "MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",
            "score": 48,
            "issue_id": 4516,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "15412dc74ea5bed3",
            "authors": [
                "Jinming Wu",
                "Zihao Deng",
                "Wei Li",
                "Yiding Liu",
                "Bo You",
                "Bo Li",
                "Zejun Ma",
                "Ziwei Liu"
            ],
            "affiliations": [
                "ByteDance",
                "S-Lab, NTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20670.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#reasoning",
                    "#games",
                    "#rl",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MMSearch-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒÑÑÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…, Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ·Ğ° Ğ¿Ğ¾Ğ¸ÑĞº. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VQA Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 30%."
                },
                "en": {
                    "title": "Efficient Multimodal Search with Reinforcement Learning",
                    "desc": "MMSearch-R1 is a novel reinforcement learning framework designed to enhance the performance of large multimodal models (LMMs) in real-world search tasks. It addresses the limitations of traditional methods like retrieval-augmented generation (RAG) by allowing LMMs to conduct efficient, on-demand, multi-turn searches using both text and image data. The framework employs an outcome-based reward system that encourages optimal search strategies while minimizing unnecessary search actions. Through extensive testing, MMSearch-R1 demonstrates superior efficiency and effectiveness compared to existing models, significantly reducing the number of search calls needed to achieve high performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€æœç´¢çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "MMSearch-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç°å®ç¯å¢ƒä¸­çš„æœç´¢æ•ˆç‡ã€‚ä¸ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ä¸åŒï¼ŒMMSearch-R1é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œæ”¯æŒæŒ‰éœ€çš„å¤šè½®æœç´¢ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·ï¼Œé€šè¿‡åŸºäºç»“æœçš„å¥–åŠ±å’Œæœç´¢æƒ©ç½šæ¥æŒ‡å¯¼æ¨¡å‹çš„æœç´¢å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMSearch-R1åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŒç­‰è§„æ¨¡çš„RAGåŸºçº¿ï¼Œå¹¶ä¸”åœ¨å‡å°‘æœç´¢è°ƒç”¨çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„RAGæ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21506",
            "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
            "url": "https://huggingface.co/papers/2506.21506",
            "abstract": "Mind2Web 2 benchmark evaluates agentic search systems with a suite of realistic, long-horizon tasks, introducing an Agent-as-a-Judge framework to assess accuracy and source attribution.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
            "score": 38,
            "issue_id": 4516,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "00a88b4b0bc63d5b",
            "authors": [
                "Boyu Gou",
                "Zanming Huang",
                "Yuting Ning",
                "Yu Gu",
                "Michael Lin",
                "Weijian Qi",
                "Andrei Kopanev",
                "Botao Yu",
                "Bernal JimÃ©nez GutiÃ©rrez",
                "Yiheng Shu",
                "Chan Hee Song",
                "Jiaman Wu",
                "Shijie Chen",
                "Hanane Nour Moussa",
                "Tianshu Zhang",
                "Jian Xie",
                "Yifei Li",
                "Tianci Xue",
                "Zeyi Liao",
                "Kai Zhang",
                "Boyuan Zheng",
                "Zhaowei Cai",
                "Viktor Rozgic",
                "Morteza Ziyadi",
                "Huan Sun",
                "Yu Su"
            ],
            "affiliations": [
                "Amazon AGI",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21506.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#benchmark"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Mind2Web 2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Mind2Web 2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 130 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-as-a-Judge Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ñ‡Ğ°ÑĞ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 9 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° (OpenAI Deep Research) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 50-70% Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Mind2Web 2: Advancing Evaluation for Agentic Search Systems",
                    "desc": "The paper presents Mind2Web 2, a benchmark designed to evaluate agentic search systems through a set of 130 realistic, long-horizon tasks that require extensive web browsing and information synthesis. It introduces the Agent-as-a-Judge framework, which uses task-specific judge agents to automatically assess the accuracy and source attribution of answers generated by these systems. This benchmark addresses the limitations of existing evaluation methods that focus on short search tasks and static responses. The findings indicate that the best-performing system, OpenAI Deep Research, achieves significant performance levels compared to human users, highlighting the potential of agentic search technologies."
                },
                "zh": {
                    "title": "Mind2Web 2ï¼šè¯„ä¼°è‡ªä¸»æœç´¢ç³»ç»Ÿçš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Mind2Web 2åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è‡ªä¸»æœç´¢ç³»ç»Ÿåœ¨ç°å®é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Agent-as-a-Judgeæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ¥æºå½’å±ã€‚è¯¥åŸºå‡†åŒ…å«130ä¸ªé«˜è´¨é‡çš„ä»»åŠ¡ï¼Œè¦æ±‚å®æ—¶æµè§ˆç½‘é¡µå¹¶ç»¼åˆä¿¡æ¯ï¼Œæ„å»ºè¿‡ç¨‹ä¸­è€—è´¹äº†è¶…è¿‡1000å°æ—¶çš„äººåŠ›ã€‚é€šè¿‡å¯¹ä¹ä¸ªå‰æ²¿è‡ªä¸»æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°çš„å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†è¿™äº›ç³»ç»Ÿåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20911",
            "title": "FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
            "url": "https://huggingface.co/papers/2506.20911",
            "abstract": "A neurosymbolic agent combines language models for fast subtask planning with A$^*$ search for detailed toolpaths, creating a cost-efficient multi-turn image editing solution.  \t\t\t\t\tAI-generated summary \t\t\t\t We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as \"Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A^* search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A^* on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A^* search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A^* search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA^* is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate.",
            "score": 37,
            "issue_id": 4519,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "f2b4bebdfb3a457f",
            "authors": [
                "Advait Gupta",
                "Rishie Raj",
                "Dang Nguyen",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20911.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº A* Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ³ĞµĞ½Ñ‚ FaSTA* Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "FaSTA*: Fast and Efficient Multi-Turn Image Editing",
                    "desc": "This paper presents a neurosymbolic agent designed for efficient multi-turn image editing tasks. It utilizes large language models (LLMs) for quick planning of high-level subtasks and employs A* search for precise toolpath execution. The agent learns from previous successful toolpaths, allowing it to reuse effective strategies for similar tasks, which reduces computational costs. The proposed method, named FaSTA*, balances fast planning with detailed execution, achieving competitive performance while being more efficient than existing approaches."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å¤šè½®å›¾åƒç¼–è¾‘ä»£ç†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç¥ç»ç¬¦å·ä»£ç†ï¼Œç”¨äºè§£å†³å¤æ‚çš„å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥ä»£ç†ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¿«é€Ÿçš„é«˜å±‚æ¬¡å­ä»»åŠ¡è§„åˆ’ï¼Œä»¥åŠA^*æœç´¢ç®—æ³•è¿›è¡Œç²¾ç¡®çš„å·¥å…·è·¯å¾„è§„åˆ’ã€‚é€šè¿‡å¯¹æˆåŠŸçš„å·¥å…·è·¯å¾„è¿›è¡Œå½’çº³æ¨ç†ï¼Œä»£ç†èƒ½å¤Ÿæå–å’Œé‡ç”¨å¸¸ç”¨çš„å­ç¨‹åºï¼Œä»è€Œåœ¨ç›¸ä¼¼ä»»åŠ¡ä¸­èŠ‚çœè®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFaSTA^*åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æˆåŠŸç‡ä¸Šä¸æœ€å…ˆè¿›çš„åŸºçº¿ä¿æŒç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21520",
            "title": "MADrive: Memory-Augmented Driving Scene Modeling",
            "url": "https://huggingface.co/papers/2506.21520",
            "abstract": "MADrive enhances scene reconstruction for autonomous driving by integrating visually similar 3D car assets from an external memory bank to achieve photorealistic synthesis of altered scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/",
            "score": 34,
            "issue_id": 4524,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "850d8a345d876231",
            "authors": [
                "Polina Karpikova",
                "Daniil Selikhanovych",
                "Kirill Struminsky",
                "Ruslan Musaev",
                "Maria Golitsyna",
                "Dmitry Baranchuk"
            ],
            "affiliations": [
                "HSE University",
                "Skoltech",
                "Yandex",
                "Yandex Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21520.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#games",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "MADrive - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MAD-Cars, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 70 Ñ‚Ñ‹ÑÑÑ‡ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. MADrive Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ ÑÑ†ĞµĞ½Ñƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with Memory-Augmented Scene Reconstruction",
                    "desc": "MADrive is a framework that improves scene reconstruction for autonomous driving by using a memory bank of 3D car models. It allows for the replacement of real vehicles in a scene with visually similar 3D assets, enhancing the realism of altered driving scenarios. The framework utilizes a curated dataset called MAD-Cars, which contains around 70,000 car videos, to retrieve and reconstruct these 3D assets. This approach enables the generation of photorealistic images of driving environments, even when significant changes are made to the scene."
                },
                "zh": {
                    "title": "MADriveï¼šæå‡è‡ªåŠ¨é©¾é©¶åœºæ™¯é‡å»ºçš„çœŸå®æ„Ÿ",
                    "desc": "MADrive æ˜¯ä¸€ä¸ªå¢å¼ºåœºæ™¯é‡å»ºçš„æ¡†æ¶ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ã€‚å®ƒé€šè¿‡ä»å¤–éƒ¨è®°å¿†åº“ä¸­æ•´åˆè§†è§‰ç›¸ä¼¼çš„ 3D è½¦è¾†èµ„äº§ï¼Œæ¥å®ç°å¯¹æ”¹å˜åœºæ™¯çš„çœŸå®æ„Ÿåˆæˆã€‚è¯¥æ¡†æ¶ä½¿ç”¨äº†ä¸€ä¸ªåä¸º MAD-Cars çš„æ•°æ®é›†ï¼ŒåŒ…å«çº¦ 70,000 ä¸ª 360 åº¦çš„æ±½è½¦è§†é¢‘ï¼Œå¹¶é€šè¿‡æ£€ç´¢æ¨¡å—æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è½¦è¾†å®ä¾‹ã€‚æœ€ç»ˆï¼ŒMADrive èƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’çš„è½¦è¾†è¡¨ç°ï¼Œæ”¯æŒæ˜¾è‘—æ”¹å˜çš„åœºæ™¯åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21539",
            "title": "WorldVLA: Towards Autoregressive Action World Model",
            "url": "https://huggingface.co/papers/2506.21539",
            "abstract": "WorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.",
            "score": 33,
            "issue_id": 4517,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "a293600a80c39e2d",
            "authors": [
                "Jun Cen",
                "Chaohui Yu",
                "Hangjie Yuan",
                "Yuming Jiang",
                "Siteng Huang",
                "Jiayan Guo",
                "Xin Li",
                "Yibing Song",
                "Hao Luo",
                "Fan Wang",
                "Deli Zhao",
                "Hao Chen"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21539.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#multimodal",
                    "#games",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "WorldVLA - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. WorldVLA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Action Prediction with WorldVLA: A Unified Vision-Language-Action Model",
                    "desc": "WorldVLA is a novel autoregressive action world model that combines vision, language, and action understanding into a single framework. It enhances action prediction and sequence generation by integrating a world model that predicts future images based on actions and visual inputs. The model demonstrates mutual enhancement, where the action model improves visual understanding, which in turn aids the world model's image generation. To tackle the issue of error propagation in action sequences, an attention mask strategy is introduced, leading to significant performance gains in generating action sequences."
                },
                "zh": {
                    "title": "ä¸–ç•Œæ¨¡å‹ä¸åŠ¨ä½œæ¨¡å‹çš„ç›¸äº’å¢å¼º",
                    "desc": "WorldVLAæ˜¯ä¸€ç§è‡ªå›å½’çš„åŠ¨ä½œä¸–ç•Œæ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å’Œä¸–ç•Œæ¨¡å‹ã€‚å®ƒé€šè¿‡ç›¸äº’ç†è§£å’Œç”Ÿæˆæ¥å¢å¼ºæ€§èƒ½ï¼Œæ”¹å–„åŠ¨ä½œé¢„æµ‹å’Œåºåˆ—ç”Ÿæˆã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŠ¨ä½œå’Œå›¾åƒç†è§£æ¥é¢„æµ‹æœªæ¥å›¾åƒï¼Œä»è€Œå­¦ä¹ ç¯å¢ƒçš„åŸºæœ¬ç‰©ç†ç‰¹æ€§ã€‚æˆ‘ä»¬æå‡ºçš„æ³¨æ„åŠ›æ©ç ç­–ç•¥æœ‰æ•ˆè§£å†³äº†è‡ªå›å½’ç”Ÿæˆä¸­åŠ¨ä½œæ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†åŠ¨ä½œç”Ÿæˆçš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21551",
            "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
            "url": "https://huggingface.co/papers/2506.21551",
            "abstract": "Grokking, or continued test performance improvement after training loss convergence, is observed during pretraining of a large language model, showcasing a memorization-to-generalization process.  \t\t\t\t\tAI-generated summary \t\t\t\t Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's \"emergence of generalization\" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound.",
            "score": 25,
            "issue_id": 4518,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "d78fbac896c81bf5",
            "authors": [
                "Ziyue Li",
                "Chenrui Fan",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Department of Computer Science University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21551.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#data",
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° (grokking) Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ“Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‰ĞµĞµÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Grokking: From Memorization to Generalization in Large Language Models",
                    "desc": "This paper explores the phenomenon of grokking, where a large language model continues to improve its test performance even after the training loss has stabilized. The authors investigate this behavior during the pretraining of a 7 billion parameter model, OLMoE, and find that grokking occurs asynchronously across different data samples. They analyze the internal dynamics of the model, revealing that the pathways through which training samples are processed evolve from random to more structured forms, indicating a shift from memorization to generalization. Additionally, the study introduces new metrics to measure the complexity of these pathways, which can predict improvements in generalization across various tasks without the need for fine-tuning."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–ä¹‹è°œ",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„â€œgrokkingâ€ç°è±¡ï¼Œå³åœ¨è®­ç»ƒæŸå¤±æ”¶æ•›åï¼Œæµ‹è¯•æ€§èƒ½ä»ç„¶æŒç»­æé«˜ã€‚æˆ‘ä»¬é¦–æ¬¡åœ¨ä¸€ä¸ª7Bå‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹OLMoEçš„å•æ¬¡é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šè¿›è¡Œç ”ç©¶ï¼ŒéªŒè¯äº†grokkingåœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„å­˜åœ¨ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒæ ·æœ¬çš„è·¯å¾„ä»éšæœºã€ç‰¹å®šå®ä¾‹é€æ¸æ¼”å˜ä¸ºæ›´ç»“æ„åŒ–å’Œå¯å…±äº«çš„å½¢å¼ï¼Œå°½ç®¡æŸå¤±å·²æ”¶æ•›ï¼Œæ ·æœ¬è·¯å¾„çš„å¤æ‚æ€§å´åœ¨é™ä½ã€‚è¿™è¡¨æ˜äº†ä»è®°å¿†åˆ°æ³›åŒ–çš„è½¬å˜ï¼Œä¸ºå»¶è¿Ÿæ³›åŒ–æä¾›äº†æœºåˆ¶è§£é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21547",
            "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
            "url": "https://huggingface.co/papers/2506.21547",
            "abstract": "SAM4D is a multi-modal and temporal foundation model for segmentation in autonomous driving using Unified Multi-modal Positional Encoding and Motion-aware Cross-modal Memory Attention, with a multi-modal automated data engine generating pseudo-labels.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D.",
            "score": 12,
            "issue_id": 4516,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "25172262b153bc59",
            "authors": [
                "Jianyun Xu",
                "Song Wang",
                "Ziqian Ni",
                "Chunyong Hu",
                "Sheng Yang",
                "Jianke Zhu",
                "Qiang Li"
            ],
            "affiliations": [
                "Unmanned Vehicle Dept., CaiNiao Inc., Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21547.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "SAM4D: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹",
                    "desc": "SAM4D - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (UMPE) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ° Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ (MCMA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. SAM4D Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Segmentation in Autonomous Driving with SAM4D",
                    "desc": "SAM4D is a cutting-edge model that integrates multiple data types, specifically camera and LiDAR, to improve segmentation tasks in autonomous driving. It uses Unified Multi-modal Positional Encoding to align features from both data sources in a shared 3D space, facilitating effective interaction between them. The model also incorporates Motion-aware Cross-modal Memory Attention to maintain consistency over time and retrieve features from long sequences, which is crucial for dynamic driving environments. To streamline the data labeling process, SAM4D employs an automated engine that generates high-quality pseudo-labels quickly, significantly reducing the need for manual annotation."
                },
                "zh": {
                    "title": "SAM4Dï¼šè‡ªåŠ¨é©¾é©¶åˆ†å‰²çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹",
                    "desc": "SAM4Dæ˜¯ä¸€ç§å¤šæ¨¡æ€å’Œæ—¶é—´åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶ä¸­çš„åˆ†å‰²ä»»åŠ¡è®¾è®¡ã€‚å®ƒä½¿ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼ˆUMPEï¼‰æ¥å¯¹é½ç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„ç‰¹å¾ï¼Œä»è€Œå®ç°æ— ç¼çš„è·¨æ¨¡æ€æç¤ºå’Œäº¤äº’ã€‚æ­¤å¤–ï¼Œè¿åŠ¨æ„ŸçŸ¥è·¨æ¨¡æ€è®°å¿†æ³¨æ„åŠ›ï¼ˆMCMAï¼‰åˆ©ç”¨è‡ªæˆ‘è¿åŠ¨è¡¥å¿æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œé•¿æ—¶é—´ç‰¹å¾æ£€ç´¢ï¼Œç¡®ä¿åœ¨åŠ¨æ€å˜åŒ–çš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­è¿›è¡Œç¨³å¥çš„åˆ†å‰²ã€‚ä¸ºäº†é¿å…æ ‡æ³¨ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€è‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆä¸ç›¸æœºå’Œæ¿€å…‰é›·è¾¾å¯¹é½çš„ä¼ªæ ‡ç­¾ï¼Œé€Ÿåº¦è¿œè¶…äººå·¥æ ‡æ³¨ï¼ŒåŒæ—¶ä¿æŒç‚¹äº‘è¡¨ç¤ºä¸­çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21103",
            "title": "Learning to Skip the Middle Layers of Transformers",
            "url": "https://huggingface.co/papers/2506.21103",
            "abstract": "A novel conditional computation architecture for Transformers dynamically skips middle layers based on input and a gating mechanism, but does not outperform dense baselines in reducing computational cost or improving validation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle.",
            "score": 10,
            "issue_id": 4524,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "3c7e7fa3beaf5d4d",
            "authors": [
                "Tim Lawson",
                "Laurence Aitchison"
            ],
            "affiliations": [
                "School of Engineering Mathematics and Technology University of Bristol Bristol, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21103.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "â­ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ»Ğ¸ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ insights Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Layer Skipping in Transformers: Innovation Meets Limitation",
                    "desc": "This paper introduces a new architecture for Transformers that uses conditional computation to skip certain middle layers based on the input data. A gating mechanism is employed to decide which layers to bypass, aiming to enhance efficiency without compromising performance. Despite the innovative approach, the results show that this method does not outperform traditional dense models in terms of reducing computational costs or improving validation accuracy. The findings suggest that while the architecture is theoretically sound, it may not provide practical benefits at the scales tested."
                },
                "zh": {
                    "title": "åŠ¨æ€è·³å±‚ï¼Œæå‡Transformeræ•ˆç‡çš„æ¢ç´¢",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡ä»¶è®¡ç®—æ¶æ„ï¼Œç”¨äºTransformeræ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åŠ¨æ€è·³è¿‡ä¸­é—´å±‚ã€‚é€šè¿‡å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼Œå†³å®šæ˜¯å¦è·³è¿‡ä¸€æ®µå¯¹ç§°çš„ä¸­å¤®å—ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡è¯¥æ–¹æ³•æ—¨åœ¨å‡å°‘è®¡ç®—éœ€æ±‚å¹¶ä¿ƒè¿›å¤šå±‚æ¬¡è¡¨ç¤ºçš„å½¢æˆï¼Œä½†åœ¨å®éªŒä¸­æœªèƒ½åœ¨éªŒè¯æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—æ˜¾è‘—æ”¹å–„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¯†é›†åŸºçº¿åœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œæé«˜éªŒè¯æ€§èƒ½æ–¹é¢ä»ç„¶è¡¨ç°æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20936",
            "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for\n  Realistic Articulated Object Modeling",
            "url": "https://huggingface.co/papers/2506.20936",
            "abstract": "A physics-based skinning and rigging framework called PhysRig uses volumetric representation and continuum mechanics for more realistic and physically plausible animations.  \t\t\t\t\tAI-generated summary \t\t\t\t Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: a differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh), which is simulated as a deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct a comprehensive synthetic dataset using meshes from Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling.",
            "score": 8,
            "issue_id": 4530,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "a4cf66e609fddfae",
            "authors": [
                "Hao Zhang",
                "Haolan Xu",
                "Chun Feng",
                "Varun Jampani",
                "Narendra Ahuja"
            ],
            "affiliations": [
                "Stability AI",
                "University of Illinois Urbana Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20936.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "PhysRig: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "PhysRig - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ ÑĞ¿Ğ»Ğ¾ÑˆĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Linear Blend Skinning (LBS), PhysRig Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¸ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼ÑĞ³ĞºĞ¸Ğµ Ñ‚ĞºĞ°Ğ½Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‚ĞºĞ¸. PhysRig Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞµĞ»ĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "PhysRig: Realistic Animation through Physics-Based Skinning",
                    "desc": "The paper introduces PhysRig, a novel skinning and rigging framework that utilizes physics-based principles for more realistic animations. Unlike traditional Linear Blend Skinning (LBS), which can cause unnatural deformations, PhysRig employs a volumetric representation and continuum mechanics to simulate soft-body dynamics. This approach allows for better modeling of elastic materials and complex shapes, such as soft tissues and flexible appendages. The framework is evaluated using a synthetic dataset and shows superior performance in generating physically plausible animations compared to existing methods."
                },
                "zh": {
                    "title": "PhysRigï¼šæ›´çœŸå®çš„åŠ¨ç”»çš®è‚¤ä¸ç»‘å®šæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„çš®è‚¤å’Œç»‘å®šæ¡†æ¶ï¼Œç§°ä¸ºPhysRigï¼Œæ—¨åœ¨å®ç°æ›´çœŸå®å’Œç‰©ç†ä¸Šåˆç†çš„åŠ¨ç”»æ•ˆæœã€‚ä¼ ç»Ÿçš„çº¿æ€§æ··åˆçš®è‚¤ï¼ˆLBSï¼‰æ–¹æ³•è™½ç„¶ç®€å•ï¼Œä½†ä¼šå¯¼è‡´ä½“ç§¯æŸå¤±å’Œä¸è‡ªç„¶çš„å˜å½¢ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿè½¯ç»„ç»‡å’Œçµæ´»çš„é™„è‚¢ã€‚PhysRigé€šè¿‡å°†åˆšæ€§éª¨æ¶åµŒå…¥ä½“ç§¯è¡¨ç¤ºä¸­ï¼Œå¹¶åˆ©ç”¨è¿ç»­ä»‹è´¨åŠ›å­¦è¿›è¡Œæ¨¡æ‹Ÿï¼Œå…‹æœäº†è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPhysRigåœ¨ç”ŸæˆçœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„LBSæ–¹æ³•ï¼Œä¸”åœ¨å§¿æ€è½¬ç§»ä»»åŠ¡ä¸­å±•ç°äº†è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21552",
            "title": "Whole-Body Conditioned Egocentric Video Prediction",
            "url": "https://huggingface.co/papers/2506.21552",
            "abstract": "A model trained on real-world egocentric video and body pose predicts video from human actions using an auto-regressive conditional diffusion transformer, evaluated with a hierarchical protocol of tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.",
            "score": 6,
            "issue_id": 4519,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "88d888f2aa383886",
            "authors": [
                "Yutong Bai",
                "Danny Tran",
                "Amir Bar",
                "Yann LeCun",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "affiliations": [
                "FAIR, Meta",
                "New York University",
                "UC Berkeley (BAIR)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21552.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#agents",
                    "#games",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PEVA, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Nymeria Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞ»Ğ°. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ñ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Predicting Video from Human Actions in First-Person View",
                    "desc": "This paper presents a model called PEVA, which predicts ego-centric video based on human actions and body poses. It utilizes an auto-regressive conditional diffusion transformer to learn how human actions influence the environment from a first-person perspective. The model is trained on a large dataset, Nymeria, which includes real-world video and body pose data. A hierarchical evaluation protocol is introduced to assess the model's performance on various tasks, highlighting its ability to understand and simulate complex interactions in real-world scenarios."
                },
                "zh": {
                    "title": "ä»äººç±»åŠ¨ä½œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œæ—¨åœ¨ä»äººç±»åŠ¨ä½œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç›¸å¯¹3Dèº«ä½“å§¿æ€ä½œä¸ºæ¡ä»¶ï¼Œç»“åˆè¿åŠ¨å­¦è½¨è¿¹ï¼Œæ¨¡æ‹Ÿäººç±»åŠ¨ä½œå¦‚ä½•å½±å“ç¯å¢ƒã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåä¸ºNymeriaçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨ã€‚é€šè¿‡è®¾è®¡åˆ†å±‚è¯„ä¼°åè®®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå…¨é¢åˆ†ææ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é¢„æµ‹å’Œæ§åˆ¶èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16655",
            "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
            "url": "https://huggingface.co/papers/2506.16655",
            "abstract": "A preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce Arch-Router, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: https://huggingface.co/katanemo/Arch-Router-1.5B.",
            "score": 6,
            "issue_id": 4519,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "09391602d5fce0b2",
            "authors": [
                "Co Tran",
                "Salman Paracha",
                "Adil Hafeez",
                "Shuguang Chen"
            ],
            "affiliations": [
                "Katanemo Labs, Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16655.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ˜Ğ˜ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Arch-Router Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 1,5 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Aligning Queries with User Preferences for Optimal Model Routing",
                    "desc": "This paper presents a new routing framework called Arch-Router, which uses a compact 1.5 billion parameter model to effectively match user queries with specific domains and action types. Unlike traditional models that rely on fixed benchmarks, this framework aligns with human preferences by incorporating subjective evaluation criteria into its routing decisions. Arch-Router allows for the easy addition of new models without the need for retraining, enhancing flexibility in model selection. Experiments show that this approach outperforms existing proprietary models, achieving state-of-the-art results in aligning queries with user preferences."
                },
                "zh": {
                    "title": "åå¥½å¯¹é½çš„æ™ºèƒ½è·¯ç”±æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åå¥½å¯¹é½çš„è·¯ç”±æ¡†æ¶ï¼Œä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„1.5Bæ¨¡å‹æœ‰æ•ˆåœ°å°†æŸ¥è¯¢ä¸ç”¨æˆ·å®šä¹‰çš„é¢†åŸŸå’ŒåŠ¨ä½œç±»å‹åŒ¹é…ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æŸ¥è¯¢ä¸ç”¨æˆ·çš„åå¥½ç›¸ç»“åˆï¼Œå…‹æœäº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è·¯ç”±æ–¹æ³•åœ¨ä¸»è§‚è¯„ä¼°æ ‡å‡†ä¸Šçš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥çš„Arch-Routeræ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å°†æŸ¥è¯¢æ˜ å°„åˆ°é¢†åŸŸ-åŠ¨ä½œåå¥½ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸äººç±»åå¥½çš„åŒ¹é…ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¶…è¶Šäº†è®¸å¤šä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20703",
            "title": "Generative Blocks World: Moving Things Around in Pictures",
            "url": "https://huggingface.co/papers/2506.20703",
            "abstract": "A generative method that edits 3D scenes using convex primitives and regenerates images with enhanced texture consistency and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.",
            "score": 5,
            "issue_id": 4535,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "5f5979ba6e2c9a34",
            "authors": [
                "Vaibhav Vavilala",
                "Seemandhar Jain",
                "Rahul Vasanth",
                "D. A. Forsyth",
                "Anand Bhattad"
            ],
            "affiliations": [
                "Toyota Technological Institute at Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20703.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµÑ‘ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ 3D-Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞŸĞ¾ÑĞ»Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ flow-based Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Edit 3D Scenes with Precision and Visual Fidelity!",
                    "desc": "This paper presents a novel generative method called Generative Blocks World, which allows users to edit 3D scenes using simple geometric shapes known as convex primitives. The method enables flexible scene representation, where the same scene can be constructed with varying numbers of these primitives, facilitating both large structural changes and fine detail adjustments. After editing the scene geometry, a flow-based image generation technique is employed, which utilizes depth information and a texture hint to enhance visual quality. The results show that this approach significantly improves texture consistency and visual fidelity compared to existing methods, making it easier to manipulate and generate realistic 3D images."
                },
                "zh": {
                    "title": "é€šè¿‡å‡¸ä½“ç´ ç¼–è¾‘3Dåœºæ™¯ï¼Œæå‡å›¾åƒè´¨é‡ä¸ä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å‡¸ä½“ç´ ç¼–è¾‘3Dåœºæ™¯ï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¢å¼ºçº¹ç†ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿçš„å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åœºæ™¯è¡¨ç¤ºä¸ºå‡¸3DåŸè¯­çš„ç»„åˆï¼Œå…è®¸ç¼–è¾‘è€…ç§»åŠ¨æ•´ä¸ªç»“æ„æˆ–å°ç»†èŠ‚ã€‚ç¼–è¾‘åœºæ™¯å‡ ä½•åï¼Œä½¿ç”¨åŸºäºæµçš„æ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œè¯¥æ–¹æ³•ä¾èµ–äºæ·±åº¦ä¿¡æ¯å’Œçº¹ç†æç¤ºã€‚æˆ‘ä»¬çš„çº¹ç†æç¤ºè€ƒè™‘äº†ä¿®æ”¹åçš„3DåŸè¯­ï¼Œè¶…è¶Šäº†ç°æœ‰å…³é”®å€¼ç¼“å­˜æŠ€æœ¯æä¾›çš„çº¹ç†ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20430",
            "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
            "url": "https://huggingface.co/papers/2506.20430",
            "abstract": "DeepRare, a large language model-based system, provides accurate rare disease diagnoses using heterogeneous clinical inputs and outperforms other diagnostic methods across various datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.",
            "score": 5,
            "issue_id": 4517,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "f12b8efd117ae9ab",
            "authors": [
                "Weike Zhao",
                "Chaoyi Wu",
                "Yanjie Fan",
                "Xiaoman Zhang",
                "Pengcheng Qiu",
                "Yuze Sun",
                "Xiao Zhou",
                "Yanfeng Wang",
                "Ya Zhang",
                "Yongguo Yu",
                "Kun Sun",
                "Weidi Xie"
            ],
            "affiliations": [
                "Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "Shanghai Jiao Tong University, Shanghai, China",
                "Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20430.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#science",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "DeepRare: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DeepRare - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ…Ğ¾ÑÑ‚Ğ° Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. DeepRare Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 100% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ 1013 Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Rare Disease Diagnosis with DeepRare",
                    "desc": "DeepRare is a novel system that utilizes a large language model to diagnose rare diseases by analyzing diverse clinical data. It generates ranked hypotheses for potential diagnoses, providing clear reasoning linked to medical evidence. The system is modular, featuring a long-term memory and specialized agents that integrate over 40 tools and current medical knowledge. DeepRare outperforms traditional diagnostic methods, achieving high accuracy and recall rates across multiple datasets, making it a significant advancement in rare disease diagnosis."
                },
                "zh": {
                    "title": "DeepRareï¼šç²¾å‡†è¯Šæ–­ç½•è§ç–¾ç—…çš„æ™ºèƒ½ç³»ç»Ÿ",
                    "desc": "DeepRareæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤šç§ä¸´åºŠè¾“å…¥æä¾›å‡†ç¡®çš„ç½•è§ç–¾ç—…è¯Šæ–­ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ’åçš„è¯Šæ–­å‡è®¾ï¼Œå¹¶æä¾›é€æ˜çš„æ¨ç†é“¾ï¼Œç¡®ä¿æ¯ä¸€æ­¥åˆ†æéƒ½ä¸å¯éªŒè¯çš„åŒ»å­¦è¯æ®ç›¸è¿æ¥ã€‚DeepRareçš„è®¾è®¡æ¨¡å—åŒ–ä¸”å¯æ‰©å±•ï¼Œé›†æˆäº†è¶…è¿‡40ç§ä¸“ä¸šå·¥å…·å’Œæœ€æ–°çš„åŒ»å­¦çŸ¥è¯†æ¥æºï¼Œç¡®ä¿è·å–æœ€æ–°çš„ä¸´åºŠä¿¡æ¯ã€‚ç»è¿‡è¯„ä¼°ï¼ŒDeepRareåœ¨2919ç§ç–¾ç—…ä¸­è¡¨ç°å‡ºè‰²ï¼Œ1013ç§ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®ç‡è¾¾åˆ°100%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21272",
            "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character",
            "url": "https://huggingface.co/papers/2506.21272",
            "abstract": "FairyGen generates story-driven cartoon videos from a single drawing by disentangling character modeling and background styling, employing MLLM for storyboards, style propagation for consistency, and MMDiT-based diffusion models for motion.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen",
            "score": 4,
            "issue_id": 4525,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "912ef8f5a86d8a2c",
            "authors": [
                "Jiayi Zheng",
                "Xiaodong Cun"
            ],
            "affiliations": [
                "GVC Lab, Great Bay University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21272.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#video",
                    "#diffusion",
                    "#story_generation"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞÑ‚ Ğ´ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ° Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñƒ: FairyGen Ğ¾Ğ¶Ğ¸Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "FairyGen - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµÑ‚ÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ¸ÑÑƒĞ½ĞºÑƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞºĞ°Ğ´Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MMDiT Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. FairyGen Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Transforming Drawings into Animated Stories with FairyGen",
                    "desc": "FairyGen is an innovative system that creates cartoon videos from a single drawing while maintaining the original artistic style. It separates character modeling from background styling, allowing for more coherent storytelling and visual consistency. The system uses a multi-level language model (MLLM) to generate detailed storyboards and employs a style propagation adapter to ensure that the character's style is preserved in the background. Additionally, it utilizes a two-stage motion customization process to create realistic animations, resulting in engaging and personalized video content."
                },
                "zh": {
                    "title": "ä»ä¸€å¹…ç”»ç”Ÿæˆæ•…äº‹åŠ¨ç”»çš„é­”æ³•",
                    "desc": "FairyGen æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆæ•…äº‹é©±åŠ¨å¡é€šè§†é¢‘çš„ç³»ç»Ÿï¼Œåªéœ€ä¸€å¹…å„¿ç«¥ç”»å³å¯ï¼ŒåŒæ—¶ä¿ç•™å…¶ç‹¬ç‰¹çš„è‰ºæœ¯é£æ ¼ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†è§’è‰²å»ºæ¨¡ä¸èƒŒæ™¯é£æ ¼ç”Ÿæˆåˆ†ç¦»ï¼Œç»“åˆ MLLM ç”Ÿæˆç»“æ„åŒ–æ•…äº‹æ¿ï¼Œç¡®ä¿è§†è§‰ä¸€è‡´æ€§ã€‚å®ƒè¿˜ä½¿ç”¨ MMDiT åŸºäºæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨åºåˆ—ï¼Œä»è€Œå®ç°åŠ¨ç”»æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒFairyGen èƒ½å¤Ÿç”Ÿæˆé£æ ¼ä¸€è‡´ã€å™äº‹ç»“æ„æ¸…æ™°çš„è‡ªç„¶è¿åŠ¨åŠ¨ç”»ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ªæ€§åŒ–æ•…äº‹åŠ¨ç”»ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21263",
            "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
            "url": "https://huggingface.co/papers/2506.21263",
            "abstract": "DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.",
            "score": 3,
            "issue_id": 4527,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "5ee2d59ef586a49c",
            "authors": [
                "Ji Qi",
                "WenPeng Zhu",
                "Li Li",
                "Ming Wu",
                "YingJun Wu",
                "Wu He",
                "Xun Gao",
                "Jason Zeng",
                "Michael Heinrich"
            ],
            "affiliations": [
                "China Mobile(Suzhou) Software Technology, JiangSu",
                "Zero Gravity Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21263.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "DiLoCoX: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DiLoCoX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². DiLoCoX Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞµÑ‚ÑÑ… ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ 1 Ğ“Ğ±Ğ¸Ñ‚/Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 357-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ AllReduce."
                },
                "en": {
                    "title": "Revolutionizing Large-Scale Model Training on Slow Networks",
                    "desc": "DiLoCoX is a decentralized training framework designed to improve the training of large-scale models, especially those with over 100 billion parameters, over slow networks. It employs techniques like pipeline parallelism and a dual optimizer policy to enhance communication efficiency and speed. The framework also introduces an adaptive gradient compression scheme, which helps in reducing the amount of data that needs to be communicated during training. Empirical results show that DiLoCoX can achieve a remarkable 357x speedup in distributed training compared to traditional methods, while still ensuring effective model convergence."
                },
                "zh": {
                    "title": "å»ä¸­å¿ƒåŒ–é›†ç¾¤è®­ç»ƒçš„é€Ÿåº¦é©å‘½",
                    "desc": "DiLoCoXæ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„é›†ç¾¤è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹åœ¨æ…¢é€Ÿç½‘ç»œä¸Šçš„è®­ç»ƒæ•ˆç‡ã€‚å®ƒç»“åˆäº†ç®¡é“å¹¶è¡Œã€åŒä¼˜åŒ–å™¨ç­–ç•¥å’Œæ¢¯åº¦å‹ç¼©ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒé€Ÿåº¦å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼ŒDiLoCoXèƒ½å¤Ÿåœ¨1Gbpsç½‘ç»œä¸ŠæˆåŠŸé¢„è®­ç»ƒè¶…è¿‡107äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„AllReduceæ–¹æ³•ç›¸æ¯”ï¼ŒDiLoCoXåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å®ç°äº†357å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ¨¡å‹æ”¶æ•›æ€§å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17533",
            "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2506.17533",
            "abstract": "A novel reward modeling framework DuaShepherd integrates correctness and potential signals into a unified multi-head architecture to enhance LLMs' mathematical reasoning capabilities and achieve state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.",
            "score": 2,
            "issue_id": 4533,
            "pub_date": "2025-06-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ½Ñ",
                "en": "June 21",
                "zh": "6æœˆ21æ—¥"
            },
            "hash": "4beafb969a7354be",
            "authors": [
                "Yuanhao Wu",
                "Juntong Song",
                "Hanning Zhang",
                "Tong Zhang",
                "Cheng Niu"
            ],
            "affiliations": [
                "NewsBreak",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17533.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "DuaShepherd - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² - ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» - Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ±Ğ¾Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ‚Ğ¸Ğ¿Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs' Math Skills with DuaShepherd",
                    "desc": "The paper introduces DuaShepherd, a new framework for reward modeling that enhances the mathematical reasoning abilities of Large Language Models (LLMs). It combines two types of reward signals: correctness, which focuses on identifying errors in reasoning steps, and potential, which assesses the likelihood of arriving at the correct answer. The authors created an automated system to build a large dataset that incorporates both signals and employed a multi-head architecture to train the models simultaneously. This approach leads to improved performance on various benchmarks, demonstrating that using both signals together yields better results than using either one alone."
                },
                "zh": {
                    "title": "DuaShepherdï¼šæå‡æ•°å­¦æ¨ç†çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶DuaShepherdï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ­£ç¡®æ€§å’Œæ½œåŠ›ä¸¤ç§äº’è¡¥çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œç”¨äºæ„å»ºåŒ…å«è¿™ä¸¤ç§ä¿¡å·çš„å¤§è§„æ¨¡å¥–åŠ±å»ºæ¨¡æ•°æ®é›†ã€‚é€šè¿‡åœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­è®­ç»ƒè¿™ä¸¤ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15196",
            "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
            "url": "https://huggingface.co/papers/2506.15196",
            "abstract": "HeurAgenix, a two-stage hyper-heuristic framework using large language models, evolves and selects heuristics dynamically for combinatorial optimization problems, achieving performance on par with specialized solvers.  \t\t\t\t\tAI-generated summary \t\t\t\t Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce HeurAgenix, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.",
            "score": 2,
            "issue_id": 4523,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "464747c8cdf8780d",
            "authors": [
                "Xianliang Yang",
                "Ling Zhang",
                "Haolong Qian",
                "Lei Song",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research Asia, Beijing, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15196.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "HeurAgenix - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. HeurAgenix Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Heuristic Evolution and Selection with LLMs",
                    "desc": "HeurAgenix is a novel two-stage hyper-heuristic framework that utilizes large language models (LLMs) to enhance the solving of combinatorial optimization (CO) problems. The framework first evolves heuristics by comparing initial solutions with better ones, extracting effective strategies for improvement. In the second stage, it dynamically selects the most suitable heuristic for each problem state, leveraging the LLM's ability to perceive and adapt. This approach not only improves performance compared to traditional methods but also rivals specialized solvers, demonstrating the potential of LLMs in optimization tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€æ¼”åŒ–ä¸é€‰æ‹©å¯å‘å¼ç®—æ³•çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "HeurAgenix æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¤é˜¶æ®µè¶…å¯å‘å¼æ¡†æ¶ï¼Œæ—¨åœ¨åŠ¨æ€æ¼”åŒ–å’Œé€‰æ‹©å¯å‘å¼ç®—æ³•ä»¥è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡æ¯”è¾ƒç§å­å¯å‘å¼è§£ä¸é«˜è´¨é‡è§£ï¼Œæå–å¯é‡ç”¨çš„æ¼”åŒ–ç­–ç•¥ã€‚ç„¶åï¼Œåœ¨è§£å†³é—®é¢˜æ—¶ï¼ŒHeurAgenix æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€æœ‰å‰æ™¯çš„å¯å‘å¼ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHeurAgenix çš„æ€§èƒ½ä¸ä¸“é—¨çš„æ±‚è§£å™¨ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†ç°æœ‰çš„åŸºäº LLM çš„è¶…å¯å‘å¼ç®—æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18729",
            "title": "MuseControlLite: Multifunctional Music Generation with Lightweight\n  Conditioners",
            "url": "https://huggingface.co/papers/2506.18729",
            "abstract": "Rotary positional embeddings enhance time-varying control in text-to-music generation models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainble parameters. Source code, model checkpoints, and demo examples are available at: https://musecontrollite.github.io/web/.",
            "score": 1,
            "issue_id": 4524,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "b1d008a79f59af7d",
            "authors": [
                "Fang-Duo Tsai",
                "Shih-Lun Wu",
                "Weijaw Lee",
                "Sheng-Ping Yang",
                "Bo-Rui Chen",
                "Hao-Chung Cheng",
                "Yi-Hsuan Yang"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology, Cambridge, MA, United States",
                "National Taiwan University, Taipei, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18729.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#diffusion",
                    "#training",
                    "#games",
                    "#data"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ›ĞµĞ³ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MuseControlLite - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ»Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ 56.6% Ğ´Ğ¾ 61.1%, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 6.75 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MuseControlLite Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ MusicGen-Large Ğ¸ Stable Audio Open ControlNet Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Enhancing Music Generation with Efficient Positional Control",
                    "desc": "This paper introduces MuseControlLite, a streamlined approach for enhancing text-to-music generation models by incorporating rotary positional embeddings. These embeddings are crucial for managing time-varying musical attributes, allowing for more precise control over generated music. The study demonstrates that adding these embeddings to cross-attention layers improves control accuracy significantly while reducing the number of trainable parameters needed. Overall, MuseControlLite offers a cost-effective solution for fine-tuning music generation models, achieving better performance with fewer resources."
                },
                "zh": {
                    "title": "æ—‹è½¬ä½ç½®åµŒå…¥æå‡æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆçš„æ§åˆ¶èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMuseControlLiteçš„è½»é‡çº§æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡æ—¶é—´å˜åŒ–çš„éŸ³ä¹å±æ€§å’Œå‚è€ƒéŸ³é¢‘ä¿¡å·æ¥ç²¾ç»†è°ƒæ•´æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆæ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œæ—‹è½¬ä½ç½®åµŒå…¥åœ¨æ–‡æœ¬æ¡ä»¶çš„è°ƒèŠ‚å™¨ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å½“æ¡ä»¶ä¸æ—¶é—´ç›¸å…³æ—¶ã€‚é€šè¿‡åœ¨è§£è€¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ä¸­æ·»åŠ æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œæ§åˆ¶ç²¾åº¦ä»56.6%æé«˜åˆ°61.1%ï¼Œä¸”æ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°æ¯”æœ€å…ˆè¿›çš„å¾®è°ƒæœºåˆ¶å°‘6.75å€ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤šç§éŸ³ä¹å±æ€§æ§åˆ¶å½¢å¼ï¼Œå±•ç¤ºäº†åœ¨è¾ƒä½çš„å¾®è°ƒæˆæœ¬ä¸‹ï¼ŒMuseControlLiteåœ¨å¯æ§æ€§æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-26.html",
    "link_next": "2025-06-30.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "26.06",
        "en": "06/26",
        "zh": "6æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.06",
        "en": "06/30",
        "zh": "6æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 5,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 7,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}