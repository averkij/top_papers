{
    "date": {
        "ru": "20 ноября",
        "en": "November 20",
        "zh": "11月20日"
    },
    "time_utc": "2024-11-20 05:10",
    "weekday": 2,
    "issue_id": 676,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.12734",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "url": "https://huggingface.co/papers/2411.12734",
            "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
            "score": 0,
            "issue_id": 676,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "f75a2283a0a8a06f",
            "authors": [
                "Yunchao Yao",
                "Uksang Yoo",
                "Jean Oh",
                "Christopher G. Atkeson",
                "Jeffrey Ichnowski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.12734.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мягкая роботизированная рука осваивает динамичные манипуляции",
                    "desc": "Статья представляет систему SWIFT для обучения динамическим задачам с использованием мягкой роботизированной руки. В отличие от предыдущих подходов, система учится вращать ручку методом проб и ошибок, используя только данные реального мира без предварительных знаний о физических свойствах объекта. После 130 пробных действий SWIFT достигает 100% успеха при вращении трех ручек с разным весом и распределением массы. Система также демонстрирует обобщающую способность на предметах другой формы и веса."
                },
                "en": {
                    "title": "SWIFT: Mastering Dynamic Manipulation with Soft Robotics",
                    "desc": "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."
                },
                "zh": {
                    "title": "软机器人动态操作的新突破",
                    "desc": "本研究提出了一种名为SWIFT的系统，用于学习动态任务，特别是在软机器人手中进行快速的物体操作。与以往依赖于模拟和精确物体模型的方法不同，SWIFT通过真实世界的数据进行试错学习，能够在没有物体物理属性先验知识的情况下，成功地旋转笔。经过130次采样操作，SWIFT在三种不同重量和分布的笔上实现了100%的成功率，展示了其在物体属性变化下的通用性和鲁棒性。该系统还能够推广到其他形状和重量的物体，如刷子和螺丝刀，分别实现了10/10和5/10的成功率，显示了软机器人在动态任务中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12240",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
            "url": "https://huggingface.co/papers/2411.12240",
            "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
            "score": 0,
            "issue_id": 675,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "aee934b73b340b71",
            "authors": [
                "S. Tamang",
                "D. J. Bora"
            ],
            "affiliations": [
                "Department of IT The Assam Kaziranga University Jorhat, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12240.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "SUTRA: Лидер токенизации для индийских языков в больших языковых моделях",
                    "desc": "В статье представлен анализ токенизаторов, используемых в 12 больших языковых моделях (LLM) для 22 официальных языков Индии. Исследователи использовали метрику нормализованной длины последовательности (NSL) для оценки эффективности токенизации. Результаты показали, что токенизатор SUTRA превзошел другие модели, включая специализированные для индийских языков, показав лучшие результаты для 14 языков. Исследование подчеркивает важность разработки целевых стратегий токенизации для многоязычных моделей и моделей, ориентированных на индийские языки."
                },
                "en": {
                    "title": "Optimizing Tokenization for Multilingual Mastery",
                    "desc": "This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings."
                },
                "zh": {
                    "title": "优化多语言模型的分词策略",
                    "desc": "本论文探讨了基于变换器架构的大型语言模型（LLMs）中的分词技术，特别是在印度官方语言中的应用。我们对12种LLMs使用的分词器进行了全面评估，重点比较了它们的分词效率。研究结果显示，SUTRA分词器在14种语言中表现优于其他模型，尤其是在处理印度语言方面。该研究强调了为多语言和以印度语言为中心的模型开发针对性分词策略的重要性，以提高语言覆盖率和模型效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11925",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2411.11925",
            "abstract": "Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD",
            "score": 0,
            "issue_id": 674,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "17049106ecc06192",
            "authors": [
                "Zili Wang",
                "Robert Zhang",
                "Kun Ding",
                "Qi Yang",
                "Fei Li",
                "Shiming Xiang"
            ],
            "affiliations": [
                "China Tower Corporation Limited",
                "Institute of Automation, Chinese Academy of Sciences, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Ускорение генерации изображений: от дискретного к непрерывному",
                    "desc": "Статья представляет новый метод ускорения генерации изображений с помощью авторегрессионных моделей с непрерывными значениями. Авторы адаптируют алгоритм спекулятивного декодирования, ранее применявшийся для ускорения больших языковых моделей, к непрерывному пространству. Они вводят специальный критерий принятия для диффузионных распределений и предлагают методы выравнивания траектории шумоподавления и предварительного заполнения токенов. Экспериментальные результаты показывают 2.33-кратное ускорение без ухудшения качества выходных данных."
                },
                "en": {
                    "title": "Speeding Up Image Generation with Continuous Speculative Decoding",
                    "desc": "This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images."
                },
                "zh": {
                    "title": "加速连续值自回归图像生成的推测解码",
                    "desc": "本文提出了一种针对连续值自回归图像生成模型的推测解码算法，旨在提高生成速度。通过分析输出分布的内在特性，建立了适合扩散分布的接受标准。为了解决推测解码输出分布的不一致性，本文引入了去噪轨迹对齐和令牌预填充方法。实验结果表明，该方法在保持输出分布的同时，实现了2.33倍的速度提升。"
                }
            }
        }
    ],
    "link_prev": "2024-11-19.html",
    "link_next": "2024-11-21.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11月19日"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了多模态大语言模型（MLLMs）的兴起及其在日常生活中的潜力。手机是部署MLLMs的最佳平台，但由于内存和计算能力有限，难以实现流畅的实时处理。文章提出了BlueLM-V-3B，一种专为移动平台设计的算法和系统。它通过重新设计动态分辨率方案和硬件感知部署优化，实现了小尺寸、快速度和强性能。\n\nTranslation:\nThis article discusses the rise of multimodal large language models (MLLMs) and their potential in daily life. Mobile phones are the best platform for deploying MLLMs, but due to memory and computational limitations, real-time processing is challenging. The article presents BlueLM-V-3B, an algorithm and system designed for mobile platforms. It achieves small size, fast speed, and strong performance through dynamic resolution scheme redesign and hardware-aware deployment optimization.",
        "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhè piān wén zhāng jiè shào le duō mó tài dà yǔ yán mó xíng (MLLMs) de xīng qǐ jí qí zài rì cháng shēng huó zhōng de qián lì. Shǒu jī shì bù shǔ MLLMs de zuì jiā píng tài, dàn yóu yú nèi cùn hé suàn jì néng lì yǒu xiàn, nán yǐ shí xiàn liú chàng de shí shí chǔ lǐ. Wén zhāng tí chū le BlueLM-V-3B, yī zhǒng zhuān wèi yí dòng píng tài shè jì de suàn fǎ hé xì tǒng. Tā tōng guò chóng xīn shè jì dòng tài fēn bié lǜ fāng ān hé yìng jiàn gǎn zhī bù shǔ yōu huà, shí xiàn le xiǎo chǐ cù, kuài sù dù hé qiáng xìng néng.\n\nTranslation:\nThis article discusses the rise of multimodal large language models (MLLMs) and their potential in daily life. Mobile phones are the best platform for deploying MLLMs, but due to memory and computational limitations, real-time processing is challenging. The article presents BlueLM-V-3B, an algorithm and system designed for mobile platforms. It achieves small size, fast speed, and strong performance through dynamic resolution scheme redesign and hardware-aware deployment optimization.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language models'},\n{'word': '兴起', 'pinyin': 'xīng qǐ', 'trans': 'rise'},\n{'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'},\n{'word': '部署', 'pinyin': 'bù shǔ', 'trans': 'deploy'},\n{'word': '平台', 'pinyin': 'píng tái', 'trans': 'platform'},\n{'word': '内存', 'pinyin': 'nèi cún', 'trans': 'memory'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computational'},\n{'word': '流畅', 'pinyin': 'liú chàng', 'trans': 'smooth'},\n{'word': '实时', 'pinyin': 'shí shí', 'trans': 'real-time'},\n{'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'processing'},\n{'word': '算法', 'pinyin': 'suàn fǎ', 'trans': 'algorithm'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '分辨率', 'pinyin': 'fēn biàn lǜ', 'trans': 'resolution'},\n{'word': '方案', 'pinyin': 'fāng àn', 'trans': 'scheme'},\n{'word': '硬件', 'pinyin': 'yìng jiàn', 'trans': 'hardware'},\n{'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'aware'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}]",
        "trans": "Here's a refined translation of the text:\n\n\"This article explores the emergence of multimodal large language models (MLLMs) and their potential applications in everyday life. While mobile phones serve as an ideal platform for deploying MLLMs, their limited memory and computational power present challenges for smooth real-time processing. To address these issues, the article introduces BlueLM-V-3B, an algorithm and system tailored for mobile platforms. By employing a redesigned dynamic resolution scheme and hardware-aware deployment optimizations, BlueLM-V-3B achieves a compact size, high speed, and robust performance.\"\n\nChanges made:\n1. Used \"explores the emergence\" instead of \"discusses the rise\" for a more formal tone.\n2. Added \"applications in\" for better flow.\n3. Changed \"difficult to achieve\" to \"present challenges for\" to improve phrasing.\n4. Used \"to address these issues\" to create a stronger connection between sentences.\n5. Changed \"it achieves\" to \"By employing... BlueLM-V-3B achieves\" for clarity.\n6. Used \"robust performance\" instead of \"strong performance\" for more natural English.",
        "update_ts": "2024-11-19 09:11"
    }
}