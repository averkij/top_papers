{
    "date": {
        "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 22",
        "zh": "12æœˆ22æ—¥"
    },
    "time_utc": "2025-12-22 21:20",
    "weekday": 0,
    "issue_id": 187,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.16969",
            "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
            "url": "https://huggingface.co/papers/2512.16969",
            "abstract": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
            "score": 78,
            "issue_id": 169,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "67364211dc24cdbf",
            "authors": [
                "Wanghan Xu",
                "Yuhao Zhou",
                "Yifan Zhou",
                "Qinglong Cao",
                "Shuo Li",
                "Jia Bu",
                "Bo Liu",
                "Yixin Chen",
                "Xuming He",
                "Xiangyu Zhao",
                "Xiang Zhuang",
                "Fengxiang Wang",
                "Zhiwang Zhou",
                "Qiantai Feng",
                "Wenxuan Huang",
                "Jiaqi Wei",
                "Hao Wu",
                "Yuejin Yang",
                "Guangshuai Wang",
                "Sheng Xu",
                "Ziyan Huang",
                "Xinyao Liu",
                "Jiyao Liu",
                "Cheng Tang",
                "Wei Li",
                "Ying Chen",
                "Junzhi Ning",
                "Pengfei Jiang",
                "Chenglong Ma",
                "Ye Du",
                "Changkai Ji",
                "Huihui Xu",
                "Ming Hu",
                "Jiangbin Zheng",
                "Xin Chen",
                "Yucheng Wu",
                "Feifei Jiang",
                "Xi Chen",
                "Xiangru Tang",
                "Yuchen Fu",
                "Yingzhou Lu",
                "Yuanyuan Zhang",
                "Lihao Sun",
                "Chengbo Li",
                "Jinzhe Ma",
                "Wanhao Liu",
                "Yating Liu",
                "Kuo-Cheng Wu",
                "Shengdu Chai",
                "Yizhou Wang",
                "Ouwen Zhangjin",
                "Chen Tang",
                "Shufei Zhang",
                "Wenbo Cao",
                "Junjie Ren",
                "Taoyong Cui",
                "Zhouheng Yao",
                "Juntao Deng",
                "Yijie Sun",
                "Feng Liu",
                "Wangxu Wei",
                "Jingyi Xu",
                "Zhangrui Li",
                "Junchao Gong",
                "Zijie Guo",
                "Zhiyu Yao",
                "Zaoyu Chen",
                "Tianhao Peng",
                "Fangchen Yu",
                "Bo Zhang",
                "Dongzhan Zhou",
                "Shixiang Tang",
                "Jiaheng Liu",
                "Fenghua Ling",
                "Yan Lu",
                "Yuchen Ren",
                "Ben Fei",
                "Zhen Zhao",
                "Xinyu Gu",
                "Rui Su",
                "Xiao-Ming Wu",
                "Weikang Si",
                "Yang Liu",
                "Hao Chen",
                "Xiangchao Yan",
                "Xue Yang",
                "Junchi Yan",
                "Jiamin Wu",
                "Qihao Zheng",
                "Chenhui Li",
                "Zhiqiang Gao",
                "Hao Kong",
                "Junjun He",
                "Mao Su",
                "Tianfan Fu",
                "Peng Ye",
                "Chunfeng Song",
                "Nanqing Dong",
                "Yuqiang Li",
                "Huazhu Fu",
                "Siqi Sun",
                "Lijing Cheng",
                "Jintai Lin",
                "Wanli Ouyang",
                "Bowen Zhou",
                "Wenlong Zhang",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16969.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#science",
                    "#dataset",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒĞºÑƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ¼ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (SGI) â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ SGI-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ´ĞµĞ¹, ÑÑƒÑ…Ğ¸Ğµ Ğ¸ Ğ²Ğ»Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹: Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ, Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ´ĞµÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Test-Time Reinforcement Learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñƒ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering AI for Scientific Discovery with SGI Framework",
                    "desc": "This paper introduces a framework for Scientific General Intelligence (SGI), which is the ability of AI to autonomously conduct scientific research and reasoning. The authors define SGI using the Practical Inquiry Model (PIM) and evaluate it through SGI-Bench, a benchmark with over 1,000 tasks related to scientific inquiry. The evaluation reveals significant shortcomings in current AI models, such as low accuracy in deep research and challenges in generating feasible ideas. To address these issues, the paper proposes Test-Time Reinforcement Learning (TTRL) to enhance the novelty of hypotheses generated by AI during inference, aiming to improve the overall scientific capabilities of AI systems."
                },
                "zh": {
                    "title": "æ„å»ºç§‘å­¦é€šç”¨æ™ºèƒ½çš„åŸºç¡€æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç§‘å­¦é€šç”¨æ™ºèƒ½ï¼ˆSGIï¼‰çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡SGI-Benchè¿›è¡Œè¯„ä¼°ï¼ŒåŒæ—¶å¼•å…¥äº†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰æ¥æ”¹è¿›ç°æœ‰æ¨¡å‹çš„ç§‘å­¦èƒ½åŠ›ã€‚SGIçš„å®šä¹‰åŸºäºå®ç”¨æ¢ç©¶æ¨¡å‹ï¼ˆPIMï¼‰ï¼Œå¹¶é€šè¿‡å››ä¸ªä¸ç§‘å­¦å®¶å¯¹é½çš„ä»»åŠ¡è¿›è¡Œæ“ä½œåŒ–ï¼ŒåŒ…æ‹¬æ·±åº¦ç ”ç©¶ã€åˆ›æ„ç”Ÿæˆã€å¹²æ¹¿å®éªŒå’Œå®éªŒæ¨ç†ã€‚SGI-BenchåŒ…å«è¶…è¿‡1000ä¸ªè·¨å­¦ç§‘çš„æ ·æœ¬ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ç ”ç©¶ã€åˆ›æ„å¯è¡Œæ€§ã€å®éªŒæ‰§è¡Œç­‰æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡å¼•å…¥TTRLï¼Œæœ¬æ–‡æ—¨åœ¨æå‡å‡è®¾çš„æ–°é¢–æ€§ï¼Œä»è€Œä¸ºAIç³»ç»Ÿåœ¨ç§‘å­¦å‘ç°ä¸­æä¾›æ›´åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16793",
            "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
            "url": "https://huggingface.co/papers/2512.16793",
            "abstract": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
            "score": 63,
            "issue_id": 169,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "c5a529ace411306b",
            "authors": [
                "Xiaopeng Lin",
                "Shijie Lian",
                "Bin Yu",
                "Ruoqi Yang",
                "Changti Wu",
                "Yuzhuo Miao",
                "Yurun Jin",
                "Yukun Shi",
                "Cong Huang",
                "Bojun Cheng",
                "Kai Chen"
            ],
            "affiliations": [
                "DeepCybo",
                "Harbin Institute of Technology",
                "Huazhong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Zhongguancun Academy",
                "Zhongguancun Institute of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16793.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ pipeline Egocentric2Embodiment, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, ÑĞ½ÑÑ‚Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ E2E-3M ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PhysBrain Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Human Perspective for Robotic Intelligence",
                    "desc": "The Egocentric2Embodiment pipeline translates first-person videos into structured training data for robots, addressing the challenge of viewpoint mismatch in robotic learning. By leveraging large-scale human egocentric videos, the pipeline generates a diverse dataset that captures rich interaction contexts and causal relationships. This structured data, known as the Egocentric2Embodiment dataset (E2E-3M), enables robots to develop better egocentric understanding and planning capabilities. The resulting model, PhysBrain, shows improved performance in robot control tasks, demonstrating the effectiveness of transferring human egocentric knowledge to robotic systems."
                },
                "zh": {
                    "title": "è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘åŠ©åŠ›æœºå™¨äººæ™ºèƒ½æå‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§Egocentric2Embodimentç®¡é“ï¼Œå°†äººç±»çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è½¬æ¢ä¸ºç»“æ„åŒ–çš„è®­ç»ƒæ•°æ®ï¼Œä»¥å¢å¼ºæœºå™¨äººå¯¹è‡ªæˆ‘ä¸­å¿ƒçš„ç†è§£å’Œä»»åŠ¡è¡¨ç°ã€‚å¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸»è¦åœ¨ç¬¬ä¸‰äººç§°æ•°æ®ä¸Šè®­ç»ƒï¼Œè¿™å¯¼è‡´äº†äººå½¢æœºå™¨äººåœ¨è§†è§’ä¸Šçš„ä¸åŒ¹é…ã€‚é€šè¿‡å°†åŸå§‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è½¬æ¢ä¸ºå¤šå±‚æ¬¡çš„ã€åŸºäºæ¨¡å¼çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç›‘ç£ï¼Œæœ¬æ–‡æ„å»ºäº†Egocentric2Embodimentæ•°æ®é›†ï¼ˆE2E-3Mï¼‰ï¼Œä»è€Œè§£å†³äº†æ•°æ®æ”¶é›†çš„é«˜æˆæœ¬å’Œå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚è®­ç»ƒå¾—åˆ°çš„PhysBrainå±•ç°äº†æ˜¾è‘—çš„è‡ªæˆ‘ä¸­å¿ƒç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨EgoThinkè§„åˆ’ä»»åŠ¡ä¸­ï¼Œè¯æ˜äº†äººç±»è‡ªæˆ‘ä¸­å¿ƒç›‘ç£å‘æœºå™¨äººæ§åˆ¶çš„æœ‰æ•ˆè½¬ç§»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17901",
            "title": "When Reasoning Meets Its Laws",
            "url": "https://huggingface.co/papers/2512.17901",
            "abstract": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
            "score": 47,
            "issue_id": 169,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "f41d8969dc0a1159",
            "authors": [
                "Junyu Zhang",
                "Yifan Sun",
                "Tianang Leng",
                "Jingyan Shen",
                "Liu Ziyin",
                "Paul Pu Liang",
                "Huan Zhang"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "NTT Research",
                "New York University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17901.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#math"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ—Ğ°ĞºĞ¾Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· compute-Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° framework Laws of Reasoning (LoRe) Ğ´Ğ»Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ĞµĞ³Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LoRe-Bench, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ; Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ fine-tuning ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Reasoning in Large Models with Laws of Reasoning",
                    "desc": "The paper introduces a framework called Laws of Reasoning (LoRe) to define and measure the reasoning behaviors of Large Reasoning Models (LRMs). It focuses on two main laws: a compute law that suggests reasoning should scale linearly with question complexity, and an accuracy law that complements this. The authors present LoRe-Bench, a benchmark designed to evaluate the properties of monotonicity and compositionality in LRMs. Their findings indicate that while many models show good monotonicity, they often struggle with compositionality, leading to the development of a finetuning method that enhances reasoning performance by aligning with the proposed compute laws."
                },
                "zh": {
                    "title": "æ¨ç†æ³•åˆ™ï¼šæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæ¨ç†æ³•åˆ™ï¼ˆLaws of Reasoning, LoReï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç†è®ºä¸Šå®šä¹‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLarge Reasoning Models, LRMï¼‰ä¸­æœŸæœ›çš„æ¨ç†è¡Œä¸ºï¼Œé‡ç‚¹å…³æ³¨è®¡ç®—æ³•åˆ™å’Œå‡†ç¡®æ€§æ³•åˆ™ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºå‡†ï¼ˆLoRe-Benchï¼‰æ¥æµ‹é‡è¿™äº›å±æ€§ã€‚å°½ç®¡å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ä¼˜è¶Šï¼Œä½†å…¶æ¨ç†è¡Œä¸ºå¸¸å¸¸ä¸ç¬¦åˆç›´è§‰ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸ä½³ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºè®¡ç®—æ³•åˆ™ï¼Œå‡è®¾æ¨ç†è®¡ç®—åº”ä¸é—®é¢˜å¤æ‚æ€§çº¿æ€§ç›¸å…³ã€‚é€šè¿‡å¼•å…¥å•è°ƒæ€§å’Œç»„åˆæ€§è¿™ä¸¤ä¸ªå¯æµ‹é‡çš„å±æ€§ï¼Œæˆ‘ä»¬å¯¹è¿™äº›å‡è®¾è¿›è¡Œäº†æ£€éªŒï¼Œå¹¶å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºè®¡ç®—æ³•åˆ™çš„ç»„åˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17260",
            "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
            "url": "https://huggingface.co/papers/2512.17260",
            "abstract": "Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
            "score": 37,
            "issue_id": 169,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "631edfe7539fe665",
            "authors": [
                "Jiangjie Chen",
                "Wenxiang Chen",
                "Jiacheng Du",
                "Jinyi Hu",
                "Zhicheng Jiang",
                "Allan Jie",
                "Xiaoran Jin",
                "Xing Jin",
                "Chenggang Li",
                "Wenlei Shi",
                "Zhihong Wang",
                "Mingxuan Wang",
                "Chenrui Wei",
                "Shufa Wei",
                "Huajian Xin",
                "Fan Yang",
                "Weihao Gao",
                "Zheng Yuan",
                "Tianyang Zhan",
                "Zeyu Zheng",
                "Tianxi Zhou",
                "Thomas Hanwen Zhu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17260.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#plp",
                    "#agents",
                    "#benchmark",
                    "#math",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "Seed-Prover 1.5 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Lean, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ workflow Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: Ñ€ĞµÑˆĞ°ĞµÑ‚ 88% Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ±Ğ°ĞºĞ°Ğ»Ğ°Ğ²Ñ€Ğ¸Ğ°Ñ‚Ğ°, 80% Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ 33% Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡Ğ¾Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with Efficient Learning",
                    "desc": "Seed-Prover 1.5 is a formal theorem-proving model that utilizes large-scale agentic reinforcement learning to improve its problem-solving capabilities in mathematics. It incorporates an efficient test-time scaling workflow that allows it to perform well with reduced computational resources. The model has shown impressive results, solving a high percentage of undergraduate, graduate, and PhD-level problems, demonstrating its effectiveness in formal theorem proving. By bridging natural and formal languages, Seed-Prover 1.5 enhances the learning process and sets a new standard for computational efficiency in mathematical reasoning."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å½¢å¼å®šç†è¯æ˜æ–°çºªå…ƒ",
                    "desc": "Seed-Prover 1.5 æ˜¯ä¸€ç§ä½¿ç”¨å¤§è§„æ¨¡ä»£ç†å¼ºåŒ–å­¦ä¹ çš„å½¢å¼å®šç†è¯æ˜æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§£å†³å„ç§æ•°å­¦é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å‡å°‘äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚è¯¥æ¨¡å‹é€šè¿‡é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿåœ¨ä¸ Lean ç­‰å·¥å…·çš„å¹¿æ³›äº¤äº’ä¸­ä¸æ–­ç§¯ç´¯ç»éªŒï¼Œä»è€Œæ˜¾è‘—æé«˜å½¢å¼å®šç†è¯æ˜çš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒSeed-Prover 1.5 åœ¨è¾ƒå°çš„è®¡ç®—é¢„ç®—ä¸‹å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ï¼Œè§£å†³äº†å¤§éƒ¨åˆ†æœ¬ç§‘ç”Ÿå’Œç ”ç©¶ç”Ÿæ°´å¹³çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»ç»éªŒä¸­å­¦ä¹ å¹¶åˆ©ç”¨é«˜è´¨é‡çš„å½¢å¼åé¦ˆï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›æ¥æ¨åŠ¨å½¢å¼æ•°å­¦æ¨ç†çš„æœªæ¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17909",
            "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
            "url": "https://huggingface.co/papers/2512.17909",
            "abstract": "A systematic framework adapts understanding-oriented encoder features for generative tasks by introducing a semantic-pixel reconstruction objective, enabling state-of-the-art image reconstruction and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
            "score": 29,
            "issue_id": 172,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "18308ea42f45ceca",
            "authors": [
                "Shilong Zhang",
                "He Zhang",
                "Zhifei Zhang",
                "Chongjian Ge",
                "Shuchen Xue",
                "Shaoteng Liu",
                "Mengwei Ren",
                "Soo Ye Kim",
                "Yuqian Zhou",
                "Qing Liu",
                "Daniil Pakhomov",
                "Kai Zhang",
                "Zhe Lin",
                "Ping Luo"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Hong Kong",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17909.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ°Ñ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Generative Models with Semantic-Pixel Reconstruction",
                    "desc": "This paper presents a new framework that enhances generative tasks by using features from understanding-oriented encoders. It introduces a semantic-pixel reconstruction objective to improve the quality of image generation and reconstruction. The framework addresses two main challenges: the lack of compact regularization in discriminative feature spaces and the weak pixel-level reconstruction from encoders. By creating a compact representation that retains both semantic information and fine details, the approach achieves state-of-the-art results in image reconstruction and text-to-image generation tasks."
                },
                "zh": {
                    "title": "ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰åƒç´ é‡å»ºç›®æ ‡ï¼Œé€‚åº”ç†è§£å¯¼å‘çš„ç¼–ç å™¨ç‰¹å¾ä»¥ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼Œç¡®ä¿äº†æ½œåœ¨ç©ºé—´çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œç´§å‡‘æ€§ã€‚æˆ‘ä»¬è®¾è®¡çš„æ¡†æ¶èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„å›¾åƒé‡å»ºï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¡¨ç¤ºç¼–ç å™¨å¯ä»¥æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºå¼ºå¤§çš„ç”Ÿæˆç»„ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17012",
            "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
            "url": "https://huggingface.co/papers/2512.17012",
            "abstract": "4D-RGPT, a specialized multimodal LLM, enhances 4D perception in video inputs through Perceptual 4D Distillation and is evaluated on R4D-Bench, a new benchmark for depth-aware dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
            "score": 28,
            "issue_id": 169,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "5415545a8a27bd3a",
            "authors": [
                "Chiao-An Yang",
                "Ryo Hachiuma",
                "Sifei Liu",
                "Subhashree Radhakrishnan",
                "Raymond A. Yeh",
                "Yu-Chiang Frank Wang",
                "Min-Hung Chen"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17012.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#transfer_learning",
                    "#reasoning",
                    "#3d",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ 4D Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 4D-RGPT, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Perceptual 4D Distillation (P4D), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº R4D-Bench, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚."
                },
                "en": {
                    "title": "Enhancing 4D Perception in Video with 4D-RGPT",
                    "desc": "The paper introduces 4D-RGPT, a specialized multimodal large language model (LLM) that improves the understanding of 4D video inputs by enhancing depth perception and temporal dynamics. It addresses the limitations of current models in reasoning over 3D structures and dynamic scenes by employing a novel training method called Perceptual 4D Distillation (P4D). This method allows 4D-RGPT to learn from a frozen expert model, enabling it to capture complex 4D representations effectively. Additionally, the authors present R4D-Bench, a new benchmark designed for evaluating depth-aware dynamic scenes with region-level prompting, demonstrating significant performance improvements in 4D video question answering tasks."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘å››ç»´æ„ŸçŸ¥çš„é©å‘½æ€§æ¨¡å‹",
                    "desc": "4D-RGPTæ˜¯ä¸€ç§ä¸“é—¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºè§†é¢‘è¾“å…¥ä¸­çš„å››ç»´æ„ŸçŸ¥ã€‚é€šè¿‡æ„ŸçŸ¥å››ç»´è’¸é¦ï¼ˆP4Dï¼‰è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»å†»ç»“çš„ä¸“å®¶æ¨¡å‹ä¸­è½¬ç§»å››ç»´è¡¨ç¤ºï¼Œä»è€Œæå‡å¯¹åŠ¨æ€åœºæ™¯çš„ç†è§£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†R4D-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºæ·±åº¦æ„ŸçŸ¥çš„åŠ¨æ€åœºæ™¯ï¼Œå¹¶å¼•å…¥åŒºåŸŸçº§æç¤ºã€‚4D-RGPTåœ¨ç°æœ‰çš„å››ç»´è§†é¢‘é—®ç­”åŸºå‡†å’Œæ–°æå‡ºçš„R4D-BenchåŸºå‡†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16041",
            "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
            "url": "https://huggingface.co/papers/2512.16041",
            "abstract": "Sage is a human-free evaluation suite for LLM-as-a-Judge, using rational choice theory to assess local and global consistency, revealing significant reliability issues with current LLM judges.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
            "score": 21,
            "issue_id": 170,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "acfa7713fec5c22a",
            "authors": [
                "Yuanning Feng",
                "Sinan Wang",
                "Zhengxiang Cheng",
                "Yao Wan",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16041.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Sage â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ (ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹) Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ (Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini-2.5-Pro Ğ¸ GPT-4 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸Ğ·-Ğ·Ğ° ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑƒĞ´ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ½ĞµĞ»ĞµĞ¹ ÑÑƒĞ´ĞµĞ¹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº."
                },
                "en": {
                    "title": "Sage: A New Standard for Evaluating LLM Judges Without Human Bias",
                    "desc": "The paper introduces Sage, an evaluation suite designed to assess large language models (LLMs) acting as judges without relying on human annotations. It utilizes principles from rational choice theory to measure local self-consistency and global logical consistency in LLM judgments. The study reveals significant reliability issues in current LLM judges, including top models like Gemini-2.5-Pro and GPT-5, which struggle with consistent preferences in challenging scenarios. Additionally, the findings suggest that fine-tuning LLMs and employing panel-based judgments can improve consistency in evaluations."
                },
                "zh": {
                    "title": "Sageï¼šæ— äººå·¥è¯„ä¼°çš„LLMè¯„åˆ¤å·¥å…·",
                    "desc": "Sageæ˜¯ä¸€ä¸ªæ— éœ€äººå·¥å‚ä¸çš„è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…çš„è¡¨ç°ã€‚è¯¥å·¥å…·åŸºäºç†æ€§é€‰æ‹©ç†è®ºï¼Œæå‡ºäº†å±€éƒ¨è‡ªä¸€è‡´æ€§å’Œå…¨å±€é€»è¾‘ä¸€è‡´æ€§ä¸¤ä¸ªæ–°æŒ‡æ ‡æ¥è¡¡é‡LLMçš„è¯„åˆ¤è´¨é‡ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«650ä¸ªé—®é¢˜çš„æ•°æ®é›†ï¼ŒSageå±•ç¤ºäº†å…¶è¯„ä¼°æŒ‡æ ‡çš„ç¨³å®šæ€§å’Œä¸ç°æœ‰åŸºå‡†çš„é«˜ç›¸å…³æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„é¡¶å°–LLMåœ¨ä½œä¸ºè¯„åˆ¤è€…æ—¶å­˜åœ¨æ˜¾è‘—çš„å¯é æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ¡ˆä¾‹æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17897",
            "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
            "url": "https://huggingface.co/papers/2512.17897",
            "abstract": "RadarGen uses diffusion models to synthesize realistic radar point clouds from multi-view camera images, incorporating depth, semantic, and motion cues from pretrained models to enhance physical plausibility.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
            "score": 16,
            "issue_id": 175,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "7187cc905e173135",
            "authors": [
                "Tomer Borreda",
                "Fangqiang Ding",
                "Sanja Fidler",
                "Shengyu Huang",
                "Or Litany"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "Technion",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17897.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“¡",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ°Ğ¼ĞµÑ€ Ğº Ñ€Ğ°Ğ´Ğ°Ñ€Ñƒ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº",
                    "desc": "RadarGen â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ñ€Ğ°Ğ´Ğ°Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ´Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ¸Ğ´Ğ° ÑĞ²ĞµÑ€Ñ…Ñƒ Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ‡ĞµĞ½Ğ¸Ñ (RCS) Ğ¸ Ğ´Ğ¾Ğ¿Ğ¿Ğ»ĞµÑ€Ğ¾Ğ²ÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "RadarGen: Bridging Visuals and Radar with Diffusion Models",
                    "desc": "RadarGen is a novel diffusion model designed to create realistic radar point clouds using images from multiple cameras. It transforms radar data into a bird's-eye view format, which helps in capturing the spatial structure and important radar attributes like radar cross section and Doppler information. By integrating depth, semantic, and motion cues from pretrained models, RadarGen ensures that the generated radar patterns are physically plausible and closely aligned with the visual scene. This approach not only enhances the realism of radar data synthesis but also allows for compatibility with existing visual datasets, paving the way for advanced multimodal generative simulations."
                },
                "zh": {
                    "title": "é›·è¾¾ç‚¹äº‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "RadarGenæ˜¯ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆçœŸå®é›·è¾¾ç‚¹äº‘çš„æŠ€æœ¯ï¼Œä¸»è¦åŸºäºå¤šè§†è§’æ‘„åƒå¤´å›¾åƒã€‚å®ƒé€šè¿‡é¸Ÿç°å›¾å½¢å¼è¡¨ç¤ºé›·è¾¾æµ‹é‡ï¼Œç»“åˆé›·è¾¾æˆªé¢ï¼ˆRCSï¼‰å’Œå¤šæ™®å‹’å±æ€§ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§ã€‚è¯¥æ¨¡å‹è¿˜åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–çš„æ·±åº¦ã€è¯­ä¹‰å’Œè¿åŠ¨çº¿ç´¢ï¼ŒæŒ‡å¯¼éšæœºç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„é›·è¾¾æ¨¡å¼æ›´ç¬¦åˆå®é™…åœºæ™¯ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒRadarGenèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é›·è¾¾æµ‹é‡çš„ç‰¹å¾åˆ†å¸ƒï¼Œç¼©å°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ„ŸçŸ¥æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17495",
            "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
            "url": "https://huggingface.co/papers/2512.17495",
            "abstract": "GroundingME benchmark evaluates multimodal large language models' visual grounding capabilities across complexity dimensions, revealing significant gaps and proposing strategies for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
            "score": 14,
            "issue_id": 172,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "8f3810f915645086",
            "authors": [
                "Rang Li",
                "Lei Li",
                "Shuhuai Ren",
                "Hao Tian",
                "Shuhao Gu",
                "Shicheng Li",
                "Zihao Yue",
                "Yudong Wang",
                "Wenhan Ma",
                "Zhe Yang",
                "Jingyuan Ma",
                "Zhifang Sui",
                "Fuli Luo"
            ],
            "affiliations": [
                "LLM-Core Xiaomi",
                "Renmin University of China",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17495.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#hallucinations",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¾Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GroundingME Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ´Ğ¾ 2.9% Ğ¸ 27.9% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Bridging Language and Vision: GroundingME Unveils MLLM Gaps",
                    "desc": "The GroundingME benchmark evaluates how well multimodal large language models (MLLMs) can connect language to visual elements, focusing on their ability to localize objects based on descriptions. It identifies significant gaps in MLLMs' performance, particularly in complex real-world scenarios where ambiguity and occlusions are common. The benchmark introduces four dimensions of complexity to rigorously test these models, revealing that even the best-performing model struggles with accuracy, especially in recognizing when grounding is impossible. To address these limitations, the paper proposes strategies like test-time scaling and data-mixture training to enhance MLLMs' grounding capabilities."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰å®šä½èƒ½åŠ›",
                    "desc": "GroundingMEåŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å®šä½èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›å·®è·ï¼Œå¹¶æå‡ºäº†æ”¹è¿›ç­–ç•¥ã€‚è§†è§‰å®šä½æ˜¯å°†è‡ªç„¶è¯­è¨€æè¿°ä¸­çš„å¯¹è±¡è¿›è¡Œå®šä½çš„å…³é”®ç¯èŠ‚ï¼Œè¿æ¥äº†è¯­è¨€å’Œè§†è§‰ç†è§£ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½æ•æ‰åˆ°ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†GroundingMEåŸºå‡†ï¼Œç³»ç»Ÿæ€§åœ°æŒ‘æˆ˜æ¨¡å‹åœ¨å››ä¸ªå…³é”®ç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚é€šè¿‡è¯„ä¼°25ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º45.1%ï¼Œè€Œå¤§å¤šæ•°åœ¨æ‹’ç»ä»»åŠ¡ä¸Šçš„å¾—åˆ†ä¸º0%ï¼Œè¿™å¼•å‘äº†éƒ¨ç½²æ—¶çš„å®‰å…¨éšæ‚£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11362",
            "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
            "url": "https://huggingface.co/papers/2512.11362",
            "abstract": "This survey provides a structured overview of Vision-Language-Action models, detailing key challenges and opportunities in areas such as representation, execution, generalization, safety, and datasets in the field of robotics.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/Survery/{project page}.",
            "score": 13,
            "issue_id": 174,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "8b7c8b26cccfb3f8",
            "authors": [
                "Chao Xu",
                "Suyu Zhang",
                "Yang Liu",
                "Baigui Sun",
                "Weihong Chen",
                "Bo Xu",
                "Qi Liu",
                "Juncheng Wang",
                "Shujun Wang",
                "Shan Luo",
                "Jan Peters",
                "Athanasios V. Vasilakos",
                "Stefanos Zafeiriou",
                "Jiankang Deng"
            ],
            "affiliations": [
                "Computer Science Department of the Technische Universitat Darmstadt",
                "Department of Computing, Imperial College London",
                "Department of Engineering, Kings College London",
                "Department of ICT and Center for AI Research, University of Agder (UiA)",
                "Hong Kong Polytechnic University",
                "IROOTECH TECHNOLOGY",
                "Wolf 1069 Lab, Sany Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11362.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#benchmark",
                    "#dataset",
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ÑĞ·Ñ‹Ğº: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ VLA Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµÑ…Ğ¸ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼: Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° - Ğ¾Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑÑ€ĞµĞ´Ñ‹. ĞĞ¿Ñ€Ğ¾Ñ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¸Ñ‡ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ÑĞ»ÑƒĞ¶Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Navigating the Future of Vision-Language-Action in Robotics",
                    "desc": "This paper surveys Vision-Language-Action (VLA) models in robotics, focusing on their ability to interpret instructions and interact with the environment. It outlines five major challenges: Representation, Execution, Generalization, Safety, and Dataset Evaluation, which are crucial for advancing VLA research. The survey is structured to guide researchers from foundational concepts to complex issues, providing insights into existing methods and future opportunities. It serves as a resource for both newcomers and seasoned researchers, aiming to enhance understanding and inspire innovation in embodied intelligence."
                },
                "zh": {
                    "title": "è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬è°ƒæŸ¥æä¾›äº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„ç»“æ„åŒ–æ¦‚è¿°ï¼Œé‡ç‚¹è®¨è®ºäº†æœºå™¨äººé¢†åŸŸä¸­çš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ï¼ŒåŒ…æ‹¬è¡¨ç¤ºã€æ‰§è¡Œã€æ³›åŒ–ã€å®‰å…¨æ€§å’Œæ•°æ®é›†ç­‰æ–¹é¢ã€‚VLAæ¨¡å‹æ­£åœ¨æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„é©å‘½ï¼Œä½¿æœºå™¨èƒ½å¤Ÿç†è§£æŒ‡ä»¤å¹¶ä¸ç‰©ç†ä¸–ç•Œäº’åŠ¨ã€‚æˆ‘ä»¬è¯¦ç»†åˆ†æäº†äº”ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶å›é¡¾äº†ç°æœ‰çš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†æœªæ¥çš„æœºä¼šã€‚è¯¥è®ºæ–‡æ—¨åœ¨ä¸ºæ–°æ‰‹æä¾›åŸºç¡€æŒ‡å—ï¼ŒåŒæ—¶ä¸ºç»éªŒä¸°å¯Œçš„ç ”ç©¶äººå‘˜æä¾›æˆ˜ç•¥è·¯çº¿å›¾ï¼Œä¿ƒè¿›å­¦ä¹ å¹¶æ¿€å‘æ–°çš„æƒ³æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17351",
            "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
            "url": "https://huggingface.co/papers/2512.17351",
            "abstract": "Canon layers, lightweight architectural components, enhance reasoning depth and breadth in language models, improving performance in synthetic and real-world pretraining tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.   We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
            "score": 12,
            "issue_id": 169,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "0949e1dcbb4c7086",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#synthetic",
                    "#architecture",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Canon layers Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Canon layers â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¾ĞºĞ¾Ğ»Ğ¾ 1.3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Canon layers, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹: Transformers, Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Canon layers Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ ÑˆĞ¸Ñ€Ğ¾Ñ‚Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² 2 Ñ€Ğ°Ğ·Ğ°, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Language Models with Canon Layers for Deeper Reasoning",
                    "desc": "This paper introduces Canon layers, which are lightweight components designed to improve the reasoning capabilities of language models. By promoting horizontal information flow among neighboring tokens, these layers enhance both the depth and breadth of reasoning in models. The authors demonstrate that Canon layers can significantly boost the performance of weaker architectures, allowing them to compete with state-of-the-art models. Additionally, the study presents a synthetic pretraining framework that helps isolate and evaluate essential model capabilities, paving the way for future advancements in model architecture."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è½»é‡çº§ç»„ä»¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œCanon Layersâ€çš„è½»é‡çº§æ¶æ„ç»„ä»¶ï¼Œæ—¨åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†æ·±åº¦å’Œå¹¿åº¦ã€‚è¿™äº›ç»„ä»¶é€šè¿‡ä¿ƒè¿›ç›¸é‚»æ ‡è®°ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è®¡ç®—é™„è¿‘æ ‡è®°è¡¨ç¤ºçš„åŠ æƒå’Œï¼Œå¹¶ä¸å„ç§åºåˆ—æ¶æ„æ— ç¼é›†æˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCanon Layerså¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œé¢„è®­ç»ƒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç”šè‡³ä½¿ä¸€äº›è¾ƒå¼±çš„æ¶æ„è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½æ°´å¹³ã€‚é€šè¿‡æ§åˆ¶åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼Œæœ¬æ–‡ä¸ºè¯„ä¼°æ ¸å¿ƒæ¨¡å‹èƒ½åŠ›æä¾›äº†ä¸€ç§ç»æµä¸”åŸåˆ™æ€§çš„é€”å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14870",
            "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
            "url": "https://huggingface.co/papers/2512.14870",
            "abstract": "HERBench is a VideoQA benchmark that tests models' ability to integrate multiple, temporally separated visual cues, revealing significant challenges in current Video-LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.",
            "score": 9,
            "issue_id": 172,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "ab072ea1954e9170",
            "authors": [
                "Dan Ben-Ami",
                "Gabriele Serussi",
                "Kobi Cohen",
                "Chaim Baskin"
            ],
            "affiliations": [
                "Ben-Gurion University of the Negev, Israel",
                "INSIGHT Lab, Ben-Gurion University of the Negev, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14870.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#survey",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½ĞµÑÑ‘Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² â€” ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "HERBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, HERBench ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 26 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ñ‚Ñ€Ñ‘Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Minimum Required Frame-Set (MRFS) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Video-LLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»Ğ¸ÑˆÑŒ 31-42% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 20% ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²."
                },
                "en": {
                    "title": "HERBench: Advancing VideoQA with Multi-Evidence Integration",
                    "desc": "HERBench is a new benchmark designed to evaluate Video Question Answering (VideoQA) models on their ability to integrate multiple visual cues from different time segments in videos. Unlike previous benchmarks that often rely on single cues, HERBench requires models to combine at least three distinct evidential cues to answer questions correctly. The benchmark includes 26,000 multiple-choice questions across various tasks, highlighting challenges in identity binding, temporal ordering, and more. Evaluation of current Video-LLMs on HERBench shows significant shortcomings, revealing issues in both evidence retrieval and information fusion, thus setting a new standard for video understanding."
                },
                "zh": {
                    "title": "HERBenchï¼šæå‡è§†é¢‘é—®ç­”æ¨¡å‹çš„å¤šè¯æ®æ•´åˆèƒ½åŠ›",
                    "desc": "HERBenchæ˜¯ä¸€ä¸ªè§†é¢‘é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹æ•´åˆå¤šä¸ªæ—¶é—´ä¸Šåˆ†ç¦»çš„è§†è§‰çº¿ç´¢çš„èƒ½åŠ›ã€‚å½“å‰çš„è§†é¢‘é—®ç­”åŸºå‡†å¾€å¾€åªä¾èµ–å•ä¸€æ˜¾è‘—çº¿ç´¢ï¼Œæœªèƒ½å……åˆ†è€ƒå¯Ÿéœ€è¦æ•´åˆå¤šç§è§†è§‰è¯æ®çš„æ¨ç†èƒ½åŠ›ã€‚HERBenchè¦æ±‚æ¯ä¸ªé—®é¢˜è‡³å°‘æ•´åˆä¸‰ä¸ªä¸é‡å çš„è¯æ®çº¿ç´¢ï¼Œç¡®ä¿æ¨¡å‹ä¸èƒ½ä»…ä¾èµ–è¯­è¨€å…ˆéªŒæˆ–å•ä¸€å¿«ç…§æ¥å›ç­”ã€‚é€šè¿‡å¼•å…¥æœ€å°æ‰€éœ€å¸§é›†ï¼ˆMRFSï¼‰ï¼ŒHERBenchæ˜¾è‘—æé«˜äº†å¯¹æ¨¡å‹çš„è¦æ±‚ï¼Œæ­ç¤ºäº†ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†é¢‘ç†è§£æ—¶çš„ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17796",
            "title": "Animate Any Character in Any World",
            "url": "https://huggingface.co/papers/2512.17796",
            "abstract": "AniX synthesizes temporally coherent videos by extending controllable-entity models to support diverse, user-defined character interactions in static 3D environments using conditional autoregressive video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
            "score": 8,
            "issue_id": 169,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "b86f25dd898b163b",
            "authors": [
                "Yitong Wang",
                "Fangyun Wei",
                "Hongyang Zhang",
                "Bo Dai",
                "Yan Lu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research",
                "The University of Hong Kong",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17796.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#3d",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D ÑÑ†ĞµĞ½Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "AniX â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ â€” Ğ¾Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering User-Defined Character Interactions in Video Generation",
                    "desc": "AniX is a novel approach in video generation that creates realistic and coherent videos by allowing users to define character interactions in 3D environments. It builds on existing controllable-entity models, enabling characters to perform a wide range of actions based on user input through natural language. The method utilizes conditional autoregressive video generation to ensure that the generated videos maintain high visual fidelity and temporal coherence. By enhancing motion dynamics and generalization across different actions and characters, AniX sets a new standard for interactive video synthesis."
                },
                "zh": {
                    "title": "AniXï¼šè®©è§’è‰²åœ¨3Dç¯å¢ƒä¸­è‡ªç”±äº’åŠ¨çš„æ™ºèƒ½è§†é¢‘ç”Ÿæˆ",
                    "desc": "AniXæ˜¯ä¸€ç§é€šè¿‡æ‰©å±•å¯æ§å®ä½“æ¨¡å‹æ¥åˆæˆæ—¶é—´ä¸€è‡´çš„è§†é¢‘çš„æ–¹æ³•ã€‚å®ƒæ”¯æŒç”¨æˆ·å®šä¹‰çš„è§’è‰²åœ¨é™æ€3Dç¯å¢ƒä¸­è¿›è¡Œå¤šæ ·åŒ–çš„äº¤äº’ã€‚ç”¨æˆ·å¯ä»¥æä¾›3Dåœºæ™¯å’Œè§’è‰²ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è®©è§’è‰²æ‰§è¡Œä»åŸºæœ¬ç§»åŠ¨åˆ°ç‰©ä½“äº¤äº’çš„å„ç§è¡Œä¸ºã€‚AniXåˆ©ç”¨æ¡ä»¶è‡ªå›å½’è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œç”Ÿæˆä¸æä¾›çš„åœºæ™¯å’Œè§’è‰²è§†è§‰ä¸€è‡´çš„æ—¶é—´è¿è´¯è§†é¢‘ç‰‡æ®µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17008",
            "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
            "url": "https://huggingface.co/papers/2512.17008",
            "abstract": "Turn-PPO, a variant of PPO tailored for multi-turn RL tasks, improves advantage estimation and performance over GRPO in environments requiring long-horizon reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
            "score": 7,
            "issue_id": 170,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "0ca7221f5783bbed",
            "authors": [
                "Junbo Li",
                "Peng Zhou",
                "Rui Meng",
                "Meet P. Vadera",
                "Lihong Li",
                "Yang Li"
            ],
            "affiliations": [
                "Amazon",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17008.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#agents",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Turn-PPO: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Turn-PPO, Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Proximal Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° GRPO Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ MDP Ğº turn-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ advantage-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… WebShop Ğ¸ Sokoban Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Turn-PPO Ğ½Ğ°Ğ´ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…."
                },
                "en": {
                    "title": "Turn-PPO: Enhancing Multi-Turn Reinforcement Learning",
                    "desc": "This paper presents Turn-PPO, a new version of the Proximal Policy Optimization (PPO) algorithm designed specifically for multi-turn reinforcement learning tasks. It addresses the limitations of the Group Relative Policy Optimization (GRPO) algorithm, particularly in environments that require long-horizon reasoning. By focusing on turn-level Markov Decision Processes (MDPs) instead of token-level MDPs, Turn-PPO improves the stability and effectiveness of advantage estimation. The results show that Turn-PPO outperforms GRPO in various tasks, demonstrating its potential for training interactive agents in complex scenarios."
                },
                "zh": {
                    "title": "Turn-PPOï¼šå¤šè½®ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ–°é€‰æ‹©",
                    "desc": "Turn-PPOæ˜¯ä¸€ç§é’ˆå¯¹å¤šè½®å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„PPOå˜ä½“ï¼Œæ—¨åœ¨æé«˜ä¼˜åŠ¿ä¼°è®¡å’Œæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”ï¼ŒTurn-PPOåœ¨éœ€è¦é•¿æ—¶é—´æ¨ç†çš„ç¯å¢ƒä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å›åˆçº§åˆ«çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå…‹æœäº†GRPOåœ¨å¤šè½®ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTurn-PPOåœ¨WebShopå’ŒSokobanæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºé•¿æ¨ç†ç»„ä»¶å’Œéé•¿æ¨ç†ç»„ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17419",
            "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
            "url": "https://huggingface.co/papers/2512.17419",
            "abstract": "SWE-Bench++ is an automated framework generating repository-level coding tasks from live GitHub pull requests, offering a scalable, multilingual benchmark for evaluating and improving code generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
            "score": 6,
            "issue_id": 169,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "01a77f618512fc9c",
            "authors": [
                "Lilin Wang",
                "Lucas Ramalho",
                "Alan Celestino",
                "Phuc Anthony Pham",
                "Yu Liu",
                "Umang Kumar Sinha",
                "Andres Portillo",
                "Onassis Osunwa",
                "Gabriel Maduekwe"
            ],
            "affiliations": [
                "Turing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17419.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#plp",
                    "#synthetic",
                    "#benchmark",
                    "#optimization",
                    "#multilingual"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ",
                    "desc": "SWE-Bench++ â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… pull request'Ğ¾Ğ² Ğ½Ğ° GitHub, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 11 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ PR Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 11,133 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ¸Ğ· 3,971 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², Ğ¸ Ğ½Ğ° Ğ½Ñ‘Ğ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Claude Ğ¸ GPT, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ: fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° SWE-Bench++ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ ĞŸĞ."
                },
                "en": {
                    "title": "Revolutionizing Code Generation Evaluation with SWE-Bench++",
                    "desc": "SWE-Bench++ is an innovative framework designed to automatically generate coding tasks from real GitHub pull requests, enhancing the evaluation of code generation models. It addresses limitations of previous benchmarks by utilizing live data and supporting multiple programming languages, rather than relying solely on static datasets. The framework processes pull requests through several stages, including sourcing, environment setup, and quality assurance, to create reproducible coding tasks. Initial results show that fine-tuning models on SWE-Bench++ can significantly improve their performance on existing benchmarks, demonstrating its effectiveness in advancing code generation capabilities."
                },
                "zh": {
                    "title": "SWE-Bench++ï¼šæå‡ä»£ç ç”Ÿæˆçš„å¤šè¯­è¨€åŸºå‡†",
                    "desc": "SWE-Bench++æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å®æ—¶çš„GitHubæ‹‰å–è¯·æ±‚ä¸­ç”Ÿæˆä»“åº“çº§åˆ«çš„ç¼–ç ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„æ‰‹åŠ¨æ•´ç†å’Œé™æ€æ•°æ®é›†ä¸åŒï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¶æ¶µç›–äº†é”™è¯¯ä¿®å¤å’ŒåŠŸèƒ½è¯·æ±‚ã€‚é€šè¿‡å››ä¸ªé˜¶æ®µçš„å¤„ç†ï¼ŒSWE-Bench++å°†GitHubæ‹‰å–è¯·æ±‚è½¬åŒ–ä¸ºå¯é‡å¤æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æç¤ºå¼•å¯¼ç”Ÿæˆè®­ç»ƒè½¨è¿¹ã€‚åˆæ­¥åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå½“å‰æœ€å¼ºçš„æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°å„å¼‚ï¼Œè¡¨æ˜è¯¥æ¡†æ¶åœ¨è¯„ä¼°å’Œæ”¹è¿›ä»£ç ç”Ÿæˆæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16848",
            "title": "Meta-RL Induces Exploration in Language Agents",
            "url": "https://huggingface.co/papers/2512.16848",
            "abstract": "LaMer, a Meta-RL framework, enhances LLM agents' exploration and adaptation capabilities in RL tasks, leading to improved performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
            "score": 5,
            "issue_id": 178,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "1927ee8c2d297559",
            "authors": [
                "Yulun Jiang",
                "Liangze Jiang",
                "Damien Teney",
                "Michael Moor",
                "Maria Brbic"
            ],
            "affiliations": [
                "EPFL",
                "ETH Zurich",
                "Idiap Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16848.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "LaMer â€” ÑÑ‚Ğ¾ Meta-RL Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, LaMer Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Empowering LLM Agents with Enhanced Exploration and Adaptation",
                    "desc": "LaMer is a Meta-Reinforcement Learning (Meta-RL) framework designed to enhance the exploration and adaptation abilities of large language model (LLM) agents in reinforcement learning tasks. It addresses the challenges faced by RL-trained agents, particularly in tasks requiring active exploration and efficient adaptation from trial-and-error experiences. The framework includes a cross-episode training component that promotes exploration and long-term reward optimization, along with in-context policy adaptation that allows agents to adjust their strategies based on feedback without needing gradient updates. Experimental results show that LaMer outperforms traditional RL methods, achieving significant performance improvements and better generalization to new tasks."
                },
                "zh": {
                    "title": "LaMerï¼šæå‡LLMä»£ç†çš„æ¢ç´¢ä¸é€‚åº”èƒ½åŠ›",
                    "desc": "LaMeræ˜¯ä¸€ä¸ªå…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„æ¢ç´¢å’Œé€‚åº”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨å›åˆè®­ç»ƒå’Œåæ€ç­–ç•¥é€‚åº”ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä¿ƒè¿›ä»£ç†åœ¨æµ‹è¯•æ—¶ä¸»åŠ¨æ¢ç´¢å’Œä»ç¯å¢ƒåé¦ˆä¸­å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMeråœ¨å¤šä¸ªç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼ŒSokobanã€MineSweeperå’ŒWebshopä»»åŠ¡çš„æ€§èƒ½åˆ†åˆ«æå‡äº†11%ã€14%å’Œ19%ã€‚æ­¤å¤–ï¼ŒLaMeråœ¨é¢å¯¹æ›´å…·æŒ‘æˆ˜æ€§æˆ–æœªè§è¿‡çš„ä»»åŠ¡æ—¶ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16483",
            "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
            "url": "https://huggingface.co/papers/2512.16483",
            "abstract": "StageVAR accelerates visual autoregressive models by selectively pruning or approximating less critical later stages, achieving significant speedup with minimal quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.",
            "score": 5,
            "issue_id": 170,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "f400cca08ac39eaa",
            "authors": [
                "Senmao Li",
                "Kai Wang",
                "Salman Khan",
                "Fahad Shahbaz Khan",
                "Jian Yang",
                "Yaxing Wang"
            ],
            "affiliations": [
                "City University of Hong Kong (Dongguan), China",
                "Linkoping University",
                "Mohamed bin Zayed University of Artificial Intelligence",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16483.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²",
                    "desc": "StageVAR â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ (pruning) Ğ¸ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 3.4 Ñ€Ğ°Ğ·Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Accelerating Image Generation with Stage-Aware Pruning",
                    "desc": "StageVAR is a framework designed to enhance the efficiency of visual autoregressive models by focusing on the importance of different stages in the image generation process. It identifies that early stages are crucial for maintaining the overall quality and structure of the generated images, while later stages can be simplified or removed to speed up the process. By applying selective pruning and approximation techniques to these less critical later stages, StageVAR achieves significant reductions in computational time without greatly affecting image quality. This approach demonstrates that understanding the varying significance of model stages can lead to more efficient and effective image generation methods."
                },
                "zh": {
                    "title": "é˜¶æ®µæ„ŸçŸ¥åŠ é€Ÿï¼Œæå‡è§†è§‰ç”Ÿæˆæ•ˆç‡",
                    "desc": "StageVARæ˜¯ä¸€ç§åŠ é€Ÿè§†è§‰è‡ªå›å½’æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°ä¿®å‰ªæˆ–è¿‘ä¼¼ä¸å¤ªé‡è¦çš„åæœŸé˜¶æ®µï¼Œå®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒå°çš„è´¨é‡æŸå¤±ã€‚è¯¥æ–¹æ³•åŸºäºå¯¹ä¸åŒç”Ÿæˆé˜¶æ®µé‡è¦æ€§çš„åˆ†æï¼Œå‘ç°æ—©æœŸæ­¥éª¤å¯¹ä¿æŒè¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§è‡³å…³é‡è¦ï¼Œè€ŒåæœŸæ­¥éª¤ä¸»è¦ç”¨äºç»†èŠ‚çš„ç²¾ç»†åŒ–ï¼Œå¯ä»¥è¿›è¡Œä¿®å‰ªæˆ–è¿‘ä¼¼å¤„ç†ã€‚StageVARå¼•å…¥äº†ä¸€ç§å³æ’å³ç”¨çš„åŠ é€Ÿç­–ç•¥ï¼Œåˆ©ç”¨åæœŸè®¡ç®—ä¸­çš„è¯­ä¹‰æ— å…³æ€§å’Œä½ç§©ç‰¹æ€§ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStageVARåœ¨GenEvalä¸Šå®ç°äº†æœ€é«˜3.4å€çš„åŠ é€Ÿï¼Œä¸”åœ¨DPGä¸Šä»…æœ‰0.26çš„ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºé˜¶æ®µæ„ŸçŸ¥è®¾è®¡åœ¨é«˜æ•ˆè§†è§‰è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15586",
            "title": "Bolmo: Byteifying the Next Generation of Language Models",
            "url": "https://huggingface.co/papers/2512.15586",
            "abstract": "Bolmo, a family of competitive byte-level language models, is trained by converting existing subword-level models, overcoming character understanding and efficiency limitations while achieving performance comparable to subword models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\\% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases.",
            "score": 5,
            "issue_id": 185,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "29f3bf55455df864",
            "authors": [
                "Benjamin Minixhofer",
                "Tyler Murray",
                "Tomasz Limisiewicz",
                "Anna Korhonen",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Edoardo M. Ponti",
                "Luca Soldaini",
                "Valentin Hofmann"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Cambridge",
                "University of Edinburgh",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15586.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture",
                    "#small_models"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»Ğ¾Ğ² Ğº Ğ±Ğ°Ğ¹Ñ‚Ğ°Ğ¼: Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Bolmo â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ¹Ñ‚-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1B Ğ¸ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ byteification Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Bolmo Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ· ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… LM, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 1% Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ²ÑĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ¹Ñ‚-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ LM ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Bolmo: Revolutionizing Byte-Level Language Models for Enhanced Performance",
                    "desc": "Bolmo is a new family of byte-level language models that are trained by converting existing subword-level models. This approach, called byteification, helps to overcome issues related to character understanding and efficiency that are common in subword tokenization. Bolmo achieves performance that is comparable to leading subword models while requiring significantly less training resources. The architecture of Bolmo allows it to excel in tasks like character understanding and coding, making byte-level models a viable option for various applications."
                },
                "zh": {
                    "title": "Bolmoï¼šå­—èŠ‚çº§è¯­è¨€æ¨¡å‹çš„æ–°é€‰æ‹©",
                    "desc": "Bolmoæ˜¯ä¸€ç§æ–°å‹çš„å­—èŠ‚çº§è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œæ—¨åœ¨å…‹æœç°æœ‰å­è¯çº§æ¨¡å‹çš„å±€é™æ€§ã€‚é€šè¿‡å°†ç°æœ‰çš„å­è¯çº§æ¨¡å‹è½¬æ¢ä¸ºå­—èŠ‚çº§æ¨¡å‹ï¼ŒBolmoåœ¨å­—ç¬¦ç†è§£å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ä¸º1Bå’Œ7Bçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä¸é¢†å…ˆçš„å­è¯çº§æ¨¡å‹ç›¸åª²ç¾ã€‚Bolmoçš„è®¾è®¡ä½¿å¾—å­—èŠ‚çº§æ¨¡å‹åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­æˆä¸ºä¸å­è¯çº§æ¨¡å‹ç«äº‰çš„å®ç”¨é€‰æ‹©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17532",
            "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
            "url": "https://huggingface.co/papers/2512.17532",
            "abstract": "A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
            "score": 4,
            "issue_id": 169,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "6de661873da75b97",
            "authors": [
                "Jiaqi Tang",
                "Jianmin Chen",
                "Wei Wei",
                "Xiaogang Xu",
                "Runtao Liu",
                "Xiangyu Wu",
                "Qipeng Xie",
                "Jiafei Wu",
                "Lei Zhang",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Chinese University of Hong Kong",
                "Hong Kong University of Science and Technology",
                "Nanjing University of Science and Technology",
                "Northwestern Polytechnical University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17532.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#security",
                    "#dataset",
                    "#alignment",
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM",
                    "desc": "Robust-R1 â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 11K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… R-Bench, MMMB, MMStar Ğ¸ RealWorldQA, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing MLLM Robustness Against Visual Degradations with Robust-R1",
                    "desc": "The paper introduces Robust-R1, a new framework designed to improve the robustness of multimodal large language models (MLLMs) against visual degradations. Unlike previous models that relied on implicit training, Robust-R1 explicitly models visual degradations using structured reasoning chains. It employs supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling to enhance performance under various degradation conditions. The framework is validated on a specialized dataset and demonstrates superior robustness compared to existing models on real-world degradation benchmarks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹é²æ£’æ€§çš„Robust-R1æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶Robust-R1ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é€€åŒ–æƒ…å†µä¸‹çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡è§†è§‰é€€åŒ–ï¼Œç»“åˆç›‘ç£å¾®è°ƒã€åŸºäºå¥–åŠ±çš„å¯¹é½å’ŒåŠ¨æ€æ¨ç†æ·±åº¦ç¼©æ”¾ï¼Œæ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚Robust-R1åœ¨çœŸå®ä¸–ç•Œçš„é€€åŒ–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨çš„11Kæ•°æ®é›†ï¼ŒåŒ…å«å››ä¸ªå…³é”®è§†è§‰å¤„ç†é˜¶æ®µçš„çœŸå®é€€åŒ–ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œåº”å¯¹è§†è§‰é€€åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17459",
            "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
            "url": "https://huggingface.co/papers/2512.17459",
            "abstract": "3D-RE-GEN reconstructs single images into modifiable 3D textured mesh scenes with comprehensive backgrounds, achieving top performance through compositional generation and scene optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.   Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.",
            "score": 4,
            "issue_id": 175,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "948e39e2efcf5891",
            "authors": [
                "Tobias Sautter",
                "Jan-Niklas Dihlmann",
                "Hendrik P. A. Lensch"
            ],
            "affiliations": [
                "University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17459.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ 3D ÑÑ†ĞµĞ½Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ",
                    "desc": "3D-RE-GEN â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ñ„Ğ¾Ğ½, Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² ÑÑ†ĞµĞ½Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ 4 ÑÑ‚ĞµĞ¿ĞµĞ½ÑĞ¼Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ VFX Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€."
                },
                "en": {
                    "title": "Transforming Images into Modifiable 3D Scenes with 3D-RE-GEN",
                    "desc": "3D-RE-GEN is a framework that transforms single images into detailed 3D textured mesh scenes, making them suitable for artists in visual effects and game development. It addresses common issues in current methods, such as incorrect object decomposition and missing backgrounds, by integrating advanced models for asset detection and scene reconstruction. The approach uses generative models to infer occluded objects and ensures consistent lighting and geometry throughout the scene. By employing a novel optimization technique, 3D-RE-GEN achieves high-quality, modifiable 3D scenes that meet the needs of artists."
                },
                "zh": {
                    "title": "3Dåœºæ™¯é‡å»ºçš„æ–°çªç ´",
                    "desc": "3D-RE-GENæ˜¯ä¸€ç§å°†å•å¼ å›¾åƒé‡å»ºä¸ºå¯ä¿®æ”¹çš„3Dçº¹ç†ç½‘æ ¼åœºæ™¯çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå…¨é¢çš„èƒŒæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»„åˆç‰¹å®šé¢†åŸŸçš„å…ˆè¿›æ¨¡å‹ï¼Œå®ç°äº†é¡¶å°–çš„åœºæ™¯é‡å»ºæ€§èƒ½ï¼Œæ»¡è¶³äº†è‰ºæœ¯å®¶çš„éœ€æ±‚ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œ3D-RE-GENåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç”Ÿæˆå…¨é¢çš„èƒŒæ™¯ï¼Œå¹¶åœ¨ä¸€è‡´çš„å…‰ç…§å’Œå‡ ä½•æ¡ä»¶ä¸‹è¿›è¡Œåœºæ™¯çº§æ¨ç†ã€‚é€šè¿‡æ–°é¢–çš„4è‡ªç”±åº¦å¯å¾®ä¼˜åŒ–æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†é‡å»ºçš„ç‰©ä½“ä¸ä¼°è®¡çš„åœ°é¢å¹³é¢å¯¹é½ï¼Œä»è€Œè·å¾—ç‰©ç†ä¸ŠçœŸå®çš„å¸ƒå±€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16978",
            "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
            "url": "https://huggingface.co/papers/2512.16978",
            "abstract": "LongShOTBench is a diagnostic benchmark for long-form multimodal video understanding with open-ended questions, dialogues, and multimodal reasoning tasks, highlighting the challenges for state-of-the-art models.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.",
            "score": 3,
            "issue_id": 178,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "6497453714be18e2",
            "authors": [
                "Mohammed Irfan Kurpath",
                "Jaseel Muhammad Kaithakkodan",
                "Jinxing Zhou",
                "Sahal Shaji Mullappilly",
                "Mohammad Almansoori",
                "Noor Ahsan",
                "Beknur Kalmakhanbet",
                "Sambal Shikhar",
                "Rishabh Lalla",
                "Jean Lahoud",
                "Mariette Awad",
                "Fahad Shahbaz Khan",
                "Salman Khan",
                "Rao Muhammad Anwer",
                "Hisham Cholakkal"
            ],
            "affiliations": [
                "American University of Beirut",
                "Linkoping University",
                "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16978.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LongShOTBench â€” Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ²ÑƒĞºĞ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ SOTA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸: Gemini-2.5-Flash Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 52.95%, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½Ğ¸Ğ¶Ğµ 30%, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Unlocking Long-Form Video Understanding with LongShOTBench",
                    "desc": "LongShOTBench is a new benchmark designed to test how well machine learning models understand long videos that include different types of information like images, speech, and sounds. It focuses on complex tasks that require reasoning over long periods and involves open-ended questions and dialogues. Unlike previous benchmarks, it provides a detailed evaluation system that helps identify where models struggle, rather than just giving a single score. The results show that current state-of-the-art models still have significant challenges in understanding long-form videos, highlighting the need for better tools and methods in this area."
                },
                "zh": {
                    "title": "é•¿ç¯‡è§†é¢‘ç†è§£çš„æ–°åŸºå‡†æŒ‘æˆ˜",
                    "desc": "LongShOTBenchæ˜¯ä¸€ä¸ªç”¨äºé•¿ç¯‡å¤šæ¨¡æ€è§†é¢‘ç†è§£çš„è¯Šæ–­åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†ç»“åˆäº†è§†è§‰ã€è¯­éŸ³å’Œç¯å¢ƒéŸ³é¢‘ï¼Œè¦æ±‚è¿›è¡Œè¿è´¯çš„é•¿è·ç¦»æ¨ç†ã€‚å®ƒåŒ…å«å¼€æ”¾å¼é—®é¢˜ã€å¯¹è¯å’Œå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå¹¶æä¾›å‚è€ƒç­”æ¡ˆå’Œè¯„åˆ†æ ‡å‡†ï¼Œä»¥ä¾¿è¿›è¡Œå¯è§£é‡Šå’Œå¯è¿½æº¯çš„è¯„ä¼°ã€‚LongShOTBenchçš„ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç†è§£é•¿è§†é¢‘æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†è¿™ä¸€é¢†åŸŸçš„å¤æ‚æ€§å’Œæ”¹è¿›çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13427",
            "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
            "url": "https://huggingface.co/papers/2512.13427",
            "abstract": "MineTheGap is a method using a genetic algorithm and a bias score to identify prompts that cause Text-to-Image models to generate biased outputs, aiming to reduce redundancy and improve diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
            "score": 2,
            "issue_id": 179,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "1b17a047a30c0635",
            "authors": [
                "Noa Cohen",
                "Nurit Spingarn-Eliezer",
                "Inbar Huberman-Spiegelglas",
                "Tomer Michaeli"
            ],
            "affiliations": [
                "Technion Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13427.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MineTheGap â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿ÑƒĞ»Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° (bias score), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ² Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ."
                },
                "en": {
                    "title": "Uncovering Bias in Text-to-Image Models with Genetic Algorithms",
                    "desc": "MineTheGap is a novel approach that utilizes a genetic algorithm to identify text prompts that lead to biased outputs in Text-to-Image (TTI) models. By employing a unique bias score, the method evaluates and ranks the severity of biases in generated images, aiming to enhance diversity and reduce redundancy in the outputs. This process involves iteratively refining prompts to expose and address biases, rather than just detecting them. The effectiveness of MineTheGap is validated using a dataset with known biases, ensuring that the method can improve user experience and mitigate societal impacts of biased image generation."
                },
                "zh": {
                    "title": "è¯†åˆ«ä¸å‡å°‘æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åè§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "MineTheGapæ˜¯ä¸€ç§åˆ©ç”¨é—ä¼ ç®—æ³•å’Œåå·®è¯„åˆ†çš„æ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«å¯¼è‡´æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåè§è¾“å‡ºçš„æç¤ºã€‚è¯¥æ–¹æ³•ä¸ä»…ä»…æ˜¯æ£€æµ‹ç‰¹å®šæç¤ºçš„åè§ï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£ä¼˜åŒ–æç¤ºæ± ï¼Œå¯»æ‰¾èƒ½å¤Ÿæ­ç¤ºåè§çš„æç¤ºã€‚ä¼˜åŒ–è¿‡ç¨‹ä¾èµ–äºä¸€ç§æ–°é¢–çš„åå·®è¯„åˆ†ï¼Œè¯¥è¯„åˆ†æ ¹æ®åè§çš„ä¸¥é‡ç¨‹åº¦å¯¹å…¶è¿›è¡Œæ’åã€‚é€šè¿‡æ¯”è¾ƒç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒä¸åŸºäºæç¤ºçš„å˜ä½“ç”Ÿæˆçš„æ–‡æœ¬åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—è¯¥è¯„åˆ†ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘å†—ä½™å¹¶æé«˜å¤šæ ·æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-19.html",
    "link_next": "2025-12-23.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 0,
        "#benchmark": 14,
        "#agents": 5,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 2,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 10,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 2,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}