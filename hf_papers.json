{
    "date": {
        "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 9",
        "zh": "1æœˆ9æ—¥"
    },
    "time_utc": "2026-01-10 02:08",
    "weekday": 4,
    "issue_id": 513,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.05242",
            "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
            "url": "https://huggingface.co/papers/2601.05242",
            "abstract": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
            "score": 96,
            "issue_id": 493,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "2644b8e88f8fe659",
            "authors": [
                "Shih-Yang Liu",
                "Xin Dong",
                "Ximing Lu",
                "Shizhe Diao",
                "Peter Belcak",
                "Mingjie Liu",
                "Min-Hung Chen",
                "Hongxu Yin",
                "Yu-Chiang Frank Wang",
                "Kwang-Ting Cheng",
                "Yejin Choi",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05242.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (reinforcement learning) Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ GRPO Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ñ… ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GDPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GDPO Ğ½Ğ°Ğ´ GRPO."
                },
                "en": {
                    "title": "Decoupling Rewards for Better Learning in Multi-Reward RL",
                    "desc": "This paper addresses the challenges faced in multi-reward reinforcement learning, particularly the issue of reward normalization collapse when using Group Relative Policy Optimization (GRPO). The authors show that GRPO can lead to identical advantage values for different rewards, which diminishes the effectiveness of the training signal and can cause suboptimal learning outcomes. To overcome this, they propose a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO), which separates the normalization of individual rewards to better maintain their differences. The results indicate that GDPO significantly improves training stability and performance across various reasoning tasks compared to GRPO."
                },
                "zh": {
                    "title": "è§£è€¦å¥–åŠ±å½’ä¸€åŒ–ï¼Œæå‡å¤šå¥–åŠ±å­¦ä¹ æ•ˆæœ",
                    "desc": "å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨ä½¿ç”¨GRPOæ—¶ä¼šå‡ºç°å¥–åŠ±å½’ä¸€åŒ–å´©æºƒçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•GDPOï¼Œé€šè¿‡è§£è€¦ä¸ªä½“å¥–åŠ±çš„å½’ä¸€åŒ–ï¼Œæ”¹å–„äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚GDPOèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å¥–åŠ±ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¤šå¥–åŠ±ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDPOåœ¨å·¥å…·è°ƒç”¨ã€æ•°å­¦æ¨ç†å’Œç¼–ç æ¨ç†ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºGRPOã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04890",
            "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
            "url": "https://huggingface.co/papers/2601.04890",
            "abstract": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
            "score": 28,
            "issue_id": 494,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "286bbc12fc07f8d9",
            "authors": [
                "Maksim Velikanov",
                "Ilyas Chahed",
                "Jingwei Zuo",
                "Dhia Eddine Rhaiem",
                "Younes Belkada",
                "Hakim Hacid"
            ],
            "affiliations": [
                "Falcon LLM Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04890.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ weight decay Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ weight decay Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° muP. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Adam, Ñ‚Ğ°Ğº Ğ¸ Ñ Muon."
                },
                "en": {
                    "title": "Learnable Multipliers: Enhancing Large Language Model Training",
                    "desc": "This paper introduces learnable multipliers to improve the training of large language models by addressing issues caused by weight decay. Traditional weight decay can create normalization artifacts that hinder model performance, but the proposed method allows the model to learn optimal scaling for weight matrices. By using learnable multipliers for individual rows and columns, the model adapts better to the data, leading to improved performance. The approach not only outperforms existing methods but also reduces the computational burden associated with tuning multipliers."
                },
                "zh": {
                    "title": "å¯å­¦ä¹ ä¹˜å­ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„ä¹˜å­ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å› æƒé‡è¡°å‡å¼•èµ·çš„å½’ä¸€åŒ–ä¼ªå½±ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„ä¹˜å­ï¼Œç ”ç©¶è€…å‘ç°å¯ä»¥æ›´å¥½åœ°é€‚åº”æ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æƒé‡è¡°å‡æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–°æ–¹æ³•åœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºæ›´ä¼˜çš„æ•ˆæœã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¯å­¦ä¹ ä¹˜å­çš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05106",
            "title": "Token-Level LLM Collaboration via FusionRoute",
            "url": "https://huggingface.co/papers/2601.05106",
            "abstract": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
            "score": 25,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "652fcf403878311c",
            "authors": [
                "Nuoya Xiong",
                "Yuhang Zhou",
                "Hanqing Zeng",
                "Zhaorun Chen",
                "Furong Huang",
                "Shuchao Bi",
                "Lizhu Zhang",
                "Zhuokai Zhao"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Meta AI",
                "Meta TBD Lab",
                "University of Chicago",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05106.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "FusionRoute â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°, Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Optimizing Multi-LLM Collaboration with FusionRoute",
                    "desc": "FusionRoute is a novel framework that enhances collaboration among multiple large language models (LLMs) by using a lightweight router to select the best expert for each token during decoding. This approach not only chooses the most suitable model but also adds complementary logits to improve the predictions of the selected expert. By addressing the limitations of traditional expert-only routing, FusionRoute allows for a more flexible and effective decoding strategy that can adapt to various tasks. Empirical results show that it outperforms existing methods in diverse applications, demonstrating its efficiency and robustness in multi-LLM settings."
                },
                "zh": {
                    "title": "å¤šLLMåä½œï¼Œä¼˜åŒ–è§£ç ç­–ç•¥çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "FusionRoute æ˜¯ä¸€ä¸ªåŸºäºä»¤ç‰Œçº§åˆ«çš„å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åä½œæ¡†æ¶ï¼Œä½¿ç”¨è½»é‡çº§è·¯ç”±å™¨é€‰æ‹©æœ€ä½³ä¸“å®¶å¹¶æ·»åŠ äº’è¡¥çš„ logitsï¼Œä»è€Œåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ï¼Œå¹¶é€šè¿‡ logits ç›¸åŠ æ¥ä¿®æ­£æ‰€é€‰ä¸“å®¶çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å¸ƒï¼Œè§£å†³äº†å•ä¸€é€šç”¨æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸè¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ä¸ä¾èµ–å›ºå®šä¸“å®¶è¾“å‡ºçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFusionRoute é€šè¿‡å¯è®­ç»ƒçš„äº’è¡¥ç”Ÿæˆå™¨æ‰©å±•äº†æœ‰æ•ˆç­–ç•¥ç±»ï¼Œèƒ½å¤Ÿåœ¨æ¸©å’Œæ¡ä»¶ä¸‹æ¢å¤æœ€ä½³ä»·å€¼å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼ŒFusionRoute åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåºåˆ—å’Œä»¤ç‰Œçº§åˆ«çš„åä½œã€æ¨¡å‹åˆå¹¶å’Œç›´æ¥å¾®è°ƒï¼ŒåŒæ—¶åœ¨å„è‡ªä»»åŠ¡ä¸Šä¸é¢†åŸŸä¸“å®¶ä¿æŒç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05249",
            "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
            "url": "https://huggingface.co/papers/2601.05249",
            "abstract": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
            "score": 24,
            "issue_id": 496,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "e9f0ea8291fddbbf",
            "authors": [
                "Yuan-Kang Lee",
                "Kuan-Lin Chen",
                "Chia-Che Chang",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "MediaTek Inc.",
                "National Taiwan University",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05249.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸŒ™",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ² Ğ½Ğ¾Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RL-AWB Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ ÑĞµÑ€Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Revolutionizing Nighttime Photography with RL-AWB",
                    "desc": "This paper introduces RL-AWB, a new framework that enhances white balance adjustment in nighttime photography using a combination of statistical methods and deep reinforcement learning. The approach starts with a statistical algorithm designed for low-light conditions, which includes detecting gray pixels and estimating illumination. It then employs deep reinforcement learning to optimize the white balance parameters dynamically, simulating the expertise of professional photographers. The authors also present a multi-sensor nighttime dataset to evaluate the framework's performance, showing that RL-AWB outperforms existing methods in both low-light and well-lit scenarios."
                },
                "zh": {
                    "title": "å¤œé—´ç™½å¹³è¡¡çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤œé—´è‰²å½©æ’å¸¸æ€§æ¡†æ¶ï¼Œç»“åˆäº†ç»Ÿè®¡æ–¹æ³•å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ”¹å–„ä½å…‰ç…§æ¡ä»¶ä¸‹çš„ç™½å¹³è¡¡è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨é’ˆå¯¹å¤œé—´åœºæ™¯çš„ç»Ÿè®¡ç®—æ³•ï¼Œç»“åˆæ˜¾è‘—ç°è‰²åƒç´ æ£€æµ‹å’Œæ–°é¢–çš„å…‰ç…§ä¼°è®¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†é¦–ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨ç»Ÿè®¡ç®—æ³•ä½œä¸ºæ ¸å¿ƒï¼ŒåŠ¨æ€ä¼˜åŒ–æ¯å¼ å›¾åƒçš„å‚æ•°ï¼Œæ¨¡æ‹Ÿä¸“ä¸šç™½å¹³è¡¡è°ƒèŠ‚ä¸“å®¶çš„è°ƒèŠ‚è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½å…‰å’Œè‰¯å¥½ç…§æ˜å›¾åƒä¸­å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05241",
            "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
            "url": "https://huggingface.co/papers/2601.05241",
            "abstract": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
            "score": 19,
            "issue_id": 494,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "92a140fc7f66033d",
            "authors": [
                "Boyang Wang",
                "Haoran Zhang",
                "Shujie Zhang",
                "Jinkun Hao",
                "Mingda Jia",
                "Qi Lv",
                "Yucheng Mao",
                "Zhaoyang Lyu",
                "Jia Zeng",
                "Xudong Xu",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05241.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#robotics",
                    "#multimodal",
                    "#training",
                    "#synthetic",
                    "#data",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ ÑÑ†ĞµĞ½. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒĞ»Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Robot Policies with Visual Identity Prompting",
                    "desc": "This paper introduces a method called visual identity prompting to improve data augmentation for robot manipulation tasks. By providing explicit visual examples to diffusion models, it enhances the generation of diverse and coherent training data. This approach addresses the limitations of traditional text prompts, which often fail to capture the complexity of real-world scenes. The results show that using this augmented data leads to better performance in both simulated environments and real-world robot applications."
                },
                "zh": {
                    "title": "è§†è§‰èº«ä»½æç¤ºæå‡æœºå™¨äººç­–ç•¥è¡¨ç°",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰èº«ä»½æç¤ºçš„æ–¹æ³•ï¼Œä»¥å¢å¼ºæœºå™¨äººç­–ç•¥çš„æ“ä½œæ•°æ®å¢å¼ºã€‚é€šè¿‡ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ˜ç¡®çš„è§†è§‰æŒ‡å¯¼ï¼Œæ”¹å–„äº†ç­–ç•¥åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ç®¡é“ï¼Œä»å¤§å‹æœºå™¨äººæ•°æ®é›†ä¸­ç­–åˆ’è§†è§‰èº«ä»½æ± ï¼Œä»¥ä¾¿ä¸ºç”Ÿæˆæ‰€éœ€åœºæ™¯è®¾ç½®æä¾›ç¤ºä¾‹å›¾åƒã€‚ä½¿ç”¨å¢å¼ºåçš„æ“ä½œæ•°æ®è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œå’Œè§†è§‰è¿åŠ¨ç­–ç•¥æ¨¡å‹ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05167",
            "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
            "url": "https://huggingface.co/papers/2601.05167",
            "abstract": "RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
            "score": 19,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "78cdcd0348ab654b",
            "authors": [
                "Chengsong Huang",
                "Tong Zheng",
                "Langlin Huang",
                "Jinyuan Li",
                "Haolin Liu",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "University of Maryland",
                "University of Virginia",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05167.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#inference",
                    "#small_models",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…",
                    "desc": "RelayLLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼, Ğ¼Ğ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 1% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 98% ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Collaboration: Small Models, Big Reasoning!",
                    "desc": "RelayLLM is a framework that enhances collaborative reasoning between small and large language models by allowing them to work together more efficiently. It uses a method called token-level dynamic invocation, where the small model can decide to call on the large model only for specific important tokens, rather than for entire queries. This approach minimizes computational costs and latency while maintaining high accuracy, as the small model handles most reasoning tasks independently. The framework includes a two-stage training process to optimize the balance between independent reasoning and when to seek help from the larger model, resulting in significant cost savings and improved performance."
                },
                "zh": {
                    "title": "RelayLLMï¼šé«˜æ•ˆçš„åä½œæ¨ç†æ–°æ¡†æ¶",
                    "desc": "RelayLLMæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡ä»¤ç‰Œçº§çš„åŠ¨æ€è°ƒç”¨å®ç°å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„é«˜æ•ˆåä½œæ¨ç†ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿåä½œæ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚RelayLLMå…è®¸SLMä½œä¸ºä¸»åŠ¨æ§åˆ¶è€…ï¼Œä»…åœ¨å…³é”®ä»¤ç‰Œæ—¶è°ƒç”¨LLMï¼Œä»è€Œå‡å°‘äº†è®¡ç®—èµ„æºçš„æµªè´¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRelayLLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†49.52%çš„å¹³å‡å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä»…è°ƒç”¨äº†1.07%çš„æ€»ç”Ÿæˆä»¤ç‰Œï¼Œæ˜¾è‘—é™ä½äº†98.2%çš„è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04767",
            "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
            "url": "https://huggingface.co/papers/2601.04767",
            "abstract": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
            "score": 19,
            "issue_id": 496,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "38825a65f1e8a007",
            "authors": [
                "Zefang Zong",
                "Dingwei Chen",
                "Yang Li",
                "Qi Yi",
                "Bo Zhou",
                "Chengming Li",
                "Bo Qian",
                "Peng Chen",
                "Jie Jiang"
            ],
            "affiliations": [
                "Shenzhen MSU-BIT University",
                "Sun Yat-Sen University",
                "Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04767.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#open_source",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ATÂ²PO â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ» Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 1,84 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Learning with ATÂ²PO Framework",
                    "desc": "The paper introduces ATÂ²PO, a new framework designed for multi-turn agentic reinforcement learning (RL). It tackles key challenges such as enhancing exploration diversity, improving credit assignment, and optimizing policies through a structured approach. By utilizing a turn-level tree structure, ATÂ²PO facilitates strategic exploration and precise reward propagation, which helps in better learning from sparse outcomes. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness in refining agentic interactions in RL tasks."
                },
                "zh": {
                    "title": "ATÂ²POï¼šæå‡å¤šå›åˆæ™ºèƒ½å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "ATÂ²POæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šå›åˆæ™ºèƒ½å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ ‘æœç´¢å’Œå›åˆçº§å­¦ä¹ ç›®æ ‡æ¥æ”¹å–„æ¢ç´¢å¤šæ ·æ€§ã€ä¿¡ç”¨åˆ†é…å’Œç­–ç•¥ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å›åˆçº§æ ‘ç»“æ„ï¼Œç»“åˆç†µå¼•å¯¼çš„æ ‘æ‰©å±•å’Œå›åˆä¿¡ç”¨åˆ†é…ï¼Œä¿ƒè¿›äº†ç¨€ç–ç»“æœçš„å¥–åŠ±ä¼ æ’­ã€‚ATÂ²POçš„ç­–ç•¥ä¼˜åŒ–ä¸æ™ºèƒ½äº¤äº’çš„è‡ªç„¶å†³ç­–ç²’åº¦ç›¸ä¸€è‡´ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATÂ²POåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºç°æœ‰æœ€ä¼˜åŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21815",
            "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.21815",
            "abstract": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
            "score": 17,
            "issue_id": 501,
            "pub_date": "2026-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "79a1c8bb88f5fe0c",
            "authors": [
                "Mengqi He",
                "Xinyu Tian",
                "Xin Shen",
                "Jinhong Ni",
                "Shu Zou",
                "Zhaoyuan Yang",
                "Jing Zhang"
            ],
            "affiliations": [
                "Australian National University",
                "GE Research",
                "The University of Queensland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21815.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ°Ğ´Ğ²ĞµÑ€ÑĞ°Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ - ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµĞ³Ğ¾ 20% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ñ… Ğ¿ĞµÑ€Ñ‚ÑƒÑ€Ğ±Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ EGA (Entropy-bank Guided Adversarial attacks) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 93-95% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ 35-49% Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº (17-26% Ğ½Ğ° Ğ½ĞµĞ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…)."
                },
                "en": {
                    "title": "Targeting Uncertainty: A New Approach to Adversarial Attacks on VLMs",
                    "desc": "This paper discusses a new method for attacking vision-language models (VLMs) by focusing on high-entropy tokens, which are points of high uncertainty in the model's output. Instead of spreading adversarial changes across all tokens, the authors show that targeting just 20% of these critical tokens can lead to significant semantic degradation with less effort. Their approach, called Entropy-bank Guided Adversarial attacks (EGA), achieves high success rates in converting benign outputs into harmful ones, revealing vulnerabilities in various VLM architectures. This research highlights the importance of understanding model uncertainty and its implications for the safety of AI systems."
                },
                "zh": {
                    "title": "é«˜ç†µæ ‡è®°çš„é€‰æ‹©æ€§å¯¹æŠ—æ”»å‡»æ­ç¤ºVLMçš„æ–°è„†å¼±æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œé«˜ç†µæ ‡è®°ï¼Œå³æ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜çš„æ ‡è®°ï¼Œèƒ½å¤Ÿæ˜¾è‘—å½±å“ç”Ÿæˆç»“æœã€‚é€šè¿‡é›†ä¸­å¯¹è¿™äº›é«˜ç†µæ ‡è®°çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å®ç°ä¸å…¨å±€æ–¹æ³•ç›¸å½“çš„è¯­ä¹‰é™çº§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§é€‰æ‹©æ€§æ”»å‡»åœ¨ä¸åŒçš„VLMæ¶æ„ä¸­å…·æœ‰å¯è½¬ç§»æ€§ï¼Œæš´éœ²äº†å½“å‰å®‰å…¨æœºåˆ¶çš„æ–°å¼±ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05175",
            "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
            "url": "https://huggingface.co/papers/2601.05175",
            "abstract": "VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
            "score": 15,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "b9a7c1c14abbe921",
            "authors": [
                "Shuming Liu",
                "Mingchen Zhuge",
                "Changsheng Zhao",
                "Jun Chen",
                "Lemeng Wu",
                "Zechun Liu",
                "Chenchen Zhu",
                "Zhipeng Cai",
                "Chong Zhou",
                "Haozhe Liu",
                "Ernie Chang",
                "Saksham Suri",
                "Hongyu Xu",
                "Qi Qian",
                "Wei Wen",
                "Balakrishnan Varadarajan",
                "Zhuang Liu",
                "Hu Xu",
                "Florian Bordes",
                "Raghuraman Krishnamoorthi",
                "Bernard Ghanem",
                "Vikas Chandra",
                "Yunyang Xiong"
            ],
            "affiliations": [
                "King Abdullah University of Science and Technology (KAUST)",
                "Meta AI",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05175.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#benchmark",
                    "#optimization",
                    "#video",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VideoAuto-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Â«Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ´Ğ²Ğ°Ğ¶Ğ´Ñ‹Â», Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ğ±Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² 3.3 Ñ€Ğ°Ğ·Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ½Ğ¾Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficient Video Understanding with Reason-When-Necessary Strategy",
                    "desc": "The VideoAuto-R1 framework introduces a novel approach to video understanding by implementing a reason-when-necessary strategy. It utilizes a Thinking Once, Answering Twice training paradigm, where the model first generates an initial answer, then reasons through it, and finally provides a reviewed answer, all while being guided by verifiable rewards. During inference, the model decides whether to engage in reasoning based on the confidence score of its initial answer, leading to improved efficiency. This method achieves state-of-the-art accuracy in video question answering and grounding tasks while significantly reducing response length."
                },
                "zh": {
                    "title": "å¿…è¦æ—¶æ¨ç†ï¼Œæå‡è§†é¢‘ç†è§£æ•ˆç‡",
                    "desc": "VideoAuto-R1æ¡†æ¶é‡‡ç”¨å¿…è¦æ—¶æ¨ç†çš„ç­–ç•¥æ¥ç†è§£è§†é¢‘ï¼Œä½¿ç”¨ä¸€æ¬¡æ€è€ƒã€ä¸¤æ¬¡å›ç­”çš„è®­ç»ƒèŒƒå¼ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»“åˆå¯éªŒè¯çš„å¥–åŠ±å’ŒåŸºäºç½®ä¿¡åº¦çš„æ¨ç†æ¿€æ´»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ï¼Œç›´æ¥å›ç­”çš„æ•ˆæœå¾€å¾€ä¸é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›¸å½“ï¼Œç”šè‡³æ›´å¥½ï¼Œå°½ç®¡CoTçš„è®¡ç®—æˆæœ¬æ›´é«˜ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒæ—¶é¦–å…ˆç”Ÿæˆåˆå§‹ç­”æ¡ˆï¼Œç„¶åè¿›è¡Œæ¨ç†ï¼Œæœ€åè¾“å‡ºç»è¿‡å®¡æŸ¥çš„ç­”æ¡ˆï¼Œå¹¶é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œç›‘ç£ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®åˆå§‹ç­”æ¡ˆçš„ç½®ä¿¡åº¦å†³å®šæ˜¯å¦è¿›è¡Œæ¨ç†ï¼Œä»è€Œåœ¨è§†é¢‘é—®ç­”å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—æé«˜çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05138",
            "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
            "url": "https://huggingface.co/papers/2601.05138",
            "abstract": "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
            "score": 11,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "4e9d7d654118b517",
            "authors": [
                "Sixiao Zheng",
                "Minghao Yin",
                "Wenbo Hu",
                "Xiaoyu Li",
                "Ying Shan",
                "Yanwei Fu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Fudan University",
                "HKU",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05138.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#data",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "VerseCrafter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ 4D-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 4D Geometric Control, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº Ñ„Ğ¾Ğ½Ğ° Ğ¸ 3D Gaussian Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼. Ğ­Ñ‚Ğ¸ 4D ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰ĞµĞ³Ğ¾ 4D ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¸ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unified 4D Control for Dynamic Video Generation",
                    "desc": "VerseCrafter is a novel 4D-aware video world model designed to enhance control over camera and object movements in video generation. It utilizes a unique 4D Geometric Control representation that captures the dynamic state of the environment through a combination of static point clouds and 3D Gaussian trajectories for each object. This approach allows for flexible and precise modeling of object dynamics, moving beyond traditional bounding boxes. To overcome the challenge of limited training data, VerseCrafter includes an automatic data engine that extracts necessary 4D controls from existing videos, enabling the model to learn from a large and varied dataset."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ§åˆ¶ç›¸æœºä¸ç‰©ä½“åŠ¨æ€çš„4Dè§†é¢‘æ¨¡å‹",
                    "desc": "VerseCrafter æ˜¯ä¸€ç§4Dæ„ŸçŸ¥çš„è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡4Då‡ ä½•æ§åˆ¶è¡¨ç¤ºå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ç»Ÿä¸€æ§åˆ¶ç›¸æœºå’Œç‰©ä½“çš„åŠ¨æ€ã€‚ç°æœ‰æ–¹æ³•åœ¨æä¾›ç›¸æœºå’Œå¤šç‰©ä½“è¿åŠ¨çš„ç»Ÿä¸€ç²¾ç¡®æ§åˆ¶æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒVerseCrafteré€šè¿‡é™æ€èƒŒæ™¯ç‚¹äº‘å’Œæ¯ä¸ªç‰©ä½“çš„3Dé«˜æ–¯è½¨è¿¹æ¥ç¼–ç ä¸–ç•ŒçŠ¶æ€ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¨¡å‹ä¸ä»…æ•æ‰ç‰©ä½“çš„è¿åŠ¨è·¯å¾„ï¼Œè¿˜èƒ½è¡¨ç¤ºå…¶éšæ—¶é—´å˜åŒ–çš„3Då ç”¨æ¦‚ç‡ï¼Œæä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œä»é‡å¤–è§†é¢‘ä¸­æå–æ‰€éœ€çš„4Dæ§åˆ¶ï¼Œä»¥ä¾¿åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03425",
            "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
            "url": "https://huggingface.co/papers/2601.03425",
            "abstract": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
            "score": 9,
            "issue_id": 492,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "102f6999339b81a5",
            "authors": [
                "Yan Wang",
                "Yitao Xu",
                "Nanhan Shen",
                "Jinyan Su",
                "Jimin Huang",
                "Zining Zhu"
            ],
            "affiliations": [
                "Cornell University",
                "Georgia Institute of Technology",
                "Stevens Institute of Technology",
                "The Fin AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03425.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Mixture of Experts: Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¸Ñ‚ĞµÑ‚Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture of Experts (MoE) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ°Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ COMMITTEEAUDIT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ñ€Ñ‘Ñ… Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° MMLU Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞŸĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞšĞ¾Ğ¼Ğ¸Ñ‚ĞµÑ‚Ğ° â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ°Ğ»Ğ¸Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑÑ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Rethinking Specialization in Mixture of Experts Models",
                    "desc": "This paper challenges the belief that Mixture of Experts (MoE) models specialize in different domains through sparse routing of experts. The authors introduce a framework called COMMITTEEAUDIT, which examines how groups of experts, rather than individual ones, influence routing decisions. They discover a consistent group of experts, termed the Standing Committee, that dominates routing across various domains and architectures. This suggests that the expected specialization in MoE models is less significant than previously thought, and current training methods may hinder the model's performance by not aligning with its natural optimization tendencies."
                },
                "zh": {
                    "title": "æ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„ä¸­å¤®å§”å‘˜ä¼šå½±å“ä¸“ä¸šåŒ–",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†æ··åˆä¸“å®¶æ¨¡å‹ä¸­é¢†åŸŸä¸“ä¸šåŒ–çš„å‡è®¾ï¼ŒæŒ‡å‡ºåœ¨ä¸åŒé¢†åŸŸå’Œæ¶æ„ä¸­å­˜åœ¨ä¸€ä¸ªæŒç»­çš„ä¸­å¤®å§”å‘˜ä¼šä¸“å®¶ï¼Œä¸»å¯¼ç€è·¯ç”±è¡Œä¸ºã€‚ç ”ç©¶å¼•å…¥äº†COMMITTEEAUDITæ¡†æ¶ï¼Œåˆ†æä¸“å®¶ç»„çš„è·¯ç”±è¡Œä¸ºï¼Œè€Œä¸æ˜¯å•ä¸ªä¸“å®¶ã€‚é€šè¿‡å¯¹ä¸‰ç§ä»£è¡¨æ€§æ¨¡å‹å’ŒMMLUåŸºå‡†çš„ç ”ç©¶ï¼Œå‘ç°äº†ä¸€ä¸ªé¢†åŸŸä¸å˜çš„å¸¸è®¾å§”å‘˜ä¼šï¼Œè¿™ä¸ªå§”å‘˜ä¼šåœ¨ä¸åŒé¢†åŸŸã€å±‚æ¬¡å’Œè·¯ç”±é¢„ç®—ä¸­å§‹ç»ˆå æ®ä¸»è¦çš„è·¯ç”±è´¨é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„ä¸“ä¸šåŒ–ç¨‹åº¦è¿œä½äºæ™®éè®¤ä¸ºçš„æ°´å¹³ï¼Œå½“å‰çš„è®­ç»ƒç›®æ ‡å¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05239",
            "title": "Plenoptic Video Generation",
            "url": "https://huggingface.co/papers/2601.05239",
            "abstract": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
            "score": 6,
            "issue_id": 493,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "e200e4f57b8dacae",
            "authors": [
                "Xiao Fu",
                "Shitao Tang",
                "Min Shi",
                "Xian Liu",
                "Jinwei Gu",
                "Ming-Yu Liu",
                "Dahua Lin",
                "Chen-Hsuan Lin"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "NVIDIA",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05239.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸",
                    "desc": "PlenopticDreamer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Synchronized Generative Hallucinations for Multi-View Video Mastery",
                    "desc": "PlenopticDreamer is a novel framework designed for multi-view video re-rendering that ensures consistent visual output across different perspectives. It utilizes synchronized generative hallucinations, which help maintain spatio-temporal coherence in the generated video content. The model employs a camera-guided retrieval system to select relevant video segments from previous generations, enhancing the quality of the output. Additionally, it incorporates progressive training techniques to improve model robustness and visual fidelity, achieving state-of-the-art results in video re-rendering tasks."
                },
                "zh": {
                    "title": "åŒæ­¥ç”Ÿæˆå¹»è§‰ï¼Œå®ç°å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“çš„çªç ´",
                    "desc": "PlenopticDreamer æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŒæ­¥ç”Ÿæˆçš„å¹»è§‰å®ç°ä¸€è‡´çš„å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ã€‚å®ƒåˆ©ç”¨ç›¸æœºå¼•å¯¼çš„æ£€ç´¢å’Œæ¸è¿›è®­ç»ƒæœºåˆ¶ï¼Œæ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå›å½’æ–¹å¼è®­ç»ƒå¤šè¾“å…¥å•è¾“å‡ºçš„è§†é¢‘æ¡ä»¶æ¨¡å‹ï¼Œå¹¶è‡ªé€‚åº”é€‰æ‹©å‰æœŸç”Ÿæˆçš„æ˜¾è‘—è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlenopticDreamer åœ¨è§†é¢‘é‡æ¸²æŸ“æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæä¾›äº†å“è¶Šçš„è§†è§’åŒæ­¥å’Œé«˜ä¿çœŸè§†è§‰æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05111",
            "title": "Agent-as-a-Judge",
            "url": "https://huggingface.co/papers/2601.05111",
            "abstract": "Large language models face limitations in evaluating complex, multi-step tasks, prompting the development of agent-based evaluation systems that utilize planning, tool-augmented verification, and multi-agent collaboration for more robust assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
            "score": 6,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "67cf270baff78987",
            "authors": [
                "Runyang You",
                "Hongru Cai",
                "Caiqi Zhang",
                "Qiancheng Xu",
                "Meng Liu",
                "Tiezheng Yu",
                "Yongqi Li",
                "Wenjie Li"
            ],
            "affiliations": [
                "Huawei Technologies",
                "Shandong Jianzhu University",
                "The Hong Kong Polytechnic University",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05111.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#agents",
                    "#survey"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑ‚ ÑÑƒĞ´ÑŒĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑÑƒĞ´ÑŒĞµ-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² AI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ Ğ¾Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ LLM-as-a-Judge Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Agent-as-a-Judge â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "From LLMs to Agentic Evaluation: A New Era in AI Assessment",
                    "desc": "This paper discusses the limitations of large language models (LLMs) in evaluating complex tasks, which has led to the creation of agent-based evaluation systems. These systems enhance assessment reliability by incorporating planning, tool-augmented verification, and collaboration among multiple agents. The authors present a comprehensive survey that outlines the evolution from LLM-as-a-Judge to Agent-as-a-Judge, highlighting key dimensions and methodologies in this transition. Additionally, they identify challenges and future research directions to guide the development of more effective agentic evaluation systems."
                },
                "zh": {
                    "title": "ä»£ç†è¯„ä¼°ï¼šè¿ˆå‘æ›´ç¨³å¥çš„AIè¯„ä¼°ç³»ç»Ÿ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤å¼€å‘äº†åŸºäºä»£ç†çš„è¯„ä¼°ç³»ç»Ÿã€‚è¿™äº›ç³»ç»Ÿåˆ©ç”¨è§„åˆ’ã€å·¥å…·å¢å¼ºéªŒè¯å’Œå¤šä»£ç†åä½œæ¥è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚å°½ç®¡ä»£ç†è¯„ä¼°ç³»ç»Ÿè¿…é€Ÿå‘å±•ï¼Œä½†è¯¥é¢†åŸŸç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶æ¥åº”å¯¹è¿™ä¸€å˜åŒ–ã€‚æœ¬æ–‡æä¾›äº†é¦–æ¬¡å…¨é¢çš„è°ƒæŸ¥ï¼Œè¯†åˆ«äº†è¿™ä¸€èŒƒå¼è½¬å˜çš„å…³é”®ç»´åº¦ï¼Œå¹¶å»ºç«‹äº†å‘å±•åˆ†ç±»æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05172",
            "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
            "url": "https://huggingface.co/papers/2601.05172",
            "abstract": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
            "score": 4,
            "issue_id": 502,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "30169d7a3fe8549f",
            "authors": [
                "Haoyu Zhao",
                "Akide Liu",
                "Zeyu Zhang",
                "Weijie Wang",
                "Feng Chen",
                "Ruihan Zhu",
                "Gholamreza Haffari",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "AIML, Adelaide University",
                "Monash University",
                "ZIP Lab, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05172.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Chain-of-View prompting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 11.56% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenEQA. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² 3D Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning in 3D with Chain-of-View Prompting",
                    "desc": "This paper introduces Chain-of-View (CoV) prompting, a novel approach that enhances vision-language models (VLMs) for embodied question answering in 3D environments. CoV allows models to dynamically select and adjust camera views to gather relevant context, overcoming limitations of fixed input views. By employing a View Selection agent and iterative reasoning, CoV improves spatial reasoning capabilities without requiring additional training. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of this model-agnostic strategy."
                },
                "zh": {
                    "title": "é“¾è§†æç¤ºï¼šæå‡ä¸‰ç»´ç©ºé—´æ¨ç†çš„æœ‰æ•ˆç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé“¾è§†æç¤ºï¼ˆChain-of-Viewï¼ŒCoVï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸‰ç»´ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚CoVé€šè¿‡é€‰æ‹©ä¸é—®é¢˜ç›¸å…³çš„è§†è§’å¹¶è¿­ä»£è°ƒæ•´ç›¸æœºä½ç½®ï¼Œå¸®åŠ©æ¨¡å‹ä¸»åŠ¨æ¢ç´¢ç¯å¢ƒï¼Œä»è€Œæ”¶é›†åˆ†æ•£åœ¨å¤šä¸ªè§†ç‚¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ç²—åˆ°ç»†çš„æ¢ç´¢è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è·å–æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVåœ¨å¤šä¸ªä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå‡æ˜¾è‘—æé«˜äº†è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œé—®é¢˜å›ç­”çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05163",
            "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
            "url": "https://huggingface.co/papers/2601.05163",
            "abstract": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.  \t\t\t\t\tAI-generated summary \t\t\t\t Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
            "score": 3,
            "issue_id": 492,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "d54786c0b7245d58",
            "authors": [
                "Qintong Zhang",
                "Xinjie Lv",
                "Jialong Wu",
                "Baixuan Li",
                "Zhengwei Tao",
                "Guochen Yan",
                "Huanyao Zhang",
                "Bin Wang",
                "Jiahao Xu",
                "Haitao Mi",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Lab",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05163.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#agents",
                    "#synthetic"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ¢Ğ°Ğ½ĞµÑ† Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "DocDancer â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Exploration-then-Synthesis Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… DocQA. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ end-to-end Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMLongBench-Doc Ğ¸ DocBench Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "DocDancer: Revolutionizing Document Question Answering with Open-Source Innovation",
                    "desc": "DocDancer is an innovative open-source document question answering (DocQA) agent that approaches the task as an information-seeking challenge. It utilizes a tool-driven framework that emphasizes both exploration of documents and synthesis of information for effective training. To tackle the issue of limited high-quality training data, the authors introduce a novel Exploration-then-Synthesis data synthesis pipeline. The performance of DocDancer is validated on two benchmarks, demonstrating its capability in long-context document understanding and providing insights for future tool design."
                },
                "zh": {
                    "title": "DocDancerï¼šå¼€æºæ–‡æ¡£é—®ç­”çš„æ–°çªç ´",
                    "desc": "DocDanceræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¼€æºæ–‡æ¡£é—®ç­”ä»£ç†ï¼Œæ—¨åœ¨å°†æ–‡æ¡£é—®ç­”ä»»åŠ¡è§†ä¸ºä¿¡æ¯è·å–é—®é¢˜ã€‚å®ƒé‡‡ç”¨å·¥å…·é©±åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡æ¢ç´¢å’Œç»¼åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜é—®ç­”çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†æ”¯æŒç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¢ç´¢-å†ç»¼åˆçš„æ•°æ®åˆæˆç®¡é“ï¼Œè§£å†³äº†é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç»è¿‡åˆæˆæ•°æ®çš„è®­ç»ƒï¼ŒDocDanceråœ¨ä¸¤ä¸ªé•¿æ–‡æœ¬ç†è§£åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05124",
            "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
            "url": "https://huggingface.co/papers/2601.05124",
            "abstract": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
            "score": 3,
            "issue_id": 495,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "ab34d211b4ce6dc5",
            "authors": [
                "Runze He",
                "Yiji Cheng",
                "Tiankai Hang",
                "Zhimin Li",
                "Yu Xu",
                "Zijin Yin",
                "Shiyi Zhang",
                "Wenxun Dai",
                "Penghui Du",
                "Ao Ma",
                "Chunyu Wang",
                "Qinglin Lu",
                "Jizhong Han",
                "Jiao Dai"
            ],
            "affiliations": [
                "Hunyuan, Tencent",
                "IIE, CAS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05124.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#alignment",
                    "#reasoning",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Re-Align Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ In-Context Chain-of-Thought (IC-CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ñ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Re-Align Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging Understanding and Generation in Image Creation",
                    "desc": "Re-Align is a new framework designed to improve in-context image generation and editing by enhancing the connection between understanding user prompts and generating images. It uses a method called In-Context Chain-of-Thought (IC-CoT) to clarify the relationship between text prompts and reference images, which helps the model better understand what users want. Additionally, Re-Align employs a reinforcement learning training approach that uses a surrogate reward to evaluate how well the generated images match the structured reasoning provided by the text. Experiments show that Re-Align performs better than other similar models in generating and editing images based on user input."
                },
                "zh": {
                    "title": "ç†è§£ä¸ç”Ÿæˆçš„æ¡¥æ¢ï¼šRe-Align",
                    "desc": "Re-Align æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ¨ç†å¼•å¯¼çš„å¯¹é½æ¥è§£å†³å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­çš„ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚å®ƒé‡‡ç”¨äº†ä¸Šä¸‹æ–‡é“¾å¼æ€ç»´ï¼ˆIC-CoTï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†è¯­ä¹‰æŒ‡å¯¼å’Œå‚è€ƒå…³è”è§£è€¦ï¼Œä»è€Œæä¾›æ¸…æ™°çš„æ–‡æœ¬ç›®æ ‡ï¼Œå‡å°‘å‚è€ƒå›¾åƒä¹‹é—´çš„æ··æ·†ã€‚æ­¤å¤–ï¼ŒRe-Align è¿˜å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨æ›¿ä»£å¥–åŠ±æ¥è¡¡é‡ç»“æ„åŒ–æ¨ç†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ•´ä½“è¡¨ç°ã€‚å¤§é‡å®éªŒéªŒè¯äº† Re-Align åœ¨åŒç±»æ¨¡å‹è§„æ¨¡å’Œèµ„æºä¸‹çš„ç«äº‰æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03559",
            "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2601.03559",
            "abstract": "DiffCoT reformulates chain-of-thought reasoning as an iterative denoising process using diffusion principles, enabling unified generation and correction of intermediate steps while maintaining causal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
            "score": 3,
            "issue_id": 492,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "add13e4a635a0515",
            "authors": [
                "Shidong Cao",
                "Hongzhan Lin",
                "Yuxuan Gu",
                "Ziyang Luo",
                "Jing Ma"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Hong Kong Baptist University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03559.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#math",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ DiffCoT â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸Ñ… Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiffCoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ€Ğ¾Ğ±ÑƒÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "DiffCoT: Enhancing Reasoning with Diffusion Principles",
                    "desc": "This paper introduces DiffCoT, a novel framework that enhances chain-of-thought (CoT) reasoning in large language models by applying diffusion principles. It reformulates CoT reasoning as an iterative denoising process, allowing for the generation and correction of intermediate reasoning steps while maintaining causal consistency. The framework employs a sliding-window mechanism to integrate diffusion at the reasoning-step level, which helps to mitigate issues like exposure bias and error accumulation. Experimental results show that DiffCoT outperforms traditional CoT optimization methods, demonstrating better robustness and error correction in multi-step reasoning tasks."
                },
                "zh": {
                    "title": "DiffCoTï¼šæå‡é“¾å¼æ€ç»´æ¨ç†çš„é²æ£’æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "DiffCoTå°†é“¾å¼æ€ç»´æ¨ç†é‡æ–°æ„å»ºä¸ºä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œåˆ©ç”¨æ‰©æ•£åŸç†æ¥ç»Ÿä¸€ç”Ÿæˆå’Œä¿®æ­£ä¸­é—´æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒå› æœä¸€è‡´æ€§ã€‚ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨ç†åœ¨å¤šæ­¥éª¤æ•°å­¦é—®é¢˜è§£å†³ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®¹æ˜“å—åˆ°æ›å…‰åå·®å’Œé”™è¯¯ç´¯ç§¯çš„å½±å“ã€‚DiffCoTé€šè¿‡æ»‘åŠ¨çª—å£æœºåˆ¶åœ¨æ¨ç†æ­¥éª¤çº§åˆ«æ•´åˆæ‰©æ•£åŸç†ï¼Œä½¿å¾—ä¸­é—´æ­¥éª¤çš„ç”Ÿæˆå’Œå›é¡¾æ€§ä¿®æ­£å˜å¾—æ›´åŠ é«˜æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffCoTåœ¨å¤šä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé”™è¯¯ä¿®æ­£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04754",
            "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2601.04754",
            "abstract": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
            "score": 2,
            "issue_id": 499,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "8bdee003577e3af9",
            "authors": [
                "Yen-Jen Chiou",
                "Wei-Tse Cheng",
                "Yuan-Fu Yang"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04754.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ² 3D Gaussian Splatting Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ProFuse Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„Ğ°Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² open-vocabulary Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ·Ğ° Ğ¿ÑÑ‚ÑŒ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "ProFuse: Fast and Semantic 3D Scene Understanding",
                    "desc": "ProFuse is a novel framework that improves 3D scene understanding by integrating semantic information into the process of 3D Gaussian Splatting (3DGS). It utilizes a context-aware approach that enhances the consistency of views and the cohesion of object masks without needing extensive fine-tuning. The method includes a pre-registration phase that accurately initializes Gaussian representations and constructs 3D Context Proposals through cross-view clustering. This allows for efficient semantic fusion and geometric refinement, achieving significant speed improvements over existing state-of-the-art methods."
                },
                "zh": {
                    "title": "ProFuseï¼šé«˜æ•ˆçš„3Dåœºæ™¯ç†è§£æ–°æ–¹æ³•",
                    "desc": "ProFuse æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3D Gaussian Splattingï¼‰å¢å¼ºå¼€æ”¾è¯æ±‡çš„ 3D åœºæ™¯ç†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥æ³¨å†Œè®¾ç½®æé«˜äº†è·¨è§†å›¾ä¸€è‡´æ€§å’Œå†…éƒ¨æ©è†œçš„å‡èšåŠ›ï¼Œä¸”å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ã€‚ProFuse å¼•å…¥äº†ä¸€ç§å¯†é›†å¯¹åº”å¼•å¯¼çš„é¢„æ³¨å†Œé˜¶æ®µï¼Œèƒ½å¤Ÿå‡†ç¡®åˆå§‹åŒ–é«˜æ–¯å¹¶é€šè¿‡è·¨è§†å›¾èšç±»å…±åŒæ„å»º 3D ä¸Šä¸‹æ–‡ææ¡ˆã€‚è¯¥æ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸­å®Œæˆè¯­ä¹‰é™„åŠ çš„é€Ÿåº¦çº¦ä¸ºäº”åˆ†é’Ÿï¼Œæ˜¯å½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„ä¸¤å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03362",
            "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
            "url": "https://huggingface.co/papers/2601.03362",
            "abstract": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
            "score": 2,
            "issue_id": 495,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "fd6d3f256e025564",
            "authors": [
                "Xiang Zhang",
                "Yang Zhang",
                "Lukas Mehl",
                "Markus Gross",
                "Christopher Schroers"
            ],
            "affiliations": [
                "DisneyResearchStudios",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03362.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸ’‡",
                "ru": {
                    "title": "ĞÑ…Ñ€Ğ°Ğ½Ğ° Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "HairGuard â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ”Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑÑ‚ĞµÑ€ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "HairGuard: Mastering Soft Boundaries in 3D Vision",
                    "desc": "HairGuard is a novel framework aimed at enhancing the recovery of fine-grained soft boundary details in 3D vision tasks. It utilizes a specialized depth fixer network that identifies and refines depth around soft boundaries, ensuring high-quality global depth. The framework also incorporates depth-based forward warping and a generative scene painter to synthesize new views while maintaining texture fidelity and eliminating background artifacts. Extensive experiments show that HairGuard outperforms existing methods in monocular depth estimation and view synthesis, particularly in handling soft boundary regions effectively."
                },
                "zh": {
                    "title": "HairGuardï¼šæ¢å¤3Dè§†è§‰ä¸­çš„ç»†ç²’åº¦è½¯è¾¹ç•Œ",
                    "desc": "HairGuardæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“é—¨çš„æ·±åº¦ç»†åŒ–å’Œè§†å›¾åˆæˆæŠ€æœ¯ï¼Œæ¢å¤3Dè§†è§‰ä»»åŠ¡ä¸­çš„ç»†ç²’åº¦è½¯è¾¹ç•Œç»†èŠ‚ã€‚è¯¥æ¡†æ¶é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œåˆ©ç”¨å›¾åƒæŠ å›¾æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ·±åº¦ä¿®å¤ç½‘ç»œï¼Œè‡ªåŠ¨è¯†åˆ«è½¯è¾¹ç•ŒåŒºåŸŸã€‚æ·±åº¦ä¿®å¤ç½‘ç»œä½¿ç”¨é—¨æ§æ®‹å·®æ¨¡å—ï¼Œç²¾ç¡®ç»†åŒ–è½¯è¾¹ç•Œå‘¨å›´çš„æ·±åº¦ï¼ŒåŒæ—¶ä¿æŒå…¨å±€æ·±åº¦è´¨é‡ã€‚é€šè¿‡æ·±åº¦åŸºç¡€çš„å‰å‘æ‰­æ›²å’Œç”Ÿæˆåœºæ™¯ç»˜åˆ¶ï¼ŒHairGuardèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´å‡ ä½•å½¢çŠ¶å’Œç»†ç²’åº¦ç»†èŠ‚çš„æ–°è§†å›¾ï¼Œæ˜¾è‘—æå‡äº†è½¯è¾¹ç•ŒåŒºåŸŸçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03111",
            "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
            "url": "https://huggingface.co/papers/2601.03111",
            "abstract": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
            "score": 2,
            "issue_id": 502,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "fd6febf737aa4e79",
            "authors": [
                "Yiyuan Li",
                "Zhen Huang",
                "Yanan Wu",
                "Weixun Wang",
                "Xuefeng Li",
                "Yijia Luo",
                "Wenbo Su",
                "Bo Zheng",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "Shanghai Jiaotong University",
                "Taobao & Tmall Group of Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03111.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ‹ÑÑÑ‡: Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼ â€” Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ, Ñ…Ğ¸Ğ¼Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² â€” Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°."
                },
                "en": {
                    "title": "Unlocking Reasoning with One Perfect Sample",
                    "desc": "This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) using just one carefully designed training sample. It introduces a new approach called polymath learning, which shows that a single, well-chosen math reasoning sample can enhance performance in various fields like physics, chemistry, and biology. The authors argue that the quality and design of training samples are more important than the quantity, challenging the traditional reliance on large datasets. Their findings suggest a shift towards sample engineering, focusing on creating optimal training samples to boost reasoning capabilities in LLMs."
                },
                "zh": {
                    "title": "æ ·æœ¬è®¾è®¡ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å•ä¸€è®­ç»ƒæ ·æœ¬æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¤šæ‰å­¦ä¹ çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†å•æ¬¡å­¦ä¹ åœ¨å¤šä¸ªå­¦ç§‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé€‰æ‹©çš„æ•°å­¦æ¨ç†æ ·æœ¬èƒ½å¤Ÿæ˜¾è‘—æé«˜åœ¨ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©ç­‰é¢†åŸŸçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ ·æœ¬çš„è´¨é‡å’Œè®¾è®¡æ¯”æ•°é‡æ›´ä¸ºé‡è¦ï¼Œè¿™å¯èƒ½æ˜¯æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23628",
            "title": "Memorization in 3D Shape Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2512.23628",
            "abstract": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
            "score": 2,
            "issue_id": 493,
            "pub_date": "2026-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "ae6b1f98f106b276",
            "authors": [
                "Shu Pu",
                "Boya Zeng",
                "Kaichen Zhou",
                "Mengyu Wang",
                "Zhuang Liu"
            ],
            "affiliations": [
                "Harvard University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23628.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#3d",
                    "#leakage"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ) Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ§ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Vecset Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² guidance. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Measuring and Mitigating Memorization in 3D Generative Models",
                    "desc": "This paper presents a framework to assess how much 3D generative models memorize their training data during the generation process. The researchers found that factors such as the type of data used and the design of the model significantly affect memorization levels. Their experiments revealed that increased data diversity and finer conditioning lead to higher memorization, while certain modeling techniques can help reduce it. The findings aim to enhance the understanding of memorization in generative models, which can help improve the quality and diversity of generated outputs."
                },
                "zh": {
                    "title": "é‡åŒ–3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡",
                    "desc": "æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæµ‹é‡3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡ï¼Œå¹¶è¯†åˆ«å½±å“å› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°æ®æ¨¡æ€å’Œæ¨¡å‹è®¾è®¡å‚æ•°ä¼šå½±å“è®­ç»ƒæ•°æ®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è®°å¿†ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°ï¼Œæˆ‘ä»¬é‡åŒ–äº†è®°å¿†ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°ï¼Œè®°å¿†ä¸æ•°æ®å¤šæ ·æ€§å’Œç»†ç²’åº¦æ¡ä»¶æœ‰å…³ã€‚æˆ‘ä»¬çš„æ¡†æ¶å’Œåˆ†æä¸ºç†è§£3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„å‡å°‘è®°å¿†çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05149",
            "title": "Multi-Scale Local Speculative Decoding for Image Generation",
            "url": "https://huggingface.co/papers/2601.05149",
            "abstract": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
            "score": 1,
            "issue_id": 505,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "3eb3439c3eef40bd",
            "authors": [
                "Elia Peruzzo",
                "Guillaume SautiÃ¨re",
                "Amirhossein Habibian"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05149.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Multi-Scale Local Speculative Decoding (MuLo-SD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ¼ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 1.7x Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ EAGLE-2 Ğ¸ LANTERN."
                },
                "en": {
                    "title": "Accelerating Image Generation with Smart Drafting and Verification",
                    "desc": "This paper presents Multi-Scale Local Speculative Decoding (MuLo-SD), a new method to speed up autoregressive image generation while preserving quality. It combines low-resolution drafting with high-resolution verification to propose and refine image tokens efficiently. The approach uses a local rejection and resampling mechanism that focuses on nearby pixels, improving error correction. The results show that MuLo-SD significantly accelerates the generation process, achieving up to 1.7 times faster performance compared to existing methods, without sacrificing semantic and perceptual quality."
                },
                "zh": {
                    "title": "å¤šå°ºåº¦å±€éƒ¨æ¨æµ‹è§£ç ï¼šåŠ é€Ÿå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦å±€éƒ¨æ¨æµ‹è§£ç æ¡†æ¶ï¼ˆMuLo-SDï¼‰ï¼Œæ—¨åœ¨åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šåˆ†è¾¨ç‡è‰å›¾å’Œç©ºé—´ä¿¡æ¯éªŒè¯ï¼Œè§£å†³äº†ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨æ ‡è®°çº§åˆ«æ¨¡ç³Šæ€§å’Œç¼ºä¹ç©ºé—´æ„è¯†çš„é—®é¢˜ã€‚é€šè¿‡ä½åˆ†è¾¨ç‡è‰å›¾ç”Ÿæˆå€™é€‰å›¾åƒæ ‡è®°ï¼Œå¹¶åˆ©ç”¨é«˜åˆ†è¾¨ç‡ç›®æ ‡æ¨¡å‹è¿›è¡Œå¹¶è¡ŒéªŒè¯ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMuLo-SDåœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œé€Ÿåº¦æå‡å¯è¾¾1.7å€ï¼Œæˆä¸ºå›¾åƒåˆæˆé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04792",
            "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
            "url": "https://huggingface.co/papers/2601.04792",
            "abstract": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
            "score": 1,
            "issue_id": 509,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "482ad7e44a3f4895",
            "authors": [
                "Denis Korzhenkov",
                "Adil Karjauv",
                "Animesh Karnewar",
                "Mohsen Ghafoorian",
                "Amirhossein Habibian"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04792.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#video",
                    "#inference",
                    "#open_source"
                ],
                "emoji": "ğŸ”º",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³: Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸: ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…, Ğ° Ğ¼ĞµĞ½ĞµĞµ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ â€” Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ…, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Inference with Pyramidal Diffusion Models",
                    "desc": "This paper introduces pyramidal diffusion models that optimize computational efficiency by processing data at different resolutions. The models operate in stages, handling noisier inputs at lower resolutions and clearer inputs at higher resolutions, which reduces the overall computational load during inference. The authors propose a method to convert existing pretrained models into pyramidal models using low-cost fine-tuning, ensuring that the quality of the output remains high. Additionally, they explore various strategies for step distillation to further improve the efficiency of these models during inference."
                },
                "zh": {
                    "title": "é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹é€šè¿‡åˆ†å±‚åˆ†è¾¨ç‡å¤„ç†æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™äº›æ¨¡å‹å°†ä¼ ç»Ÿçš„å‰å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œåœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹è¿è¡Œã€‚å®ƒä»¬åœ¨è¾ƒä½åˆ†è¾¨ç‡ä¸‹å¤„ç†é«˜å™ªå£°è¾“å…¥ï¼Œè€Œåœ¨è¾ƒé«˜åˆ†è¾¨ç‡ä¸‹å¤„ç†ä½å™ªå£°è¾“å…¥ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¤šæ­¥å»å™ªæ¨¡å‹çš„æ¨ç†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä½æˆæœ¬å¾®è°ƒå°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è½¬æ¢ä¸ºé‡‘å­—å¡”æ¨¡å‹çš„æµç¨‹ï¼Œç¡®ä¿è¾“å‡ºè§†é¢‘è´¨é‡ä¸ä¸‹é™ï¼ŒåŒæ—¶è¿˜ç ”ç©¶äº†é‡‘å­—å¡”æ¨¡å‹ä¸­çš„æ­¥éª¤è’¸é¦ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04620",
            "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
            "url": "https://huggingface.co/papers/2601.04620",
            "abstract": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
            "score": 1,
            "issue_id": 499,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "0dcce10378a61ef8",
            "authors": [
                "Di Zhang"
            ],
            "affiliations": [
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04620.jpg",
            "data": {
                "categories": [
                    "#open_source"
                ],
                "emoji": "ğŸ“¦",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ¾Ñ„Ñ‚Ğ²ĞµÑ€: Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²",
                    "desc": "AgentDevel Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° LLM, ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ° Ğ½Ğ° Ñ€ĞµĞ»Ğ¸Ğ· Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾Ñ†Ğ¸Ñ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ AgentDevel Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ÑĞ¼Ğ¸ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Stable and Auditable Improvements for LLM Agents",
                    "desc": "AgentDevel introduces a new way to improve large language model (LLM) agents by treating them like software products that can be released and updated. Instead of embedding self-improvement directly in the agents, it uses a structured release engineering approach that focuses on stability and auditability. The system generates quality signals from the agents' performance, allowing for better diagnosis of issues without needing to look inside the agent's code. This method ensures that improvements are consistent and verifiable, reducing the chances of regressions and making the development process more reliable."
                },
                "zh": {
                    "title": "AgentDevelï¼šç¨³å®šå¯å®¡è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†å‘å¸ƒå·¥ç¨‹",
                    "desc": "AgentDevelæå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„å‘å¸ƒå·¥ç¨‹æ–¹æ³•ï¼Œå°†å…¶è§†ä¸ºå¯äº¤ä»˜çš„å·¥ä»¶ï¼Œå¹¶å¼ºè°ƒé€šè¿‡å¤–éƒ¨æµ‹è¯•å’Œè¯Šæ–­è¿‡ç¨‹å®ç°ç¨³å®šã€å¯å®¡è®¡çš„æ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•å°†ä»£ç†çš„æ”¹è¿›å¤–éƒ¨åŒ–åˆ°ä¸€ä¸ªå›å½’æ„ŸçŸ¥çš„å‘å¸ƒç®¡é“ä¸­ã€‚AgentDevelçš„æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ä¸€ä¸ªä¸ä¾èµ–å®ç°çš„LLMè¯„è®ºå™¨ã€åŸºäºè„šæœ¬çš„å¯æ‰§è¡Œè¯Šæ–­å’Œä»¥ç¿»è½¬ä¸ºä¸­å¿ƒçš„é—¨æ§æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAgentDevelèƒ½å¤Ÿåœ¨å‡å°‘å›å½’çš„åŒæ—¶ï¼Œå®ç°ç¨³å®šçš„æ”¹è¿›ï¼Œå¹¶ç”Ÿæˆå¯é‡å¤ã€å¯å®¡è®¡çš„å·¥ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04575",
            "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
            "url": "https://huggingface.co/papers/2601.04575",
            "abstract": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.  \t\t\t\t\tAI-generated summary \t\t\t\t Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
            "score": 1,
            "issue_id": 504,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "ca12e042748c296d",
            "authors": [
                "Yuguang Yue",
                "Irakli Salia",
                "Samuel Hunt",
                "Chris Green",
                "Wenzhe Shi",
                "Jonathan J Hunt"
            ],
            "affiliations": [
                "Player2, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04575.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#3d",
                    "#reasoning",
                    "#agents",
                    "#video",
                    "#open_source"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğµ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8300 Ñ‡Ğ°ÑĞ¾Ğ² Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞµÑĞ¾Ğ¼ Ğ´Ğ¾ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Scaling Up Behavior Cloning for Human-Level Gameplay",
                    "desc": "This paper discusses the resurgence of behavior cloning in machine learning, particularly for training models to play 3D video games. By scaling up both the model size and the amount of training data, the authors demonstrate that their foundation model can achieve human-level performance in gameplay. They provide an open-source framework that includes extensive gameplay data, training code, and pretrained models, allowing others to replicate their results. The study also explores how increasing model depth and training data enhances the model's ability to learn causal relationships, revealing important insights into the scaling laws of behavior cloning."
                },
                "zh": {
                    "title": "è¡Œä¸ºå…‹éš†ï¼šé€šè¿‡æ‰©å±•å®ç°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°",
                    "desc": "è¡Œä¸ºå…‹éš†é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®ï¼Œå±•ç¤ºäº†åœ¨3Dè§†é¢‘æ¸¸æˆä¸­è¾¾åˆ°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°å’Œå› æœæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æ”¾çš„è®­ç»ƒè§†é¢‘æ¸¸æˆåŸºç¡€æ¨¡å‹çš„é…æ–¹ï¼Œæ—¨åœ¨å®æ—¶æ¨ç†å¹¶åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚æˆ‘ä»¬å‘å¸ƒäº†8300å¤šä¸ªå°æ—¶çš„é«˜è´¨é‡äººç±»æ¸¸æˆæ•°æ®ã€è®­ç»ƒå’Œæ¨ç†ä»£ç ï¼Œä»¥åŠé¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œå‡åœ¨å¼€æ”¾è®¸å¯ä¸‹æä¾›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ€ä½³æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§3Dè§†é¢‘æ¸¸æˆä¸­ä¸äººç±»ç©å®¶ç«äº‰ï¼Œä¸”é€šè¿‡ç³»ç»Ÿæ€§ç ”ç©¶è¡Œä¸ºå…‹éš†çš„æ‰©å±•è§„å¾‹ï¼Œæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½å’Œå› æœæ¨ç†å¦‚ä½•éšæ¨¡å‹å’Œæ•°æ®è§„æ¨¡å˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04342",
            "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2601.04342",
            "abstract": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
            "score": 1,
            "issue_id": 509,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "1816aeec79b1b6b5",
            "authors": [
                "Mohsen Ghafoorian",
                "Amirhossein Habibian"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04342.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ReHyAt Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° softmax Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ softmax, ReHyAt Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² ÑÑ‚Ğ¾ Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² (~160 GPU Ñ‡Ğ°ÑĞ¾Ğ²) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹."
                },
                "en": {
                    "title": "ReHyAt: Efficient Video Generation with Hybrid Attention",
                    "desc": "ReHyAt presents a novel Recurrent Hybrid Attention mechanism that merges the advantages of softmax and linear attention, facilitating efficient video generation. This approach addresses the challenge of quadratic attention complexity in transformer-based models, which hampers scalability for longer video sequences. By enabling chunk-wise recurrent processing and maintaining constant memory usage, ReHyAt significantly reduces training costs while preserving high video quality. The method has been validated through experiments, showing it achieves state-of-the-art results in video generation with improved efficiency."
                },
                "zh": {
                    "title": "ReHyAtï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "ReHyAtæ˜¯ä¸€ç§é€’å½’æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆäº†softmaxå’Œçº¿æ€§æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆè§†é¢‘ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜å¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½¿ç”¨åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œè™½ç„¶ç”Ÿæˆæ•ˆæœä¼˜ç§€ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¼šé¢ä¸´äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚åº¦çš„é—®é¢˜ã€‚ReHyAté€šè¿‡å—çŠ¶é€’å½’é‡æ„å’Œæ’å®šå†…å­˜ä½¿ç”¨ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæˆæœ¬ä¸Šå‡å°‘äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReHyAtåœ¨è§†é¢‘è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒæ—¶å°†æ³¨æ„åŠ›æˆæœ¬ä»äºŒæ¬¡é™ä½åˆ°çº¿æ€§ï¼Œé€‚ç”¨äºé•¿æ—¶é•¿å’Œè®¾å¤‡ä¸Šçš„è§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04300",
            "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
            "url": "https://huggingface.co/papers/2601.04300",
            "abstract": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
            "score": 1,
            "issue_id": 498,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "465dd03a32e42d19",
            "authors": [
                "Chenye Meng",
                "Zejian Li",
                "Zhongni Liu",
                "Yize Li",
                "Changle Xie",
                "Kaixin Jia",
                "Ling Yang",
                "Huanghuang Deng",
                "Shiying Ding",
                "Shengyuan Zhang",
                "Jiayi Li",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "University of Electronic Science and Technology of China",
                "University of Nottingham Ningbo China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04300.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#alignment"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ supervised fine-tuning Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Complex Preference Optimization (CPO), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ DPO Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Hierarchical Expert Alignment",
                    "desc": "This paper presents a two-stage framework designed to improve the alignment of diffusion models with complex human preferences in image generation. It begins by creating a hierarchical evaluation system that breaks down image quality into detailed attributes, allowing for a more nuanced assessment. The framework then employs Supervised Fine-Tuning to incorporate expert knowledge into the diffusion model, followed by Complex Preference Optimization (CPO) to refine the model's outputs based on these detailed criteria. Experimental results show that this approach significantly enhances both the quality of generated images and their alignment with expert evaluations."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹å¯¹é½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œé‡‡ç”¨åˆ†å±‚è¯„ä¼°æ ‡å‡†å’Œå¤æ‚åå¥½ä¼˜åŒ–ã€‚ä¼ ç»Ÿçš„åè®­ç»ƒå¯¹é½æ–¹æ³•ä¾èµ–äºç®€åŒ–ä¿¡å·ï¼Œå¦‚æ ‡é‡å¥–åŠ±æˆ–äºŒå…ƒåå¥½ï¼Œè¿™é™åˆ¶äº†ä¸å¤æ‚äººç±»ä¸“ä¸šçŸ¥è¯†çš„å¯¹é½ã€‚æˆ‘ä»¬é¦–å…ˆä¸é¢†åŸŸä¸“å®¶æ„å»ºäº†ä¸€ä¸ªåˆ†å±‚çš„ç»†ç²’åº¦è¯„ä¼°æ ‡å‡†ï¼Œå°†å›¾åƒè´¨é‡åˆ†è§£ä¸ºå¤šä¸ªæ­£é¢å’Œè´Ÿé¢å±æ€§ï¼Œå¹¶ä»¥æ ‘çŠ¶ç»“æ„ç»„ç»‡ã€‚é€šè¿‡å¼•å…¥å¤æ‚åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŒæ—¶æœ€å¤§åŒ–æ­£é¢å±æ€§çš„æ¦‚ç‡ï¼ŒåŒæ—¶æœ€å°åŒ–è´Ÿé¢å±æ€§çš„æ¦‚ç‡ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡å’Œä¸ä¸“ä¸šçŸ¥è¯†çš„å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02016",
            "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
            "url": "https://huggingface.co/papers/2601.02016",
            "abstract": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
            "score": 1,
            "issue_id": 496,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "661151975c914811",
            "authors": [
                "Matthias Bartolo",
                "Dylan Seychell",
                "Gabriel Hili",
                "Matthew Montebello",
                "Carl James Debono",
                "Saviour Formosa",
                "Konstantinos Makantasis"
            ],
            "affiliations": [
                "Department of Artificial Communications of Malta",
                "Department of Communications and Computer Engineering, Faculty of Information and Communications Technology, University of Malta",
                "Department of Criminology, Faculty of Social Wellbeing, University of Malta",
                "Faculty Intelligence, Technology, University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02016.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (LUPI) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾ĞºÑĞ¾Ğ², ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ UAV-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Pascal VOC 2012. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ LUPI, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Boosting Object Detection with Privileged Information",
                    "desc": "This paper explores the Learning Using Privileged Information (LUPI) approach to improve object detection by utilizing extra information available during training. It presents a flexible method to incorporate privileged data, like bounding box masks and depth cues, into existing deep learning models using a teacher-student framework. The experiments show that models trained with LUPI significantly enhance detection accuracy without adding complexity during inference. The results indicate that this method is particularly beneficial for detecting medium and large objects, making it a valuable strategy for real-world applications."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç‰¹æƒä¿¡æ¯æå‡ç›®æ ‡æ£€æµ‹ç²¾åº¦",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨ç›®æ ‡æ£€æµ‹ä¸­æ•´åˆç‰¹æƒä¿¡æ¯å­¦ä¹ ï¼ˆLUPIï¼‰èŒƒå¼ï¼Œä»¥åˆ©ç”¨è®­ç»ƒæœŸé—´å¯ç”¨ä½†æ¨ç†æ—¶ä¸å¯ç”¨çš„ç»†ç²’åº¦æè¿°ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„ã€ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„å°†ç‰¹æƒä¿¡æ¯ï¼ˆå¦‚è¾¹ç•Œæ¡†æ©ç ã€æ˜¾è‘—æ€§å›¾å’Œæ·±åº¦çº¿ç´¢ï¼‰æ³¨å…¥åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹å™¨ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡LUPIè®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨ä¸­å¤§ç‰©ä½“çš„æ£€æµ‹ä¸­è¡¨ç°æ˜¾è‘—æå‡ã€‚ç ”ç©¶ç»“æœç¡®è®¤LUPIæ¡†æ¶ä¸ºåœ¨èµ„æºå—é™å’Œç°å®ä¸–ç•Œç¯å¢ƒä¸­æå‡ç›®æ ‡æ£€æµ‹ç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å®ç”¨çš„ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05125",
            "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
            "url": "https://huggingface.co/papers/2601.05125",
            "abstract": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
            "score": 0,
            "issue_id": 500,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "2d5ee3972f5a6371",
            "authors": [
                "Ignacio de Rodrigo",
                "Alvaro J. Lopez-Lopez",
                "Jaime Boal"
            ],
            "affiliations": [
                "Institute for Research in Technology, ICAI School of Engineering, Comillas Pontifical University, Calle Rey Francisco, 4, Madrid, 28008, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05125.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#multimodal",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ñƒ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "VERSE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ F1 Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (Donut, Idefics2) ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ (GPT-4, Pixtral)."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with VERSE",
                    "desc": "VERSE is a new method designed to analyze and improve Vision-Language Models specifically for understanding visually-rich documents. It allows researchers to visualize the hidden representations of these models, making it easier to identify areas where the model struggles. By generating synthetic data for these challenging areas, VERSE enhances the model's performance without losing its ability to generalize to new data. The methodology has been validated using the MERIT Dataset, showing significant improvements in model accuracy compared to existing solutions."
                },
                "zh": {
                    "title": "VERSEï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ©å™¨",
                    "desc": "VERSEæ˜¯ä¸€ç§ç”¨äºåˆ†æå’Œæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä¸­ã€‚å®ƒé€šè¿‡å¯è§†åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œå¸®åŠ©è¯„ä¼°æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶è¯†åˆ«å‡ºé—®é¢˜åŒºåŸŸã€‚è¯¥æ–¹æ³•è¿˜æŒ‡å¯¼ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä»¥æé«˜åœ¨è¿™äº›é”™è¯¯æ˜“å‘é›†ç¾¤ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨åˆæˆçš„MERITæ•°æ®é›†ä¸Šè®­ç»ƒå¹¶åœ¨çœŸå®çš„MERIT Secretæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œç»“æœè¡¨æ˜VERSEèƒ½å¤Ÿæ­ç¤ºä¸é”™è¯¯é›†ç¾¤ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶æ˜¾è‘—æå‡F1æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02702",
            "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
            "url": "https://huggingface.co/papers/2601.02702",
            "abstract": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
            "score": 0,
            "issue_id": 508,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "9b399a11240ed646",
            "authors": [
                "Shuhaib Mehri",
                "Priyanka Kargupta",
                "Tal August",
                "Dilek Hakkani-TÃ¼r"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02702.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MultiSessionCollab Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚."
                },
                "en": {
                    "title": "Enhancing Collaboration with Memory-Driven Agents",
                    "desc": "The MultiSessionCollab benchmark assesses how well agents can learn and adapt to user preferences over time using persistent memory systems. This approach allows agents to enhance the quality of collaboration by refining their understanding of user needs through multiple interactions. By leveraging learning signals from user simulator behavior, agents can improve their memory updates and generate better reflections on user preferences. Experimental results indicate that agents with memory not only achieve higher task success rates but also create more efficient interactions, ultimately leading to a better user experience."
                },
                "zh": {
                    "title": "æå‡é•¿æœŸåˆä½œè´¨é‡çš„æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿ",
                    "desc": "MultiSessionCollabåŸºå‡†æµ‹è¯•è¯„ä¼°æ™ºèƒ½ä½“åœ¨é•¿æœŸåˆä½œä¸­å­¦ä¹ å’Œé€‚åº”ç”¨æˆ·åå¥½çš„èƒ½åŠ›ã€‚é€šè¿‡æŒä¹…çš„è®°å¿†ç³»ç»Ÿï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤šä¸ªä¼šè¯ä¸­ä¸æ–­æ”¹è¿›åˆä½œè´¨é‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç”¨æˆ·æ¨¡æ‹Ÿå™¨çš„è¡Œä¸ºä¿¡å·æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ›´æ–°è®°å¿†å¹¶ç”Ÿæˆæ›´å…¨é¢çš„åæ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…·å¤‡è®°å¿†çš„æ™ºèƒ½ä½“åœ¨é•¿æœŸåˆä½œä¸­è¡¨ç°æ›´ä½³ï¼Œä»»åŠ¡æˆåŠŸç‡æ›´é«˜ï¼Œäº¤äº’æ•ˆç‡æ›´é«˜ï¼Œç”¨æˆ·çš„åŠªåŠ›ä¹Ÿå‡å°‘äº†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01887",
            "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
            "url": "https://huggingface.co/papers/2601.01887",
            "abstract": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
            "score": 0,
            "issue_id": 492,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "3f5eb740e47ff8ee",
            "authors": [
                "Jiawen Zhang",
                "Lipeng He",
                "Kejia Chen",
                "Jian Lou",
                "Jian Liu",
                "Xiaohu Yang",
                "Ruoxi Jia"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "University of Waterloo",
                "Virginia Tech",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01887.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿Ğ¾Ñ… Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "One Example is Enough: Efficient Safety Alignment for LLMs",
                    "desc": "This paper presents a novel approach to safety alignment in large language models (LLMs), showing that a single safety example can effectively restore safety without compromising model performance. Traditional methods often require numerous safety samples, leading to increased computational costs and reduced utility. The authors reveal that the low-rank structure of the safety gradient allows for rapid convergence in just a few training epochs. Their findings are validated across multiple LLMs and datasets, highlighting the efficiency and general applicability of their method."
                },
                "zh": {
                    "title": "å•ä¸ªç¤ºä¾‹å³å¯æ¢å¤å®‰å…¨å¯¹é½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ç”¨ä¸€ä¸ªå®‰å…¨ç¤ºä¾‹å°±èƒ½å®Œå…¨æ¢å¤æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè€Œä¸ä¼šå½±å“æ¨¡å‹çš„å®ç”¨æ€§ã€‚é€šè¿‡è¯†åˆ«ä½ç§©æ¢¯åº¦ç»“æ„ï¼Œæ¨¡å‹åœ¨å°‘æ•°è®­ç»ƒå‘¨æœŸå†…å³å¯å®ç°æ”¶æ•›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœåœ¨å¤šä¸ªå®‰å…¨å¯¹é½çš„è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸ŠéªŒè¯äº†è¿™ä¸€æ–¹æ³•çš„æ™®éé€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04233",
            "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
            "url": "https://huggingface.co/papers/2601.04233",
            "abstract": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
            "score": 0,
            "issue_id": 503,
            "pub_date": "2026-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "ce6f38f3022fb1cb",
            "authors": [
                "Zhiyuan Zhao",
                "Lijian Lin",
                "Ye Zhu",
                "Kai Xie",
                "Yunfei Liu",
                "Yu Li"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04233.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#low_resource",
                    "#optimization",
                    "#architecture",
                    "#dataset",
                    "#multilingual",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LEMAS-Dataset â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ñ€ĞµÑ‡Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 150 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: LEMAS-TTS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ flow-matching Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ adversarial Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ° LEMAS-Edit Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· autoregressive decoder Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº ÑĞ»Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ prompt'Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Unlocking Multilingual Speech Synthesis with LEMAS-Dataset",
                    "desc": "The LEMAS-Dataset is a large open-source multilingual speech corpus designed for high-quality speech synthesis and editing. It includes over 150,000 hours of audio in 10 languages, with precise word-level timestamps to enhance data quality. Two models, LEMAS-TTS and LEMAS-Edit, utilize different architectures to demonstrate effective multilingual synthesis and speech editing capabilities. The dataset's innovative training techniques, including accent-adversarial training, improve performance across diverse languages and accents, paving the way for future advancements in speech generation systems."
                },
                "zh": {
                    "title": "LEMAS-Datasetï¼šå¤šè¯­è¨€è¯­éŸ³åˆæˆä¸ç¼–è¾‘çš„æœªæ¥",
                    "desc": "LEMAS-Datasetæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šè¯­è¨€è¯­éŸ³åˆæˆå’Œç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡150,000å°æ—¶çš„è¯­éŸ³æ•°æ®ï¼Œè¦†ç›–10ç§ä¸»è¦è¯­è¨€ã€‚è¯¥æ•°æ®é›†é€šè¿‡é«˜æ•ˆçš„æ•°æ®å¤„ç†æµç¨‹æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®å’Œæ³¨é‡Šçš„é«˜è´¨é‡ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ç§åŸºå‡†æ¨¡å‹ï¼Œåˆ†åˆ«é‡‡ç”¨éè‡ªå›å½’å’Œè‡ªå›å½’æ¶æ„ï¼Œä»¥éªŒè¯LEMAS-Datasetåœ¨ä¸åŒç”ŸæˆèŒƒå¼ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLEMAS-Datasetè®­ç»ƒçš„æ¨¡å‹åœ¨è¯­éŸ³åˆæˆå’Œç¼–è¾‘æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24160",
            "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
            "url": "https://huggingface.co/papers/2512.24160",
            "abstract": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
            "score": 0,
            "issue_id": 497,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "9eb9ce39c2f3de67",
            "authors": [
                "TsaiChing Ni",
                "ZhenQi Chen",
                "YuanFu Yang"
            ],
            "affiliations": [
                "Institute of Intelligent Systems, National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24160.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#transfer_learning",
                    "#diffusion",
                    "#synthetic",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° IMDD-1M â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 60 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ 400 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ foundation model, ÑĞ¾Ğ²Ğ¼ĞµÑ‰Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ³ĞºÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Manufacturing with IMDD-1M: A Million Defects for Smarter AI",
                    "desc": "The paper introduces IMDD-1M, a comprehensive dataset containing 1 million image-text pairs focused on industrial defects, aimed at enhancing multimodal learning in manufacturing. This dataset features high-resolution images of defects across various materials, each paired with detailed textual descriptions that include annotations on defect characteristics. The authors develop a diffusion-based vision-language foundation model that can be fine-tuned for specific industrial tasks, demonstrating its versatility and efficiency. Remarkably, this model requires significantly less task-specific data while maintaining high performance, showcasing the effectiveness of foundation model adaptation in industrial applications."
                },
                "zh": {
                    "title": "æ¨åŠ¨å·¥ä¸šæ™ºèƒ½çš„å¤šæ¨¡æ€ç¼ºé™·æ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†IMDD-1Mï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡å·¥ä¸šå¤šæ¨¡æ€ç¼ºé™·æ•°æ®é›†ï¼ŒåŒ…å«100ä¸‡ä¸ªå¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ—¨åœ¨æ¨åŠ¨åˆ¶é€ å’Œè´¨é‡æ£€æµ‹çš„å¤šæ¨¡æ€å­¦ä¹ ã€‚è¯¥æ•°æ®é›†æ¶µç›–60å¤šç§ææ–™ç±»åˆ«å’Œ400å¤šç§ç¼ºé™·ç±»å‹ï¼Œé…æœ‰ä¸“å®¶éªŒè¯çš„æ³¨é‡Šå’Œè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼Œè¯´æ˜ç¼ºé™·çš„ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸Šä¸‹æ–‡å±æ€§ã€‚IMDD-1Mæ”¯æŒåˆ†ç±»ã€åˆ†å‰²ã€æ£€ç´¢ã€æè¿°ç”Ÿæˆå’Œç”Ÿæˆå»ºæ¨¡ç­‰å¤šç§åº”ç”¨ã€‚åŸºäºIMDD-1Mï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å·¥ä¸šåœºæ™¯ï¼Œå±•ç¤ºäº†æ•°æ®é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹é€‚åº”æ€§åœ¨å·¥ä¸šæ£€æµ‹å’Œç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-08.html",
    "link_next": "2026-01-12.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "12.01",
        "en": "01/12",
        "zh": "1æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 5,
        "#benchmark": 9,
        "#agents": 6,
        "#cv": 5,
        "#rl": 5,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 6,
        "#audio": 1,
        "#video": 6,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 20,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 11,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 11,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    }
}