{
    "date": {
        "ru": "25 Ğ¸ÑĞ½Ñ",
        "en": "June 25",
        "zh": "6æœˆ25æ—¥"
    },
    "time_utc": "2025-06-25 02:45",
    "weekday": 2,
    "issue_id": 4470,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.19851",
            "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2506.19851",
            "abstract": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.",
            "score": 5,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "ce9a811b1a9e7d9d",
            "authors": [
                "Zehuan Huang",
                "Haoran Feng",
                "Yangtian Sun",
                "Yuanchen Guo",
                "Yanpei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, China",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19851.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#transfer_learning",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "AnimaX: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "AnimaX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… 2D-ĞºĞ°Ñ€Ñ‚ Ğ¿Ğ¾Ğ· Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. AnimaX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ·. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging Video Motion and 3D Animation with AnimaX",
                    "desc": "AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters."
                },
                "zh": {
                    "title": "AnimaXï¼šæ— ç±»åˆ« 3D åŠ¨ç”»çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "AnimaX æ˜¯ä¸€ä¸ªå‰é¦ˆå¼çš„ 3D åŠ¨ç”»æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒå’ŒåŸºäºéª¨éª¼çš„å¯æ§ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„è¿åŠ¨åˆæˆæ–¹æ³•ä¸åŒï¼ŒAnimaX æ”¯æŒä»»æ„éª¨éª¼çš„å¤šæ ·åŒ–å…³èŠ‚ç½‘æ ¼ï¼Œå¹¶æœ‰æ•ˆåœ°å°†è§†é¢‘ä¸­çš„è¿åŠ¨çŸ¥è¯†è½¬ç§»åˆ° 3D é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè§†è§’ã€å¤šå¸§çš„ 2D å§¿æ€å›¾è¡¨ç¤º 3D è¿åŠ¨ï¼Œå¹¶åˆ©ç”¨å…±äº«ä½ç½®ç¼–ç å’Œæ¨¡æ€æ„ŸçŸ¥åµŒå…¥ç¡®ä¿è§†é¢‘å’Œå§¿æ€åºåˆ—ä¹‹é—´çš„æ—¶ç©ºå¯¹é½ã€‚ç»è¿‡åœ¨ä¸€ä¸ªåŒ…å« 160,000 ä¸ªç»‘å®šåºåˆ—çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒAnimaX åœ¨ VBench ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç±»åˆ« 3D åŠ¨ç”»è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19838",
            "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
            "url": "https://huggingface.co/papers/2506.19838",
            "abstract": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.",
            "score": 0,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "a00bfa00a7f0f869",
            "authors": [
                "Liangbin Xie",
                "Yu Li",
                "Shian Du",
                "Menghan Xia",
                "Xintao Wang",
                "Fanghua Yu",
                "Ziyan Chen",
                "Pengfei Wan",
                "Jiantao Zhou",
                "Chao Dong"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Shenzhen University of Advanced Technology",
                "State Key Laboratory of Internet of Things for Smart City, University of Macau",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19838.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Quality with Smart Cascaded Super-Resolution",
                    "desc": "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘è¶…åˆ†è¾¨ç‡ç”Ÿæˆçš„è®¾è®¡åŸåˆ™",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†çº§è”è§†é¢‘è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„è®¾è®¡åŸåˆ™ï¼Œä»¥æé«˜é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†é€€åŒ–ç­–ç•¥ã€æ—¶é—´æ­¥é‡‡æ ·ã€å™ªå£°å¢å¼ºå’Œç¨€ç–å±€éƒ¨æ³¨æ„åŠ›ç­‰æ–¹æ³•ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥å’Œå™ªå£°å¢å¼ºå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥çš„å½±å“ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹æ¶æ„å’Œè®­ç»ƒåˆ›æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥é«˜æ•ˆçº§è”åˆæˆç³»ç»Ÿçš„å‘å±•æä¾›äº†å®ç”¨çš„è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-24.html",
    "link_next": "2025-06-26.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.06",
        "en": "06/26",
        "zh": "6æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}