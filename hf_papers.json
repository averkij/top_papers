{
    "date": {
        "ru": "24 июля",
        "en": "July 24",
        "zh": "7月24日"
    },
    "time_utc": "2025-07-24 03:58",
    "weekday": 3,
    "issue_id": 4983,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.17512",
            "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2507.17512",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.",
            "score": 9,
            "issue_id": 4983,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "d637b6ffc2166e10",
            "authors": [
                "Yu Li",
                "Zhuoshi Pan",
                "Honglin Lin",
                "Mengyuan Sun",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "OpenDataLab",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17512.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Многодоменное рассуждение: раскрывая потенциал ИИ через обучение с подкреплением",
                    "desc": "Исследование посвящено обучению с подкреплением с проверяемыми наградами (RLVR) для улучшения способностей больших языковых моделей к рассуждению в нескольких областях одновременно. Авторы провели систематический анализ взаимодействия между математическими рассуждениями, генерацией кода и решением логических задач в рамках RLVR. Эксперименты включали оценку улучшений внутри доменов и обобщения между доменами, изучение взаимодействий при комбинированном обучении, сравнение базовых и инструктированных моделей, а также исследование влияния различных аспектов обучения с подкреплением. Результаты предоставляют ценные insights о динамике взаимодействия доменов и факторах, влияющих на специализированную и обобщенную производительность в рассуждениях."
                },
                "en": {
                    "title": "Enhancing Multi-Domain Reasoning in LLMs with RLVR",
                    "desc": "This paper introduces Reinforcement Learning with Verifiable Rewards (RLVR) as a method to improve the reasoning abilities of large language models (LLMs) across multiple domains. It highlights the need for integrated reasoning skills in real-world scenarios, moving beyond isolated tasks like math or coding. The study investigates how different reasoning domains interact during training, using the GRPO algorithm and the Qwen-2.5-7B model family to assess improvements and generalization. Key findings reveal important dynamics in multi-domain training, offering insights for enhancing RL techniques to develop more versatile reasoning capabilities in LLMs."
                },
                "zh": {
                    "title": "多领域推理的强化学习新探索",
                    "desc": "强化学习与可验证奖励（RLVR）是一种增强大型语言模型（LLMs）推理能力的有效方法。现有研究主要集中在数学问题解决、编码任务和逻辑推理等孤立的推理领域。然而，现实世界的推理场景需要多种认知技能的综合应用。本文系统研究了RLVR框架下的多领域推理，重点分析数学推理、代码生成和逻辑难题解决之间的相互作用。"
                }
            }
        }
    ],
    "link_prev": "2025-07-23.html",
    "link_next": "2025-07-25.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7月23日"
    },
    "short_date_next": {
        "ru": "25.07",
        "en": "07/25",
        "zh": "7月25日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}